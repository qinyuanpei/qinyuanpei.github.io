<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>可视化 on 元视角</title><link>https://qinyuanpei.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/</link><description>Recent content in 可视化 on 元视角</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sun, 24 Jan 2021 22:36:47 +0000</lastBuildDate><atom:link href="https://qinyuanpei.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/index.xml" rel="self" type="application/rss+xml"/><item><title>通过 Python 分析 2020 年全年微博热搜数据</title><link>https://qinyuanpei.github.io/posts/2758545080/</link><pubDate>Sun, 24 Jan 2021 22:36:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/2758545080/</guid><description>几天前， Catcher Wong 大佬告诉我，他终于写完了 2020 年的年终总结。在看完大佬的年终总结以后，我有一种“前浪被后浪拍死在沙滩上”的感觉，正如当学生时都看“别人家的孩子”，工作以后看的都是“别人的年终总结”。我们的生活，其实就是由“别人”和“我们”交织在一起，而更多的时候，是成为“大多数”的“我们”，去关注成为“少数”的“别人”。我想说的是，世间万物互为装饰，就像卞之琳在《断章》里写道，“明月装饰了你的窗子，你装饰了别人的梦”。即便一个人在历史长河中，尤如一叶飘泊不定的孤舟在波涛中摇荡，可每一朵浪花都曾以自己的方式美丽过，所以，看“别人”的生活，联想“我们”的生活，这便是我同 2020 告别的一种方式，为此，博主决定抓取 2020 年全年 366 天的微博热搜，通过可视化的方式来串联起 2020 年的回忆。
热搜抓取 首先，我们来考虑微博热搜的数据来源。 微博 官方提供了一个热搜排行榜的页面：https://s.weibo.com/top/summary，可惜这个网站只支持查看当天的热搜，显然这无法满足我们的需求。在搜索引擎的帮助下，找到了两个网站，它们分别是：微博时光机 和 热搜神器。经过一番权衡，决定选择页面结构更简单一点的 微博时光机 。
通过抓包，可以快速获得两个关键的接口，它们分别是 获取 timeId 接口 和 获取历史热搜接口。
Firefox抓包示意图简单来说，我们指定一个日期，第一个接口会返回timeId。接下来，通过这个timeId调用第二个接口就可以获得热搜数据。仔细观察的话，第一个接口传递的data参数像是一个BASE64加密后的结果，尝试解密后发现我的猜想是对的，加密前的内容如下：
[&amp;#34;getclosesttime&amp;#34;,[&amp;#34;2021-01-20T23:08:02&amp;#34;]] 这意味着我们只需要改变这里的日期就可以啦，因此，我们的思路无非就是从 2020 年 1 月 1 日开始，依次请求热搜接口获取数据，直到 2020 年 12 月 31 日。这里想顺便吐槽下这个网站的接口设计，居然清一色地全部用数组来返回结果，难道是为了省掉这几个字段来节省流量吗？
接口返回值说明-1接口返回值说明-2吐槽归吐槽，这里我们可以非常容易地写出对应的代码，由于日期和timeId的对应关系是固定的，为了减少后续的请求数量，我们使用MongoDB来对数据进行持久化。同样地，抓取热搜采用了类似的方式，因为历史热搜同样是确定的数据，这里只给出关键的代码，并不代表你可以无脑地复制、粘贴：
# 获取指定日期对应的timeId def get_timeId(date, cookie): cacheKey = date.strftime(&amp;#39;%Y-%m-%d&amp;#39;) records = list(store.find(TABLE_TIME_ID, {&amp;#39;date&amp;#39;: cacheKey})) if len(records) &amp;gt; 0: return records[0][&amp;#39;timeId&amp;#39;] else: data = &amp;#34;[\&amp;#34;getclosesttime\&amp;#34;,[\&amp;#34;{d}\&amp;#34;]]&amp;#34;.format(d=cacheKey) data = base64.</description></item><item><title>厉害了！打工人用 Python 分析西安市职位信息</title><link>https://qinyuanpei.github.io/posts/2147036181/</link><pubDate>Sat, 05 Dec 2020 12:49:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/2147036181/</guid><description>在上一篇博客中，我和大家分享了整个 11 月份找工作的心路历程，而在找工作的过程中，博主发现西安大小周、单休这种变相“996”的公司越来越多，感慨整个行业越来越“内卷”的同时，不免会对未来的人生有一点迷茫，因为深圳已经开始试运行“996”了，如果有一天“996”被合法化并成为一种常态，那么，我们又该如何去面对“人会一天天衰老，总有一天肝不动”的客观规律呢？我注意到 Boss 直聘移动端会展示某个公司的作息时间，所以，我有了抓取西安市职位和公司信息并对其进行数据分析的想法，我想知道，这到底是我一个人的感受呢？还是整个世界的确是这样子的呢？带着这样的想法，博主有了今天这篇博客。所以，在今天这篇博客里，博主会从Boss 直聘、智联招聘以及前程无忧上抓取职位和公司信息，并使用 MongoDB 对数据进行持久化，最终通过pyecharts对结果进行可视化展示。虽然不大确定 2021 年会不会变得更好，可生活最迷人的地方就在于它的不确定性，正如数据分析唯一可以做的，就是帮助我们从变化的事物中挖掘出不变的规律一样。
爬虫编写 其实，这种类似的数据分析，博主此前做过挺多的啦，譬如 基于 Python 实现的微信好友数据分析 以及 基于新浪微博的男女性择偶观数据分析(下) 这两篇博客。总体上来说，大部分学习 Python 的朋友都是从编写爬虫开始的，而在博主看来，数据分析是最终的目的，编写爬虫则是达到这一目的的手段。而从始至终，“爬”与“反爬”的较量从未停止过，Requests、BeautifulSoup、Selenium、Phantom 等等的技术层出不穷。考虑到现在编写爬虫存在风险，所以，我不会在博客里透露过多的“爬虫”细节，换言之，我不想成为一个教别人写爬虫的人，因为这篇博客的标签是数据分析，关于爬虫的部分，我点到为止，不再过多地去探讨它的实现，希望大家理解。而之所以要从这三个招聘网站上抓取，主要还是为了增加样本的多样性，因为 Boss 直聘上西安市的职位居然只有 3 页，这实在是太让人费解了！
Boss 直聘 通过抓包，我们可以分析出 Boss 直聘的地址：https://www.zhipin.com/job_detail/?query={query}&amp;amp;city={cityCode}&amp;amp;industry=&amp;amp;position=&amp;amp;page={page}。其中，query为待查询关键词，cityCode为待查询城市代码，page为待查询的页数。可以注意到，industry和position两个参数没有维护，它们分别表示待查询的行业和待查询的职称。因为我们面向的是更一般的“打工人”，所以，这些都可以进行简化。对于cityCode这个参数，我们可以通过下面的接口获得：https://www.zhipin.com/wapi/zpCommon/data/city.json。这里，简单定义一个方法extractCity()来提取城市代码：
def extractCity(self, cityName=None): if (os.path.exists(&amp;#39;bossCity.json&amp;#39;) and cityName != None): with open(&amp;#39;bossCity.json&amp;#39;, &amp;#39;rt&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as fp: cityList = json.load(fp) for city in cityList: if (city[&amp;#39;name&amp;#39;] == cityName): return city[&amp;#39;code&amp;#39;] else: response = requests.get(self.cityUrl) response.raise_for_status() json_data = response.json(); if (json_data[&amp;#39;code&amp;#39;] == 0 and json_data[&amp;#39;zpData&amp;#39;] !</description></item></channel></rss>