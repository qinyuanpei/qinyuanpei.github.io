<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Binlog on 元视角</title><link>http://example.org/tags/binlog/</link><description>Recent content in Binlog on 元视角</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Fri, 31 Jul 2020 12:01:14 +0000</lastBuildDate><atom:link href="http://example.org/tags/binlog/index.xml" rel="self" type="application/rss+xml"/><item><title>利用 MySQL 的 Binlog 实现数据同步与订阅(下)：EventBus 篇</title><link>http://example.org/posts/3424138425/</link><pubDate>Fri, 31 Jul 2020 12:01:14 +0000</pubDate><guid>http://example.org/posts/3424138425/</guid><description>终于到这个系列的最后一篇，在前两篇博客中，我们分别了介绍了Binlog的概念和事件总线(EventBus)的实现，在完成前面这将近好几千字的铺垫以后，我们终于可以进入正题，即通过 EventBus 发布 Binlog，再通过编写对应的 EventHandler 来订阅这些 Binlog，这样就实现了我们“最初的梦想”。坦白说，这个过程实在有一点漫长，庆幸的是，它终于还是来了。
Binlog 读取与解析 首先，我们通过 Python-Mysql-Replication 这个项目来读取 Binlog，直接通过pip install mysql-replication安装即可。接下来，我们编写一个简单的脚本文件，这再次印证那句名言——人生苦短，我用 Python：
def readBinLog(): stream = BinLogStreamReader( # 填写IP、账号、密码即可 connection_settings = { &amp;#39;host&amp;#39;: &amp;#39;&amp;#39;, &amp;#39;port&amp;#39;: 3306, &amp;#39;user&amp;#39;: &amp;#39;&amp;#39;, &amp;#39;passwd&amp;#39;: &amp;#39;&amp;#39; }, # 每台服务器唯一 server_id = 3, # 主库Binlog读写完毕时是否阻塞连接 blocking = True, # 筛选指定的表 only_tables = [&amp;#39;order_info&amp;#39;, &amp;#39;log_info&amp;#39;], # 筛选指定的事件 only_events = [DeleteRowsEvent, WriteRowsEvent, UpdateRowsEvent]) for binlogevent in stream: for row in binlogevent.rows: event = { &amp;#34;schema&amp;#34;: binlogevent.</description></item><item><title>利用 MySQL 的 Binlog 实现数据同步与订阅(上)：基础篇</title><link>http://example.org/posts/1333693167/</link><pubDate>Tue, 07 Jul 2020 09:23:59 +0000</pubDate><guid>http://example.org/posts/1333693167/</guid><description>终于等到了周末，在经历了一周的忙碌后，终于可以利用空闲写篇博客。其实，博主有一点困惑，困惑于这个世界早已“堆积”起人类难以想象的“大”数据，而我们又好像执着于去“造”一个又一个“差不多”的“内容管理系统”，从前我们说互联网的精神是开放和分享，可不知从什么时候起，我们亲手打造了一个又一个的“信息孤岛”。而为了打通这些“关节”，就不得不去造一张巨大无比的蜘蛛网，你说这就是互联网的本质，对此我表示无法反驳。我更关心的是这其中最脆弱的部分，即：一条数据怎么从 A 系统流转到 B 系统。可能你会想到API或者ETL这样的关键词，而我今天想说的关键词则是Binlog。假如你经常需要让数据近乎实时地在两个系统间流转，那么你应该停下来听我——一个不甘心整天写CRUD换取996福报的程序员，讲讲如何通过Binlog实现数据同步和订阅的故事。
什么是 Binlog 首先，来回答第一个问题，什么是 Binlog？Binlog 即 Binary Log，是 MySQL 中的一种二进制日志文件。它可以记录MySQL内部对数据库的所有修改，故，设计 Binlog 最主要的目的是满足数据库主从复制和增量恢复的需要。对于主从复制，想必大家都耳熟能详呢，因为但凡提及数据库性能优化，大家首先想到的所谓的“读写分离”，而无论是物理层面的一主多从，还是架构层面的CQRS，这背后最大的功臣当属主从复制，而实现主从复制的更底层原因，则要从 Binlog 说起。而对于数据库恢复，身为互联网从业者，对于像“rm -f”和“删库”、“跑路”这些梗，更是喜闻乐见，比如像今年的绿盟删库事件，在数据被删除以后，工程师花了好几天时间去抢救数据，这其中就用到了 Binlog。
可能大家会好奇，为什么 Binlog 可以做到这些事情。其实，从 Binlog 的三种模式上，我们就可以窥其一二，它们分别是：Statement、Row、Mixed，其中Statement模式记录的是所有数据库操作对应的 SQL 语句，如 INSERT、UPDATE 、DELETE 等 DML 语句，CREATE 、DROP 、ALTER 等 DDL，所以，从理论上讲，只要按顺序执行这些 SQL 语句，就可以实现不同数据库间的数据复制。而Row模式更关心每一行的变更，这种在实际应用中会更普遍一点，因为有时候更关心数据的变化情况，例如一个订单被创建出来，司机通过 App 接收了某个运输任务等。而Mixed模式可以认为是Statement模式和Row模式的混合体，因为Statement模式和Row模式都有各自的不足，前者可能会导致数据不一致，而后者则会占用大量的存储空间。在实际使用中，我们往往会借助各种各样的工具，譬如官方自带的mysqlbinlog、支持 Binlog 解析的StreamSets等等。
好了，下面我们简单介绍下 Binlog 相关的知识点。在使用 Binlog 前，首先需要确认是否开启了 Binlog，此时，我们可以使用下面的命令：
SHOW VARIABLES LIKE &amp;#39;LOG_BIN&amp;#39; 如果可以看到下面的结果，则表示 Binlog 功能已开启。 Binlog已开启示意图如果 Binlog 没有开启怎么办呢？此时，就需要我们手动来开启，为此我们需要修改 MySQL 的my.conf文件，通常情况下，该文件位于/etc/my.cnf路径，在[mysqld]下写入如下内容：
# 设置Binlog存储目录 log_bin = /var/lib/mysql/bin-log # 设置Binlog索引存储目录 log_bin_index = /var/lib/mysql/mysql-bin.</description></item></channel></rss>