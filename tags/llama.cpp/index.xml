<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Llama.cpp on 元视角</title><link>http://example.org/tags/llama.cpp/</link><description>Recent content in Llama.cpp on 元视角</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sun, 04 Feb 2024 12:30:47 +0000</lastBuildDate><atom:link href="http://example.org/tags/llama.cpp/index.xml" rel="self" type="application/rss+xml"/><item><title>使用 llama.cpp 在本地部署 AI 大模型的一次尝试</title><link>http://example.org/posts/an-attempt-to-deploy-a-large-ai-model-locally-using-llama.cpp/</link><pubDate>Sun, 04 Feb 2024 12:30:47 +0000</pubDate><guid>http://example.org/posts/an-attempt-to-deploy-a-large-ai-model-locally-using-llama.cpp/</guid><description>对于刚刚落下帷幕的2023年，人们曾经给予其高度评价——AIGC元年。随着 ChatGPT 的火爆出圈，大语言模型、AI 生成内容、多模态、提示词、量化&amp;hellip;等等名词开始相继频频出现在人们的视野当中，而在这场足以引发第四次工业革命的技术浪潮里，人们对于人工智能的态度，正从一开始的惊喜慢慢地变成担忧。因为 AI 在生成文字、代码、图像、音频和视频等方面的能力越来越强大，强大到需要 “冷门歌手” 孙燕姿亲自发文回应，强大到连山姆·奥特曼都被 OpenAI 解雇。在经历过 OpenAI 套壳、New Bing、GitHub Copilot 以及各式 AI 应用、各类大语言模型的持续轰炸后，我们终于迎来了人工智能的 “安卓时刻”，即除了 ChatGPT、Gemini 等专有模型以外，我们现在有更多的开源大模型可以选择。可这难免会让我们感到困惑，人工智能的尽头到底是什么呢？2013年的时候，我以为未来属于提示词工程(Prompt Engineering)，可后来好像是 RAG 以及 GPTs 更受欢迎？
从哪里开始 在经历过早期调用 OpenAI API 各种障碍后，我觉得大语言模型，最终还是需要回归到私有化部署这条路上来。毕竟，连最近新上市的手机都开始内置大语言模型了，我先后在手机上体验了有大语言模型加持的小爱同学，以及抖音的豆包，不能说体验有多好，可终归是聊胜于无。目前，整个人工智能领域大致可以分为三个层次，即：算力、模型和应用。其中，算力，本质上就是芯片，对大模型来说特指高性能显卡；模型，现在在 Hugging Face 可以找到各种开源的模型，即便可以节省训练模型的成本，可对这些模型的微调和改进依然是 “最后一公里” 的痛点；应用，目前 GPTs 极大地推动了各类 AI 应用的落地，而像 Poe 这类聚合式的 AI 应用功能要更强大一点。最终，我决定先在 CPU 环境下利用 llama.cpp 部署一个 AI 大模型，等打通上下游关节后，再考虑使用 GPU 环境实现最终落地。从头开始训练一个模型是不大现实的，可如果通过 LangChain 这类框架接入本地知识库还是有希望的。
编译 llama.cpp llama.cpp 是一个纯 C/C++ 实现的 LLaMA 模型推理工具，由于其具有极高的性能，因此，它可以同时在 GPU 和 CPU 环境下运行，这给了像博主这种寻常百姓可操作的空间。在 Meta 半开源了其 LLaMA 模型以后，斯坦福大学发布了其基于 LLaMA-7B 模型微调而来的模型 Alpaca，在开源社区的积极响应下，在 Hugging Face 上面相继衍生出了更多的基于 LLaMA 模型的模型，这意味着这些由 LLaMA 衍生而来的模型，都可以交给 llama.</description></item></channel></rss>