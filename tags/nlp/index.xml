<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on 元视角</title><link>https://qinyuanpei.github.io/tags/nlp/</link><description>Recent content in NLP on 元视角</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Fri, 12 May 2023 20:49:47 +0000</lastBuildDate><atom:link href="https://qinyuanpei.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>后 GPT 时代，NLP 不存在了？</title><link>https://qinyuanpei.github.io/posts/in-the-post-gpt-era-nlp-no-longer-exists/</link><pubDate>Fri, 12 May 2023 20:49:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/in-the-post-gpt-era-nlp-no-longer-exists/</guid><description>在刘慈欣老师的《三体》小说中，整个故事是以杨冬的死亡线索展开的，而她自杀的原因是物理学不存在了。随着 GPT-4 的发布，『NLP已死』和『NLP不存在了』的声音开始不绝于耳。如果说杨冬认为物理学被颠覆源于智子的“欺骗”，那么，现在的大型语言模型对于 NLP 的冲击，实际上改变了AI与最终用户互动的方式。传统的 NLP 技术方向涵盖了信息抽取、文本挖掘、机器翻译、语音合成、语音识别、语义理解、句法分析，这些都被视为自然语言处理的中间任务。因此，传统的 NLP 模式是在每个领域中提供各种不同的工具。当需要对自然语言进行处理时，你不得不将这些不同的工具结合起来，就像博主曾经使用过的结巴分词、SnowNLP、词袋模型、情感分析、TF-IDF一样。然而，现在的大型语言模型更像是一个直接面对最终任务的“智者”，跳过了这些中间任务。因为我们最终的目的就是要让程序产生智能，并让人类相信它能够“理解”他们的意图。显然，在这一点上，ChatGPT 和 Midjourney 都做到了。作为非科班的程序员，我无法科学、客观地评判 LLM 和 NLP 的优劣。但是我想分享一下我在开发 “贾维斯” 过程中获得的一点心得，希望对大家有所启发。
Hey Jarvis 在将小爱同学接入 ChatGPT 以后，我开始思考怎么样在智能和功能上取得平衡，尽管 ChatGPT 提升了小爱同学在聊天方面的智能，可这同时破坏了当下智能音箱普遍使用的“指令集”，你无法运用 ChatGPT 的这种“聪慧”让它真正地帮你做点事情。所以，我大概率要再次发明“智能音箱”，可我想知道，有了 ChatGPT 的加持以后，它到底能达到什么样的智能水平？带着这样一个想法，我开始从头编写贾维斯，一个基于 ChatGPT 的人工智能管家，其灵感来源于钢铁侠的人工智能管家 Jarvis。目前，它可以查询日期/时间、检索信息、播放音乐、控制米家/电脑、打开应用、讲笑话、查询天气、编程。以下是我在 Bilibili 上发布的视频演示：
你可能会好奇这些功能都是如何实现的？请允许我做一个简单说明，下面是“贾维斯”的整体设计思路：
贾维斯整体设计思路说明从整体而言，整个程序并不算特别复杂，因为语音唤醒、语音识别、语音合成这些均已有非常成熟的方案。在接入 ChatGPT 以后，我开始尝试为它扩展更多的功能。这个时候，我了解到自然语言理解(NLU)的这个方向，并且它依然属于自然语言处理(NLP)这个范畴，更具体的关键词则是意图识别或者意图提取。以查询日期这个功能为例，我只需要在某个函数上添加“路由”，即可为贾维斯设计不同的“技能”：
@trigger.route(keywords=[&amp;#39;查询日期&amp;#39;,&amp;#39;询问日期&amp;#39;,&amp;#39;日期查询&amp;#39;]) def report_date(input): now = datetime.datetime.now() week_list = [&amp;#39;星期一&amp;#39;,&amp;#39;星期二&amp;#39;,&amp;#39;星期三&amp;#39;,&amp;#39;星期四&amp;#39;,&amp;#39;星期五&amp;#39;,&amp;#39;星期六&amp;#39;,&amp;#39;星期日&amp;#39;] formated = now.strftime(&amp;#39;%Y年%m月%d日&amp;#39;) week_day = week_list[now.weekday()] history = today_on_history() if history != None: return f&amp;#39;今天是{formated}，{week_day}。{history}。&amp;#39; else: return f&amp;#39;今天是{formated}，{week_day}。&amp;#39; 此时，问题就被转化为如何识别或者提取一句话中的真实意图。坦白地讲，面对人类这种口是心非的动物，想要了解其真实意图是非常困难的。譬如，人类都希望别人能够“懂我”，可没有人会喜欢被别人一眼看穿。因此，我们这里讨论的意图，特指那些动宾结构的指令型用语，例如：打开客厅的灯、查询明天的天气等等。不论屏幕前的你对 AI 持何种态度，我想说的是，AI 已然参与到我日常的编程和写作中，这正是我开发贾维斯的动力所在，我希望它能参与到更多的事项中去，甚至想让它取代家里的小爱同学。</description></item><item><title>SnowNLP 使用自定义语料进行模型训练</title><link>https://qinyuanpei.github.io/posts/1772340994/</link><pubDate>Wed, 19 May 2021 21:22:41 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/1772340994/</guid><description>SnowNLP 是一个功能强大的中文文本处理库，它囊括了中文分词、词性标注、情感分析、文本分类、关键字/摘要提取、TF/IDF、文本相似度等诸多功能，像隐马尔科夫模型、朴素贝叶斯、TextRank等算法均在这个库中有对应的应用。如果大家仔细观察过博主的博客，就会发现博主使用了摘要提取这一功能来增强博客的SEO，即通过自然语言处理(NLP)技术，提取每一篇文章中的摘要信息。因为 SnowNLP 本身使用的语料是电商网站评论，所以，当我们面对不同的使用场景时，它自带的这个模型难免会出现“水土不服”。因此，如果我们希望得到更接近实际的结果，最好的方案是使用自定义语料进行模型训练。值得庆幸的是，这一切在 SnowNLP 中实施起来非常简单，并不需要我们去钻研那些高深莫测的算法。至此，就引出了今天这篇博客的主题，即 SnowNLP 使用自定义语料进行模型训练。
不知道大家是否还有印象，博主曾经在 《通过 Python 分析 2020 年全年微博热搜数据》 这篇文章中提到过 SnowNLP 的模型训练。当时，博主采集了整个 2020 年的微博热搜话题，因为要体现整个一年里的情感变化，博主特意找了两份微博语料，并以此为基础训练出了一个模型文件。
2020全年微博热搜情感变化趋势那么，具体是怎么样做的呢？我们一起来看一下：
from snownlp import sentiment sentiment.train(&amp;#39;./train/neg60000.txt&amp;#39;, &amp;#39;./train/pos60000.txt&amp;#39;) sentiment.save(&amp;#39;weibo.marshal&amp;#39;) 千万不要怀疑你的眼睛，因为它真的只有短短的三行代码。简单来说，我们只需要准备一个“积极”的语料文件，一个“消极”的语料文件，它就可以训练出一个模型文件。特别注意的是，如果是在Python 3.X的版本下，最终生成的模型文件的扩展名将会是.3，下图是博主这里训练出的模型文件：
SnowNLP 使用自定义语料进行模型训练好了，一旦训练出这个模型文件，我们就可以考虑替换掉 SnowNLP 的默认模型文件，我们可以在以下位置：\Lib\site-packages\snownlp\sentiment 找到下列文件。为了安全起见，我们首先将原来的模型文件重命名，然后再放入我们自己的模型文件。
SnowNLP 使用自定义模型替换默认模型此时，我们就可以利用训练好的模型，分析某一条微博的情感倾向。这里我选取了几条我的微博，看看这个情感倾向预测的结果如何：
from snownlp import SnowNLP s = SnowNLP(u&amp;#39;我爱你，并不期待回声&amp;#39;) s.sentiments # 0.8760737296091975 s = SnowNLP(u&amp;#39;想找一个人，一起做老爷爷、老奶奶才做的事情，比如，替我拔一拔头上的白头发……[二哈] ​​&amp;#39;) s.sentiments # 0.001629297651780881 s = SnowNLP(u&amp;#39;如果两个人都不爱了，一别两宽，各生欢喜，其实是挺好的结局；可如果还有一个人爱着，对那个人来说，爱又是什么呢？&amp;#39;) s.sentiments # 0.809651945221708 s = SnowNLP(u&amp;#39;为了发张自拍，特意出来跑步，还有谁？[doge] ​​​&amp;#39;) s.sentiments # 0.4041057894917053 有人说，双子座是一个白天自愈、晚上孤独的星座，我确信这是真的，因为从我出生的那一刻起，那种宏大宇宙中的孤独感就一直笼罩着我，用一句话来形容，大概就是“热闹是人家的，我什么都没有”，因为内心世界里的两个灵魂，从来没有一刻闲歇地在纠缠和撕裂。我一直都想了解一件事情，如果这些基于概率或者是公式的算法，都可以琢磨出人类某个时刻的心境，我们期望别人能懂自己是不是太过矫情，我们是真的了解自己吗？</description></item><item><title>使用 Python 抽取《半泽直树》原著小说人物关系</title><link>https://qinyuanpei.github.io/posts/1427872047/</link><pubDate>Tue, 08 Dec 2020 22:49:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/1427872047/</guid><description>此时此刻，2020 年的最后一个月，不管过去这一年给我们留下了怎样的记忆，时间终究自顾自地往前走，留给我们的怀念已时日无多。如果要说 2020 年的年度日剧，我想《半泽直树》实至名归，这部在时隔七年后上映的续集，豆瓣评分高达 9.4 分，一度超越 2013 年第一部的 9.3 分，是当之无愧的现象级电视剧，期间甚至因为疫情原因而推迟播出，这不能不感谢为此付出辛勤努力的演职人员们。身为一个“打工人”，主角半泽直树那种百折不挠、恩怨分明的性格，难免会引起你我这种“社畜”们的共鸣，即使做不到“以牙还牙，加倍奉还”，至少可以活得像一个活生生的人。电视剧或许大家都看过了，那么，电视剧相对于原著小说有哪些改动呢？今天，就让我们使用 Python 来抽取半泽直树原著小说中的人物关系吧！
准备工作 在开始今天的博客内容前，我们有一点准备工作要完成。考虑到小说人物关系抽取，属于自然语言处理(NLP)领域的内容，所以，除了准备好 Python 环境以外，我们需要提前准备相关的中文语料，在这里主要有：半泽直树原著小说、 半泽直树人名词典、半泽直树别名词典、中文分词停用词表。除此之外，我们需要安装结巴分词、PyECharts两个第三方库(注，可以通过 pip 直接安装)，以及用于展示人物关系的软件Gephi(注，这个软件依赖 Java 环境)。所以，你基本可以想到，我们会使用结巴分词对小说文本进行分词处理，而半泽直树人名列表则作为用户词典供结巴分词使用，经过一系列处理后，我们最终通过Gephi和PyECharts对结果进行可视化，通过分析人物间的关系，结合我们对电视剧剧情的掌握情况，我们就可以对本文所采用方法的效果进行评估，也许你认为两个人毫无联系，可最终他们以某种特殊的形式建立了联系，这就是我们要做这件事情的意义所在。本项目已托管在 Github上，供大家自由查阅。
原理说明 这篇博客主要参考了 Python 基于共现提取《釜山行》人物关系 这个课程，该项目已在 Github 上开源，可以参考：https://github.com/Forec/text-cooccurrence。这篇文章中提到了一种称之为“共现网络”的方法，它本质上是一种基于统计的信息提取方法。其基本原理是，当我们在阅读书籍或者观看影视作品时，在同一时间段内同时出现的人物，通常都会存在某种联系。所以，如果我们将小说中的每个人物都看作一个节点，将人物间的关系都看作一条连线，最终我们将会得到一个图(指数据结构中的Graph)。因为Gephi和PyECharts以及NetworkX都提供了针对Graph的可视化功能，因此，我们可以使用这种方法，对《半泽直树》原著小说中的人物关系进行抽取。当然，这种方法本身会存在一点局限性，这些我们会放在总结思考这部分来进行说明，而我们之所以需要准备人名词典，主要还是为了排除单纯的分词产生的干扰词汇的影响；准备别名词典，则是考虑到同一个人物，在不同的语境下会有不同的称谓。
过程实现 这里，我们定义一个RelationExtractor类来实现小说人物关系的抽取。其中，extract()方法用于抽取制定小说文本中的人物关系，exportGephi()方法用于输出 Gephi 格式的节点和边信息， exportECharts()方法则可以使用ECharts对人物关系进行渲染和输出：
import os, sys import jieba, codecs, math import jieba.posseg as pseg from pyecharts import options as opts from pyecharts.charts import Graph class RelationExtractor: def __init__(self, fpStopWords, fpNameDicts, fpAliasNames): # 人名词典 self.name_dicts = [line.strip().split(&amp;#39; &amp;#39;)[0] for line in open(fpNameDicts,&amp;#39;rt&amp;#39;,encoding=&amp;#39;utf-8&amp;#39;).</description></item></channel></rss>