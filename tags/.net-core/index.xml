<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>.NET Core on 元视角</title><link>http://example.org/tags/.net-core/</link><description>Recent content in .NET Core on 元视角</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sun, 07 Aug 2022 16:01:13 +0000</lastBuildDate><atom:link href="http://example.org/tags/.net-core/index.xml" rel="self" type="application/rss+xml"/><item><title>.NET Core + ELK 搭建可视化日志分析平台(下)</title><link>http://example.org/posts/3687594959/</link><pubDate>Sun, 07 Aug 2022 16:01:13 +0000</pubDate><guid>http://example.org/posts/3687594959/</guid><description>最近，我收到一位读者朋友的私信，问我 ELK 为什么没有下篇，道德感极强的我不得不坦诚相告，显然这一篇鸽了。这就是说，鸽子不单单会出现在吴宇森的电影里，只要你试图拖延或者逃避，你一样有鸽子可以放飞。话说回来，新冠疫情已然持续了三年，而这篇文章其实是我在新冠元年写下的。某年某月，彼时彼刻，立春过后紧接着是上元节，阳光已透过玻璃宣示着春天的到来，可在这一墙之隔的里里外外，仿佛是两个气候迥异的世界。记忆里那种每天都和消毒水、口罩打交道的日子，后来就变成了一种习以为常、甚至有一点唏嘘的常态化生活。在这过去的三年里，恍惚中已经发生太多的事情，譬如 ELK 早已变成了 EFK，譬如前女友有了新的男朋友，在一切的物是人非背后，在一切的断壁残垣下面，我想，我还是用这个旧题目来讲一个新的故事罢！
从 Logstash 到 Filebeat 当初准备写这个系列的时候，ELK 还是经典的 Elastaicsearch 、 Logstash 和 Kibana 组合，如下图所示，Logstash 从各种不同的数据源收集数据，通过内置的管道对输入的数据进行加工。最终，这些数据会被存储到 Elastaicsearch 中供 Kibana 完成数据可视化。 即使放到三年后的今天来看，这张图依然是非常经典的一幅图。为什么这么说呢？因为自此以后，可视化日志分析平台的搭建，基本都是围绕这三个方面展开，甚至 Logstash 的继任者 Filebeat、Fluentd、Fluent-Bit 等等无一不沿用了 Logstash 的这套管道设计，足可见其对后来者的影响之深远。不过，作为先驱出现的 Logstash，其本身是采用 Java 语言开发的，其插件则是采用 Ruby 语言开发的，特别是第一点，一直让 Logstash 在性能问题上遭人垢病。在实际使用中，你常常需要在每一台服务器上安装 Logstash ，这意味着它在 CPU 和内存上的占用会比较高。
经典的 ELK 全家桶组合为了解决这个问题，Elastic 官方推出了被称为 Beats 的下一代日志收集方案， 这是一种基于 Go 语言开发、更加轻量级的、资源占用更少的日志收集方案，可以认为是 Logstash 的替代品, 而 Filebeat 正好是其中一种实现。关于这两者的区别，我想，使用下面的比喻或许会更恰当一点， Logstash 就像一个轰鸣声不断的垃圾转运车，虽然可以让你直接把垃圾丢车上拉走，可你不得不忍受一整天的噪音；Filebeat 则像一个拎着扫帚和簸萁的环卫工人，那里有需要就去哪里清扫，不单单效率高而且不会让你感觉扰民，下面是一张来自 Elastic 官方文档 中的示意图：
Filebeat 日志收集示意图从这里我们可以看出， Filebeat 由两个主要的组件 Inputs 和 Harvester 组成。其中， Harvester 是一个负责读取单个文件内容的采集器，它可以打开和关闭一个文件，并将内容发送到指定的输出；Inputs 顾名思义就是输入，对于 Filebeat 而言，其实就是指各种不同类型的日志文件，譬如 Filebeat 可以支持 Kafka、Redis、MQTT、TCP、UDP、Stdin、Syslog 等等的输入。从某种意义上讲，你可以把 Filebeat 理解为一个文件扫描服务。例如，下面的配置表示 Filebeat 将会从一个指定的路径读取日志文件：</description></item><item><title>源代码探案系列之 .NET Core 限流中间件 AspNetCoreRateLimit</title><link>http://example.org/posts/2396015802/</link><pubDate>Wed, 10 Mar 2021 21:52:47 +0000</pubDate><guid>http://example.org/posts/2396015802/</guid><description>在上一篇文章中，博主带领大家一起深入了解 ConcurrencyLimiter 这个中间件，正当我得意洋洋地向 Catcher Wong 大佬吹嘘这一点小收获时，大佬一脸嫌弃地说，一个单机版的方案有什么好得意的啊。大佬言下之意，显然是指，这个中间件在分布式环境中毫无用武之地。其实，你只需要稍微想一下，就能想明白这个问题。毕竟，它只是通过SeamphoreSlim控制线程数量而已，一旦放到分布式环境中，这个并发控制就被大大地削弱。所以，在今天这篇文章中，博主会带领大家一起“探案” ASP.NET Core 中的限流中间件 AspNetCoreRateLimite，希望大家可以从中感悟到不一样的东西。对我而言，这可能是人到中年的焦虑感所催生出来的一种源动力，同时亦是为了不让那些订阅专栏的同学失望。
关于“限流”这个话题，我个人以为，它可以引申出非常多的东西，譬如“熔断”和“限流”，其实可以看作是同一类问题的“一体两面”。最早接触熔断，是源于 Spring Cloud 中的 Hystrix，它其实是指当服务不可用的时候，客户端应该采取什么样的措施去应对，实际使用中我们可能会考虑重试、超时、降级等策略。相应地，当服务端在面对来自客户端的异常流量时，就产生了“限流”这个概念，“限流”可以是线程隔离**(线程数 + 队列大小限制)，可以是信号量隔离(设置最大并发请求数目)，可以是限制QPS。这里，我们讨论的主要是第三种，而实现限流的常见算法主要有计数器算法、漏桶算法和令牌桶算法。这里，AspNetCoreRateLimit 这个中间件，则主要使用了计数器算法，并配合 IMemoryCache 和 IDistributedCache 分别实现了基于内存和基于分布式缓存的持久化逻辑。
源代码解读 首先，使用者通过配置定义了一个或者多个规则，这些规则决定了每个客户端在访问特定终结点时，一段时间内可以访问的最大次数。 RateLimitMiddleware 通过注入的IRateLimitProcessor 来匹配规则，然后依次判断每个规则是否达到了限流条件。一旦达到限流条件，中间件会改变 HTTP 响应的状态码、响应头、返回值，告知使用者已达到最大调用次数。而针对每一种 IRateLimitProcessor ，主要通过ProcessRequestAsync() 方法来实现计数，如果上一次的请求对应的时间戳 + 规则中时间间隔 &amp;gt;= 当前时间，则说明请求没有过期，此时，就需要给这个计数增加1。好了，现在我们来针对 AspNetCoreRateLimit 中的核心部件逐个进行解读。
RateLimitProcessor RateLimitProcessor，是一个抽象类，实现了IRateLimitProcessor接口，公开的方法有 3 个：ProcessRequestAsync()、IsWhitelisted() 和 GetRateLimitHeaders()。在此基础上，派生出ClientRateLimitProcessor和IpRateLimitProcessor两个子类。两者最大的不同在于，其所依赖的Store不同，前者为IClientPolicyStore，后者IIpPolicyStore，它们都实现了同一个接口IRateLimitStore：
public interface IRateLimitStore&amp;lt;T&amp;gt; { Task&amp;lt;bool&amp;gt; ExistsAsync(string id, CancellationToken cancellationToken = default); Task&amp;lt;T&amp;gt; GetAsync(string id, CancellationToken cancellationToken = default); Task RemoveAsync(string id, CancellationToken cancellationToken = default); Task SetAsync(string id, T entry, TimeSpan?</description></item><item><title>源代码探案系列之 .NET Core 并发限制中间件 ConcurrencyLimiter</title><link>http://example.org/posts/18417412/</link><pubDate>Thu, 04 Mar 2021 20:13:47 +0000</pubDate><guid>http://example.org/posts/18417412/</guid><description>打算开一个新的专栏——源代码探案系列，目的是通过源代码来探索更广阔的技术世界。因为我越来越意识到，我可能缺乏一个结构化的知识体系，虽然处在一个碎片化的时代，从外界接收了大量的信息，可这些碎片化的信息，到底能不能转化为自身可用的知识，其实是需要去认真思考一番。尤其是当我注意到，许多人工作多年，在经历过从“生手”到“熟练工”这种蜕变以后，居然还是会害怕原理性内容的考察。我承认，程序员这个职业更像是一个“手艺人”，可我更想说一句古人的话——君子不器。什么是器呢？“形而上者谓之道，形而下者谓之器”，用一句更直白的话来说，就是“不能知其然而不知其所以然”，这是我一个非CS科班出身的程序员，想去写这样一个专栏的初衷，因为在我看来，“器”是永远学不完的，而“道”虽然听起来虚无缥缈，实则“朝闻道，夕死可矣”。
作为这个专栏的第一篇博客，我打算从 ASP.NET Core 中的 ConcurrencyLimiter 这个中间件开始。并发是一个爱恨交织的话题，我们喜欢高并发，因为这是程序员跻身高手行列的好机会；我们厌恶并发，因为它引入了多线程、锁、信号量这些复杂的东西。相信大家都曾被并发困扰过，古人云：他山之石，可以攻玉，还有什么比阅读源代码更朴实无华的“学习”呢？你找大牛，大牛可能忙着开会、做PPT；你找同事，同事里可能十个有八个都不知道啊。这个中间件的核心是 IQueuePolicy ，其位于以下位置，它定义了两个核心的方法：TryEnterAsync() 和 OnExit()：
public interface IQueuePolicy { ValueTask&amp;lt;bool&amp;gt; TryEnterAsync(); void OnExit(); } 在其默认实现QueuePolicy中，TryEnterAsync()方法，决定着一个请求是会被拒绝还是接受。具体是怎么做呢？它定义了一个最大的并发请求数目，如果实际数超过了最大的并发请求数目，那么请求将会被拒绝。反之，请求将被接受。再仔细看，我们就会发现，它内部使用了SeamphoreSlim和Interlocked，所以，聪明的小伙伴们应该立马会联想到，这两种锁各自的作用是什么。
其中，Seamphore 是一个 Windows 内核中的一个同步信号量，适用于在多个有限的线程资源中共享内存资源，它就像一个栅栏，本身具有一定的容量，当线程数量达到这个容量后，新的线程就无法再通过，直到某个线程执行完成。SeamphoreSlim是Seamphore优化后的版本，在性能上表现更好一点，更推荐大家使用SeamphoreSlim。
而 Interlocked 的则是我们熟悉的原子操作，它可以在多个线程中，对共享的内存资源进行原子加或者原子减操作。在这里，Interlocked主要用来控制并发请求数的加和减。如果当前的并发请求数小于最大的并发请求数，表示还可以允许新的请求进来，此时，TryEnterAsync()方法会返回true。如果此时的并发请求数大于最大的并发请求数，则需要对当前请求数进行减操作，此时，TryEnterAsync()方法会返回false。
一旦搞清楚这一点，结合中间件的代码，我们可以非常容易地想明白,这个并发控制的实现思路。下面是QueuePolicy中TryEnterAsync()和OnExit()两个方法的实现，分别代表了“加锁”和“解锁”两个不同的阶段。某种程度上，Seamphore更像一个水闸，每次可以通过的“流量”是固定的，超出的部分会被直接“拒绝”：
//“加锁” public ValueTask&amp;lt;bool&amp;gt; TryEnterAsync() { // a return value of &amp;#39;false&amp;#39; indicates that the request is rejected // a return value of &amp;#39;true&amp;#39; indicates that the request may proceed // _serverSemaphore.Release is *not* called in this method, // it is called externally when requests leave the server int totalRequests = Interlocked.</description></item><item><title>通过 EmbededFileProvider 实现 Blazor 的静态文件访问</title><link>http://example.org/posts/3789745079/</link><pubDate>Tue, 23 Feb 2021 05:37:47 +0000</pubDate><guid>http://example.org/posts/3789745079/</guid><description>重构我的 独立博客 ，是博主今年的计划之一，这个基于 Hexo 的静态博客，最早搭建于2014年，可以说是比女朋友更亲密的存在，陪伴着博主走过了毕业、求职以及此刻的而立之年。其间虽然尝试过像 Jekyll 和 Hugo 这样的静态博客生成器，可是考虑到模板、插件等周边生态，这个想法一直被搁置下来。直到最近，突然涌现出通过 Blazor 重写博客的想法，尤其是它对于 WebAssembly 的支持，而类似 Vue 和 React的组件化开发模式，在开发体验上有着同样不错的表现。所以，今天这篇博客就来聊聊在重写博客过程中的一点收获，即如何让 Blazor 访问本地的静态文件。
从内嵌资源说起 首先，我们要引入一个概念，即：内嵌资源。我们平时接触的更多的是本地文件系统，或者是 FTP 、对象存储这类运行在远程服务器上的文件系统，这些都是非内嵌资源，所以，内嵌资源主要是指那些没有目录层级的文件资源，因为它会在编译的时候“嵌入”到动态链接库(DLL)中。一个典型的例子是Swagger，它在.NET Core平台下的实现是Swashbuckle.AspNetCore，它允许使用自定义的HTML页面。这里可以注意到，它使用到了GetManifestResourceStream()方法：
app.UseSwaggerUI(c =&amp;gt; { // requires file to be added as an embedded resource c.IndexStream = () =&amp;gt; GetType().Assembly .GetManifestResourceStream(&amp;#34;CustomUIIndex.Swagger.index.html&amp;#34;); }); 其实，这里使用的就是一个内嵌资源。关于内嵌资源，我们有两种方式来定义它：
在 Visual Studio 中选中指定文件，在其属性窗口中选择生成操作为嵌入的资源： 如何定义一个文件资源为内嵌资源在项目文件(.csproj)中修改对应ItemGroup节点，参考示例如下： &amp;lt;Project Sdk=&amp;#34;Microsoft.NET.Sdk.Web&amp;#34;&amp;gt; &amp;lt;!-- ... --&amp;gt; &amp;lt;ItemGroup&amp;gt; &amp;lt;EmbeddedResource Include=&amp;#34;_config.yml&amp;#34;&amp;gt; &amp;lt;CopyToOutputDirectory&amp;gt;Always&amp;lt;/CopyToOutputDirectory&amp;gt; &amp;lt;/EmbeddedResource&amp;gt; &amp;lt;/ItemGroup&amp;gt; &amp;lt;!-- ... --&amp;gt; &amp;lt;/Project&amp;gt; 这样，我们就完成了内嵌资源的定义。而定义内嵌资源，本质上还是为了在运行时期间去读取和使用，那么，自然而然地，我们不禁要问，该怎么读取这些内嵌资源呢？在Assembly类中，微软为我们提供了下列接口来处理内嵌资源：
public virtual ManifestResourceInfo GetManifestResourceInfo(string resourceName); public virtual string[] GetManifestResourceNames(); public virtual Stream GetManifestResourceStream(Type type, string name); public virtual Stream GetManifestResourceStream(string name); 其中，GetManifestResourceNames()方法用来返回所有内嵌资源的名称，GetManifestResourceInfo()方法用来返回指定内嵌资源的描述信息，GetManifestResourceStream()方法用来返回指定内嵌资源的文件流。为了方便大家理解，这里我们准备了一个简单的示例：</description></item><item><title>基于选项模式实现.NET Core 的配置热更新</title><link>http://example.org/posts/835719605/</link><pubDate>Sun, 11 Oct 2020 12:19:02 +0000</pubDate><guid>http://example.org/posts/835719605/</guid><description>最近在面试的时候，遇到了一个关于 .NET Core 配置热更新的问题，顾名思义，就是在应用程序的配置发生变化时，如何在不重启应用的情况下使用当前配置。从 .NET Framework 一路走来，对于 Web.Config 以及 App.Config 这两个配置文件，我们应该是非常熟悉了，通常情况下， IIS 会检测这两个配置文件的变化，并自动完成配置的加载，可以说它天然支持热更新，可当我们的视野伸向分布式环境的时候，这种配置方式就变得繁琐起来，因为你需要修改一个又一个配置文件，更不用说这些配置文件可能都是放在容器内部。而有经验的朋友，可能会想到，利用 Redis 的发布-订阅来实现配置的下发，这的确是一个非常好的思路。总而言之，我们希望应用可以随时感知配置的变化，所以，在今天这篇博客里，我们来一起聊聊 .NET Core 中配置热更新相关的话题，这里特指全新的选项模式(Options)。
Options 三剑客 在 .NET Core 中，选项模式(Options)使用类来对一组配置信息进行强类型访问，因为按照接口分隔原则(ISP)和关注点分离这两个工程原则，应用的不同部件的配置应该是各自独立的，这意味着每一个用于访问配置信息的类，应该是只依赖它所需要的配置信息的。举一个简单的例子，虽然 Redis 和 MySQL 都属于数据持久化层的设施，但是两者属于不同类型的部件，它们拥有属于各自的配置信息，而这两套配置信息应该是相互独立的，即 MySQL 不会因为 Redis 的配置存在问题而停止工作。此时，选项模式(Options)推荐使用两个不同的类来访问各自的配置。我们从下面这个例子开始：
{ &amp;#34;Learning&amp;#34;: { &amp;#34;Years&amp;#34;: 5, &amp;#34;Topic&amp;#34;: [ &amp;#34;Hotfix&amp;#34;, &amp;#34;.NET Core&amp;#34;, &amp;#34;Options&amp;#34; ], &amp;#34;Skill&amp;#34;: [ { &amp;#34;Lang&amp;#34;: &amp;#34;C#&amp;#34;, &amp;#34;Score&amp;#34;: 3.9 }, { &amp;#34;Lang&amp;#34;: &amp;#34;Python&amp;#34;, &amp;#34;Score&amp;#34;: 2.6 }, { &amp;#34;Lang&amp;#34;: &amp;#34;JavaScript&amp;#34;, &amp;#34;Score&amp;#34;: 2.8 } ] } } 此时，如果希望访问Learning节点下的信息，我们有很多种实现方式：
//方式1 var learningSection = Configuration.</description></item><item><title>.NET Core 中对象池(Object Pool)的使用</title><link>http://example.org/posts/2414960312/</link><pubDate>Sat, 15 Aug 2020 16:37:23 +0000</pubDate><guid>http://example.org/posts/2414960312/</guid><description>在此前的博客中，博主参考 eShopOnContainers 实现了一个基于 RabbitMQ 的事件总线(EventBus)。在这个项目中，它提供了一个持久化连接的类DefaultRabbitMQPersistentConnection，主要解决了 RabbitMQ 在连接断开后自动重连的问题，可实际上我们都知道，RabbitMQ 提供的连接数是有一个上限的，如果频繁地使用短连接的方式，即通过ConnectionFactory的CreateConnection()方法来创建一个连接，从本质上讲，一个Connection对象就是一个 TCP 连接，而Channel则是每个Connection对象下有限的虚拟连接，注意“有限”这个限定词，这意味着Channel和Connection一样，都不能毫无节制的创建下去。此时，官方推荐的做法有两种：(1)：一个Connection对应多个Channel同时保证每个Channel线程独占；(2)：创建一个Connection池同时定期清除无效连接。这里的第二种做法，显然就是我们今天要说的对象池(Object Pool)啦，我们将从这里拉开这篇博客的帷幕。
什么是对象池 首先，我们来回答第一个问题，什么是对象池？简单来说，它就是一种为对象提供可复用性能力的软件设计思路。俗话说**“有借有还，再借不难”**，而对象池就是通过“借”和“还”这样两个动作来保证对象可以被重复使用，进而节省频繁创建对象的性能开销。对象池在游戏设计中使用的更普遍一点，因为游戏中大量存在着像子弹、怪物等等这类可复用的对象，你在玩第一人称射击游戏(FPS)时，总是有源源不断的子弹或者丧尸出现，可事实上这不过是数字世界的循环再生，因为玩家的电脑内存始终都有一个上限。而在数据库的世界里，则存在着一个被称为“连接池”的东西，每当出现数据库无法连接的情况时，经验丰富的开发人员往往会先检查“连接池”是否满了，这其实就是对象池模式在特定领域的具体实现啦，所以，对象池本质上就是负责一组对象创建和销毁的容器，下面是一个基本的对象池示意图：
对象池示意图可以注意到， 对象池最大的优势就是可以自主地管理“池子”内的每个对象，决定它们是需要被回收还是可以重复使用。我们都知道，创建一个新的对象，需要消耗一定的系统资源，而一旦这些对象可以重复地使用，就能有效地节省系统资源的开销，这对于我们提高系统性能会非常有帮助。也许，现在计算机的硬件水平越来越好，可我们还是要重新拾起这个领域的基础知识，即数据结构、算法、数学和英语。如果你完全理解了对象池模式，你应该可以非常轻松地给出你的实现：
public class ObjectPool&amp;lt;T&amp;gt; : IObjectPool&amp;lt;T&amp;gt; { private Func&amp;lt;T&amp;gt; _instanceFactory; private ConcurrentBag&amp;lt;T&amp;gt; _instanceItems; public ObjectPool(Func&amp;lt;T&amp;gt; instanceFactory) { _instanceFactory = instanceFactory ?? throw new ArgumentNullException(nameof(instanceFactory)); _instanceItems = new ConcurrentBag&amp;lt;T&amp;gt;(); } public T Get() { T item; if (_instanceItems.TryTake(out item)) return item; return _instanceFactory(); } public void Return(T item) { _instanceItems.Add(item); } } 注：以上代码片段来自微软的一篇文档：How to: Create an Object Pool by Using a ConcurrentBag。实际上，除了ConcurrentBag&amp;lt;T&amp;gt;，我们可以选择的数据结构还可以是Stack&amp;lt;T&amp;gt;、Queue&amp;lt;T&amp;gt;以及BlockingCollection&amp;lt;T&amp;gt;，此中差别，大家可以自己去体会。</description></item><item><title>.NET Core 原生 DI 扩展之属性注入实现</title><link>http://example.org/posts/1658310834/</link><pubDate>Sat, 20 Jun 2020 13:10:31 +0000</pubDate><guid>http://example.org/posts/1658310834/</guid><description>在上一篇博客里，我们为.NET Core原生 DI 扩展了基于名称的注入功能。而今天，我们要来聊一聊属性注入。关于属性注入，历来争议不断，支持派认为，构造函数注入会让构造函数变得冗余，其立意点主要在代码的可读性。而反对派则认为，属性注入会让组件间的依赖关系变得模糊，其立意点主要在代码是否利于测试。我认识的一位前辈更是留下一句话：只要构造函数中超过 5 个以上的参数，我就觉得无法忍受。我个人是支持派，因为我写这篇博客的动机，正是一位朋友向我吐槽公司项目，说一个控制器里单单是构造函数里的参数就有十来个。在这其中最大的痛点是，有些在构造函数中注入的类型其实是重复的，譬如ILogger&amp;lt;&amp;gt;、IMapper、IRepository&amp;lt;&amp;gt;以及用户上下文信息等，虽然继承可以让痛苦减轻一点，可随之而来的就是冗长的 base 调用链。博主参与的项目里不乏有大量使用静态类、静态方法的，譬如 LogEx、UserContext 等等，可这种实践显然与依赖注入的思想背道而驰，为吾所不取也，这就是这篇博客产生的背景啦！
好了，当视角正式切入属性注入的时候，我们不妨先来考虑这样一件事情，即：当我们从容器里 Resolve 一个特定的类型的时候，这个实例到底是怎么被创建出来的呢？这个问题如果给到三年前的我，我会不假思索的说出两个字——反射。的确，这是最简单的一种实现方式，换句话说，首先，容器收集构造函数中的类型信息，并根据这些类型信息 Resolve 对应的实例；其次，这些实例最终会被放到一个object[]里，并作为参数传递给Activator.CreateInstance()方法。这是一个一般意义上的 Ioc 容器的工作机制。那么，相对应地，关于属性注入，我们可以认为容器 Reslove 一个特定类型的时候，这个类型提供了一个空的构造函数(这一点非常重要)，再创建完实例以后，再去 Reslove 这个类型中的字段或者是属性。所以，为了在微软自带的 DI 上实现属性注入，我们就必须实现自己的 ServiceProvider——AutowiredServiceProvider，这个 ServiceProvider 相比默认的 ServiceProvider 多了一部分功能，即反射属性或者字段的过程。一旦想通这一点，我们可以考虑装饰器模式。
public class AutowiredServiceProvider : IServiceProvider, ISupportRequiredService { private readonly IServiceProvider _serviceProvider; public AutowiredServiceProvider (IServiceProvider serviceProvider) { _serviceProvider = serviceProvider; } public object GetRequiredService (Type serviceType) { return GetService (serviceType); } public object GetService (Type serviceType) { var instance = _serviceProvider.GetService (serviceType); Autowried (instance); return instance; } private void Autowried (object instance) { if (_serviceProvider == null || instance == null) return; var flags = BindingFlags.</description></item><item><title>.NET Core 原生 DI 扩展之基于名称的注入实现</title><link>http://example.org/posts/1734098504/</link><pubDate>Wed, 10 Jun 2020 13:08:03 +0000</pubDate><guid>http://example.org/posts/1734098504/</guid><description>接触 .NET Core 有一段时间了，最大的感受无外乎无所不在的依赖注入，以及抽象化程度更高的全新框架设计。想起三年前 Peter 大神手写 IoC 容器时的惊艳，此时此刻，也许会有不一样的体会。的确，那个基于字典实现的 IoC 容器相当“简陋”，就像 .NET Core 里的依赖注入，默认(原生)都是采用构造函数注入的方式，可其实从整个依赖注入的理论上而言，属性注入和方法注入的方式，同样是依赖注入的实现方式啊。最近一位朋友找我讨论，.NET Core 里该如何实现 Autowried，这位朋友本身是 Java 出身，一番攀谈了解到原来是指属性注入啊。所以，我打算用两篇博客来聊聊 .NET Core 中的原生 DI 的扩展，而今天这篇，则单讲基于名称的注入的实现。
Autofac是一个非常不错的 IoC 容器，通常我们会使用它来替换微软内置的 IoC 容器。为什么要这样做呢？其实，微软在其官方文档中早已给出了说明，即微软内置的 IoC 容器实际上是不支持以下特性的： 属性注入、基于名称的注入、子容器、自定义生存期管理、对迟缓初始化的 Func 支持、基于约定的注册。这是我们为什么要替换微软内置的 IoC 容器的原因，除了 Autofac 以外，我们还可以考虑 Unity 、Castle 等容器，对我个人而言，其实最需要的一个功能是“扫描”，即它可以针对程序集中的组件或者服务进行自动注册。这个功能可以让人写起代码更省心一点，果然，人类的本质就是让自己变得更加懒惰呢。好了，话题拉回到本文主题，我们为什么需要基于名称的注入呢？它其实针对的是“同一个接口对应多种不同的实现”这种场景。
OK ，假设我们现在有一个接口 ISayHello，它对外提供一个方法 SayHello：
public interface ISayHello { string SayHello(string receiver); } 相对应地，我们有两个实现类，ChineseSayHello 和 EnglishSayHello：
//ChineseSayHello public class ChineseSayHello : ISayHello { public string SayHello(string receiver) { return $&amp;#34;你好，{receiver}&amp;#34;; } } //EnglishSayHello public class EnglishSayHello : ISayHello { public string SayHello(string receiver) { return $&amp;#34;Hello，{receiver}&amp;#34;; } } 接下来，一顿操作猛如虎：</description></item><item><title>.NET Core + ELK 搭建可视化日志分析平台(上)</title><link>http://example.org/posts/3687594958/</link><pubDate>Sat, 15 Feb 2020 16:01:13 +0000</pubDate><guid>http://example.org/posts/3687594958/</guid><description>Hi，各位朋友，大家好！欢迎大家关注我的博客，我的博客地址是: https://blog.yuanpei.me。今天是远程办公以来的第一个周末，虽然公司计划在远程两周后恢复正常办公，可面对着每天都有人离开的疫情，深知这一切都不会那么容易。窗外的阳光透过玻璃照射进屋子，这一切都昭示着春天的脚步渐渐近了。可春天来了，有的人却没有再回来。那些在 2019 年结束时许下的美好期待、豪言壮语，在这样一场灾难面前，终究是如此的无力而苍白。可不管怎么样，生活还是要继续，在这些无法出门的日子里，在这样一个印象深刻的春节长假里，除了做好勤洗手、多通风、戴口罩这些防疫保护措施以外，博主还是希望大家能够抽空学习，通过知识来充实这“枯燥&amp;quot;的生活。所以，从今天开始，我将为大家带来 .NET Core + ELK 搭建可视化日志分析平台 系列文章，希望大家喜欢。
什么是 ELK 当接触到一个新的事物的时候，我们最好是从它的概念开始入手。那么，什么是 ELK 呢？ELK，是 Elastaicsearch 、 Logstash 和 Kibana 三款软件的简称。其中，Elastaicsearch 是一个开源的全文搜索引擎。如果你没有听说过它，那至少应该听说过 Lucene 这个开源搜索引擎。事实上，Elastaicsearch 是 Lucene 的封装，它提供了 REST API 的操作接口 。而 Logstash 则是一个开源的数据收集引擎，具有实时的管道，它可以动态地将不同的数据源的数据统一起来。最后，Kibana 是一个日志可视化分析的平台，它提供了一系列日志分析的 Web 接口，可以使用它对日志进行高效地搜索、分析和可视化操作。至此，我们可以给 ELK 一个简单的定义：
ELK 是一个集日志收集、搜索、日志聚合和日志分析于一身的完整解决方案。
下面这张图，展示了 Elastaicsearch 、 Logstash 和 Kibana 三款软件间的协作关系。可以注意到，Logstash 负责从应用服务器收集日志。我们知道，现在的应用程序都是跨端应用，程序可能运行在 PC、移动端、H5、小程序等等各种各样的终端上，而 Logstash 则可以将这些不同的日志信息通过管道转换为统一的数据接口。这些日志将被存储到 Elasticsearch 中。我们提到 Elastaicsearch 是一个开源的全文搜索引擎，故而它在数据查询上相对传统的数据库有着更好的优势，并且 Elasticsearch 可以根据需要搭建单机或者集群。最终，Kibana 从 Elasticsearch 中查询数据并绘制可视化图表，并展示在浏览器中。在最新的 ELK 架构中，新增了FireBeat这个软件，它是它是一个轻量级的日志收集处理工具(Agent)，适合于在各个服务器上搜集日志后传输给 Logstash。
ELK-01.png总而言之，ELK 可以让我们以一种更优雅的方式来收集日志，传统的日志收集通常会把日志写到文件或者数据库中。前者，不利于日志的集中管理和查询；后者，则无法应对海量文本检索的需求。所以，使用 ELK 可以为我们带来下面这些便利：分布式日志数据集中式查询和管理；系统监控，譬如对系统硬件和应用各个组件的监控；故障排查；报表功能；日志查询，问题排查，上线检查； 服务器监控、应用监控、错误报警；性能分析、用户行为分析、时间管理等等。
如何安装 ELK 安装 ELK 的方式，首推以 Docker 方式安装。关于 Docker 的安装、使用请大家查阅官方文档：https://docs.</description></item><item><title>从 .NET Core 2.2 升级到 3.1 的踩坑之旅</title><link>http://example.org/posts/3099575458/</link><pubDate>Wed, 22 Jan 2020 10:23:08 +0000</pubDate><guid>http://example.org/posts/3099575458/</guid><description>有时候，版本更新太快并不是一件好事。虽然，两周一个迭代的“敏捷”开发依然被客户嫌弃交付缓慢，可一边是前端领域“求不要再更新了，学不动了”的声音，一边则是.NET Core从1.x到2.x再到3.x的高歌猛进。版本更新太快，带来的是API的频繁变动，无法形成有效的知识沉淀，就像转眼到了2020年，Python 2.x和Windows 7都引来了“寿终正寝”，可能你都还没有认真地学习过这些知识，突然就被告知这些知识要过期了，想想还是觉得挺疯狂啊。最近一直在捣鼓，如何让.NET Core应用跑在Heroku平台上，因为Docker镜像里使用最新的.NET Core 3.1运行时，所以，痛定思痛之余，决定把手头项目升级到3.1。上一次痛苦还是在2.1升级2.2，这还真没过多长时间。所以呢，这篇博客主要梳理下从2.2升级到3.1过程中遇到的问题。
更新项目文件 调整目标框架为netcoreapp3.1 删除引用项：Microsoft.AspNetCore.App、Microsoft.AspNetCore.Razor.Design 删除AspNetCoreHostingModel，如果项目文件中的值为InProcess(因为ASP.NET Core 3.0 或更高版本项目默认为进程内承载模型） 更新程序入口 CreateWebHostBuilder()方法的返回值类型由IWebHostBuilder调整为IHostBuilder 增加引用项：Microsoft.Extensions.Hosting Kestrel配置变更至ConfigureWebHostDefaults()方法 public static IHostBuilder CreateWebHostBuilder(string[] args) =&amp;gt;Host.CreateDefaultBuilder(args).ConfigureWebHostDefaults(webBuilder =&amp;gt;{webBuilder.ConfigureKestrel(serverOptions =&amp;gt;{// Set properties and call methods on options}).UseStartup&amp;lt;Startup&amp;gt;();}); 如果通过 HostBuilder手动创建宿主，则需要在 ConfigureWebHostDefaults()方法中显式调用·UseKestrel()：
public static void Main (string[] args) {var host = new HostBuilder ().UseContentRoot (Directory.GetCurrentDirectory ()).ConfigureWebHostDefaults (webBuilder =&amp;gt; {webBuilder.UseKestrel (serverOptions =&amp;gt; {// Set properties and call methods on options}).</description></item><item><title>.NET Core POCOController 在动态 Web API 中的应用</title><link>http://example.org/posts/116795088/</link><pubDate>Thu, 01 Aug 2019 16:44:59 +0000</pubDate><guid>http://example.org/posts/116795088/</guid><description>Hi，大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是：https://blog.yuanpei.me。在上一篇文章中，我和大家分享了 ASP.NET 中动态 Web API 的实现，这种方案的现实意义是，它可以让我们把任意一个接口转换为 Web API，所以，不单单局限在文章里提到的 WCF 迁移到 Web API，任意领域驱动开发(DDD)中的服务层，甚至是更为普遍的传统三层，都可以通过这种方式快速构建前后端分离的应用。可能大家会觉得直接把 Service 层暴露为 API，会引发一系列关于鉴权、参数设置(FromQuery/FromBody)等等的问题，甚至更极端的想法是，这样和手写的没什么区别，通过中间件反射能达到相同的目的，就像我们每天都在写各种接口，经常有人告诉我们说，不要在 Controller 层写太重的业务逻辑，所以，我们的做法就是不断地在 Service 层里增加新接口，然后再把 Service 层通过 Controller 层暴露出来，这样子真的是对的吗？
可我个人相信，技术总是在不断向前发展的，大家觉得 RESTful 完全够用啦，结果 GraphQL 突然发现了。大家写了这么多年后端，其实一直都在绕着数据转，可如果数据库本身就支持 RESTful 风格的接口，或者是数据库本身就支持某种 ORM，我们后端会立马变得无趣起来。其实，在 ASP.NET Core 中已经提供了这种特性，这就是我们今天要说的 POCOController，所以，这也许是个正确的思路，对吧？为什么 Service 层本身不能就是 Controller 层呢？通过今天这篇文章，或许你会接受这种想法，因为 POCOController，就是弱化 Controller 本身的特殊性，一个 Controller 未必需要继承自 Controller，或者在名称中含有 Controller 相关的字眼，如果 Controller 同普通的类没有区别会怎么样呢？答案就是 Service 层和 Controller 层的界限越来越模糊。扪心自问，我们真的需要中间这一层封装吗？
什么是 POCOController POCOController 是 ASP.NET Core 中提供的一个新特性，按照约定大于配置的原则，在 ASP.NET Core 项目中，所有带有 Controller 后缀的类，或者是使用了[Controller]标记的类，即使它没有像模板中一样继承 Controller 类，ASP.NET Core 依然会将其识别为 Controller，并拥有和普通 Controller 一样的功能，说到这里，你是不是有点兴奋了呢，因为我们在 ASP.</description></item><item><title>使用 ASP.NET Core 和 Hangfire 实现 HTTP 异步化方案</title><link>http://example.org/posts/1071063696/</link><pubDate>Thu, 04 Jul 2019 08:56:28 +0000</pubDate><guid>http://example.org/posts/1071063696/</guid><description>Hi，大家好，我是 Payne，欢迎大家一如既往地关注我的博客。今天这篇博客里的故事背景，来自我工作中的一次业务对接，因为客户方提供的是长达上百行的 XML，所以一度让更喜欢使用 JSON 的博主感到沮丧，我这里不是想讨论 XML 和 JSON 彼此的优缺点，而是我不明白 AJAX 里的 X 现在基本都被 JSON 替代了，为什么还有这么多的人坚持使用并友好的 XML 作为数据的交换协议呢？也许你会说，因为有这样或者那样等等的理由，就像 SOA、ESB、SAP 等等类似的技术在企业级用户依然大量流行一样，而这些正是“消费”XML 的主力军。我真正想说的是，在对接这类接口时，我们会遇到一个异步化的 HTTP 协议场景，这里的异步和多线程、async/await 没有直接关系，因为它描述的实际上是业务流程上的一种“异步”。
引子-想对 XML 说不 我们知道，HTTP 协议是一个典型的请求-响应模型，由调用方(Client)调用服务提供者(Server)提供的接口，在理想状态下，后者在处理完请求后会直接返回结果。可是当后者面对的是一个“耗时”任务时，这种方式的问题就立马凸显出来，此时调用者有两个选择：一直等对方返回直至超时(同步)、隔一会儿就看看对方是否处理完了(轮询)。这两种方式，相信大家都非常熟悉了，如果继续延伸下去，我们会联想到长/短轮询、SignalR、WebSocket。其实，更好的方式是，我们接收到一个“耗时”任务时，立即返回表明我们接收了任务，等任务执行完以后再通知调用者，这就是我们今天要说的 HTTP 异步化方案。因为对接过程中，客户采用的就是这种方案，ESB 这类消息总线本身就提供了这种功能，可作为调用方的博主就非常难受啦，因为明明能“同步”地处理完的事情，现在全部要变成“异步”处理，就像一个习惯了 async/await 语法糖的人，突然间就要重新开始写 APM 风格的代码，宝宝心里苦啊，“异步”处理就异步处理嘛，可要按人家要求去返回上百行的 XML，博主表示想死的心都有了好嘛……
好了，吐槽归吐槽，吐槽完我们继续梳理下 HTTP 异步化的方案，这种方式在现实生活中还是相当普遍的，毕竟人类都是“异步”做事，譬如“等你哪天有空一起吃个饭”，测试同事对我说得最多的话就是，“等你这个 Bug 改完了同我说一声”，更不用说，JavaScript 里典型的异步单线程的应用等等……实现“异步”的思路其实是非常多的，比如同样在 JavaScript 里流行的回调函数，比如通过一张中间表存起来，比如推送消息到消息队列里。在面向数据库编程的时候，我听到最多的话就是，没有什么问题是不能用一张中间表来解决的，如果一张不行那就用两张。项目上我是用 Quartz+中间表的方式实现的，因为这是最为普通的方式。这里，我想和大家分享下，关于使用 Hangfire 来实现类似 Quartz 定时任务的相关内容，果然，我这次又做了一次标题党呢，希望大家会对今天的内容感兴趣。简单来说，我们会提供一个接口，调用方提供参数和回调地址，调用后通过 Hangfire 创建后台任务，等任务处理结束后，再通过回调地址返回结果给调用方，这就是所谓的 HTTP 异步化。
开箱即用的 Hangfire 我们项目上是使用 Quartz 来实现后台任务的，因为它采用了反射的方式来调用具体的 Job，因此，它的任务调度和任务实现是耦合在同一个项目里的，常常出现单个 Job 引发整个系统卡顿的情况，尤其是是它的触发器，常常导致一个 Job 停都停不下来，直到后来才渐渐开始通过 Web API 来分离这两个部分。Quartz 几乎没有一个自己的可视化界面，我们为此专门为它开发了一套 UI。我这里要介绍的 Hangfire，可以说它刚好可以作为 Quartz 的替代品，它是一个开箱即用的、轻量级的、开源后台任务系统，想想以前为 Windows 开发定时任务，只能通过定时器(Timer)来实现，尚不知道 CRON 为何物，而且只能用命令行那种拙劣的方式来安装/卸载，我至今都记得，测试同事问我，能不能不要每次都弹个黑窗口出来，这一起想起来还真是让人感慨啊。好了，下面我们开始今天的实践吧！首先，第一步自然是安装 Hangfire 啦，这里我们新建一个 ASP.</description></item><item><title>记通过 EF 生成不同数据库 SQL 脚本的一次尝试</title><link>http://example.org/posts/795474045/</link><pubDate>Mon, 17 Sep 2018 09:42:23 +0000</pubDate><guid>http://example.org/posts/795474045/</guid><description>接触新项目有段时间了，如果让我用一句话来形容此刻的感受，大概就是**“痛并快乐着”。痛苦之一是面对 TFS，因为它的分支管理实在是一言难尽，无时无刻不在体验着人肉合代码的“趣味”。而痛苦之二是同时维护三套数据库的脚本，这让我想到一个梗，在讲到设计模式的时候，一个常常被提到的场景是，怎么样从设计上支持不同数据库的切换。我想，这个问题是非常容易回答的，真正的问题是我们真的需要切换数据库吗？原谅我的年少无知，我们的产品因为要同时支持公有云和私有化部署，所以在数据库的选择上，覆盖到了主流 MySQL、Oracle 和 SQL Server，这直接导致我们要维护三套数据库的脚本，你说这样子能不痛苦吗？而快乐的地方在于，终于有机会在一个有一定用户体量的产品上参与研发，以及从下周开始我们将从 TFS 切换到 Git。好了，今天这篇文章的主题是，通过 EF 来生成不同数据库的 SQL 脚本，这是痛苦中的一次尝试，所谓“痛并快乐着”**。
基本原理 我们知道数据库和面向对象这两者间存在着天然阻抗，这是因为两者在事物的认知上存在差异，数据库关注的是二维表、是集合间的关系，而面向对象关注的是封装、是细节的隐藏，所以，不管到什么时候，这两者都只能以某种尴尬的方式共存，SQL 执行效率高，这是以牺牲可读性为代价的； ORM 迎合了面向对象，这是以牺牲性能为代价的，所以，即使到了今天，关于 SQL 和 ORM 的争论从来没有停止过，甚至写 SQL 的人不知不觉间“造”出了 ORM，而使用 ORM 的人有时需要 SQL。所以，面对这样一个需要同时维护三套数据库脚本的工作，我个人倾向于用工具去生成，或许是出于程序员对“懒”这种美德的极致追求，或许是出于我对 SQL 这种“方言”天生的排斥，总而言之，我不是很喜欢手写 SQL 除非特别必要，因为它和正则一样，只有写得人懂它真正的含义。
那么，说到这里，我们就知道了一件事情，ORM 可以帮助我们生成 SQL，所以，我们为什么不让它帮我们生成不同数据库的 SQL 脚本呢？虽然 ORM 的性能总是为人所诟病，因为它严格遵循某种规则，所以注定做不到像人类一样“灵活”。我们始终认为不“灵活”的就是“笨拙”的，可即便如此 ORM 生成的 SQL 依然比人类写得要好看。故而，我们的思路是，在 ORM 生成 SQL 语句的时候将其记录下来，然后按照一定规则生成不同数据库的脚本。毕竟 SQL 语言更接近“方言”，每一种数据库的 SQL 脚本都存在着细微的差别。所以，后来人们不得不发明 T-SQL，可任何东西归根结底不都是权力和利益带来的附属品吗？人类为了互相竞争而形成差异化，可当一切差异都不甚明显时，最终又不得不花费精力来解决这些差异。可一个只有垄断存在的世界，除了让人想起 1984 里的 Big Brother 以外，还能想起什么呢？
尝试过程 好了，顺着这个思路，我们就会想到在 ORM 中添加拦截器或者是日志的方式，来获得由 ORM 生成的 SQL 语句，这里我们以 Entity Framework(以下简称 EF)为例，这是.NET 中最常见的 ORM，因为目前官方的 Web 开发框架有 ASP.</description></item><item><title>基于 WebSocket 和 Redis 实现 Bilibili 弹幕效果</title><link>http://example.org/posts/3269605707/</link><pubDate>Wed, 22 Aug 2018 14:07:23 +0000</pubDate><guid>http://example.org/posts/3269605707/</guid><description>嗨，大家好，欢迎大家关注我的博客，我是 Payne，我的博客地址是https://qinyuanpei.github.io。在上一篇博客中，我们使用了.NET Core 和 Vue 搭建了一个基于 WebSocket 的聊天室。在今天这篇文章中，我们会继续深入这个话题。博主研究 WebSocket 的初衷是，我们的项目上有需要实时去推送数据来完成图表展示的业务，而博主本人对这个内容比较感兴趣，因为博主有对爬虫抓取的内容进行数据可视化(ECharts)的想法。可遗憾的是，这些数据量都不算太大，因为难以支持实时推送这个想法，当然更遗憾的是，我无法在项目中验证以上脑洞，所以，最终退而求其次，博主打算用 Redis 和 WebSocket 做一个弹幕的 Demo，之所以用 Redis，是因为博主懒到不想折腾 RabbitMQ。的确，这世界上有很多事情都是没有道理的啊……
其实，作为一个业余的数据分析爱好者，我是非常乐意看到炫酷的 ECharts 图表呈现在我的面前的，可当你无法从一个项目中收获到什么的时候，你唯一的选择就是项目以外的地方啦，所以，在今天这样一个精细化分工的时代，即使你没有机会独立地完成一个项目，我依然鼓励大家去了解项目的“上下文”，因为单单了解一个点并不足以了解事物的全貌。好了，下面我们来简单说明下这个 Demo 整体的设计思路，即我们通过 Redis 来“模拟”一个简单的消息队列，客户端发送的弹幕会被推送到消息队列中。当 WebSocket 完成握手以后，我们定时从消息队列中取出弹幕，并推送到所有客户端。当客户端接收到服务端推送的消息后，我们通过 Canvas API 完成对弹幕的绘制，这样就可以实现一个基本的弹幕系统啦！
编写消息推送中间件 首先，我们来实现服务端的消息推送，其基本原理是：在客户端和服务端完成“握手”后，我们循环地从消息队列中取出消息，并将消息群发至每一个客户端，这样就完成了消息的推送。同上一篇文章一样，我们继续基于“中间件”的形式，来编写消息推送相关的服务。这样，两个 WebSocket 服务可以独立运行而不受到相互的干扰，因为我们将采用两个不同的路由。在上一篇文章中，我们给“聊天”中间件 WebSocketChat 配置的路由为**/wsws。这里，我们将“消息推送”中间件 WebSocketPush 配置的路由为/push**。这块儿我们做了简化，不再对所有 WebSocket 的连接状态进行维护，因为对一个弹幕系统而言，它不需要让别人了解某个用户的状态是否发生了变化。所以，这里我们给出关键的代码。
public async Task Invoke(HttpContext context) { if (!IsWebSocket(context)) { await _next.Invoke(context); return; } var webSocket = await context.WebSockets.AcceptWebSocketAsync(); _socketList.Add(webSocket); while (webSocket.State == WebSocketState.Open) { var message = _messageQueue.Pull(&amp;#34;barrage&amp;#34;,TimeSpan.FromMilliseconds(2)); foreach(var socket in _socketList) { await SendMessage(socket,message); } } await webSocket.</description></item><item><title>使用 .NET Core 和 Vue 搭建 WebSocket 聊天室</title><link>http://example.org/posts/1989654282/</link><pubDate>Wed, 01 Aug 2018 15:42:23 +0000</pubDate><guid>http://example.org/posts/1989654282/</guid><description>Hi，大家好，我是Payne，欢迎大家关注我的博客，我的博客地址是：https://qinyuanpei.github.io。今天这篇博客，我们来说说WebSocket。各位可能会疑惑，为什么我会突然间对WebSocket感兴趣，这是因为最近接触到了部分“实时”的业务场景，譬如：用户希望在远程视频通话过程中，实时地监控接入方的通话状态，实时地将接入方的响应时间、通话时长以及接通率等信息推送到后台。与此同时，用户可以通过监控平台看到实时变化着的图表。坦白地讲，这种业务场景陌生吗？不，每一年的双11，都能见到小伙伴们实时地“剁手”。所以，在今天这篇文章中，我们会以WebSocket聊天室为例，来讲解如何基于WebSocket构建实时应用。
WebSocket概述 WebSocket是HTML5标准中的一部分，从Socket这个字眼我们就可以知道，这是一种网络通信协议。WebSocket是为了弥补HTTP协议的不足而产生的，我们知道，HTTP协议有一个重要的缺陷，即：请求只能由客户端发起。这是因为HTTP协议采用了经典的请求-响应模型，这就限制了服务端主动向客户端推送消息的可能。与此同时，HTTP协议是无状态的，这意味着连接在请求得到响应以后就关闭了，所以，每次请求都是独立的、上下文无关的请求。这种单向请求的特点，注定了客户端无法实时地获取服务端的状态变化，如果服务端的状态发生连续地变化，客户端就不得不通过“轮询”的方式来获知这种变化。毫无疑问，轮询的方式不仅效率低下，而且浪费网络资源，在这种背景下，WebSocket应运而生。
WebSocket协议最早于2008年被提出，并于2011年成为国际标准。目前，主流的浏览器都已经提供了对WebSocket的支持。在WebSocket协议中，客户端和服务器之间只需要做一次握手操作，就可以在客户端和服务器之间实现双向通信，所以，WebSocket可以作为**服务器推送**的实现技术之一。因为它本身以HTTP协议为基础，所以对HTTP协议有着更好的兼容性，无论是通信效率还是传输的安全性都能得到保证。WebSocket没有同源限制，客户端可以和任意服务器端进行通信，因此具备通过一个单一连接来支持上下游通信的能力。从本质上来讲，WebSocket是一个在握手阶段使用HTTP协议的TCP/IP协议，换句话说，一旦握手成功，WebSocket就和HTTP协议再无瓜葛，下图展示了它与HTTP协议的区别：
HTTP与WebSocket的区别构建一个聊天室 OK，在对WebSocket有了一个基本的认识以后，接下来，我们以一个最简单的场景来体验下WebSocket。这个场景是什么呢？你已经知道了，答案就是网络聊天室。这是一个非常典型的实时场景。这里我们分为服务端实现和客户端实现，其中：服务端实现自豪地采用.NET Core，而客户端实现采用Vue的双向绑定特性。现在是公元2018年了，当jQuery已成往事，操作DOM这种事情交给框架去做就好，而且我本人很喜欢MVVM这种模式，Vue的渐进式框架，非常适合我这种不会写ES6的伪前端。
.NET Core与中间件 关于.NET Core中对WebSocket的支持，这里主要参考了官方文档，在这篇文档中，演示了一个最基本的Echo示例，即服务端如何接收客户端消息并返回消息给客户端。这里，我们首先需要安装Microsoft.AspNetCore.WebSockets这个库，直接通过Visual Studio Code内置的终端安装即可。接下来，我们需要在Startup类的Configure方法中添加WebSocket中间件：
app.UseWebSockets() 更一般地，我们可以配置以下两个配置，其中，KeepAliveInterval表示向客户端发送Ping帧的时间间隔；ReceiveBufferSize表示接收数据的缓冲区大小：
var webSocketOptions = new WebSocketOptions() { KeepAliveInterval = TimeSpan.FromSeconds(120), ReceiveBufferSize = 4 * 1024 }; app.UseWebSockets(webSocketOptions); 好了，那么怎么接收一个来自客户端的请求呢？这里以官方文档中的示例代码为例来说明。首先，我们需要判断下请求的地址，这是客户端和服务端约定好的地址，默认为**/，这里我们以/ws为例；接下来，我们需要判断当前的请求上下文是否为WebSocket请求，通过context.WebSockets.IsWebSocketRequest来判断。当这两个条件同时满足时，我们就可以通过context.WebSockets.AcceptWebSocketAsync()**方法来得到WebSocket对象，这样就表示“握手”完成，这样我们就可以开始接收或者发送消息啦。
if (context.Request.Path == &amp;#34;/ws&amp;#34;) { if (context.WebSockets.IsWebSocketRequest) { WebSocket webSocket = await context.WebSockets.AcceptWebSocketAsync(); //TODO } }); 一旦建立了Socket连接，客户端和服务端之间就可以开始通信，这是我们从Socket中收获的经验，这个经验同样适用于WebSocket。这里分别给出WebSocket发送和接收消息的实现，并针对代码做简单的分析。
private async Task SendMessage&amp;lt;TEntity&amp;gt;(WebSocket webSocket, TEntity entity) { var Json = JsonConvert.SerializeObject(entity); var bytes = Encoding.UTF8.GetBytes(Json); await webSocket.SendAsync( new ArraySegment&amp;lt;byte&amp;gt;(bytes), WebSocketMessageType.</description></item></channel></rss>