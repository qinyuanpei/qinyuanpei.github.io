<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Python on 元视角</title><link>https://qinyuanpei.github.io/tags/python/</link><description>Recent content in Python on 元视角</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 29 May 2023 20:49:47 +0000</lastBuildDate><atom:link href="https://qinyuanpei.github.io/tags/python/index.xml" rel="self" type="application/rss+xml"/><item><title>温故而知新，再话 Python 动态导入</title><link>https://qinyuanpei.github.io/posts/discussing-dynamic-import-in-python-again/</link><pubDate>Mon, 29 May 2023 20:49:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/discussing-dynamic-import-in-python-again/</guid><description>多年前，我曾写过一篇关于 Python 动态导入的文章，当时想要解决的问题是，如何通过动态导入 Python 脚本来实现插件机制，即整个应用程序由主程序和插件两部分组成，主程序通过 importlib 模块中的 import_module 方法动态地导入一个 Python 脚本，最终通过 getattr、setattr 等方法实现反射调用。时过境迁，代码还是那些代码，江湖故人早已不知所踪。我向来都是一个喜欢怀旧的人，我怀念的是那些遗忘在寒江孤影里的江湖故人，我怀念的是那些湮灭在时光尘埃里的代码片段。或许，在屏幕前的你看来，一个每天都在经历着“更新换代”的技术人员，更应该对这一切的消逝习以为常。可正如这世界上的风、沙、星辰等流动的事物一样，无论我们愿意与否，时间总会在不经意间将那些熟悉而珍贵的东西一一带走，不放弃对过去的回忆和珍视，这便是我在世事变幻的洪流中追求的安宁与平静。正所谓“温故而知新”，今天我想要怀旧的话题是 Python 里的动态导入。
众所周知，这段时间我一直在开发基于 ChatGPT 的人工智能管家 Jarvis，在整个探索过程中，类似语音识别、语音合成这些领域，博主先后考察了微软、百度、腾讯&amp;hellip;这些大厂的方案，这可以说是非常符合我作为 Python “调包侠” 的人设啦！以语音识别为例，最终，你可能会得到类似下面这样的代码：
class ASREngineFactory: @staticmethod def create(config, type): if type == ASREngineProvider.Baidu: return BaiduASR(config[&amp;#39;BAIDU_APP_ID&amp;#39;], config[&amp;#39;BAIDU_API_KEY&amp;#39;], config[&amp;#39;BAIDU_SECRET_KEY&amp;#39;]) elif type == ASREngineProvider.PaddleSpeech: return PaddleSpeechASR() elif type == ASREngineProvider.OpenAIWhisper: return WhisperASR() 没错，非常经典的简单工厂模式，你只需要告诉工厂类，你需要使用哪种语音识别引擎，它就可以自动帮你创建出对应的示例，如下图所示，这看起来非常合理，对吧？
config = load_config_from_env(env_file) engine = ASREngineFactory.create(config, ASREngineProvider.PaddleSpeech) 这里，其实有一段小插曲，博主最近开始尝试使用 virtualenv 来管理不同的 Python 版本，这样做的好处是，我只需要在不同的工作场所拉取代码、激活环境，就可以享受到完全一样的开发环境。当然，这一切都只是理论上的，实际使用下来的感受是，它并不能完全抹平环境上的差异。譬如，当我试图在个人电脑上安装 PaddleSpeech 和 Rasa 这两个库时，依然免不了遇到各种错误，即使是在同一个 Python 环境下。
此时，你会发现一个非常尴尬的问题，即使我不使用 PaddleSpeech 来作为 Jarvis 的语音识别引擎，它依然无法正常工作，原因是我环境中没有安装 PaddleSpeech，我不得不注释掉项目中所有和 PaddleSpeech 有关的代码，而这一切的根源其实是，我们在代码中使用了静态导入的方式，如下图所示：</description></item><item><title>视频是不能 P 的系列：使用 Milvus 实现海量人脸快速检索</title><link>https://qinyuanpei.github.io/posts/use-milvus-to-quickly-retrieve-massive-faces/</link><pubDate>Mon, 24 Apr 2023 20:49:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/use-milvus-to-quickly-retrieve-massive-faces/</guid><description>最近我一直在优化一个人脸识别项目，这个过程令我深感科学的尽头永远都是殊途同归。一年前，我使用 dlib 实现人脸识别时遇到了两个悬而未决的问题：一是因为人脸样本数目增加导致性能下降问题；二是如何快速地判断目标人脸是否在人脸样本中。然而，在经过虹软人脸识别 SDK 的折磨后，我意识到这两个问题实际上从未消失。它们总会在某个合适的时机突然跳出来，然后开始无声无息地敲打你的灵魂。果然，“出来混还是要还的”。现在重新审视这两个问题，我认为，它们本质上是1：1 和 1：N 的问题。在使用虹软人脸识别 SDK 的过程中，我遇到了一个非常棘手的难题，即：当目标人脸在人脸数据库中时，识别过程非常流畅；可当目标人脸不在人脸数据库中时，识别过程就异常卡顿。结合使用 dlib 做人脸识别的经验，我猜测魁祸首可能是频繁的特征对比。相比于输出一个枯燥的结论，我更喜欢梳理解决问题的思路。因此，这篇博客的主题是，利用 Milvus 实现海量人脸快速检索的实现过程。
从人脸识别到向量 故事应该从哪里讲起呢？我想，可以从人脸数据库这个角度来切入。当我们把人脸特征存储到 CSV 或者数据库中时，本质上是将 1:N 问题转化为 1:1 问题。因此，我们不得不遍历人脸数据库的每个样本，然后选取与目标人脸最相似或最匹配的那个。这意味着，人脸识别的效率将受到到样本数量和相似度/距离计算方法等因素的影响。以虹软人脸识别 SDK 为例，其免费版提供了 1:1 人脸特征对比的接口，付费版提供了 1:N 人脸特征对比的接口。当然，据热心网友透露，官方这个 1:N 其实还是通过 1:1 循环来实现的。可即便如此，在相同的时间复杂度下，想要写好这样一个循环，这件事情本身并不容易。所以，影响人脸识别效率的因素里，还应该考虑到人的因素。在这个硬件性能过剩的时代，“锱铢必较”大抵会成为一种难能可贵的品质。谁能想到，如今训练模型的门槛变成了一块显卡呢？
通过 one-hot 编码实现的文本向量化表示示意图 如果我们从另一个角度思考这个问题，就会发现向量作为全新的数据类型，是所有这些问题的根源。无论是通过 CSV 还是关系型数据库进行数据处理，对向量数据进行过滤和筛选都是不可直接实现的。这迫使我们需要在内存中加载所有的人脸特征数据，再通过逐个计算和对比的方式来查找目标数据。当目标人脸在数据库中不存在时，这项工作就会变得困难和耗时。这实际上代表着数据从结构化到非结构化的转变趋势。例如，在 NLP 领域，计算文本相似度的理论依据就是向量的余弦公式。而在最近最火热的 ChatGPT 中，Embeddings 模型同样是基于文本的向量化表示。如果你有学习过机器学习的相关知识，就会更加深刻地认识到向量的重要性。正如刘慈欣在《三体》中所描述的那样，高维文明可以对低维文明实施降维打击。如果我们把向量看作是一种将高维度信息压缩为低维度信息的技术，那么，时下这场 AI 革命是不是可以同样视为降维打击呢？试想一下，那些如同咒语一般的提示词(Prompt)背后，不正是由无数个超出人类认知范围的多维向量在参与着复杂计算吗？
Milvus 向量数据库 正如我们所看到的，AIGC 改变了我们对这个世界的编程方式，即从 DSL/GPL 逐步地转向自然语言。在 OpenAI 的 GPT4 以及百度的文心一言中，我们会注意到这些大语言模型(LLM)开始支持图片。也许，以后还会支持音频、视频、文件……等等不同的形式，而这其实就是我们经常听到“多模态”的概念。可以预见的是，未来会有更多的非结构化数据加入其中，传统的关系型数据库将不再适合 AI 时代。譬如，最为典型的“以图搜图”功能，传统的模糊查询已经无法满足复杂的匹配需求。从这个角度来说，向量数据库将会是未来 AI 应用不可或缺的基础设施，就像此刻的关系型数据库对于 CRUD 一样重要。目前，向量数据库主要有 Facebook 的 Faiss、Pinecone、Vespa、国内创业公司 Zilliz 的 Milvus，以及京东的 Varch 等等，笔者这里以 Milvus 为例来展示向量数据库的核心功能——相似度检索。</description></item><item><title>视频是不能 P 的系列：使用 Dlib 实现人脸识别</title><link>https://qinyuanpei.github.io/posts/dlib-face-recognition-with-machine-learning/</link><pubDate>Tue, 01 Nov 2022 22:49:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/dlib-face-recognition-with-machine-learning/</guid><description>本文是 #视频是不能 P 的系列# 的第三篇。此前，我们已经可以通过 OpenCV 或者 Dlib 实现对人脸的检测，并在此基础上实现了某种相对有趣的应用。譬如，利用人脸特征点提取面部轮廓并生成表情包、将图片中的人脸批量替换为精神污染神烦狗 等等。当然，在真实的应用场景中，如果只是检测到人脸，那显然远远不够的，我们更希望识别出这张人脸是谁。此时，我们的思绪将会被再次拉回到人脸识别这个话题。在探索未知世界的过程中，博主发现 OpenCV 自带的 LBPH 方法，即局部二值模式直方图方法，识别精度完全达不到预期效果。所以，博主最终选择了 Dlib 里的特征值方法，即：对每一张人脸计算一个 128 维的向量，再通过计算两个向量间的欧式距离来判断是不是同一张人脸。在此基础上，博主尝试结合 支持向量机 来实现模型训练。因此，这篇文章其实是对整个探索过程的梳理和记录，希望能给大家带来一点启发。
原理说明 如下图所示，假设对于每一个人物 X ，我们有 N 个人脸样本，通过 Dlib 提供的 compute_face_descriptor() 方法，我们可以计算出该人脸样本的特征值，这是一个 128 维度的向量。如果我们对这些人脸样本做同样的处理，我们就可以得到人物 X 的特征值列表 feature_list_of_person_x。在此基础上，利用 MumPy 中的 mean() 方法，我们就可以计算出人物 X 的平均特征 features_mean_person_x。最终，我们把人物 X 的平均特征和名称一起写入到一个 CSV 文件里面。至此，我们已经拥有了一个简单的人脸数据库。
Dlib 人脸识别原理说明图可以预见的是，一旦我们把人脸特征数值化，那么，人脸识别就从一个图形学问题变成了数学问题。对于图中的待检测人脸，我们只需要按同样地方式计算出特征值，然后从 CSV 文件中找一个距离它最近的特征即可。这里，博主使用的是欧式距离，并且人为规定了一个阈值 0.4， 即：当这个距离小于 0.4 时，我们认为人脸匹配成功；当这个距离大于 0.4 时，我们认为人脸匹配失败。下面的例子展示了使用 Dlib 计算人脸特征值的基本过程：
detector = dlib.get_frontal_face_detector() predictor = dlib.shape_predictor(&amp;#39;shape_predictor_68_face_landmarks.dat&amp;#39;) face_reco_model = dlib.face_recognition_model_v1(&amp;#34;dlib_face_recognition_resnet_model_v1.dat&amp;#34;) # 计算特征值 image = Image.</description></item><item><title>视频是不能 P 的系列：OpenCV 和 Dlib 实现表情包</title><link>https://qinyuanpei.github.io/posts/make-memes-with-opencv-and-dlib/</link><pubDate>Fri, 01 Jul 2022 22:49:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/make-memes-with-opencv-and-dlib/</guid><description>2020 年年底的时候，博主曾心血来潮地开启过一个系列：视频是不能 P 的，其灵感则是来源于互联网上的一个梗，即：视频不能 P 所以是真的。不过，在一个美颜盛行的时代，辨别真伪实在是一件奢侈的事情，在各种深度学习框架光环的加持下，在视频中实现“改头换面”已然不再是新鲜事儿，AI 换脸风靡一时的背后，带来是关乎隐私和伦理的一系列问题，你越来越难以确认，屏幕对面的那个到底是不是真实的人类。古典小说《红楼梦》里的太虚幻境，其牌坊上有幅对联写道，“假作真时真亦假，无为有处有还无”。果然，在这个亦真亦幻的世界里，哪里还有什么东西是不能 PS 的呢？在“鸽”了很久很久之后，博主决定要来更新这个系列啦，让我们继续以 OpenCV 作为起点，来探索那些好玩、有趣的视频/图像处理思路，这一次呢，我们来聊聊 OpenCV、Dlib 和 表情包，希望寓教于乐的方式能让大家感受到编程的快乐！
环境准备 python -m pip install opencv-python python -m pip install opencv-contrib-python python -m pip install Pillow python -m pip install numpy python -m pip install imutils python -m pip install dlib 请注意，如果通过 pip 安装 dlib 不大顺利，你可以到 https://github.com/sachadee/Dlib 这个仓库中下载对应的 .whl 文件。例如，博主使用的是 64 位的 Windows 系统，而我的 Python 版本是 3.7，因此，我下载的是 dlib-19.22.99-cp37-cp37m-win_amd64.whl 这个文件。此时，我们可以用下面的方式来安装 dlib：
python -m pip install dlib-19.22.99-cp37-cp37m-win_amd64.whl 除此以外，我们还需要下载 dlib 所需的模型文件，下载地址为：http://dlib.</description></item><item><title>Python 图像风格化迁移助力画家梦想</title><link>https://qinyuanpei.github.io/posts/a-introduction-to-stylized-migration-of-python/</link><pubDate>Sun, 01 May 2022 13:32:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/a-introduction-to-stylized-migration-of-python/</guid><description>很多年前，星爷在《食神》这部电影里大彻大悟，「只要用心，人人都是食神」。从那个时候起，这句话就隐隐约约带着返璞归真、回归本心的意思。如同电影里描绘的餐饮行业一样，在资本市场的裹挟下，造神这项运动显得轻而易举，这个食神可以是史蒂·周，可以是唐牛，可以是任何人。因此，当穷困潦倒的史蒂芬·周，因为一碗叉烧饭而落泪的时候，我想，这或许是一种直面自我的顿悟。毕竟，电影里的星爷原本就不会做饭。《舌尖上的中国》带火了一句话，“高端的食材，往往只需要最简单的烹饪”，在我看来，这同样是一种“人人都是食神”的自我暗示。多年以后，互联网行业炙手可热的彼时彼刻，一句“人人都是产品经理”让无数人发现，提需求的门槛居然如此的低。其实，早在 1967 年，德国艺术家约瑟夫·博伊斯就曾语出惊人，“人人都是艺术家”，联想到“鸡娃”教育下的各种艺术特长培训班，这句话大概是真的。你内心深处是否同样保留着某种艺术家的梦呢？那么，此时此刻，博主想和大家分享的话题是图像的风格化迁移。
走近风格化迁移 提到风格化迁移这个概念的时候，大家可能会感到陌生，所以，我们不妨用相近的概念来进行类比。纵观人类的历史长河，初唐四杰、唐宋八大家的诗文各有千秋，李杜诗篇、苏辛长短句各领风骚，更不必说书法上的颜筋柳骨、苏黄米蔡。我曾经在碑林博物馆密密麻麻的石碑中，近距离看到人们如何将石碑上的文字拓下，我开始在脑海里徜徉，是否人类一切伟大的创造都是起源于模仿？这种思绪最终在艾伦·图灵的传记电影 《模仿游戏》 中找到了某种回应，就像人工智能领域里的神经网络，其实就是在模仿人类的大脑进行思考，甚至退一万步讲，当我们还是一个婴儿的时候，襁褓中的牙牙学语、蹒跚学步，这其实还是一种模仿。那么，如果要给风格化迁移下一个定义的话，其实就是让人工智能来对某种风格或者特点进行“模仿”，以图像的风格化迁移为例，它可以将梵高、莫奈或者毕加索的绘画风格“移植”到一张目标图片上，如下图所示：
Neural-Style-Transformer 示意图它可以借由梵高《星空》这副作品中的色彩，来「绘制」一副不一样的向日葵，虽然，梵高一生中创作了无数幅向日葵，在他人生的不同阶段，或表达对生命的渴望，或刻画出死亡的压抑。由此可见，风格化迁移其实可以理解为，不同流派绘画风格的一种“模仿”。当然，这一切都是由计算机通过特定的算法来实现，你可以想象一下，当你通过描摹字帖的方式来练字时，本质上就是在模仿那些书法家们的笔划，而如果将一切的行为都转化为数学公式，这其实就是一种风格化迁移啦！
卷积神经网络(CNN)在图像风格化迁移上的应用目前，图像的风格化迁移，主要的算法支撑来自下面这两篇文章：
A Neural Algorithm of Artistic Style Instance Normalization: The Missing Ingredient for Fast Stylization 其中，前者提出“用神经网络来解决图像风格化迁移”的思路，而后者则是在此基础上引入了“可感知的损失”这一概念，如果大家有兴趣的话，不妨读一读下面这篇文章，它更像是一篇综述性质的文章，可以帮助你快速了解图像风格化迁移的前世今生，个人感觉，读这类文章会让你快速地认识到自己的无知，这或许是一件好事。
Neural Style Transfer: A Review 坦白讲，博主是第一次接触神经网络。所以，要学习陶渊明，「好读书，不求甚解」。如果大家确实对这块内容感兴趣的话，还是建议亲自去读一下这些文章，我就不在这里班门弄斧啦！(逃
体验风格化迁移 好了，当我们对图像风格化迁移有了一定的了解以后，下面我们来快速体验下图像风格化迁移。OpenCV 在 3.3 版本后，正式引入了 DNN ，这使得我们可以在 OpenCV 中使用 Caffe、TensorFlow、Torch/PyTorch 等主流框架中训练好的模型。这里，我们主要参考了 OpenCV 官方的 示例代码:
def style_transfer(pathIn=&amp;#39;&amp;#39;, pathOut=&amp;#39;&amp;#39;, model=&amp;#39;&amp;#39;, width=None, jpg_quality=80): &amp;#39;&amp;#39;&amp;#39; pathIn: 原始图片的路径 pathOut: 风格化图片的路径 model: 预训练模型的路径 width: 设置风格化图片的宽度，默认为None, 即原始图片尺寸 jpg_quality: 0-100，设置输出图片的质量，默认80，越大图片质量越好 &amp;#39;&amp;#39;&amp;#39; ## 读入原始图片，同时调整图片至所需尺寸 img = cv2.</description></item><item><title>通过 Python 预测 2021 年双十一交易额</title><link>https://qinyuanpei.github.io/posts/735074641/</link><pubDate>Tue, 26 Oct 2021 16:10:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/735074641/</guid><description>突然间，十月以某种始料未及的方式结束了，也许是因为今年雨水变多的缘故，总觉得这个秋天过去得平平无奇，仿佛只有观音禅寺的满地银杏叶儿，真正地宣布着秋天的到来，直到看见朋友在朋友圈里借景抒怀，『 霜叶红于二月花 』，秋天终于没能迁就我的一厢情愿，我确信她真的来了。当然，秋天不单单会带来这些诗情画意的东西，更多的时候我们听到的是双十一、双十二，这些曾经由光棍节而催生出的营销活动，在过去的十多年间渐渐成为了一种文化现象，虽然我们的法定节日永远都只有那么几天，可这并不妨碍我们自己创造出无数的节日，从那一刻开始，每个节日都可以和购物产生联系，这种社会氛围让我们有了某种仪式感，比如，零点时为了抢购商品恨不得戳破屏幕。再比如，在复杂的满减、红包、优惠券算法中复习数学知识。可当时间节点来到 1202 年，你是否依然对剁手这件事情乐此不疲呢，在新一轮剁手行动开始前，让我们来试试通过 Python 预测一下今年的交易额，因为在这场狂欢过后，没有人会关心你买了什么，而那个朴实无华的数字，看起来总比真实的人类要生动得多。
思路说明 其实，我一直觉得这个东西，完全不需要特意写一篇文章，因为用毕导的话说，这个东西我们在小学二年级就学过。相信只要我说出 最小二乘法 和 线性回归 这样两个关键词，各位就知道我在说什么了！博主从网上收集了从 2009 年至今历年双十一的交易额数据，如果我们将其绘制在二维坐标系内，就会得到一张散点图，而我们要做的事情，就是找到一条曲线或者方程，来对这些散点进行拟合，一旦我们确定了这样一条曲线或者方程，我们就可以预测某一年双十一的交易额。如图所示，是 2009 年至今历年双十一的交易额数据，在 Excel 中我们可以非常容易地得到对应的散点图：
在 Excel 中绘制散点图如果有朋友做过化学或者生物实验，对接下来的事情应该不会感到陌生，通常我们会在这类图表中添加趋势线，由此得到一个公式，实际上这就是一个回归或者说拟合的过程，因为 Excel 内置了线性、指数型、对数型等多种曲线模型，所以，我们可以非常容易地切换到不同的曲线，而评估一个方程好坏与否的指标为 $R^2$，该值越接近 1 表示拟合效果越好，如图是博主在 Excel 中得到的一条拟合方程：
在 Excel 中添加趋势线那么，在 Python 中我们如何实现类似的效果呢？答案是 scikit-learn，这是 Python 中一个常用的机器学习算法库，主要覆盖了以下功能：分类、回归、聚类、数据降维、模型选择 和 数据预处理，我们这里主要利用了回归这部分的 LinearRegression 类，顾名思义，它就是我们通常说的线性回归。事实上，这个线性并不是单指一元一次方程，因为我们还可以使用二次或者三次多项式，因为上面的 Excel 图表早已告诉我们，一元一次方程误差太大。
实现过程 OK，具体是如何实现的呢？首先，我们从 CSV 文件中加载数据，这个非常简单，利用 Pandas 库中的 read_csv() 方法即可：
df = pd.read_csv(&amp;#39;./历年双十一交易额.csv&amp;#39;, index_col = 0) 接下来，我们使用下面的方法来获取 年份 和 交易额 这两列数据：
year = np.array(df.index.tolist()) sales = np.array(df.Sales.tolist()) 此时，我们可以非常容易地用 matplotlib 库绘制出对应的图表：</description></item><item><title>使用 Python 自动识别防疫健康码</title><link>https://qinyuanpei.github.io/posts/1509692610/</link><pubDate>Thu, 19 Aug 2021 14:13:32 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/1509692610/</guid><description>这个月月初的时候，朋友兴奋地和我描述着他的计划——准备带孩子到宁夏自驾游。朋友感慨道，“小孩只在书本上见过黄河、见过沙漠，这样的人生多少有一点遗憾”，可正如新冠病毒会变异为德尔塔一样，生活里唯一不变的变化本身，局部地区疫情卷土重来，朋友为了孩子的健康着想，不得不取消这次计划，因为他原本就想去宁夏看看的。回想过去这一年多，口罩和二维码，是每天打交道最多的东西。也许，这会成为未来几年里的常态。在西安，不管是坐公交还是地铁，都会有人去检查防疫二维码，甚至由此而创造了不少的工作岗位。每次看到那些年轻人，我都有种失落感，因为二十九岁高龄的我，已然不那么年轻了，而这些比我更努力读书、学历更高的年轻人，看起来在做着和学历/知识并不相称的工作。也许，自卑的应该是我，因为国家刚刚给程序员群体定性——新生代农民工。可是，我这个农民工，今天想做一点和学历/知识相称的事情，利用 Python 来自动识别防疫二维码。
原理说明 对于防疫二维码而言，靠肉眼去看的话，其实主要关注两个颜色，即标识健康状态的颜色和标识疫苗注射状态的颜色。与此同时，为了追踪人的地理位置变化，防疫/安检人员还会关注地理位置信息，因此，如果要自动识别防疫二维码，核心就是读出其中的颜色以及文字信息。对于颜色的识别，我们可以利用 OpenCV 中的 inRange() 函数来实现，只要我们定义好对应颜色的 HSV 区间即可；对于文字的识别，我们可以利用 PaddleOCR 库来进行提取。基于以上原理，我们会通过 OpenCV 来处理摄像头的图像，只要我们将手机二维码对准摄像头，即可以完成防疫二维码的自动识别功能。考虑到检测不到二维码或者颜色识别不到这类问题，程序中增加了蜂鸣报警的功能。写作本文的原因，单纯是我觉得这样好玩，我无意借此来让人们失业。可生而为人，说到底不能像机器一样活着，大家不都追求有趣的灵魂吗？下面是本文中使用到的第三方 Python 库的清单：
pyzbar == 0.1.8 opencv-contrib-python == 4.4.0.46 opencv-python == 4.5.3.56 paddleocr == 2.2.0.2 paddlepaddle == 2.0.0 图块检测 下面是一张从手机上截取的防疫二维码图片，从这张图片中我们看出，整个防疫二维码，可以分为三个部分，即：上方的定位信息图块，中间的二维码信息图块，以及下方的核酸检验信息图块。
“西安一码通” 防疫二维码对于二维码的检测，我们可以直接使用 pyzbar 这个库来解析，可如果直接对整张图进行解析，因为其中的干扰项实在太多，偶尔会出现明明有二维码，结果无法进行解析的情况。所以，我们可以考虑对图片进行切分，而切分的依据就是图中的这三个图块。这里，我们利用二值化函数 threshold() 和 轮廓提取函数 findContours() 来实现图块的检测：
# 灰度化 &amp;amp; 二值化 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) _, binary = cv2.threshold(gray, 135, 255, cv2.THRESH_BINARY) # 检测轮廓，获得对应的矩形 contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2] for i in range(len(contours)): block_rect = cv2.</description></item><item><title>通过 Python 分析 2020 年全年微博热搜数据</title><link>https://qinyuanpei.github.io/posts/2758545080/</link><pubDate>Sun, 24 Jan 2021 22:36:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/2758545080/</guid><description>几天前， Catcher Wong 大佬告诉我，他终于写完了 2020 年的年终总结。在看完大佬的年终总结以后，我有一种“前浪被后浪拍死在沙滩上”的感觉，正如当学生时都看“别人家的孩子”，工作以后看的都是“别人的年终总结”。我们的生活，其实就是由“别人”和“我们”交织在一起，而更多的时候，是成为“大多数”的“我们”，去关注成为“少数”的“别人”。我想说的是，世间万物互为装饰，就像卞之琳在《断章》里写道，“明月装饰了你的窗子，你装饰了别人的梦”。即便一个人在历史长河中，尤如一叶飘泊不定的孤舟在波涛中摇荡，可每一朵浪花都曾以自己的方式美丽过，所以，看“别人”的生活，联想“我们”的生活，这便是我同 2020 告别的一种方式，为此，博主决定抓取 2020 年全年 366 天的微博热搜，通过可视化的方式来串联起 2020 年的回忆。
热搜抓取 首先，我们来考虑微博热搜的数据来源。 微博 官方提供了一个热搜排行榜的页面：https://s.weibo.com/top/summary，可惜这个网站只支持查看当天的热搜，显然这无法满足我们的需求。在搜索引擎的帮助下，找到了两个网站，它们分别是：微博时光机 和 热搜神器。经过一番权衡，决定选择页面结构更简单一点的 微博时光机 。
通过抓包，可以快速获得两个关键的接口，它们分别是 获取 timeId 接口 和 获取历史热搜接口。
Firefox抓包示意图简单来说，我们指定一个日期，第一个接口会返回timeId。接下来，通过这个timeId调用第二个接口就可以获得热搜数据。仔细观察的话，第一个接口传递的data参数像是一个BASE64加密后的结果，尝试解密后发现我的猜想是对的，加密前的内容如下：
[&amp;#34;getclosesttime&amp;#34;,[&amp;#34;2021-01-20T23:08:02&amp;#34;]] 这意味着我们只需要改变这里的日期就可以啦，因此，我们的思路无非就是从 2020 年 1 月 1 日开始，依次请求热搜接口获取数据，直到 2020 年 12 月 31 日。这里想顺便吐槽下这个网站的接口设计，居然清一色地全部用数组来返回结果，难道是为了省掉这几个字段来节省流量吗？
接口返回值说明-1接口返回值说明-2吐槽归吐槽，这里我们可以非常容易地写出对应的代码，由于日期和timeId的对应关系是固定的，为了减少后续的请求数量，我们使用MongoDB来对数据进行持久化。同样地，抓取热搜采用了类似的方式，因为历史热搜同样是确定的数据，这里只给出关键的代码，并不代表你可以无脑地复制、粘贴：
# 获取指定日期对应的timeId def get_timeId(date, cookie): cacheKey = date.strftime(&amp;#39;%Y-%m-%d&amp;#39;) records = list(store.find(TABLE_TIME_ID, {&amp;#39;date&amp;#39;: cacheKey})) if len(records) &amp;gt; 0: return records[0][&amp;#39;timeId&amp;#39;] else: data = &amp;#34;[\&amp;#34;getclosesttime\&amp;#34;,[\&amp;#34;{d}\&amp;#34;]]&amp;#34;.format(d=cacheKey) data = base64.</description></item><item><title>基于 Python 和 Selenium 实现 CSDN 一键三连自动化</title><link>https://qinyuanpei.github.io/posts/3148958651/</link><pubDate>Tue, 19 Jan 2021 22:35:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/3148958651/</guid><description>最近一段时间，博主感觉到了某种危机感，或者说是每一个不再年轻的人都会面对的问题，即，怎么面对来自更年轻的“后浪”们的压力，自打国内 IT 行业有了 35 岁这个不成文的“门槛”以后，年轻的“后浪”们仿佛有了更多将“前浪”们拍死在岸上的勇气，我辈忍不住要叹一声后生可畏啊！我认识的 Catcher Wong 正是这样一位大佬，此君虽然比我小三岁，可在技术的广/深度以及经验的丰富程度上，足以令我这个”老人”汗颜，单单 EasyCaching 这一项，就令人望尘莫及啦！我看着他的时候，一如当年 Wesley 大哥看着我的时候，可能这就是某种轮回，姑且执浊酒一杯，致我们终将老去的青春。
不正经的 Kimol 君 关注Kimol 君，最早源于他在我博客里留言，作为礼尚往来，我回访了他的博客，然后发现此人人如其名，非常的”不正经”，他的博客访问量出奇地高，在 CSDN 里写博客多年，深知现在不比从前有运营梦鸽和大白两位小姐姐帮忙推荐到首页，普通的内容很少有机会拥有这样的曝光机会，而像 郭霖 这种从 10 年前后开始写移动开发系列博客的“大神”或者是以图形学为主要写作方向的 诗人“浅墨” ，在通篇都是干货的情况下，长期保持着不错的人气。
这萌萌哒求赞的表情我是做不来的起初，我以为此君的流量来自于标题党，譬如《学会这招，小姐姐看你的眼神将不一样》 和 《震惊！小伙竟然用 Python 找出了马大师视频中的名场面》这几篇，非常像 UC 编辑部和微信公众号的风格。我是一个擅长学习的人，主动去借鉴了他博客中的优点，比如尝试使用轻松、幽默的文风，在文章开头放入目录，适当“蹭”热点等等，我甚至专门致敬了一篇博客： 《厉害了！打工人用 Python 分析西安市职位信息》。而整个 1 月份，我就只有一篇博客流量高一点，就这还不是特别正经的”技术”博客，而此君的流量则是一个又一个的 1w+ ，可我实在想不通，一个不到 100 行的 Python 脚本，真就值得花那么多的流量，真就值得上百条的评论吗？这里放张图大家感受一下：
不知道该说什么好仔细研究了他博客里评论的风格，发现有大量类似“夸夸群”风格的评论，就是那种读起来确实像对方读过了你的文章，可实际一想就觉得这是那种“放之四海而皆准”的话。我最近知道了一位大佬的博客，我惊奇地发现，此君居然在上面留过言，我顺着大佬的博客继续找，发现一个非常有意思的事情，此君曾经给我留言过的内容，居然出现在了别人的博客底下，而从这篇博客的评论里继续找，你会发现好像有一个团队专门在做这种事情，互相点赞、互相评论，甚至这些留言都是来自一篇博客都没有的”新人”，至此，基本可以断定，此君“不讲武德”，用作弊的方式在刷流量！当然，他自己都承认了：
作弊实锤年轻人不讲”武德” OK，既然现在的年轻人都把心思用到这种事情上，作为一个老年人，必须要让他知道什么叫“耗子尾汁”，我们技术做一点正经事儿不行吗？其实，博客园的博客质量相比 CSDN 是要高出许多的，而正因为如此，CSDN 在全力转在线教育/课程以后，博客这个板块就再无往日的“生气”，如果每个人都像他一样，天天跑别人底下刷评论，发一点不痛不痒的话，甚至是推广某个小圈子里的 QQ 群，那真正优质的内容又如何能被大家看到呢？博主曾经加过这样的 QQ 群，你以为是交流技术的群吗？其实是为了推广某个 Python 课程，博主本想交流一下“半泽直树”，然后就被群管理员给删除了！此君大概是抓取 Python 板块排名靠前的博客，通过程序来刷存在感。
对此，我想说，这玩意儿用 Selenium + Python 简直和闹着玩一样，毕竟在了解网页结构以后，直接上 jQuery 操作 DOM 即可，甚至连抓包都不需要，不信你看：
import requests from bs4 import BeautifulSoup import fake_useragent import os, json, time, random from selenium import webdriver from selenium.</description></item><item><title>使用多线程为你的 Python 爬虫提速的 N 种姿势，你会几种？</title><link>https://qinyuanpei.github.io/posts/3247093203/</link><pubDate>Thu, 14 Jan 2021 20:35:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/3247093203/</guid><description>最近博主在优化一个爬虫程序，它是博主在 2017 年左右刚接触 Python 时写下的一个程序。时过境迁，当 Python 2.X 终于寿终正寝成为过去，当博主终于一只脚迈进 30 岁的大门，一切都来得猝不及防，像一阵龙卷风裹挟着回忆呼啸而去。和大多数学习 Python 的人一样，博主学习 Python 是从写爬虫开始的，而这个爬虫程序刚好是那种抓取“宅男女神”的程序，下载图片无疑是整个流程里最关键的环节，所以，整个优化的核心，无外乎提升程序的稳定性、提高抓取速度。所以，接下来，我会带大家走近 Python 中的多线程编程，涉及到的概念主要有线程(池)、进程(池)、异步I/O、协程、GIL等，而理解这些概念，对我们而言是非常重要的，因为它将会告诉你选择什么方案更好一点。想让你的爬虫更高效、更快吗？在这里就能找到你的答案。
楔子 现在，假设我们有一组图片的地址(URL)，我们希望通过requests来实现图片的下载，为此我们定义了Spider类。在这个类中，我们提供了getImage()方法来完成下载这个动作。我们可以非常容易地写出一个“单线程”的版本，但这显然这不是我们今天这篇博客的目的。此时，我们来考虑一个问题，怎么样实现一个“多线程”的版本？
class Spider: def __init__(self, urls): self.session = requests.session() self.session.headers[&amp;#39;User-Agent&amp;#39;] = fake_useragent.UserAgent().random self.session.headers[&amp;#34;Referer&amp;#34;] = &amp;#34;https://www.nvshens.org&amp;#34; self.urls = urls # 下载图片 def getImage(self, url, fileName, retries=5): try: print(f&amp;#39;{threading.currentThread().name} -&amp;gt; {url}&amp;#39;) response = self.session.get(url, allow_redirects=False, timeout=10, proxies=None ) response.raise_for_status() data = response.content imgFile = open(fileName, &amp;#39;wb&amp;#39;) imgFile.write(data) imgFile.close() return True except : while retries &amp;gt; 0: retries -= 1 if self.</description></item><item><title>实现网页长截图的常见思路总结</title><link>https://qinyuanpei.github.io/posts/3406626380/</link><pubDate>Sat, 09 Jan 2021 20:37:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/3406626380/</guid><description>作为一个经常写博客的人，我有时会在微博上分享博客内容，可不知道从什么时候开始，国内互联网越来越丧失信仰，所有的厂商都在试图打造一个**“只进不出”的信息孤岛，进而达到增强“用户黏度”的目的。以微博为例，微博中的外链永远都会被转化为短地址，并且无法通过微博内置的浏览器进行跳转。即使你通过手动复制链接的方式打开链接，你依然需要至少两个步骤方能见到“庐山真面目”。借鉴/抄袭这一陋习的还有简书，花时间做了一个第三方链接跳转提示页面，唯独不愿意在上面加一个 a 标签，你还是要手动复制黏贴。坦白说，我觉得国内互联网正在丧失着信仰，看起来电商、物流、外卖、打车、支付……此起彼伏逐渐渗透到我们生活的方方面面，成为名副其实的“互联网+”，可在信息泛滥的今天，我们越来越难找到真正有价值的信息……既然外链注定要被屏蔽掉，那我就勉为其难地顺应潮流发“长截图”咯，所以，接下来我会为大家分享实现网页“长截图”**的常见思路，希望对有类似烦恼或者需求的小伙伴们有所帮助。
通过浏览器实现 要实现网页长截图，显然是和网页打交道，而和网页打交道最多的是谁呢？自然是我们每天都要用的浏览器啦！值得庆幸的是，不管是 Chrome 还是 Firefox ，我们都可以通过它们来是实现这个想法。
Chrome 对于 Chrome 来说，我们只需要“F12”打开开发者工具，并在其中找到“控制台”选项卡，在平时输入 JavaScript 脚本的地方(即 Console 选项卡)输入Ctrl + Shift + P命令，然后你会得到一个类似 VSCode 命令行体验的输入窗口，接下来，输入：Capture full size screenshot并回车。此时，我们就可以得到完整的页面截图。而如果你希望截取网页中的一部分，则可以在选中指定 DOM 元素后采用相同的方式输入命令：Capture node screenshot。此外，更常用的截取浏览器可见范围内的内容，可以使用：Capture screenshot。可能相对于一般可以进行拖拽截图的工具而言，这个方案显得有点笨拙且简陋，可它真的可以完美地实现我们的想法，而且不需要安装任何扩展或者插件。
使用 Chrome 的截图功能Firefox 对于 Firefox 而言，它本身自带截图功能，并且支持拖拽截图，对于我们这些需要长截图的人而言，唯一需要做的就是点击几下数据，确实要比敲命令行要简单一点、友好一点，我个人更喜欢用 Firefox 一点，因为 Chrome 正在从屠龙少年变成恶龙，为了让这个世界上不是只有 Chrome 一种浏览器内核，我决定支持一下 Firefox ，2020 年因为疫情的原因， Mozila 裁员 25%约 250 人，这家几乎靠着理想主义在维护 Gecko 内核的公司，之后可能再无法和 Google 的 Chrome 抗衡，而这个世界只有一种浏览器的时代我们都曾经经历过，它的名字叫做 IE6 ，不禁令人感慨，简直是开放 Web 的罗曼蒂克消亡史。
使用 Firefox 的截图功能通过 Selenium 实现 在我的认知中，有浏览器的地方就有爬虫，而有爬虫的地方就有 Selenium 。原本好端端的 UI 自动化测试框架，怎么就助纣为虐做起爬虫来了呢？其实，主要原因是它提供了一个可以和浏览器交互的环境，从某种意义上来讲，Selenium 、PhantomJS 以及 Playwright 都可以认为是类似的技术，这里我们以 Selenium 为例，而通过 Selenium 实现网页长截图则主要有两种方式：其一，是构造一个足够“大”的浏览器，然后调用save_screenshot()方法进行截图；其二，是通过“拖拽”滚动条来滚动截图，然后再通过PIL进行拼接，下面来看具体的代码实现：</description></item><item><title>视频是不能 P 的系列：OpenCV 人脸检测</title><link>https://qinyuanpei.github.io/posts/2997581895/</link><pubDate>Fri, 25 Dec 2020 22:49:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/2997581895/</guid><description>恍惚间，2020 年已接近尾声，回首过去这一年，无论是疫情、失业还是“996”，均以某种特殊的方式铭刻着这一年的记忆。也许，是这个冬天的西安雾霾更少一点。所以，有时透过中午的一抹冬阳，居然意外地觉得春天的脚步渐渐近了，甚至连圣诞节这种“洋节日”都感到亲切而且期待，我想，这大概是我丧了一段时间的缘故吧！可不管怎样，人们对未来的生活时常有一种“迷之自信”，果然生还还是要继续下去的呀！趁着最近的时间比较充裕，我决定开启一个信息的系列：视频是不能 P 的。这是互联网上流传的一个老梗了，正所谓“视频是不能 P 的，所以是真的”。其实，在如今这个亦真亦假的世界里，哪里还有什么东西是不能 PS 的呢？借助人工智能“改头换面”越来越轻而易举，而这背后关于隐私和伦理的一连串问题随之而来，你越来越难以确认屏幕对面的那个是不是真实的人类。所以，这个系列会以 OpenCV 作为起点，去探索那些好玩、有趣的视频/图像处理思路，通过技术来证明视频是可以被 PS 的。而作为这个系列的第一篇，我们将从一个最简单的地方开始，它就是人脸检测。
第一个入门示例 学习 OpenCV 最好的方式，就是从官方的示例开始，我个人非常推荐的两个教程是 OpenCV: Cascade Classifier 和 Python OpenCV Tutorial，其次是 浅墨大神 的【OpenCV】入门教程，不同的是， 浅墨大神 的教程主要是使用 C++，对于像博主这样的“不学无术”的人，这简直无异于从入门到放弃，所以，建议大家结合自己的实际情况，选择适合自己的“难度”。好了，思绪拉回我们这里，在 OpenCV 中实现人脸检测，主要分为以下三个步骤，即，首先，定义联级分类器CascadeClassifier并载入指定的模型文件；其次，对待检测目标进行灰度化和直方图均衡化处理；最后，对灰度图调用detectMultiScale()方法进行检测。下面是一个简化过的入门示例，使用世界上最省心的 Python 语言进行编写：
import cv2 # 步骤1: 定义联级分类器CascadeClassifier并载入指定的模型文件 faceCascade = cv2.CascadeClassifier(&amp;#39;./haarcascades/haarcascade_frontalface_alt2.xml&amp;#39;) # 步骤2: 对待检测目标进行灰度化和直方图均衡化处理 target = cv2.imread(&amp;#39;target.jpg&amp;#39;) target_gray = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY) target_gray = cv2.equalizeHist(target_gray) # 步骤3: 人脸检测 faces = faceCascade.detectMultiScale(target_gray) for (x,y,w,h) in faces: cv2.rectangle(target, (x, y), (x + w, y + h), (0, 255, 0), 2) # 步骤4: 展示结果 cv2.</description></item><item><title>作为技术宅的我，是这样追鬼滅の刃的</title><link>https://qinyuanpei.github.io/posts/3602353334/</link><pubDate>Tue, 15 Dec 2020 22:49:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/3602353334/</guid><description>有人说，“男人至死都是少年”，而这句听起来有一点中二的话，其实是出自一部同样有一点中二的动漫——银魂。我个人的理解是，知世故而不世故。也许，年轻时那些天马行空的想法，就像堂吉诃德大战风车一样荒诞，可依然愿意去怀着这样的梦想去生活。正如罗曼罗兰所言，“世上只有一种英雄主义，就是在认清生活真相之后依然热爱生活”。所以，继《浪客剑心》之后，我再次被一部叫做《鬼灭之刃》的动漫吸引，毕竟男人的快乐往往就是这么朴实无华且枯燥。一个快三十岁的人，如果还能被一部热血少年番吸引，大概可以说明，他身体里的中二少年连同中二少年魂还活着。最早的印象来自朋友圈里的一位二次元“少年”，他和自己儿子站一起，有种浑然天成的协调感，整个人是非常年轻的感觉。所以，大概，男人至死都是少年。
漫画的抓取 鬼滅の刃的漫画早已更完，令我不舍昼夜去追的，实际上是动画版的鬼滅の刃。虽然 B 站上提供中配版本，可一周更新两集的节奏，还是让我追得有一点焦灼(PS：我没有大会员呢)，甚至熬着夜提前“刷”了无限列车(PS：见文章末尾小程序码)。其实，鬼滅の刃前期并没有特别吸引人的地方，直到那田蜘蛛山那一话开始渐入佳境，鬼杀队和鬼两个阵营所构成的人物群像开始一点一点的展开。它的表达方式有点接近刺客信条，即反派都是在死亡一刹那间有了自我表达的机会，而玩家/观众都可以了解反派的过去。由于鬼是由人转变而来，所以，在热血和厮杀之外，同样有了一点关乎人性的思考。作为一名“自封”的技术宅，我必须要在追番的时候做点什么，从哪里开始好呢？既然漫画版早已更新完毕，我们要不先抓取漫画下来提前过过瘾？
OK，这里博主找了一个动漫网站，它上面有完整的鬼滅の刃漫画。我意识到从网上抓取漫画的行为是不对的，可这家网站提供的漫画明显是通过扫描获得的，因为正常的漫画都是通过购买杂志的方式获得的。所以，如果经济条件允许的情况下，还是希望大家可以支持正版，这里博主主要还是为了研究技术(逃，无意对这些资源做二次加工或者以任何方式盈利，所以，请大家不要向博主索取任何资源，我对自己的定位永远是一名软件工程师，谁让我无法成为尤小右这样的“美妆”博主呢？这一点希望大家可以理解哈！
鬼滅の刃作品页面鬼滅の刃章节页面简单分析下动漫网站结构，可以发现，它主要有两种界面，即作品页面和章节页面。作品页面里面会显示所有的章节，而每个章节里会显示所有的图片。所以，我们的思路是，首先，通过作品页面获取所有章节的链接。其次，针对每一个章节，获取总页数后逐页下载图片即可。注意到这个网站有部分内容是通过 JavaScript 动态生成的，所以，requests针对这种情况会有点力不从心。幸好，我们还有Selenium这个神器可以使用，我们一起来看这部分内容如何实现：
import requests from bs4 import BeautifulSoup import fake_useragent import json import urllib from selenium import webdriver from selenium.webdriver.support.ui import Select from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC import os, time import threading import threadpool class DemonSlayer: def __init__(self, baseUrl): self.baseUrl = baseUrl self.session = requests.session() self.headers = { &amp;#39;User-Agent&amp;#39;: fake_useragent.UserAgent(verify_ssl=False).random } # 使用无头浏览器 fireFoxOptions = webdriver.</description></item><item><title>使用 Python 抽取《半泽直树》原著小说人物关系</title><link>https://qinyuanpei.github.io/posts/1427872047/</link><pubDate>Tue, 08 Dec 2020 22:49:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/1427872047/</guid><description>此时此刻，2020 年的最后一个月，不管过去这一年给我们留下了怎样的记忆，时间终究自顾自地往前走，留给我们的怀念已时日无多。如果要说 2020 年的年度日剧，我想《半泽直树》实至名归，这部在时隔七年后上映的续集，豆瓣评分高达 9.4 分，一度超越 2013 年第一部的 9.3 分，是当之无愧的现象级电视剧，期间甚至因为疫情原因而推迟播出，这不能不感谢为此付出辛勤努力的演职人员们。身为一个“打工人”，主角半泽直树那种百折不挠、恩怨分明的性格，难免会引起你我这种“社畜”们的共鸣，即使做不到“以牙还牙，加倍奉还”，至少可以活得像一个活生生的人。电视剧或许大家都看过了，那么，电视剧相对于原著小说有哪些改动呢？今天，就让我们使用 Python 来抽取半泽直树原著小说中的人物关系吧！
准备工作 在开始今天的博客内容前，我们有一点准备工作要完成。考虑到小说人物关系抽取，属于自然语言处理(NLP)领域的内容，所以，除了准备好 Python 环境以外，我们需要提前准备相关的中文语料，在这里主要有：半泽直树原著小说、 半泽直树人名词典、半泽直树别名词典、中文分词停用词表。除此之外，我们需要安装结巴分词、PyECharts两个第三方库(注，可以通过 pip 直接安装)，以及用于展示人物关系的软件Gephi(注，这个软件依赖 Java 环境)。所以，你基本可以想到，我们会使用结巴分词对小说文本进行分词处理，而半泽直树人名列表则作为用户词典供结巴分词使用，经过一系列处理后，我们最终通过Gephi和PyECharts对结果进行可视化，通过分析人物间的关系，结合我们对电视剧剧情的掌握情况，我们就可以对本文所采用方法的效果进行评估，也许你认为两个人毫无联系，可最终他们以某种特殊的形式建立了联系，这就是我们要做这件事情的意义所在。本项目已托管在 Github上，供大家自由查阅。
原理说明 这篇博客主要参考了 Python 基于共现提取《釜山行》人物关系 这个课程，该项目已在 Github 上开源，可以参考：https://github.com/Forec/text-cooccurrence。这篇文章中提到了一种称之为“共现网络”的方法，它本质上是一种基于统计的信息提取方法。其基本原理是，当我们在阅读书籍或者观看影视作品时，在同一时间段内同时出现的人物，通常都会存在某种联系。所以，如果我们将小说中的每个人物都看作一个节点，将人物间的关系都看作一条连线，最终我们将会得到一个图(指数据结构中的Graph)。因为Gephi和PyECharts以及NetworkX都提供了针对Graph的可视化功能，因此，我们可以使用这种方法，对《半泽直树》原著小说中的人物关系进行抽取。当然，这种方法本身会存在一点局限性，这些我们会放在总结思考这部分来进行说明，而我们之所以需要准备人名词典，主要还是为了排除单纯的分词产生的干扰词汇的影响；准备别名词典，则是考虑到同一个人物，在不同的语境下会有不同的称谓。
过程实现 这里，我们定义一个RelationExtractor类来实现小说人物关系的抽取。其中，extract()方法用于抽取制定小说文本中的人物关系，exportGephi()方法用于输出 Gephi 格式的节点和边信息， exportECharts()方法则可以使用ECharts对人物关系进行渲染和输出：
import os, sys import jieba, codecs, math import jieba.posseg as pseg from pyecharts import options as opts from pyecharts.charts import Graph class RelationExtractor: def __init__(self, fpStopWords, fpNameDicts, fpAliasNames): # 人名词典 self.name_dicts = [line.strip().split(&amp;#39; &amp;#39;)[0] for line in open(fpNameDicts,&amp;#39;rt&amp;#39;,encoding=&amp;#39;utf-8&amp;#39;).</description></item><item><title>厉害了！打工人用 Python 分析西安市职位信息</title><link>https://qinyuanpei.github.io/posts/2147036181/</link><pubDate>Sat, 05 Dec 2020 12:49:47 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/2147036181/</guid><description>在上一篇博客中，我和大家分享了整个 11 月份找工作的心路历程，而在找工作的过程中，博主发现西安大小周、单休这种变相“996”的公司越来越多，感慨整个行业越来越“内卷”的同时，不免会对未来的人生有一点迷茫，因为深圳已经开始试运行“996”了，如果有一天“996”被合法化并成为一种常态，那么，我们又该如何去面对“人会一天天衰老，总有一天肝不动”的客观规律呢？我注意到 Boss 直聘移动端会展示某个公司的作息时间，所以，我有了抓取西安市职位和公司信息并对其进行数据分析的想法，我想知道，这到底是我一个人的感受呢？还是整个世界的确是这样子的呢？带着这样的想法，博主有了今天这篇博客。所以，在今天这篇博客里，博主会从Boss 直聘、智联招聘以及前程无忧上抓取职位和公司信息，并使用 MongoDB 对数据进行持久化，最终通过pyecharts对结果进行可视化展示。虽然不大确定 2021 年会不会变得更好，可生活最迷人的地方就在于它的不确定性，正如数据分析唯一可以做的，就是帮助我们从变化的事物中挖掘出不变的规律一样。
爬虫编写 其实，这种类似的数据分析，博主此前做过挺多的啦，譬如 基于 Python 实现的微信好友数据分析 以及 基于新浪微博的男女性择偶观数据分析(下) 这两篇博客。总体上来说，大部分学习 Python 的朋友都是从编写爬虫开始的，而在博主看来，数据分析是最终的目的，编写爬虫则是达到这一目的的手段。而从始至终，“爬”与“反爬”的较量从未停止过，Requests、BeautifulSoup、Selenium、Phantom 等等的技术层出不穷。考虑到现在编写爬虫存在风险，所以，我不会在博客里透露过多的“爬虫”细节，换言之，我不想成为一个教别人写爬虫的人，因为这篇博客的标签是数据分析，关于爬虫的部分，我点到为止，不再过多地去探讨它的实现，希望大家理解。而之所以要从这三个招聘网站上抓取，主要还是为了增加样本的多样性，因为 Boss 直聘上西安市的职位居然只有 3 页，这实在是太让人费解了！
Boss 直聘 通过抓包，我们可以分析出 Boss 直聘的地址：https://www.zhipin.com/job_detail/?query={query}&amp;amp;city={cityCode}&amp;amp;industry=&amp;amp;position=&amp;amp;page={page}。其中，query为待查询关键词，cityCode为待查询城市代码，page为待查询的页数。可以注意到，industry和position两个参数没有维护，它们分别表示待查询的行业和待查询的职称。因为我们面向的是更一般的“打工人”，所以，这些都可以进行简化。对于cityCode这个参数，我们可以通过下面的接口获得：https://www.zhipin.com/wapi/zpCommon/data/city.json。这里，简单定义一个方法extractCity()来提取城市代码：
def extractCity(self, cityName=None): if (os.path.exists(&amp;#39;bossCity.json&amp;#39;) and cityName != None): with open(&amp;#39;bossCity.json&amp;#39;, &amp;#39;rt&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as fp: cityList = json.load(fp) for city in cityList: if (city[&amp;#39;name&amp;#39;] == cityName): return city[&amp;#39;code&amp;#39;] else: response = requests.get(self.cityUrl) response.raise_for_status() json_data = response.json(); if (json_data[&amp;#39;code&amp;#39;] == 0 and json_data[&amp;#39;zpData&amp;#39;] !</description></item><item><title>使用 Python 开发插件化应用程序</title><link>https://qinyuanpei.github.io/posts/1960676615/</link><pubDate>Fri, 11 Oct 2019 08:56:27 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/1960676615/</guid><description>插件化应用是个老话题啦，在我们的日常生活中更是屡见不鲜。无论是多年来臃肿不堪的 Eclipse，亦或者是扩展丰富著称的 Chrome，乃至近年来最优秀的编辑器 VSCode，插件都是这其中重要的组成部分。插件的意义在于扩展应用程序的功能，这其实有点像 iPhone 手机和 AppStore 的关系，没有应用程序的手机无非就是一部手机，而拥有了应用程序的手机则可以是 Everything。显然，安装或卸载应用程序并不会影响手机的基本功能，而应用程序离开了手机同样无法单独运行。所以，所谓“插件”，实际上是一种按照一定规范开发的应用程序，它只能运行在特定的软件平台/应用程序且无法运行。这里，最重要的一点是应用程序可以不依赖插件单独运行，这是这类“插件式”应用的基本要求。
好了，在了解了插件的概念以后，我们来切入今天的正文。博主曾经在《基于 Python 实现 Windows 下壁纸切换功能》这篇文章中编写了一个小程序，它可以配合 Windows 注册表实现从 Unsplash 上抓取壁纸的功能。最近，博主想为这个小程序增加 必应壁纸 和 WallHaven 两个壁纸来源，考虑到大多数的壁纸抓取流程是一样的，博主决定以“插件”的方式完成这次迭代，换句话说，主程序不需要再做任何调整，当我们希望增加新的数据源的时候，只需要写一个.py 脚本即可，这就是今天这篇文章的写作缘由。同样的功能，如果使用 Java 或者 C#这类编译型语言来做，我们可能会想到为插件定义一个 IPlugin 接口，这样每一个插件实际上都是 IPlugin 接口的实现类，自然而然地，我们会想到通过反射来调用接口里的方法，这是编译型语言的做法。而面对 Python 这样的解释型语言，我们同样有解释型语言的做法。
首先，我们从一个最简单的例子入手。我们知道，Python 中的 import 语法可以用来引入一个模块，这个模块可以是 Python 标准库、第三方库和自定义模块。现在，假设我们有两个模块：foo.py 和 bar.py。
#foo.py import sys class Chat: def send(self,uid,msg): print(&amp;#39;给{uid}发送消息：{msg}&amp;#39;.format(uid=uid,msg=msg)) def sendAll(self,msg): print(&amp;#39;群发消息：{msg}&amp;#39;.format(msg=msg)) #bar.py import sys class Echo: def say(self): print(&amp;#34;人生苦短，我用Python&amp;#34;) def cry(): print(&amp;#34;男人哭吧哭吧不是罪&amp;#34;) 通常, 为了在当前模块(main.py)中使用这两个模块，我们可以使用以下语句：
import foo from bar import * 这是一种简单粗暴的做法，因为它会导入模块中的全部内容。一种更好的做法是按需加载，例如下面的语句：</description></item><item><title>博客图片迁移折腾记</title><link>https://qinyuanpei.github.io/posts/3444626340/</link><pubDate>Fri, 18 Jan 2019 09:27:35 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/3444626340/</guid><description>去年国庆的时候，七牛官方开始回收测试域名，这直接导致博客中大量图片出现无法访问的情况，虽然博主第一时间启用了新的域名：https://blog.yuanpei.me，可是因为七牛官方要求域名必须备案，所以，这件事情一直耽搁着没有往下进行。至于为什么会一直拖到 2019 年，我想大家都能猜到一二，没错，我就是懒得去弄域名备案这些事情:joy:。最近花了点时间，把博客里的图片从七牛和 CSDN 迁移了出来，所以，今天这篇博客，主要想和大家分享下这个折腾的过程，如果能帮助到和我一样，因为七牛官方回收了域名而无法导出图片的朋友，在下开心之至。虽然今天没有回望过去，没有给新的一年立 flag，就如此平淡地过渡到了 2019 年，可或许这才是生活本来的样子吧！
七牛的测试域名被官方回收了以后，我们有两种思路去导出这些图片，其一，是临时像官方提工单申请一个测试域名，这样在测试域名被回收前，我们可以直接使用官方提供的qrsctl或者qshell工具进行批量导出，因为此时我们可以直接在配置文件里配置测试域名，具体可以参考这篇文章：跑路之后七牛图片如何导出备份至本地，甚至你可以直接到七牛的管理控制台手动下载，可这样就一点都不极客了对吗？我们是一生追求做极客的人好伐。其二，同样是借助官方提供的qshell工具，因为没有域名，我们没有办法批量导出，可是工具中提供了两个非常有用的命令，它们分别是：qshell listbucket、qshell get。通过这两个命令，我们就可以列举出指定 bucket 中的文件以及下载指定文件，所以，这就是我们的第一步，首先把图片从七牛那里导出到本地。以博主的blogspace为例：
qshell account &amp;lt;ak&amp;gt; &amp;lt;sk&amp;gt; &amp;#39;qinyuanpei@163.com&amp;#39; /* 请使用你的ak/sk，谢谢 */ qshell listbucket blogspace 使用listbucket列举指定bucket内文件事实上，通过第一列的 Key，即文件名，我们就可以下载该资源到本地，因为七牛实际上是采用对象存储的方式来组织资源的，这里我们以第一张图片05549344-BF85-4e8c-BCBC-1F63DFE80E43.png为例：
qshell get blogspace 05549344-BF85-4e8c-BCBC-1F63DFE80E43.png 默认情况下，该图片会下载到当前目录下，本地文件和远程文件名保持一致。当然，我们还可以通过-o 参数来指定输出文件：
使用get命令下载指定文件好了，有了这个基础，我们就可以着手博客图片的迁移啦。博主最初的想法是，先获取到指定 bucket 下的全部文件，然后再对结果进行拆分，循环执行 qshell get 命令，可惜再 PowerShell 下并没有类似 grep 的命令，所以，这个想法放弃。其实，你仔细观察七牛图片外链的格式就会发现，除了域名部分以外，剩下的就是该文件在 bucket 里对应的 key 啦，所以，博主的想法开始从 Markdown 文件入手，最终我们的思路是，解析博客对应的 Markdown 文件，通过正则匹配所有的图片链接，截取出图片的文件名并通过 qshell 下载到本地。人生苦短，我用 Python。具体写出来，大概是下面这个样子：
def sync(root,ak,sk,account,bucket): files = [] children = os.listdir(root) for child in children: path = os.</description></item><item><title>基于 Python 实现的微信好友数据分析</title><link>https://qinyuanpei.github.io/posts/2805694118/</link><pubDate>Sat, 24 Feb 2018 12:50:52 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/2805694118/</guid><description>最近微信迎来了一次重要的更新，允许用户对&amp;quot;发现&amp;quot;页面进行定制。不知道从什么时候开始，微信朋友圈变得越来越复杂，当越来越多的人选择&amp;quot;仅展示最近三天的朋友圈&amp;quot;，大概连微信官方都是一脸的无可奈何。逐步泛化的好友关系，让微信从熟人社交逐渐过渡到陌生人社交，而朋友圈里亦真亦幻的状态更新，仿佛在努力证明每一个个体的&amp;quot;有趣&amp;quot;。有人选择在朋友圈里记录生活的点滴，有人选择在朋友圈里展示观点的异同，可归根到底，人们无时无刻不在窥探着别人的生活，唯独怕别人过多地了解自己的生活。人性中交织着的光明与黑暗，像一只浑身长满刺的刺猬，离得太远会感觉到寒冷，而靠得太近则害怕被刺扎到。朋友圈就像过年走亲戚，即便你心中有一万个不痛快，总是不愿意撕破脸，或屏蔽对方，或不给对方看，或仅展示最后三天，于是通讯录里的联系人越来越多，朋友圈越来越大，可再不会有能真正触动你内心的&amp;quot;小红点&amp;quot;出现，人类让一个产品变得越来越复杂，然后说它无法满足人类的需求，这大概是一开始就始料不及的吧！
引言 有人说，人性远比计算机编程更复杂，因为即使是人类迄今为止最伟大的发明——计算机，在面对人类的自然语言时同样会张惶失措 。人类有多少语言存在着模棱两可的含义，我认为语言是人类最大的误解，人类时常喜欢揣测语言背后隐藏的含义，好像在沟通时表达清晰的含义会让人类没有面子，更不用说网络上流行的猜测女朋友真实意图的案例。金庸先生的武侠小说《射雕英雄传》里，在信息闭塞的南宋时期，江湖上裘千丈的一句鬼话，就搅得整个武林天翻地覆。其实，一两句话说清楚不好吗？黄药师、全真七子、江南六怪间的种种纠葛，哪一场不是误会？一众儿武功震古烁今的武林高手，怎么没有丝毫的去伪存真的能力，语言造成了多少误会。
可即便人类的语言复杂得像一本无字天书，可人类还是从这些语言中寻觅到蛛丝马迹。古人有文王&amp;quot;拘而演周易&amp;quot;、东方朔测字卜卦，这种带有&amp;quot;迷信&amp;quot;色彩的原始崇拜，就如同今天人们迷信星座运势一般，都是人类在上千年的演变中不断对经验进行总结和训练的结果。如此说起来，我们的人工智能未尝不是一种更加科学化的&amp;quot;迷信&amp;quot;，因为数据和算法让我们在不断地相信，这一切都是真实地。生活在数字时代的我们，无疑是悲哀的，一面努力地在别人面前隐藏真实地自己，一面不无遗憾地感慨自己无处遁逃，每一根数字神经都紧紧地联系着你和我，你不能渴望任何一部数字设备具备真正的智能，可你生命里的每个瞬间，都在悄然间被数据地折射出来。
今天这篇文章会基于 Python 对微信好友进行数据分析，这里选择的维度主要有：性别、头像、签名、位置，主要采用图表和词云两种形式来呈现结果，其中，对文本类信息会采用词频分析和情感分析两种方法。常言道：工欲善其事，必先利其器也。在正式开始这篇文章前，简单介绍下本文中使用到的第三方模块：
itchat：微信网页版接口封装 Python 版本，在本文中用以获取微信好友信息。 jieba：结巴分词的 Python 版本，在本文中用以对文本信息进行分词处理。 matplotlib： Python 中图表绘制模块，在本文中用以绘制柱形图和饼图 snownlp：一个 Python 中的中文分词模块，在本文中用以对文本信息进行情感判断。 PIL： Python 中的图像处理模块，在本文中用以对图片进行处理。 numpy： Python 中 的数值计算模块，在本文中配合 wordcloud 模块使用。 wordcloud： Python 中的词云模块，在本文中用以绘制词云图片。 TencentYoutuyun：腾讯优图提供的 Python 版本 SDK ，在本文中用以识别人脸及提取图片标签信息。 以上模块均可通过 pip 安装，关于各个模块使用的详细说明，请自行查阅各自文档。 数据分析 分析微信好友数据的前提是获得好友信息，通过使用 itchat 这个模块，这一切会变得非常简单，我们通过下面两行代码就可以实现：
itchat.auto_login(hotReload = True) friends = itchat.get_friends(update = True) 同平时登录网页版微信一样，我们使用手机扫描二维码就可以登录，这里返回的 friends 对象是一个集合，第一个元素是当前用户。所以，在下面的数据分析流程中，我们始终取 friends[1:]作为原始输入数据，集合中的每一个元素都是一个字典结构，以我本人为例，可以注意到这里有 Sex、City、Province、HeadImgUrl、Signature 这四个字段，我们下面的分析就从这四个字段入手：
好友信息结构展示好友性别 分析好友性别，我们首先要获得所有好友的性别信息，这里我们将每一个好友信息的 Sex 字段提取出来，然后分别统计出 Male、Female 和 Unkonw 的数目，我们将这三个数值组装到一个列表中，即可使用 matplotlib 模块绘制出饼图来，其代码实现如下：
def analyseSex(firends): sexs = list(map(lambda x:x[&amp;#39;Sex&amp;#39;],friends[1:])) counts = list(map(lambda x:x[1],Counter(sexs).</description></item><item><title>使用 Python 生成博客目录并自动更新 README</title><link>https://qinyuanpei.github.io/posts/1329254441/</link><pubDate>Fri, 23 Feb 2018 09:32:45 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/1329254441/</guid><description>各位朋友，大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是：https://qinyuanpei.github.io。首先在这里祝大家春节快乐，作为过完年以后的第一篇文章，博主想写点内容风格相对轻松的内容。自从博主的博客采用 TravisCI 提供的持续集成(CI)服务以以来，博客的更新部署变得越来越简单，所有的流程都被简化为 Git 工作流下的 提交(commit) 和 推送(push) 操作。考虑到博客是托管在 Github 上的，一直希望可以自动更新仓库主页的 README 文件，这样可以显示每次提交代码后的变更历史。基于这样一个构想，我想到了为博客生成目录并自动更新 README，其好处是可以为读者建立良好的文档导航，而且 Markdown 是一种简单友好的文档格式，Github 等代码托管平台天生就支持 Markdown 文档的渲染。关于博客采用 TravisCI 提供持续集成(CI)服务相关内容，可以参考 持续集成在 Hexo 自动化部署上的实践 这篇文章。
好了，现在考虑如何为博客生成目录，我们这里需要三个要素，即标题、链接和时间。标题和时间可以直接从 _posts 目录下的 Markdown 文档中读取出来，链接从何而来呢？我最初想到的办法是读取每个 Markdown 文档的文件名，因为我的使用习惯是采用英文命名，这样当博客的永久链接(permalink)采用默认的 :year/:month/:day/:title/ 形式时，每个 Markdown 文档的文件名等价于文章链接。事实证明这是一个愚蠢的想法，因为当你改变永久链接(permalink)的形式时，这种明显投机的策略就会彻底的失败。相信你在浏览器种打开这篇文章时，已然注意到链接形式发生了变化，当然这是我们在稍后的文章中讨论的话题啦。至此，我们不得不寻找新的思路，那么这个问题该如何解决呢？
我意识到我的博客配置了 hexo-generator-json-content 插件，这个插件最初的目的是为博客提供离线的搜索能力，该插件会在博客的根目录里生成一个 content.json 文件，而这个文件中含有我们想要的一切信息，因此我们的思路转变为解析这个文件，人生苦短啊，我果断选择了我最喜欢的 Python，这里我们会提取出所有的文章信息，按照日期由近到远排序后生成列表。Python 强大到让我觉得这篇文章无法下笔，所以这里直接给出代码啦：
# -*- coding: utf-8 -*- import os import re import sys import json import datetime # 文档实体结构定义 class Post: def __init__(self,date,link,title): self.date = date self.link = link self.</description></item><item><title>基于 Python 实现 Windows 下壁纸切换功能</title><link>https://qinyuanpei.github.io/posts/2822230423/</link><pubDate>Mon, 05 Feb 2018 16:48:39 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/2822230423/</guid><description>在过去一年多的时间里，我尝试改变博客的写作风格，努力让自己不再写教程类文章，即使在这个过程中，不断地面临着写作内容枯竭的痛苦。因为我渐渐地意识到，告诉别人如何去做一件事情，始终停留在&amp;quot;术&amp;quot;的层面，而比这个更为重要的是，告诉别人为什么要这样做，这样就可以过渡到&amp;quot;道&amp;quot;的层面。古人云：形而上者谓之道，形而下者谓之器。我们常常希望通过量变来产生质变，可是如果在这个过程中不能及时反思和总结，我们认为的努力或许仅仅是重复的劳作而已。如你所见，在这篇文章里，我们将通过 Python 和 Windows 注册表实现壁纸切换功能，主要涉及到的 Python 中的 requests、pyinstaller 这两个模块的使用，希望大家喜欢。
故事缘由 人们常常相信事出有因，可这世界上有些事情，哪里会有什么原因啊，比如喜欢与不喜欢。做这样一个小功能的初衷，起源于我对桌面壁纸的挑剔。作为一个不完全的强迫症患者，我需要花费大量时间去挑选一张壁纸，丝毫不亚于在网上挑选一件喜欢的商品。我注意到知乎上有这样的话题：有哪些无版权图片网站值得推荐？，因此对于桌面壁纸的筛选，我渐渐地开始摆脱对搜索引擎的依赖，我个人比较喜欢Pexels和Unsplash这两个网站，所以我想到了从这两个网站抓取图片来设置 Windows 壁纸的方案。市面上类似的商业软件有百度壁纸、搜狗壁纸等，可这些软件都不纯粹，或多或少地掺杂了额外功能，个中缘由想来大家都是知道的。联想到微信最新版本的更新，&amp;ldquo;发现&amp;quot;页面支持所有项目的隐藏，甚至是盟友京东的电商入口和腾讯最赚钱的游戏入口，这让我开始正视腾讯这家公司，我收回曾经因为抄袭对腾讯产生的不满，腾讯是一家值得尊重的互联网公司。做一个纯粹的应用程序，这就是我的初心。
设计实现 好了，现在我们考虑如何来实现这个功能，我们的思路是从Unsplash这个网站抓取图片，并将其存储在指定路径，然后通过 Windows API 完成壁纸的设置。Python 脚本会通过 pyinstaller 模块打包成可执行文件，我们通过修改注册表的方式，在右键菜单内加入切换壁纸的选项，这样我们可以直接通过右键菜单实现壁纸切换功能。在编写脚本的时候，起初想到的是抓包这样的常规思路，因为请求过程相对复杂而失败，后来意外地发现官方提供了 API 接口。事实上Pexels和Unsplash都提供了 API 接口，通过调用这些 API 接口，我们的探索进行得非常顺利，下面是具体脚本实现：
# Query Images searchURL = &amp;#39;https://unsplash.com/napi/search?client_id=%s&amp;amp;query=%s&amp;amp;page=1&amp;#39; client_id = &amp;#39;fa60305aa82e74134cabc7093ef54c8e2c370c47e73152f72371c828daedfcd7&amp;#39; categories = [&amp;#39;nature&amp;#39;,&amp;#39;flowers&amp;#39;,&amp;#39;wallpaper&amp;#39;,&amp;#39;landscape&amp;#39;,&amp;#39;sky&amp;#39;] searchURL = searchURL % (client_id,random.choice(categories)) response = requests.get(searchURL) print(u&amp;#39;正在从Unsplash上搜索图片...&amp;#39;) # Parse Images data = json.loads(response.text) results = data[&amp;#39;photos&amp;#39;][&amp;#39;results&amp;#39;] print(u&amp;#39;已为您检索到图片共%s张&amp;#39; % str(len(results))) results = list(filter(lambda x:float(x[&amp;#39;width&amp;#39;])/x[&amp;#39;height&amp;#39;] &amp;gt;=1.33,results)) result = random.choice(results) resultId = str(result[&amp;#39;id&amp;#39;]) resultURL = result[&amp;#39;urls&amp;#39;][&amp;#39;regular&amp;#39;] # Download Images print(u&amp;#39;正在为您下载图片:%s.</description></item><item><title>深入浅出理解 Python 装饰器</title><link>https://qinyuanpei.github.io/posts/2829333122/</link><pubDate>Tue, 23 Jan 2018 15:55:13 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/2829333122/</guid><description>各位朋友，大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是https://qinyuanpei.github.io。今天我想和大家一起探讨的话题是 Python 中的装饰器。因为工作关系最近这段时间在频繁地使用 Python，而我渐渐意识到这是一个非常有趣的话题。无论是在 Python 标准库还是第三方库中，我们越来越频繁地看到装饰器的身影，从某种程度上而言，Python 中的装饰器是 Python 进阶者的一条必由之路，正确合理地使用装饰器可以让我们的开发如虎添翼。装饰器天然地和函数式编程、设计模式、AOP 等概念产生联系，这更加让我对 Python 中的这个特性产生兴趣。所以，在这篇文章中我将带领大家一起来剖析 Python 中的装饰器，希望对大家学习 Python 有所帮助。
什么是装饰器 什么是装饰器？这是一个问题。在我的认知中，装饰器是一种语法糖，其本质就是函数。我们注意到 Python 具备函数式编程的特征，譬如 lambda 演算，map、filter 和 reduce 等高阶函数。在函数式编程中，函数是一等公民，即“一切皆函数”。Python 的函数式编程特性由早期版本通过渐进式开发而来，所以对“一切皆对象”的 Python 来说，函数像普通对象一样使用，这是自然而然的结果。为了验证这个想法，我们一起来看下面的示例。
函数对象 def square(n): return n * n func = square print func #&amp;lt;function square at 0x01FF9FB0&amp;gt; print func(5) #25 可以注意到，我们将一个函数直接赋值给一个变量，此时该变量表示的是一个函数对象的实例，什么叫做函数对象呢？就是说你可以将这个对象像函数一样使用，所以当它带括号和参数时，表示立即调用一个函数；当它不带括号和参数时，表示一个函数。在 C#中我们有一个相近的概念被称为委托，而委托本质上是一个函数指针，即表示指向一个方法的引用，从这个角度来看，C#中的委托类似于这里的函数对象，因为 Python 是一个动态语言，所以我们可以直接将一个函数赋值给一个对象，而无需借助 Delegate 这样的特殊类型。
使用函数作为参数 def sum_square(f,m,n): return f(m) + f(n) print sum_square(square,3,4) #25 使用函数作为返回值 def square_wrapper(): def square(n): return n * n return square wrapper = square_wrapper() print wrapper(5) #25 既然在 Python 中存在函数对象这样的类型，可以让我们像使用普通对象一样使用函数。那么，我们自然可以将函数推广到普通对象适用的所有场合，即考虑让函数作为参数和返回值，因为普通对象都都具备这样的能力。为什么要提到这两点呢？因为让函数作为参数和返回值，这不仅是函数式编程中高阶函数的基本概念，而且是闭包、匿名方法和 lambda 等特性的理论基础。从 ES6 中的箭头函数、Promise、React 等可以看出，函数式编程在前端开发中越来越流行，而这些概念在原理上是相通的，这或许为我们学习函数式编程提供了一种新的思路。在这个示例中，**sum_square()和square_wrapper()**两个函数，分别为我们展示了使用函数作为参数和返回值的可行性。</description></item><item><title>基于新浪微博的男女性择偶观数据分析(上)</title><link>https://qinyuanpei.github.io/posts/1386017461/</link><pubDate>Sat, 23 Dec 2017 20:28:40 +0000</pubDate><guid>https://qinyuanpei.github.io/posts/1386017461/</guid><description>或许是因为我喜欢的姑娘从来都不喜欢我，而感情上的挫折一度让我陷入无尽的自卑。朋友在朋友圈里发布一条关于皮影戏的动态，我开玩笑说这个皮影戏结局应该是个悲剧，因为我注意到在剧中，无论一个人如何卖力地表演甚至双腿跪倒在地，有的人从故事开场到结束一直对此无动于衷。朋友回复我说，这不就是你现在的状态吗？我沉默半天终于熄灭手机屏幕。我听到过各种各样让我放弃她的话，即使这种念头在我脑海里萌生已久，是幻想让我硬生生地拖了这么久。当你努力想要融入对方的生活，而等待你的是一道冰冷的墙。这种感觉像什么呢？大概就是一个又一个“好友”安安静静地躺在联系人的置顶名单里，不敢发消息让对方知道，更不愿残忍地把对方删除。我安慰自己说，对我而言，我失去的是一个不喜欢你的人；而对对方而言，失去的是一个喜欢她的人。你当然可以说我没有那么喜欢她，如果一定要喜欢到卑微如尘土的地步，我宁愿一个人单身到天荒地老。
当我意识到人与人间，即使亲近如父母尚且无法完全理解彼此的时候，我忽然发现一个有趣的现象，我们喜欢一个人的时候，首先注意到的会是外表，我们将其称之为眼缘，所以人与人间的感情纽带最初会是吸引，而后是了解彼此的优缺点，最终是相互理解和扶持。可我们知道，外表是可以伪装出来的，所以我们习惯通过外表和言语来评价一个人，这就像是数学归纳法，我们总认为推倒第一块多诺米骨牌，就意味着所有多诺米骨牌都会倒下。可现实世界矛盾的地方就在于，我们认为理所当然正确的事情，或许正是我们无法证明其正确性的，这在数学上称为哥德尔不完备定理。所以，一件残酷的事情是，当你无法吸引一个人的时候，通往内心世界的路就被堵死了。朋友圈里精彩纷呈的社交互动，并不代表有人愿意真正了解你的生活，何况是你吸引不到的人呢？我很想知道，我们在选择伴侣的时候到底看中什么，所以我一直在关注@西安月老牵线上发布的征婚交友类微博，本文的故事从这里正式展开。
身高 175 的悲伤 或许你以为我会无聊到试图从微博上找到女朋友，可事实上作为一个程序员的我，即使整天投入精力在编程上，依然无法避免对象空引用的异常出现。如果说找到女朋友是个小概率事件，那么在我看来，找到一个真正懂我、喜欢我的女朋友，基本上是不可能事件。你不要觉得我对没有调整好心态、对生活过分悲观，如果你了解贝叶斯公式就会真正地理解我说的话。这个微博开始引起我的注意，是我发现身高在 155 到 165 左右的女生，对男生的要求基本上无疑例外地是 175+到 180+，我想知道到底有多少女生是有这样的想法，这是我想要抓取新浪微博的数据进行分析的初衷。更重要的是，身高不到 175 的我在面对这种要求的时候是悲伤的，因为我想起了《巴黎圣母院》中的卡西莫多，一个外表丑陋而拥有高尚人格的“丑八怪”。现代人整天都特别忙碌，以至于没有人会有耐心，园艺在忍受着你丑陋的外表的同时，同你讲一只小兔子亲了它喜欢的长颈鹿一下这种故事。
我听到这样一句话，“好看的皮囊千篇一律，有趣的灵魂万里挑一”，可谁会觉得像卡西莫夫这样的人，会拥有或者配拥有高尚的人格呢？我们这副皮囊不管好看与否，它们都是父母给予我们的最好的礼物。难道一个所谓情商高的人，会在收到别人的时候因为礼物不好看而生气吗？ 我想起《画心》里懊悔受狐妖小唯皮相蛊惑而自毁双目的霍心，美丑都是父母赐予我们的，不该被我们拿来一番大肆炫耀，可我还是想知道，我们评价一个人的标准到底是什么？因为我渐渐明白，有些人不喜欢我们，并不是我们不好，而仅仅是某一点和对方不匹配。喜欢一个人的时候，像拔下身上的一根根刺，因为你越是得不到回应，就越像变成对方期待的样子，这个过程会让你觉得自己一无是处。直到今天看到一句话，一句足以热泪盈眶的话，如果不曾喜欢你，我本来非常可爱的。有时候，人做一件事情，或许就是在和自己过不去，比如说这件事情。
花点时间爬爬微博 好了，现在我们来考虑从新浪微博上抓取@西安月老牵线上发布的微博，因为这是我们进行数据分析的前提。事实上，在写这篇文章前我曾花了大量时间来调试爬虫，然后用了一天的时间对数据进行清洗，最终利用晚上下班的时间生成词云。由此我们可以理出整体的思路：
流程图通过流程图我们可以注意到，在这里我选择了 Python 来实现整个功能。转眼间我已经 25 岁了，这是种什么样的概念呢？两年前我 23 岁的时候，听别人讨论结婚这个问题，我觉得它离我还很遥远。如今看着周围人都结婚了，我竟有种“高处不胜寒”的感觉。所以呢，人生苦短，当你不能阻止时间一天天消逝的时候，你只能趁着现在去做你想做的事情，为了节省时间去做技术以外的尝试，我选择拥有全世界最丰富的库的 Python。
这段时间学习数据分析，我渐渐意识到我们所熟悉的这个世界，如果以一种理性的角度，完全通过数据来解构的话，我们在这个数字时代里留下的每一条讯息，都冷冰冰地暴露着我们的喜怒哀乐，每一张照片里细微的表情变化，每一段文字里隐匿着的真实意图，都能被人脸识别和自然语言处理等等，这类人工智能为代表的技术所解读，我们努力想在朋友圈里隐藏些什么，当朋友圈的访问范围从半年逐渐缩小到三天，我们究竟能隐藏下什么呢？
微博爬虫分析 首先，我们需要从微博上抓取数据下来，我没有去做抓包分析这样的重复性工作，因为我注意到这个问题，在网络上有很多朋友在讨论，我主要参考了以下内容：
Python 爬虫如何机器登录新浪微博并抓取内容？ https://github.com/xchaoinfo/fuck-login 用 Python 写一个简单的微博爬虫 通过以上内容，我了解到在抓取新浪微博数据的问题上，我们基本会有以下思路：
保存 cookie 信息，利用 requests 库发起请求并带上 cookie 利用 requests 库模拟登录新浪微博并在请求过程中保持 cookie 利用 selenium 库模拟登录新浪微博然后取得页面内容 利用 PhantomJS 库模拟登录新浪微博然后取得页面内容 可以看出差异主要集中在 cookie 的获取以及是否支持 headless 模式，并且我们得到一个共识，抓取新浪微博移动版要比PC 版要容易，因为移动版优先为小尺寸屏幕设备提供服务，因而页面结构相对整洁便于我们提取数据。起初博主认为第一种方式太简单粗暴，坚持要采用第二种方式去实现，最终证明还是太年轻了啊，新浪微博的登录给人的感觉就是蛋疼，这里就简单介绍下思路哈。
首先我们会向服务器发出一次 GET 请求，它返回的结果是一段 JavaScript 代码，然后我们需要用正则匹配出其中的 JSON 字符，这样我们就获得了第二次请求需要用到的参数；接下来，第二次请求是一个 POST 请求，我们需要将用户名采用 Base64 加密，密码则采用 RSA 加密，需要用到第一次请求返回的参数。实际上，新浪微博官方给我们提供 API 获取微博数据，可这个 API 可以获取的微博数据非常有限，更让人难以接受的是新浪微博的应用授权方式，如果我们采用调用 API 的方式，在这里会有第三次 POST 请求，有朋友分析了完整的模拟登录过程，可我对此表示毫无兴趣啊。最早我采用了模拟这种方式，抓取第一页的时候还是登录的状态，可等到抓取第二页的时候变成了注销的状态，整个过程使用的是同一个 session 对象，所以我最后果断放弃了这种方式。</description></item></channel></rss>