[{"categories":null,"content":"很高兴你能来到元视角！这里不仅是一个分享技术见解的平台，更是一个记录生活点滴、交流思想的温暖角落。\n每一次的交流都是一次成长的机会。如果你觉得这里的某些内容对你有所启发，或是发现有任何可以改进的地方，都请不吝赐教。你的每一个建议，都将帮助这个网站变得更好。\n技术问题？生活感悟？或是对未来的思考？不妨在下方留言区分享你的想法。这里的每一条留言，都是我们共同进步的见证。\n期待与你展开一场愉快的对话！\n","date":"2025-04-06T00:00:00Z","image":null,"permalink":"https://qinyuanpei.github.io/comments/","slug":"comments","tags":null,"title":"留言"},{"categories":["编程语言"],"content":"时光飞逝，转眼间已步入阳春三月，可我却迟迟未曾动笔写下 2025 年的第一篇 AI 博客。不知大家心中作何感想，从年初 DeepSeek 的爆火出圈，到近期 Manus 的刷屏热议，AI 领域的发展可谓是日新月异。例如，DeepSeek R1 的出现，让人们开始接受慢思考，可我们同样注意到，OpenAI 的 Deep Research 选择了一条和 R1 截然不同的路线，模型与智能体之间的界限开始变得模糊。对于这一点，使用过 Cursor Composer 或者 Deep Research 的朋友，相信你们会有更深刻的感悟。有人说，Agent 会成为 2025 年的 AI 主旋律。我不知道大家是否清楚 AutoGPT 与 Manus 的差别，对我个人而言，最重要的事情是在喧嚣过后找到 “值得亲手去做的事情”。所以，今天这篇博客，我想分享一个 “熟悉而陌生” 的东西：MCP，即：模型上下文协议，并尝试将这个协议和 Semantic Kernel 连接起来。\nMCP 介绍 [TL;DR] MCP 是由 Anthropic 设计的开放协议，其定位类似于 AI 领域的 USB 接口，旨在通过统一接口解决大模型连接不同数据源和工具的问题。该协议通过 JSON-RPC 规范定义了 Prompt 管理、资源访问和工具调用三大核心能力，使得任何支持 Function Calling 的模型都能无缝对接外部系统，从而帮助大语言模型实现 “万物互联”。\n什么是 MCP? MCP（Model Context Protocol）是由 Anthropic 设计的一种开放协议，旨在标准化应用程序向大语言模型（LLMs）提供上下文的方式，使大模型能够以统一的方法连接各种数据源和工具。你可以将其理解为 AI 应用的 USB 接口，为 AI 模型连接到不同的数据源和工具提供了标准化的方法。架构设计上，MCP 采用了经典的 C/S 架构，客户端可以使用该协议灵活地连接多个 MCP Server，从而获取丰富的数据和功能支持，如下图所示：\nMCP 基本架构\r具体而言，MCP 架构中包括四个核心角色：\nMCP Host：承载用户交互的终端，如 Claude Desktop、Cusror、VSCode 等，负责发起请求 MCP Client：协议客户端，负责建立、维护与服务器端的一对一连接，通常需要集成 SDK 到 MCP Host MCP Server: 协议服务器端，对外暴露三种核心能力：Prompts、Resources 和 Tools Data Source：数据源，是本地资源（如 SQLite、文件系统）与远程服务（如 Github API）的集合 为什么选择 MCP? 在过去的这一年里，AI 智能体的技术生态逐渐呈现出两种典型的演进方向。首先，是以 LangChain、Semantic Kernel 等为代表的 AI 框架；其次，是以 Dify、Coze 等为代表的智能体编排平台。这实际上揭示了当前智能体技术发展的双重路径，即：人们正试图从框架层和平台层两个维度去攻克 Agent 技术的高峰。\n为什么选择 MCP?\r然而，当你真正地深入实践这一切的时候，你会在这些框架和平台中发现许多痛点。例如：\n语言框架的割裂性：不同技术栈中对 Agent 基础元素的定义存在着根本性差异。例如，LangChain 中采用 Python 的 @tool 装饰器来标注工具方法，而 C# 系列的 Semantic Kernel 则通过 [KernelFunction] 特性来实现功能注册。这种语法层面的分歧，无形中增加了跨平台协作的成本。 平台生态的封闭性：以 Coze 和 Dify 的插件系统为例，虽然二者均支持集成 Jina AI 插件，但是其工作流编排和配置的规范完全不同。这种生态壁垒加剧了不同技术体系间的 “数字鸿沟”，造成应用迁移成本过高，最终导致智能体平台沦为信息孤岛。 开发资源的重复消耗：目前，无论是服务供应商还是开发者，均需要参与智能体平台的适配工作，容易造成重复性工作，这对于 AI 时代而言是一种注意力的浪费。更重要是，这不利于 AI 技术的进一步发展，真正具有突破性的技术创新难以获得足够关注。 如你所见，有了 MCP 以后，开发人员只需要和 MCP 打交道，这是真正意义上的 “Attention Is All You Need”。\nMCP 如何工作? 现在，当我们将目光聚焦在 MCP 上面时，我们会发现情况开始有所好转，因为 Anthropic 使用 JSON-RPC 规范定义了一套与语言、平台无关的协议。在该协议中，定义了 Requests、Responses 和 Notifications 三种消息类型：\nType Description Requirements Requests Messages sent to initiate an operation Must include unique ID and method name Responses Messages sent in reply to requests Must include same ID as request Notifications One-way messages with no reply Must not include an ID 在上文中我们提到，MCP 支持 Prompts、Resources 和 Tools 三大核心能力。以 Tools 这个最常见的能力为例，MCP 支持工具的发现、调用和更新，其交互过程通常如下图所示：\nMCP-Tools 能力示意图\r此时，我们会注意到，MCP 针对工具调用主要提供了三个 API：tools/list、tools/call 以及 notifications/tools/list_changed。其中，notifications/tools/list_changed 是可选的，属于 Notification 的一部分。顾名思义，当服务器端提供的工具列表发生变化时，它能够以通知的形式告知客户端这一变化。如果你熟悉 JSON-RPC 规范，相信你已经在脑海中推测出具体的消息结构。首先，客户端通过 tools/list 方法向服务器端发起请求：\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 1, \u0026#34;method\u0026#34;: \u0026#34;tools/list\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;cursor\u0026#34;: \u0026#34;optional-cursor-value\u0026#34; } } 接下来，服务器端会返回它目前支持的工具列表。这里，我们以经典的 get_weather 方法为例：\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 1, \u0026#34;result\u0026#34;: { \u0026#34;tools\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;get current weather information for a location\u0026#34;, \u0026#34;inputSchema\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;city name or zip code\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;] } }], \u0026#34;nextCursor\u0026#34;: \u0026#34;next-page-cursor\u0026#34; } } 如果你接触过 ReAct、Tool Use、Function Calling 这些概念，你会发现这一切是如此地熟悉和亲切。当我们将这些工具提供给 LLM 以后，由 LLM 决定是否要调用指定的工具。此时，我们可以通过 tools/call 方法来调用指定的工具。这里，同样以经典的 get_weather 方法为例：\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 2, \u0026#34;method\u0026#34;: \u0026#34;tools/call\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;arguments\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;New York\u0026#34; } } } 此时，我们会收到服务器端的响应消息，如下所示：\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 2, \u0026#34;result\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Current weather in New York:\\nTemperature: 72°F\\nConditions: Partly cloudy\u0026#34; } ], \u0026#34;isError\u0026#34;: false } } 读到这里，诸位看官心里一定在吐槽：有没有搞错，就这？这好像和 Function Calling 没什么区别嘛！我的理解是，MCP、Function Calling 和 Agent 本质上是一个层层递进的关系，MCP 提供一种与模型、语言、框架无关的工具抽象，任何支持 Function Calling 的模型都可以调用这些工具，而 Agent 框架则在此基础上对工具进行规划与编排。\nMCP、Function Calling 和 Agent 三者间的联系\r例如，去年年底的时候，Anthropic 和智谱相继发布了 Cumputer Use 功能，可这些功能大多都仅限于在厂商自家的产品中使用。如果你想在国内的 DeekSeek 或者 Kimi 上面尝试，基本上是痴心妄想。可有了 MCP 以后，情况就大不相同。你只需要使用 Playwright MCP Server 或者 Browser-Use MCP Server 便可以轻松 “尝鲜”。这次 Manus 爆火后，社区在几个小时内迅速复刻出了OpenManus，这与该团队直接使用第三方库 browser-use 息息相关。由此可见，一个健康、开放的 AI 生态会极大地促进 AI 应用的繁荣。事实上，自去年 MCP 发布以来，社区里涌现出了大量的第三方 MCP 服务器，这些服务器极大地扩展了 AI 的能力边界。现在，AI 可以连接到 Notion、Slack、Github、Elasticsearch 等众多平台，如下图所示，mcpservers.org、mcp.so 等网站收录了许多 MCP Server：\nAwesome MCP Servers\r所以，我们为什么要了解 MCP 呢？因为只要接入了 MCP， 便可以拥抱 MCP 背后的整个生态，这意味着 AI 领域的 “万物互联” 时刻已悄然到来。唯一的问题在于，国内外的 AI 厂商是否有意愿一起将 MCP 发展为行业标准。我想，届时无论是服务供应商还是个人开发者，都能从 MCP 这个协议中受益。除了 Tools，MCP 还支持 Resources 和 Prompts 相关的功能，它们负责对提示词、文件等进行管理。当然，这些并不是本文关注的重点，这里不再赘述。我们只需要知道一件事情，对一个 MCP Server 而言，最重要的是实现 tools/list 和 tools/call 这两个方法。目前，官方 SDK 支持 Python、TypeScript、Java 和 Kotlin 这四种语言，我们可以使用这些 SDK 来集成或者开发一个 MCP Server。下面是一个 Python 版本的 SQLite Explorer 示例：\nfrom mcp.server.fastmcp import FastMCP import sqlite3 mcp = FastMCP(\u0026#34;SQLite Explorer\u0026#34;) @mcp.resource(\u0026#34;schema://main\u0026#34;) def get_schema() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Provide the database schema as a resource\u0026#34;\u0026#34;\u0026#34; conn = sqlite3.connect(\u0026#34;database.db\u0026#34;) schema = conn.execute( \u0026#34;SELECT sql FROM sqlite_master WHERE type=\u0026#39;table\u0026#39;\u0026#34; ).fetchall() return \u0026#34;\\n\u0026#34;.join(sql[0] for sql in schema if sql[0]) @mcp.tool() def query_data(sql: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Execute SQL queries safely\u0026#34;\u0026#34;\u0026#34; conn = sqlite3.connect(\u0026#34;database.db\u0026#34;) try: result = conn.execute(sql).fetchall() return \u0026#34;\\n\u0026#34;.join(str(row) for row in result) except Exception as e: return f\u0026#34;Error: {str(e)}\u0026#34; 如你所见，在该示例中，MCP Server 提供了一个 resource、一个 tool，前者负责返回当前数据库中的 DDL，后者提供一个查询数据的方法。恭喜你，现在你可以开始着手设计一个针对 Text2SQL 的 Agent 了。\nMCP 的局限性 当然，我们需要学会辩证地看待事物，MCP 并非完美无瑕。首先，我们不清楚国内外厂商适配这一协议的热情到底有多少；其次，类似于大多数 AI 框架，MCP 正处在迅速发展阶段，该协议的最新版本是 2024-11-05。截止目前，官方在 2025 年上半年的 Roadmap 主要集中在：认证/授权、服务发现、无状态操作。所以，未来走向到底如何，着实充满了变数。例如，按照官方的设计，MCP 在传输层（Transports）支持 stdio 和 HTTP with Server-Sent Events (SSE)，可目前大多数的 MCP Server 都是运行在本地的 stdio。对于终端用户而言，使用 MCP 依然需要了解 Python、Node.js 甚至 Docker，不得不说，这其实是一种隐形的成本。\nSemantic Kernel x MCP Semantic Kernel 集成 MCP 流程示意图\r在 Semantic Kernel 中，我们使用插件（Plugin）这个概念来描述一组工具，而每个工具则是一个 KernelFunction。因此，如果希望在 Semantic Kernel 中集成 MCP，本质上就是将 MCP 中的 Tools 转换为 Semantic Kernel 中的 KernelFunction。如上图所示，我们将在 Semantic Kernel 中集成一个 MCP 客户端，然后利用 tools/list 和 tools/call 这两个 API 分别实现工具获取、工具调用这两个流程。\n工具获取 截止目前，MCP 官方还没有提供对 .NET 的支持，不过社区里还是出现了第三方实现。例如：\nMCPSharp: https://github.com/afrise/MCPSharp mcpdotnet: https://github.com/PederHP/mcpdotnet 博主这里选择的是 mcpdotnet，假设我们希望在 Semantic Kernel 中集成 Playwright MCP Server。此时，我们可以编写下面的代码来连接到对应的 MCP Server：\nvar clientOptions = new McpClientOptions() { ClientInfo = new McpDotNet.Protocol.Types.Implementation() { Name = name, Version = \u0026#34;1.0.0\u0026#34; }, }; var serverConfig = new McpServerConfig() { Id = \u0026#34;playwright\u0026#34;, Name = \u0026#34;playwright\u0026#34;, TransportType = \u0026#34;stdio\u0026#34;, TransportOptions = new Dictionary\u0026lt;string, string\u0026gt; { [\u0026#34;command\u0026#34;] = \u0026#34;npx\u0026#34;, [\u0026#34;arguments\u0026#34;] = \u0026#34;-y @executeautomation/playwright-mcp-server\u0026#34;, } }; var loggerFactory = kernel.Services.GetRequiredService\u0026lt;ILoggerFactory\u0026gt;(); var clientFactory = new McpClientFactory( [serverConfig], clientOptions, NullLoggerFactory.Instance ); var client = await clientFactory.GetClientAsync(serverConfig.Id).ConfigureAwait(false); 从 clientFactory 获取 IMcpClient 实例时，客户端会先调用 initialize() 方法。服务器端初始化完成后，会给客户端发送 notifications/initialized 通知，表明服务器端已完成初始化。此时，可调用 ListToolsAsync() 方法，获取 MCP 服务器端提供的工具列表:\nvar listToolsResult = await client.ListToolsAsync().ConfigureAwait(false); var tools = listToolsResult.Tools; 协议转换 从前文中可知，MCP 使用 JSONSchema 来描述工具的输入参数，返回值则被定义为一个数组，如下图所示：\nMCP 工具调用输入 \u0026amp;amp; 输出\r因此，我们需要写一个中间层，将 MCP 的工具转换为 KernelFunction，这部分内容非常简单，不再赘述：\n// 将 MCP 中的 Tool 转换为 KernelFunction private static KernelFunction ToKernelFunction(this Tool tool, IMcpClient client) { async Task\u0026lt;string\u0026gt; InvokeToolAsync( Kernel kernel, KernelFunction function, KernelArguments arguments, CancellationToken cancellationToken ) { try { var mcpArguments = new Dictionary\u0026lt;string, object\u0026gt;(); foreach (var arg in arguments) { if (arg.Value is not null) mcpArguments[arg.Key] = function.ToArgumentValue(arg.Key, arg.Value); } var result = await client.CallToolAsync( tool.Name, mcpArguments, cancellationToken: cancellationToken ).ConfigureAwait(false); return string.Join(\u0026#34;\\n\u0026#34;, result.Content .Where(c =\u0026gt; c.Type == \u0026#34;text\u0026#34;) .Select(c =\u0026gt; c.Text)); } catch { throw; } } return KernelFunctionFactory.CreateFromMethod( method: InvokeToolAsync, functionName: tool.Name, description: tool.Description, parameters: ToKernelParameters(tool), returnParameter: ToKernelReturnParameter() ); } // 将 MCP 中工具的输入转换为 KernelFunction 输入 private static List\u0026lt;KernelParameterMetadata\u0026gt; ToKernelParameters(Tool tool) { var inputSchema = tool.InputSchema; var properties = inputSchema?.Properties; if (properties == null) return []; HashSet\u0026lt;string\u0026gt; requiredProperties = new(inputSchema!.Required ?? []); return properties.Select(kvp =\u0026gt; new KernelParameterMetadata(kvp.Key) { Description = kvp.Value.Description, ParameterType = ConvertParameterDataType(kvp.Value, requiredProperties.Contains(kvp.Key)), IsRequired = requiredProperties.Contains(kvp.Key) }) .ToList(); } // 将 JSONSchema 中的数据类型转换为 C# 的数据类型 private static Type ConvertParameterDataType(JsonSchemaProperty property, bool required) { var type = property.Type switch { \u0026#34;string\u0026#34; =\u0026gt; typeof(string), \u0026#34;integer\u0026#34; =\u0026gt; typeof(int), \u0026#34;number\u0026#34; =\u0026gt; typeof(double), \u0026#34;boolean\u0026#34; =\u0026gt; typeof(bool), \u0026#34;array\u0026#34; =\u0026gt; typeof(List\u0026lt;string\u0026gt;), \u0026#34;object\u0026#34; =\u0026gt; typeof(Dictionary\u0026lt;string, object\u0026gt;), _ =\u0026gt; typeof(object) }; return !required \u0026amp;\u0026amp; type.IsValueType ? typeof(Nullable\u0026lt;\u0026gt;).MakeGenericType(type) : type; } // 转换返回值，简化处理，直接返回字符串类型 private static KernelReturnParameterMetadata? ToKernelReturnParameter() { return new KernelReturnParameterMetadata() { ParameterType = typeof(string), }; } // 将 KernelFunction 参数转换为 object private static object ToArgumentValue(this KernelFunction function, string name, object value) { var parameter = function.Metadata.Parameters.FirstOrDefault(p =\u0026gt; p.Name == name); return parameter?.ParameterType switch { Type t when Nullable.GetUnderlyingType(t) == typeof(int) =\u0026gt; Convert.ToInt32(value), Type t when Nullable.GetUnderlyingType(t) == typeof(double) =\u0026gt; Convert.ToDouble(value), Type t when Nullable.GetUnderlyingType(t) == typeof(bool) =\u0026gt; Convert.ToBoolean(value), Type t when t == typeof(List\u0026lt;string\u0026gt;) =\u0026gt; (value as IEnumerable\u0026lt;object\u0026gt;)?.ToList(), Type t when t == typeof(Dictionary\u0026lt;string, object\u0026gt;) =\u0026gt; (value as Dictionary\u0026lt;string, object\u0026gt;)?.ToDictionary(kvp =\u0026gt; kvp.Key, kvp =\u0026gt; kvp.Value), _ =\u0026gt; value, } ?? value; } 现在，一切就变得简单了，我们可以封装一个如下的扩展方法：\npublic static async Task\u0026lt;IEnumerable\u0026lt;KernelFunction\u0026gt;\u0026gt; GetKernelFunctionsAsync(this IMcpClient client) { var listToolsResult = await client.ListToolsAsync().ConfigureAwait(false); return listToolsResult.Tools.Select(tool =\u0026gt; ToKernelFunction(tool, client)).ToList(); } 工具调用 在 MCP 中，客户端调用服务器端提供的工具，可以直接使用 CallToolAsync() 方法：\nvar result = await client.CallToolAsync( tool.Name, mcpArguments, cancellationToken: cancellationToken ).ConfigureAwait(false); 当我们转换为 KernelFunction 以后，只需要调用 InvokeAsync() 方法即可调用对应的插件函数。考虑到，在 Agent 中，插件函数通常是由 LLM 来调用的，我们将编写下面的扩展方法来实现工具的注册：\n// 注册 MCP Server public static async Task AddMCPServer( this Kernel kernel, string name, string command, string version = \u0026#34;1.0.0\u0026#34;, string[] args = null, Dictionary\u0026lt;string, string\u0026gt; env = null ) { var clientOptions = new McpClientOptions() { ClientInfo = new McpDotNet.Protocol.Types.Implementation() { Name = name, Version = \u0026#34;1.0.0\u0026#34; }, }; var serverConfig = new McpServerConfig() { Id = name, Name = name, TransportType = \u0026#34;stdio\u0026#34;, TransportOptions = new Dictionary\u0026lt;string, string\u0026gt; { [\u0026#34;command\u0026#34;] = command, [\u0026#34;arguments\u0026#34;] = string.Join(\u0026#39; \u0026#39;, args ?? []), } }; var loggerFactory = kernel.Services.GetRequiredService\u0026lt;ILoggerFactory\u0026gt;(); var clientFactory = new McpClientFactory([serverConfig], clientOptions, loggerFactory); var client = await clientFactory.GetClientAsync(serverConfig.Id).ConfigureAwait(false); var kernelFunctions = await client.GetKernelFunctionsAsync(); kernel.Plugins.AddFromFunctions(name, kernelFunctions); } 至此，我们便完成了 MCP 在 Semantic Kernel 中的集成。现在，你只需要使用下面的代码片段即可：\n// 添加 playwright-mcp-server await kernel.AddMCPServer( name: \u0026#34;playwright\u0026#34;, command: \u0026#34;npx\u0026#34;, version: \u0026#34;1.0.0\u0026#34;, args: [\u0026#34;-y\u0026#34;, \u0026#34;@executeautomation/playwright-mcp-server\u0026#34;], env: null ); 场景化效果展示 好的，当我们给 Semantic Kernel 集成 MCP 以后，现在我们来一起看看它具体能帮我们做什么事情？\n操作浏览器 如图所示，用户请求：打开 Bing 主页，搜索 \u0026ldquo;Model Context Protocol\u0026rdquo;\nMCP-操作浏览器-A\rMCP-操作浏览器-B\r访问文件系统 如图所示，用户请求：我在 D:\\Projects\\2024 这个 Git 仓库中都提交过那些代码，最近的一次的更新是什么？\nMCP-访问文件、读取 Git 提交记录\r读取 Github 仓库 如图所示，用户请求：阅读 OpenManus 仓库的代码，帮我分析其架构、设计相关的细节\nMCP-读取 Github 仓库\r博主先后体验了四个 MCP Server。其中，Knowledge Graph Memory Server 可以在本地构建知识图谱，从对话中提取并持久化三元组，实现长期记忆。当然，这里的关键在于，确定三元组的读写时机。今年，我准备将现有 RAG 和 Agent 融合，升级为 Agentic RAG。目前，考虑的是将 ReAct 模式应用于 RAG，显然，这里同样会遇到一个问题，即： LLM 如何判定上下文足以生成最终答案。因篇幅限制，此处不再展示更多截图，一切都需要大家去亲自体验。\n本文小结 MCP 是 Anthropic 设计的开放协议，其定位类似于 AI 领域的 USB 接口，希望通过统一接口解决大模型连接不同数据源和工具的问题。Semantic Kernel 是微软开源的 Agent 框架，两者的结合可以让 .NET 开发者快速、高效地接入社区中的 MCP 服务器，减少重复性的平台对接工作。除工具调用外，还可以考虑将项目中的提示词模板统一放置到 MCP Server 上管理。博主撰写此文时，网络正在传播着被破解的 Manus 源代码。虽然经常有人说提示工程已不存在了，可在实际的项目中提示词依旧不可或缺。从这个角度来看，尽管提示词技术含量不高，但若能得到妥善管理，至少会比项目被破解、提示词被泄露更显体面。关于更多 MCP 的细节，请参考官方文档：Introduction - Model Context Protocol ，无论你是开发者还是普通用户，相信都能在那里找到答案。\n参考链接 Model Context Protocol Model Context Protocol Specification LLM Function-Calling vs. Model Context Protocol (MCP) Integrating Model Context Protocol Tools with Semantic Kernel: A Step-by-Step Guide 什么是模型上下文协议（MCP）？它如何比传统API更简单地集成AI？ ","date":"2025-03-09T20:42:23Z","image":"/posts/semantic-kernel-mcp-agent-context-enhanced-exploration/MCP_Overview.png","permalink":"https://qinyuanpei.github.io/posts/semantic-kernel-mcp-agent-context-enhanced-exploration/","slug":"Semantic-Kernel-MCP-Agent-Context-Enhanced-Exploration","tags":["MCP","Agent","Semantic Kernel","Function Calling"],"title":"Semantic Kernel × MCP：智能体的上下文增强探索"},{"categories":["生活感悟"],"content":"\r在金庸先生的《射雕英雄传》中，有一个情节令人难以忘怀。一向被视为离经叛道的“东邪”黄药师，在得知女儿黄蓉对郭靖情根深种时，亦不免喟然长叹：“且夫天地为炉兮，造化为工；阴阳为炭兮，万物为铜”。贾谊的郁郁而终，靖蓉的长岭遇雨，在遥远的历史的长河中千回百转，可谓“不谋而遐迩自同”。或许，无论是事业与爱情，其底色都难免带着些许苦涩，否则，天地何以成为熔炉，让万物在其中备受煎熬。当然，今天我想聊的不是金庸，而是春节档票房冠军《哪吒之魔童闹海》。这部改编自中国古代神话的动画电影，融合传统神话与现代叙事，在冰与火交织的冲突下，在大银幕上演绎了一部普罗米修斯式的抗争史，带给观众一场视觉与心灵的双重震撼。\n当哪吒不再困扰于魔丸身份\r从身份认同到体系反思 如果说，《哪吒》系列第一部《魔童降世》的核心冲突源于身份认同，那么，这一次《魔童闹海》开始将视野拓展至对体系与秩序的思考。帕拉图曾提出“人生三问”——我是谁？我从哪里来？我要到哪里去？哪吒曾为“魔丸”和“灵珠”的身份困扰，然而，在其父母的影响下，他坚定地喊出：“我命由我不由天”，是魔是仙要由自己说了算，表达对命运的不屈抗争。他完全不在意世俗的眼光，毅然决然地等待天雷落下。如果用一句话概括：“若命运不公，就和它斗到底”，我个人认为，这句话是《哪吒》系列最好的注脚。剧情上，这部作品与前作一脉相承，哪吒和敖丙在天劫中大难不死，虽然肉身毁灭，可魂魄得以在七色宝莲中保全。当然，这一切的代价是太乙真人被夺去顶上三花。\n闭嘴！你这只愚蠢的土拨鼠\r因此，第二部的故事便围绕着“劫后余生”展开。因为哪吒和敖丙的肉身损坏，所以，太乙真人借助七色宝莲和藕粉为二人重塑身体。然而，此时申公豹借四海龙王之力，引海底妖族攻入陈塘关。敖丙为护陈塘关擅自移动身体，导致塑造的肉身再次损坏。适逢七色宝莲灵力枯竭，为了得到能让七色宝莲再次盛开的玉液琼浆，哪吒和敖丙被迫共用一个身体，前往昆仑山玉虚宫升仙考核。这种剧情上的转折、递进，节奏紧凑，扣人心弦，可以说是全程无尿点。重塑身体与升仙考核两大情节，贡献了整部电影的大部分的笑点，特别是哪吒塑型这一步，简直就是现实中甲方乙方的真实写照。影片中中真正深刻的东西，我认为，直到陈塘关被屠城这一刻才开始逐渐呈现出来。\n命运熔炉中的权力游戏 教员有首词叫做《贺新郎·读史》，其中有一句“铜铁炉中翻火焰，为问何时猜得”。众所周知，铜器时代和铁器时代是人类历史上的两个重要阶段，它们标志着人类在金属加工技术、社会结构和文明进步方面的巨大飞跃。结合第一部的剧情，哪吒的宿命，原本就开始于一场精心设计的“冶金游戏”，元始天尊将混元珠投入天元鼎，试图在一个相生相克的世界中提炼出永恒秩序。自此，被标记为“魔丸”的哪吒一生下来，便承受着世俗偏见带来“先天原罪”。如申公豹所言，“人心中的成见是一座大山”。龙族接受天庭“招安”，负责为其镇压海底妖兽，可这处名为“龙宫”的所在，又何尝不是关押龙族的“地牢”呢？因此，敖丙从一出生就被赋予了振兴龙族的使命，那件万龙甲便是最好的证明。\n无量仙翁开始露出狐狸尾巴\r电影中的偏见不止于此，当两只结界兽赶到玉虚宫报信时，有多少人想当然地认为，是申公豹屠戮了陈塘关的百姓呢？这同样是一种偏见。如果说，土坡鼠、申正道和石矶娘娘这三道试题，只是让你隐隐对阐教的正义性产生了一丝怀疑。那么，当巨大的天元鼎垂直插入东海海底的时候，你是否从无量仙翁那张道貌岸然的脸上，读出了某种献祭的意味呢？因为妖族修行比人类更勤勉，哪怕它们没做过坏事；因为龙族是实力强大的妖族，哪怕它们早已归顺；因为要嫁祸龙族而大举屠城，哪怕陈塘关的百姓无辜冤死。《贺新郎》里写道：“人世难逢开口笑，上疆场彼此弯弓月。流遍了，郊原血”。回顾历史，宏大叙事下的“电车难题”持续上演，十字军东征时的“上帝旨意”、殖民主义者的“文明开化”……，无一不是在用别人的血肉浇筑“封神”的圣坛。《封神演义》中阐截二教，到底孰是孰非呢？\n自由与秩序的永恒斗争 诚然，电影依旧没能突破束缚，即：“高层隐身、中层搞事、底层遭殃”的叙事套路，甚至我们无从得知，无量仙翁的行为，到底是私心作祟下的自作主张，还是元始天尊的暗中授意。可这种“个体失范代替制度失范”的切入点，还是能给观众带来某种思考：从来如此，便对吗？善与恶，是魔是仙，从来都不应该由种族和身份决定，申公豹有自己的坚守与珍视，阐教有在封神大战前壮大实力的私心，敖光有保护龙族而委曲求全的无奈，在体系框架内循规蹈矩固然重要，可当个体困境变成群体困境、退无可退的时候，或许，跳出陈规、尝试打破规则，一切自然会柳暗花明、迎来转机。当哪吒的肉身在三昧真火中完成重塑、当人族和妖族齐心协力打破天元鼎的桎梏、当定海神针在落日下缓缓浮出海面……我想，哪吒撕裂的或许是人类文明进程中的永恒困境：在命运熔炉的烈焰中，我们到底是等待煅烧的铜铁，还是执掌锤柄的工匠？“怀璧其罪”、“欲加之罪”，本质上都是一种群体对个体道德上的绑架和霸凌。\n哪吒浴火重生\r古希腊哲人赫拉克利特曾说过：“世界是一团永恒的活火”。而哪吒的选择则昭示着：与其在烈焰中化为灰烬，不如让火焰成为涅槃的锤砧。我们无需过渡解读，说昆仑山的玉虚宫像白宫、天元鼎上的符文像美元、成仙的玉牌像绿卡、压住丹炉的像棵摇钱树，甚至无量仙翁、鹿童、鹤童比妖怪更像妖怪……当哪吒与敖丙联手击败无量仙翁，一切的建筑都灰飞烟灭，那个定义神仙妖魔的符号体系，更是被彻底解构、直至崩塌。在这部电影中，所有的海底妖兽都被用锁链束缚在定海神针上面。或许，只需阐教的一道咒语，便能解开这些枷锁。可要解开人心的锁链，显然要艰难百倍。对哪吒而言，是不再纠结于魔丸的身份；对敖丙而言，是不再背负振兴家族的使命。加缪在《反抗者》中写道：“我反抗，故我们存在”。影片最后，妖族与阐教的决战，难道不比《复仇者联盟》更燃？我相信，如果贾谊看过这部电影，一定不会如此草率地写下“天地为炉”。因为在自由面前，秩序终将被反抗者打破。\n","date":"2025-02-06T11:17:23Z","image":"/posts/ne-zha-2/P2917853848.jpg","permalink":"https://qinyuanpei.github.io/posts/ne-zha-2/","slug":"Ne-Zha-2","tags":["影评","哪吒","封神","动漫"],"title":"命运、偏见与自由：《魔童之哪吒闹海》的终极抗争"},{"categories":["编程语言"],"content":" 注：本文在创作过程中得到了 ChatGPT、DeepSeek、Kimi 的智能辅助支持，由作者本人完成最终审阅。\n在 “视频是不能 P 的” 系列文章中，博主曾先后分享过人脸检测、人脸识别等相关主题的内容。今天，博主想和大家讨论的是人脸分类问题。你是否曾在人群中认错人，或是盯着熟人的照片却一时想不出对方的名字？这种 “脸盲症” 的困扰，不仅在生活中令人感到尴尬，在整理照片时更是让人头疼不已。想象一下，某次聚会结束后，你的手机里存了上百张照片——有你的笑脸、朋友的自拍，甚至还有一部分陌生面孔混杂其中。手动将这些照片按人物分类，不仅费时费力，还可能会因为 “脸盲” 而频繁出错。此时，你是否期待有一种技术，可以像魔法一样，自动将这些照片按人物分类？事实上，这种 “魔法” 已经存在，它的名字叫做 K-Means 聚类分析。作为一种经典的无监督学习算法，K-Means 能够通过分析人脸特征，自动将相似的面孔归类到一起，完全无需人工干预。接下来，为了彻底根治 “脸盲症”，我们将详细介绍如何使用 K-Means 聚类分析来实现这一目标，哈利·波特拥有魔法，而我们则拥有科技。\n实现过程 如图所示，我们将按照下面的流程来达成 “自动分类人脸” 这一目标。其中，Dlib 负责提取人脸特征向量、Scikit-Learn 中的 K-Means 负责聚类分析、Matplotlib 负责结果的可视化：\n基于 K-Means 聚类分析实现人脸照片的快速分类示意图\rK-Means 简介 K-Means 是一种广泛应用的聚类算法，其基本原理是将数据集分成 K 个簇，目标是让每个簇内的数据点尽可能相似，而不同簇之间的数据点尽可能差异明显。K-Means 的执行过程如下：\n随机选取 K 个初始中心点。\n将每个数据点分配到距离最近的中心点所对应的簇。\n更新每个簇的中心点，通常取簇内所有数据点的均值。\n重复步骤 2 和 3，直到中心点不再发生变化或达到预设的最大迭代次数。\n如下图所示，图中展示了四种不同的聚类数据分布情况，按照从左到右、自上而下的顺序：\n图一：簇划分不正确或者簇数量假设错误 图二：数据分布具有各向异性，簇的形状是一个拉长的椭圆形，而不是对称的圆形 图三：各个簇之间的方差不同，绿色簇分布更紧密，而黄色簇分布更稀疏 图四：簇的大小不均匀，黄色簇数据点较少，而紫色簇数据点较多 四种不同的聚类数据分布情况\r因此，适用于 K-Means 的数据通常满足：\n簇是球状且分布均匀 簇的大小相近 簇无明显噪声点或者离群点 数据是各向同性分布 簇的数量已知 数据维度适中 如何确定 K 值 在使用 K-Means 之前，我们需要确定 K 值，即簇的数量。下面是三种常用的确定 K 值的方法：\n肘部法则/手肘图法（Elbow Method）：通过计算不同 K 值下的聚类误差平方和（SSE, Sum of Squared Errors），找到误差下降速度明显减缓的 “拐点”，这个拐点对应的 K 值即为最佳聚类数。 轮廓系数法（Silhouette Coefficient）：通过计算每个数据点的轮廓系数，评估聚类效果，选择轮廓系数最大的 K 值。轮廓系数结合了聚类的紧密度(同一簇内样本的相似度)和分离度(不同簇之间的差异度)。 戴维斯-博尔丁评分（Davies-Bouldin Index）：通过计算每个簇的簇内距离（样本到簇中心的平均距离）与簇间距离（不同簇中心之间的距离）的比值，评估聚类效果，选择评分最低的 K 值。 如图所示，分别展示了不同方法下的最佳 K 值，综合考虑三种评估方案，此处取 K=6：\n肘部法则/手肘图法\r轮廓系数法\r戴维斯-博尔丁评分\r提取人脸特征 为了对人脸照片进行聚类，我们首先需要提取人脸特征。为此，我们使用 Dlib 库，它提供了一个基于深度学习的预训练模型，该模型能够高效地将人脸图像转换为 128 维的特征向量。\nimport dlib import cv2 import numpy as np # 加载人脸检测器和特征提取器 detector = dlib.get_frontal_face_detector() predictor = dlib.shape_predictor(\u0026#34;./models/shape_predictor_68_face_landmarks.dat\u0026#34;) face_model = dlib.face_recognition_model_v1(\u0026#34;./models/dlib_face_recognition_resnet_model_v1.dat\u0026#34;) # 提取人脸特征 def extract_face_features(image_path): img = cv2.imread(image_path) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = detector(gray) if not faces: return None max_face = max(faces, key=lambda rect: rect.width() * rect.height()) shape = predictor(gray, max_face) feature = face_model.compute_face_descriptor(img, shape) return np.array(feature) K-Means 聚类 接下来，在提取了人脸特征值后，我们可以使用 Scikit-Learn 中的 K-Means 方法进行聚类:\nfrom sklearn.cluster import KMeans X, image_paths = load_dataset(dataset_path) scaler = StandardScaler() X = scaler.fit_transform(X) # 使用 K-means 进行聚类 num_clusters = 6 kmeans = KMeans(n_clusters=num_clusters, n_init=50, random_state=42, init=\u0026#39;k-means++\u0026#39;) kmeans.fit(X) # 获取聚类标签 labels = kmeans.labels_ 其中，load_dataset() 方法用于返回特征值和图片路径，因为最终我们需要将这些图片移动到不同的文件夹中，以实现图片分类。下面是该方法的具体实现：\ndef load_dataset(dataset_path): results = [ (features, image_path) for image_name in os.listdir(dataset_path) if (image_path := os.path.join(dataset_path, image_name)) and (features := extract_face_features(image_path)) is not None ] X, image_paths = zip(*results) if results else ([], []) return np.array(X), list(image_paths) 这里，简单说一下各个参数对于 K-Means 的影响：\n使用 StandardScaler 对矩阵 X 进行标准化处理，将每一列的均值调整为 0，标准差调整为 1。标准化后的数据更适合 K-Means 聚类，因为 K-Means 对特征的尺度比较敏感。 参数 n_clusters 取值为 6，表示将数据分为 6 个簇，该值由最佳的 K 值确定。 参数 n_init 取值为 50，表示运行 K-Means 的次数，每次将使用不同的初始质心，确保可以得到最优结果。 参数 random_state 取值为 42，设置随机种子，确保结果可以复现。 参数 init 取值为 k-means++，表示使用 K-Means++ 算法初始化质心，避免陷入局部最优。 更多的参数及细节，请参考官方文档：https://scikit-learn.org.cn/view/108.html#2.3.2.%20K-means\nPCA 降维与可视化 理论上，这一步可以省略。不过，为了更加直观地展示聚类结果，我们可以使用 PCA（Principal Component Analysis，主成分分析）对人脸特征进行降维，将高维特征映射到低维空间。在本文中，我们会将 128 维的特征向量映射到 2 维平面，并通过 Matplotlib 库进行可视化展示，其中的关键代码展示如下：\nfrom sklearn.decomposition import PCA import matplotlib.pyplot as plt from matplotlib.offsetbox import OffsetImage, AnnotationBbox # 使用 PCA 降维到 2 维 pca = PCA(n_components=2) X_reduced = pca.fit_transform(X) # 获取聚类质心 centers = kmeans.cluster_centers_ centers_reduced = pca.transform(centers) # 绘制每个聚类的散点 plt.figure(figsize=(12, 10)) # 对于每个聚类，绘制聚类的点和代表性图片 for cluster in range(num_clusters): cluster_mask = labels == cluster cluster_points = X_reduced[cluster_mask] # 获取降维后的聚类中心坐标 x_center, y_center = centers_reduced[cluster] # 每个聚类的二维坐标 # 绘制聚类中的数据点 plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\u0026#39;Cluster {cluster}\u0026#39;) # 选择每个聚类中最接近聚类中心的图像 cluster_indices = np.where(cluster_mask)[0] distances = np.linalg.norm(X_reduced[cluster_mask] - centers_reduced[cluster], axis=1) closest_image_idx = cluster_indices[np.argmin(distances)] closest_image_path = image_paths[closest_image_idx] # 读取并处理该图片 img = extract_face_rect(closest_image_path) h, w = img.shape[:2] # 设定目标图像大小 target_size = 100 aspect_ratio = w / h # 根据长宽比计算新宽高 if aspect_ratio \u0026gt; 1: new_w = target_size new_h = int(target_size / aspect_ratio) else: new_h = target_size new_w = int(target_size * aspect_ratio) # 调整图片大小 img_resized = cv2.resize(img, (new_w, new_h)) # 在聚类中心绘制代表性图片 imagebox = OffsetImage(img_resized, zoom=0.5) imagebox.image.axes = plt.gca() ab = AnnotationBbox(imagebox, (x_center, y_center),frameon=True,pad=0.5) plt.gca().add_artist(ab) # 绘制聚类中心 plt.scatter(centers_reduced[:, 0], centers_reduced[:, 1], c=\u0026#39;red\u0026#39;, marker=\u0026#39;X\u0026#39;, s=200, label=\u0026#39;CentroIds\u0026#39;) # 标题、标签和图例 plt.title(\u0026#39;K-means Clustering Results (PCA Reduced)\u0026#39;) plt.xlabel(\u0026#39;PCA Dimension 1\u0026#39;) plt.ylabel(\u0026#39;PCA Dimension 2\u0026#39;) plt.legend() plt.grid(True) plt.show() 这段代码看起来非常复杂，它具体做了什么事情呢？实际上，在通过 K-Means 聚类获得分类标签以后，我们开始尝试在每个簇的中心绘制一张人脸图片，这张图片是如何找到的呢？这里我们选取的的是离簇中心最近的那张图片。考虑到，通过 Dlib 提取的人脸特征值都是 128 维的，如果希望将其绘制到二维平面上，就需要通过 PCA 来完成降维。如图所示，通过 PCA 降维后，我们可以将聚类结果可视化。不同颜色代表不同的簇，同一簇内的点表示相似的人脸照片。通过这张图片，相信你会更加直观地理解核心点、边界点、聚类中心、重叠点等概念：\nK-Means 聚类结果可视化\r本文小结 本文尝试探索运用 K-Means 聚类技术对人脸照片进行快速分类的方案。首先，我们介绍了 K-Means 的基本原理及其适用范围，并探讨了如何合理确定簇数 K。随后，博主详细阐述了人脸特征的提取方法、K-Means 聚类的具体实施步骤，以及借助 PCA 降维技术实现数据可视化的流程。这一系列步骤帮助我们高效地对大量人脸照片进行分类。然而，K-Means 并非完美无瑕。它需要预先设定簇数 K，对初始中心点的选择较为敏感，且默认簇为球形或凸形。这导致其在处理非凸形簇或含噪声数据时表现不佳。针对这些局限性，实际应用中可以考虑使用 DBSCAN 等聚类算法，这类方法通常能够自动确定簇的数量，并对噪声数据具有更强的适应性。作为一种经典的聚类算法，K-Means 具有计算高效、实现简单的优点。然而，在普适性、精确度以及对深度特征的利用上，相较于基于深度学习的聚类方法，K-Means 存在一定不足。本文旨在为读者提供一种基础思路，激发其对于聚类分析这一领域的兴趣。本文代码已上传至 Github，更多细节请参考：https://github.com/Regularly-Archive/2024/blob/main/face-classify/kmeans_cluster_classify.py。\n","date":"2025-01-14T12:52:10Z","image":"/posts/face-photo-fast-classification-using-k-means-clustering/K-means-Clusters.png","permalink":"https://qinyuanpei.github.io/posts/face-photo-fast-classification-using-k-means-clustering/","slug":"Face-Photo-Fast-Classification-Using-K-Means-Clustering","tags":["机器学习","人脸分类","K-Means","Scikit-Learn","聚类"],"title":"基于 K-Means 聚类分析实现人脸照片的快速分类"},{"categories":["编程语言"],"content":"最近，我一直在尝试复刻 OpenAI 的 Canvas 功能，它与 Claude 的 Artifacts 功能非常相似。当然，两者在侧重点上有所不同——Artifacts 更注重于 “预览” 功能，而 Canvas 则专注于编程和写作领域。尽管 Artifacts 珠玉在前，可 Canvas 无疑为交互式体验带来更多可能性。对此，OpenAI 研究主管 Karina Nguyen 曾表示：我心目中的终极 AGI 界面是一张空白画布（Canvas）。在当前推崇 “慢思考” 的背景下，我有时会觉得下半年的大语言模型（LLM）发展 “不温不火”，给人一种即将停滞不前的的感觉。我想，这可能与四季更迭、万物枯荣的规律有关，正所谓 “环球同此凉热”。直到这两天，Claude 发布了 Computer Use，智谱发布了 AutoGLM，这个冬天再次变得热闹起来，为了不辜负这份幸运，我决定更新一篇博客，这次的主题是：容器技术驱动下的代码沙箱实践与思考。\nLangChain 开源的 OpenCanvas\r为什么需要代码解释器？ 在当前生成式 AI 的浪潮中，代码生成首当其冲，从 CodeGeex 到通义灵码，从 Github Copilot 到 Cursor，可谓是层出不穷，其交互方式亦从代码补全逐渐过渡到代码执行。你会注意到，在 OpenAI 的 Canvas 以及 Claude 的 Artifacts 中，都支持前端代码的实时预览，这意味着 AI 生成的不再是冷冰冰的代码，而是所见即所得的、可交互的成果。其实，早在 ChatGPT-3.5 中，OpenAI 就提供了 Code Interpreter 插件，可见让 AI 生成代码并执行代码的思路由来已久。究其本质，编程是一项持续改进的活动，必须根据反馈不断地完善代码。如果你使用过 Cursor 这个编辑器，相信你会对这一过程印象深刻，你可以实时地看到修改代码带来的变化，快速验证想法，加快调试和迭代的速度。毫无疑问，这种即时反馈的交互模式大大提高了编程的效率和趣味。\nOpenAI 的 Canvas 功能\r在实现 AI 智能体的过程中，我尝试为 Semantic Kernel 开发过一个 Code Interpreter 插件，我觉得这对于扩展（LLM）的能力边界意义重大。以 “9.11 和9.8 哪个大” 这一经典问题为例，在各路大模型普遍翻车的情况下，搭配 Code Interpreter 插件 Agent 表现稍好，因为它可以通过编写 Python 脚本来解决问题。同样地，像 “strawberry 中有多少个 r” 这样的问题，都可以通过该插件得到有效解决。或许有人觉得这种方式不够 “智能”，可如果使用工具是人类进步的体现，我们为什么要否认 AI 通过工具获得的答案呢？难道只有 o1 这种 “推理流” 才算思考，而 ReAct 这种 “规划流” 就不是思考？米兰·昆德拉说，“人类一思考，上帝就发笑”，人类的思想和行为，在宇宙或者更高存在的眼中可能微不足道，甚至有些可笑，以你我有限的认知去揣度智能，这本来就是一种虚妄。实际上，人类的思维过程并非总是纯粹的推理，我们同样依赖着工具、经验和试错。因此，我认为，无论是通过纯粹的推理还是借助工具，只要能有效地达成目标，都应该被视为智能的表现。\nCode Interpreter 插件\r目前，这个 Code Interpreter 面临的问题是，它并未与宿主环境实现完全隔离。例如，在执行 C# 脚本时，可以选择 Roslyn、Mono、CS-Script 以及 dotnet-script；运行 Python 脚本时，可以选择 Python.NET；运行 JavaScript 时，可以选择 Jint。理论上，可以添加更多运行时环境，只要它们能嵌入.NET 框架。但这样做的风险是，一旦 AI 生成恶意代码，可能就会影响到整个 Agent 的运行。我曾考虑过 Python 的虚拟环境，但这个方案并不通用。更重要的是，Python.NET 依赖的是 Python 的动态链接库而非可执行文件，这意味着虚拟环境并未实现真正的隔离。此外，AI 生成的代码可能依赖第三方库，直接安装到宿主环境可能会引起版本冲突。在大量使用 AI 生成的代码时，我们不得不考虑这些代码的安全性。我的老板曾问我，“你敢在生产环境中直接使用大模型生成的SQL语句吗？”，基于这些考虑，我们必须寻找一种更优的代码沙箱方案。\n容器技术驱动的代码沙箱 我们的目标是：创建一个与宿主环境完全隔离的代码沙箱，它能够独立运行代码并管理依赖。此时，我们发现容器技术非常适合这一需求，因为 Docker 利用 Linux 的 Namespace 功能实现了资源隔离。具体来说，我们会为每一种语言准备一个镜像，这个镜像中通常包含了该语言的编译器或者解释器，例如，Java 使用 OpenJDK 镜像，C# 使用.NET SDK 镜像。运行代码时，首先将代码文件挂载到容器中，然后使用编译器或解释器处理该文件。以下是一个简化的流程说明图：\n基于容器技术的代码沙箱流程示意图\r以 Python 为例，假设我们有一个名为 code_runner/python3 的镜像，其对应的 Dockerfile 定义如下：\nFROM python:3.9.18 RUN useradd --create-home --no-log-init --shell /bin/bash sandbox \\ \u0026amp;\u0026amp; adduser sandbox sudo USER sandbox WORKDIR /home/sandbox 你会注意到，这里创建了一个名为 sandbox 的用户，同时指定了其工作目录为 /home/sandbox。 这一点非常重要，因为我们希望使用统一的方式来处理不同的镜像。接下来，对于每一个前端请求，我们将在后端生成一个目录，并在其中存放待执行的代码文件，这里我们将其命名为 code.py。此时，通过下面的命令就可以创建容器：\n# runner_b8de2d4e-9b72-4e16-99ad-b0a4db6def7f 为后端自动生成的目录 # 其中存放着待执行的代码文件 code.py docker run -t -d \\ -v runner_b8de2d4e-9b72-4e16-99ad-b0a4db6def7f:/home/sandbox:rw \\ code_runner/python3 python code.py 如果一切顺利的话，你会看到容器开始运行。至此，在容器中执行代码已成为现实。那么，如何得到代码的执行结果呢？通常情况下，你有下面这两种选择：\n容器日志：通过容器日志来捕获程序输出，即：docker logs \u0026lt;容器Id\u0026gt; 输出重定向：将标准输出重定向到文件，然后再通过读取该文件来捕获程序输出 如果选择第二种方案，你需要将脚本修改为类似下面这样：\ndocker run -t -d \\ -v runner_b8de2d4e-9b72-4e16-99ad-b0a4db6def7f:/home/sandbox:rw \\ code_runner/python3 python code.py \u0026gt; output.txt 这里，我们选择第一种方案，如图所示，程序输出的结果是：Hello World.：\n通过容器日志捕获程序输出\r接下来的流程就变得简单啦，因为我们只需要删除容器、清理目录、返回结果，这个过程非常简单不再赘述。在实际场景中，我们更希望将这一切自动化。当然，你可以使用类似 os.system() 的方案执行 docker 命令。但是，更好的选择是，直接使用 Docker 的 Python SDK。此时，上述流程可以被简化为以下代码片段：\nimport uuid， docker container_name = f\u0026#34;./runner_{uuid.uuid4()}\u0026#34; container = client.containers.run( image=\u0026#39;code_runner/python3\u0026#39;， command=\u0026#39;python code.py\u0026#39;， volumes={os.path.abspath(container_name): { \u0026#39;bind\u0026#39;: \u0026#39;/home/sandbox\u0026#39;， \u0026#39;mode\u0026#39;: \u0026#39;rw\u0026#39; }}， tty=True， detach=True ) container.wait() output = container.logs().decode(\u0026#39;utf-8\u0026#39;) container.stop() container.remove(force=True) 基于这一原理，我们将镜像和命令与特定语言关联起来，从而得到一个配置文件。实际上，在设计的流程中，后端接收前端请求以后，首要任务是根据选定语言获取相应的镜像和命令。这使我们能够统一处理多种语言。如需支持新语言，只需准备相应的镜像和配置信息。以下配置文件展示了 Python、JavaScript、C# 和 C++ 的实现细节：\n{ \u0026#39;python3\u0026#39;: { \u0026#39;image\u0026#39;: \u0026#39;code_runner/python3\u0026#39;， \u0026#39;command\u0026#39;: \u0026#39;python code.py\u0026#39;， \u0026#39;extension\u0026#39;: \u0026#39;py\u0026#39; }， \u0026#39;javascript\u0026#39;: { \u0026#39;image\u0026#39;: \u0026#39;code_runner/nodejs\u0026#39;， \u0026#39;command\u0026#39;: \u0026#39;node code.js\u0026#39;， \u0026#39;extension\u0026#39;: \u0026#39;js\u0026#39; }， \u0026#39;csharp\u0026#39;: { \u0026#39;image\u0026#39;: \u0026#39;code_runner/dotnet\u0026#39;， \u0026#39;command\u0026#39;: \u0026#39;dotnet script code.csx\u0026#39;， \u0026#39;extension\u0026#39;: \u0026#39;csx\u0026#39; }， \u0026#39;csharp-mono\u0026#39;: { \u0026#39;image\u0026#39;: \u0026#39;code_runner/mono\u0026#39;， \u0026#39;command\u0026#39;: \u0026#34;sh -c \u0026#39;mcs -out:code code.cs \u0026amp;\u0026amp; mono ./code\u0026#39;\u0026#34;， \u0026#39;extension\u0026#39;: \u0026#39;cs\u0026#39; }， \u0026#39;cpp\u0026#39;: { \u0026#39;image\u0026#39;: \u0026#39;code_runner/cpp\u0026#39;， \u0026#39;command\u0026#39;: \u0026#34;sh -c \u0026#39;g++ code.cpp -o code \u0026amp;\u0026amp; ./code\u0026#39;\u0026#34;， \u0026#39;extension\u0026#39;: \u0026#39;cpp\u0026#39; } } 如图所示，我们现在可以在代码沙箱中运行 C# 代码，这个沙箱目前利用 Mono 来编译和执行程序：\n通过代码沙箱运行 C# 代码\r实践过程中遇到的挑战 到目前为止，一切都算顺利，可如果你认为这就是全部的话，那可就太天真啦！首当其冲的是各种编译型语言，以 C++ 为例，通常的编译命令如下，它实际上是将编译和运行两个步骤合二为一：\ng++ code.cpp -o code \u0026amp;\u0026amp; ./code 可如果你直接将这个命令应用到 Docker 上，你会发现这个 \u0026amp;\u0026amp; 无法被正确地识别。此时，你需要使用 sh -c 命令对当前命令进行包装，这样就可以完美地规避这个问题：\nsh -c \u0026#39;g++ code.cpp -o code \u0026amp;\u0026amp; ./code\u0026#39; 如果你运气不佳的话，你可能还会遇到 g++、mono、mcs 这些命令行工具在容器内找不到的情况，此时，你还需要添加必要的环境变量，以 Mono 为例，你可以在 Dockerfile 中做如下处理：\nENV PATH=\u0026#34;/usr/bin:/usr/local/bin:${PATH}\u0026#34; 继续探索，博主这里考虑了两种 C# 脚本化的方案，它们分别是 Mono 和 dotnet-script，其中：Mono 使用的是标准的 C# 语法，这意味着它必须要有入口方法 Main()，而 dotnet-script 使用的是顶级语句，所有代码会被放置在一个隐式的 Main() 方法中执行。如果说这种语法上的细微差异尚可接受，那么，接下来请你做好破防的准备：\n破防时刻，控制台乱码\r当然，这并不是你的错，根本原因在于 Docker 输出的日志中包含特殊的 ANSI 转义序列，这些特殊字符通常用于控制终端输出时的颜色和样式，此刻变成了某种噪音。这里，博主介绍两种思路来解决这个问题：\n思路一：正则替换法，网络上广泛流传的正则表达式 r\u0026rsquo;\\x1b[[0-9;]*[mK]\u0026rsquo;，经验证，无效！😹 def remove_ansi_sequences(input_string): ansi_escape = re.compile(r\u0026#39;\\x1b\\[([0-?]*[ -/]*[@-~])\u0026#39;) return ansi_escape.sub(\u0026#39;\u0026#39;， input_string).replace(\u0026#39;\\x1b=\u0026#39;，\u0026#39;\u0026#39;) 思路二：重定向法，将终端的标准输出重定向到一个文本文件。果然，大道至简！😂 dotnet script code.csx \u0026gt; ./output.txt 个人更推荐这种方案，万一正则表达式再次失效了呢？到目前为止，这个代码沙箱里运行的都是代码片段，这意味着我们可以先暂时忘掉工程实践中的层级结构，你不必通过工程文件或者解决方案来描述各个文件之间的关系。可这样自然就会产生一个问题，代码中的依赖的第三方库该怎么办？此时你有两种策略可供选择：\n策略一：在镜像里通过包管理器提前安装好常用的第三方库，最典型的例子是 Anaconda，它预装了大量与科学计算相关的第三方库，如 NumPy、Pandas 等，可以做到一般意义上的开箱即用。 策略二：在代码沙箱运行时自动安装第三方库，如果你使用过 Jupyter Notebook，相信你会非常熟悉它管理依赖的方式。如下图所示，我们可以在 Jupyter Notebook 中使用 pip、NuGet 这类包管理器： 在 Jupyter Notebook 中使用 pip\r在 Jupyter Notebook 中使用 NuGet\r在这种情况下，虽然我们依然是在 Jupyter Notebook 的单元格里写代码，可工程化的思维早已深入人心，“代码即工程，工程即代码”。只要大模型可以同时生成代码和依赖项，那么，理论上这些代码上就可以通过 Jupyter Notebook 运行起来。所以，我接下来的想法是，在有了 dotnet-script 和 Mono 这两种 C# 的编译器以后，我想继续扩展出基于 Jupyter Notebook 的方案，至少现在解决了第三方库的依赖问题，对吧？\nJupyter 与结果可视化 你可能会疑惑，既然 dotnet-script 和 Mono 都能运行 C#，为什么还要考虑 Jupyter？除了处理第三方库依赖问题以外，Jupyter 的可视化功能是我最为看重的特性。比如， 对于数据分析这类任务，如果我们能让大模型将分析结果以图表形式展示出来，这无疑会极大地提升产品的用户体验。再比如，我之前尝试 Text2SQL 的时候，如果 SQL 查询结果能以表格或 Excel 格式展现出来，效果会不会更佳？我认为，Code Interpreter 适用于两种场景，即：非展示类场景和展示类场景，前者是用程序执行的结果作为上下文，而后者是需要对程序执行的结果进行展示。显然，在后面这种场景中，Jupyter Notebook 表现上要更胜一筹。如下图所示，我们可以在 Jupyter Notebook 中绘制出各式各样的图表：\n通过 Jupyter 实现可视化\r好了，现在让我们来思考，如何将 Jupyter 与我们在前面收获到的经验结合起来。这里需要用到两个重要的工具: nbformat 以及 nbconvet。其中，前者负责将代码片段转化为 .ipynb 格式的笔记本，后者负责执行笔记本、获取结果以及格式转换等。如图所示，下面是与 Jupyter 相关的流程示意图：\n基于 Jupyter 的代码执行流程示意图\r在整个流程中，第一步是从代码片段创建 .ipynb 格式的笔记本文件，而从 nbformat 的文档中我们可以了解到，Jupyter 的笔记本文件其实是一个 JSON 文件，其标准结构通常类似于下面这样：\n{ \u0026#34;metadata\u0026#34;: { \u0026#34;kernel_info\u0026#34;: { # if kernel_info is defined， its name field is required. \u0026#34;name\u0026#34;: \u0026#34;the name of the kernel\u0026#34; }， \u0026#34;language_info\u0026#34;: { # if language_info is defined， its name field is required. \u0026#34;name\u0026#34;: \u0026#34;the programming language of the kernel\u0026#34;， \u0026#34;version\u0026#34;: \u0026#34;the version of the language\u0026#34;， \u0026#34;codemirror_mode\u0026#34;: \u0026#34;The name of the codemirror mode to use [optional]\u0026#34;， }， }， \u0026#34;nbformat\u0026#34;: 4， \u0026#34;nbformat_minor\u0026#34;: 0， \u0026#34;cells\u0026#34;: [ # list of cell dictionaries， see below ]， } 对我们而言，这里最关键的是其中的 cells 节点，因为我们需要将代码片段放在这里，在 Jupyter 中通常有三种形式的单元格，即：Code、Markdown 以及 Raw，它们之间唯一的区别是 Code 类型的单元格可以有输出、执行次数等信息，这些细节均可以在文档中找到，这里不再赘述。通常，单元格的标准结构定义如下：\n{ \u0026#34;cell_type\u0026#34;: \u0026#34;type\u0026#34;， \u0026#34;metadata\u0026#34;: {}， \u0026#34;source\u0026#34;: \u0026#34;single string or [list， of， strings]\u0026#34;， } 不要紧张，我们不会愚蠢到要去手写这个文件的地步，相信我，通过 nbformat 包这一切会变得非常简单：\nimport nbformat def code_to_ipynb(code_string， notebook_name=\u0026#39;output_notebook.ipynb\u0026#39;): nb = nbformat.v4.new_notebook() code_cell = nbformat.v4.new_code_cell(code_string) nb[\u0026#39;cells\u0026#39;].append(code_cell) with open(notebook_name， \u0026#39;w\u0026#39;， encoding=\u0026#39;utf-8\u0026#39;) as f: nbformat.write(nb， f) 现在，我们拥有了一个 .ipynb 格式的笔记本文件，你可以直接通过 Jupyter Notebook 或者 JupyterLab 打开它。当然，我们这里更希望它可以在后台静默执行，因为我们并不需要用到 Jupyter 的可视化界面。此时，我们可以使用 nbconvert 这个工具，它可以将 .ipynb 格式的笔记本文件转换为 HTML、LateX、PDF、Markdown 等常见格式，在指定 Kernel 的情况下，你甚至可以运行整个笔记本文件中的代码，下面是部分命令示例：\n# 将笔记本转换为 HTML jupyter nbconvert --to html notebook.ipynb --output notebook.html # 运行笔记本，然后转换为 HTML jupyter nbconvert --execute --to html notebook.ipynb --output notebook.html # 运行笔记本，忽略输入，然后转换为 HTML jupyter nbconvert --execute --no-input --to html notebook.ipynb --output notebook.html # 运行笔记本，忽略输入，指定 Kernel，然后转换为 HTML jupyter nbconvert --execute --no-input --to html --ExecutePreprocessor.kernel_name=python3 notebook.ipynb --output notebook.html # 运行笔记本，直接覆盖原来的笔记本 jupyter nbconvert --to notebook --execute --inplace notebook.ipynb 如果你不喜欢命令行，你还可以使用 nbconvert 的类库，这一刻，风和你都是自由的：\nimport nbformat from nbconvert import HTMLExporter notebook = nbformat.read(\u0026#39;notebook.ipynb\u0026#39;， as_version=4) html_exporter = HTMLExporter(template_name=\u0026#34;classic\u0026#34;) (body， resources) = html_exporter.from_notebook_node(notebook) 至此，整个 Jupyter 相关的处理流程便在逻辑上实现了闭环，只要我们生成 .ipynb 格式的笔记本并将其挂载到容器中，然后再通过 nbconvert 完成转换即可。考虑到我们未来需要在前端中展示 Jupyter 的运行结果，我们直接返回 HTML 即可。那么，接下来，就是见证奇迹的时刻：\n执行 Jupyter 脚本并在前端展示结果-1\r执行 Jupyter 脚本并在前端展示结果-2\r本文小结 从 OpenAI 的 Code Interpreter 到 Canvas，从 Claude 的 Artifacts 到 Computer Use，我们一起见证了 AI 编程领域的快速发展。在此过程中，代码执行环境的安全性和隔离性成为了一个重要议题。本文探讨了基于容器技术的代码沙箱方案，这是对挑战的积极探索和实践。容器技术完美解决了代码执行的隔离问题，每个代码片段都在独立的容器中运行，拥有自己的运行时环境，不影响宿主系统。这种方案不仅确保了安全性，还提供了统一的环境管理能力。实践过程充满挑战、一波三折，可容器技术带来的便利性不容忽视。更重要的是，这种方案具有良好的可扩展性，支持多种编程语言，为 AI 运行代码提供了坚实的技术基础。展望未来，随着生成式 AI 技术的进步，代码沙箱的重要性将日益凸显，或许正如 OpenAI 研究主管所说的 “终极 AGI 界面是一张空白画布”，代码沙箱将会成为通向这个愿景的重要基础设施之一，生成代码、执行代码、可视化结果将成为AGI世界的标准能力。\n参考链接 如何实现自己的 ChatGPT Code Interpreter https://github.com/xjq7/runcode Code online with One Compiler ","date":"2024-10-28T12:52:10Z","image":"/posts/container-technology-driven-code-sandbox-practice-and-reflection/developer-8829709_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/container-technology-driven-code-sandbox-practice-and-reflection/","slug":"Container-Technology-Driven-Code-Sandbox-Practice-and-Reflection","tags":["容器","沙箱","Jupyter","代码解释器"],"title":"容器技术驱动下的代码沙箱实践与思考"},{"categories":["编程语言"],"content":"最近，我一直在体验 Cursor 这款产品，与先前的 CodeGeex、通义灵码 等 “插件类” 产品相比，Cursor 在产品形态上更接近 Github Copilot。在多项测评中，Cursor 甚至一度超越了 Github Copilot。尽管我没有体验过 Github Copilot，但从用户体验的角度来看，Cursor 基于 VS Code 进行了深度定制。除了基础的代码自动补全功能外，它还可以允许你从原型图生成代码、将整个工程作为 Codebase、一键应用代码到本地。最令我印象深刻的是，它指导我完成了一个 Vue 的小项目，从零开始。诚然，“幻觉” 的存在让它在 Vue 2 和 Vue 3 之间反复横跳，其编程能力的提升主要得益于 Claude 3.5 系列模型，可我还是像《三体》中的杨冬一样感到震惊：物理学不存在了，那前端呢？有人说，程序员真正的护城河是沟通能力，因为执行层面的工作可以交给 AI。实际上，我并不担心 AI 取代人类，我更倾向于与 AI 沟通和合作，你可能想象不到，这篇文章中的思考正是来自于我和 Claude 老师的日常交流。\nCRUD Boys 的日常 程序员普遍喜欢自嘲，以博主为例，作为一名后端工程师，我的日常工作主要就是 CRUD，因此，你可以叫我们 CRUD Boys。鲁迅先生曾作《自嘲》一诗，“破帽遮颜过闹市，漏船载酒泛中流”。面对软件世界里里的复杂性和不确定性，如果没有乐观的心态和耐心，哪怕是最基础的 CRUD，你不见得就能做到得心应手。你可能听说过这样一句话，“上岸第一剑，先斩意中人”，AI 领域的第一把火，永远烧向程序员自己，自打一众 AI 辅助编程工具问世以来，各种程序员被 AI 取代的声音不绝于耳，甚至 Cursor 可以在 45 分钟内让一个 8 岁小孩搭建出聊天网站，更不必说，在 OpenAI 发布全新的 o1 模型后，很多人觉得连提示工程、Agent 这些东西都不存在了。其实，代码生成、低代码/无代码相关的技术一直都存在，在很久以前，我们就在通过 T4 模板生成业务代码，自不必说各种代码生成器。截止到目前，Excel 依然是这个地球上最强大的低代码工具，可又有谁能掌握 Excel 的全部功能呢?\n你猜用 Cursor 写一个这样的页面需要多久？\r退一步讲，即使的最简单的 CRUD，虽然业务的推进会不断地演化出新的问题。譬如，当你为了加快查询效率引入了缓存，你需要去解决数据库和缓存一致性、缓存失效等问题；当你发现数据库读/写不平衡引入读写分离、分库分表，你就需要去解决主从一致、分布式事务、跨库查询等问题；当你发现单点性能不足引入了多机器、多线程，你需要去解决负载均衡、线程同步等问题……单单一个查询就如此棘手，你还会觉得后端的 CRUD 简单吗？我承认，后端的确都是 CRUD，可在不同的维度上这些 CRUD 并不完全相同，譬如，分布式的相关算法如 Paxos、Raft 等，难道不是针对分布式环境中的节点做 CRUD 吗？可此时你还会觉得它简单吗？Cursor 的确可以帮你生成代码，但真正让它出圈的是背后的 Claude 模型。我始终相信某位前辈曾经讲过的话：“没有银弹”，在软件行业里，复杂度永远不会消失，它只会以一种新的方式出现。如果你觉得 CRUD 简单，或许是你从未接触过那些千姿百态的查询接口：\n// OData 风格 Products?$filter=Category eq \u0026#39;Electronics\u0026#39;\u0026amp;$orderby=Price desc\u0026amp;$top=10\u0026amp;$skip=20 // GraphQL 风格 { products(category: \u0026#34;Electronics\u0026#34;, first: 10, skip: 20, orderBy: {field: PRICE, direction: DESC}) { name price } } // RESTful 风格 products?category=Electronics\u0026amp;sort=-price\u0026amp;limit=10\u0026amp;offset=20 products?filter[category]=Electronics\u0026amp;sort=-price\u0026amp;page[limit]=10\u0026amp;page[offset]=20 从 Gridify 中得到的启发 Gridify 是一个卓越的动态 LINQ 库，它可以将字符串转化为 LINQ 查询。因此，它非常适合处理分页、过滤、排序等问题。如以下代码示例所示，你只需要在控制器方法中添加一个 GridifyQuery 类型的参数，即可实现一个通用的分页查询接口，可以说是非常的 Amazing 啊！👌\n[HttpGet] public Paging\u0026lt;Product\u0026gt; GetPagingProducts([FromQuery]GridifyQuery query) { var queryable = _context.Products.AsQueryable(); return queryable.Gridify(query); } 事实上，Gridify 为 IQueryable 接口提供了丰富的扩展方法，这使得我们可以如此优雅地实现分页查询功能。此时，我们可以使用类似下面这样的语法来查询产品信息：\n// 查询品牌中含有 Nick 的产品 api/products?page=1\u0026amp;pageSize=10\u0026amp;filter=brand=*Nike // 查询价格大于或等于 10 元的产品 api/products?page=1\u0026amp;pageSize=10\u0026amp;filter=price\u0026gt;=10 // 查询名称以 Nick 开头的产品 api/products?page=1\u0026amp;pageSize=10\u0026amp;filter=name=^Nick // 查询品牌中含有 Nick 的产品，并按 SKU 降序排列 api/products?page=1\u0026amp;pageSize=10\u0026amp;filter=brand=*Nike\u0026amp;orderBy=sku desc 你可以理解为，Gridify 定义了一套过滤和排序的语法，这些语法将会被解析、编译为表达式，并最终应用到 IQueryable 接口上面。对于 .NET 开发者来说，常见的做法是：使用 Where() 筛选数据，使用 Skip() 和 Take() 实现分页查询，以及使用 OrderBy()、OrderByDescending()、ThenBy() 和 ThenByDescending() 对数据进行排序。目前，Gridify 支持的操作符如下所示：\nGridify 中定义的操作符\r这个方案在某种程度上与我过去开源的项目 DynamicSearch 非常相似，都是通过构建表达式实现动态查询。事实上，之前已经有一个名为 System.Linq.Dynamic 的项目专门解决这类动态查询问题。唯一的挑战是，这些方案对前端同事来说过于复杂了，即便 Gridify 中非常贴心的提供了前端实现。当然，一个更要的原因是，我更倾向于避免解析字符串信息。那么，是否有更好的解决方案呢？诸如 Java 程序员在 XML 里写 SQL 的做法，窃为我所不取也！\n一个通用查询方案的实现 首先，我们参照 GridifyQuery 定义一个泛型类 QueryParameter:\nclass QueryParameter\u0026lt;TEntity,TFilter\u0026gt; where TFilter : class, IQueryableFilter\u0026lt;TEntity\u0026gt; { public int PageIndex { get; set; } public int PageSize { get; set; } public TFilter Filter { get; set; } public string SortBy { get; set; } public bool IsDescending { get; set; } } 其中，参数 TFilter 需要满足一定的约束条件，即：实现 IQueryableFilter 接口，该接口定义如下：\ninterface IQueryableFilter\u0026lt;TEntity\u0026gt; { ISugarQueryable\u0026lt;TEntity\u0026gt; Apply(ISugarQueryable\u0026lt;TEntity\u0026gt; queryable); } 博主这里使用的 ORM 是 SqlSugar，因此，这个接口需要依赖 ISugarQueryable 接口，如果你使用 EntityFramework，可以将其替换为 IQueryable。当我们需要对特定数据进行筛选时，我们只需要为其定义一个或者多个过滤条件即可。例如，通过下面的 LlmModelQueryableFilter 类，即可完成对大模型信息的检索：\nclass LlmModelQueryableFilter : IQueryableFilter\u0026lt;LlmModel\u0026gt; { public string ModelName { get; set; } public int? ModelType { get; set; } public int? ServiceProvider { get; set; } public ISugarQueryable\u0026lt;LlmModel\u0026gt; Apply(ISugarQueryable\u0026lt;LlmModel\u0026gt; queryable) { if (!string.IsNullOrEmpty(ModelName)) queryable = queryable.Where(x =\u0026gt; x.ModelName.Contains(ModelName)); if (ModelType.HasValue) queryable = queryable.Where(x =\u0026gt; x.ModelType == ModelType); if (ServiceProvider.HasValue) queryable = queryable.Where(x =\u0026gt; x.ServiceProvider == ServiceProvider); return queryable; } } 通常情况下，你只需要继承 CrudBaseController，并将其与实体类关联起来即可：\n[Route(\u0026#34;api/[controller]\u0026#34;)] [ApiController] class LlmModelController : CrudBaseController\u0026lt;LlmModel,LlmModelQueryableFilter\u0026gt; { private readonly IRepository\u0026lt;LlmModel\u0026gt; _llmModelRepository; public LlmModelController(CrudBaseService\u0026lt;LlmModel\u0026gt; crudBaseService, IRepository\u0026lt;LlmModel\u0026gt; llmModelRepository) : base(crudBaseService) { _llmModelRepository = llmModelRepository; } } 此时，你将得到一组 RESTful 风格的 CRUD 接口，如下图所示：\n自动解锁一组 RESTful 风格的 CRUD 接口\r其中， api/LlmModel/paginate 和 api/LlmModel/list 具有相同的过滤条件，真正做到了 “一次编写，到处运行”。关于 CrudBaseController 的具体实现，我认为并不复杂，关键代码片段展示如下：\n// 分页查询 async Task\u0026lt;PagedResult\u0026lt;T\u0026gt;\u0026gt; PaginateAsync\u0026lt;TQueryFilter\u0026gt;( QueryParameter\u0026lt;T, TQueryFilter\u0026gt; queryParameter, ISugarQueryable\u0026lt;T\u0026gt; queryable = null ) where TQueryFilter : class, IQueryableFilter\u0026lt;T\u0026gt; { queryable = queryable ?? base.AsQueryable(); if (queryParameter.Filter != null) queryable = queryParameter.Filter.Apply(queryable); var total = await queryable.CountAsync(); queryable = queryable.Skip((queryParameter.PageIndex - 1) * queryParameter.PageSize).Take(queryParameter.PageSize); if (!string.IsNullOrEmpty(queryParameter.SortBy)) queryable = queryable.OrderByPropertyName(queryParameter.SortBy, queryParameter.IsDescending ? OrderByType.Desc : OrderByType.Asc); var list = await queryable.ToListAsync(); return new PagedResult\u0026lt;T\u0026gt; { TotalCount = total, Rows = list }; } // 列表查询 Task\u0026lt;List\u0026lt;T\u0026gt;\u0026gt; FindListAsync\u0026lt;TQueryFilter\u0026gt;( TQueryFilter filter, ISugarQueryable\u0026lt;T\u0026gt; queryable = null ) where TQueryFilter : class, IQueryableFilter\u0026lt;T\u0026gt; { queryable = queryable ?? base.AsQueryable(); if (filter != null) queryable = filter.Apply(queryable); return queryable.ToListAsync(); } 大家可能已经注意到，代码本身并没有变，但是它解决了一个关键问题：如何让整个控制器层变得整洁。回想一下，我们过去是如何处理这类问题的？是不是经常不断地向控制器添加参数？虽然我可能只是一个 CRUD Boy，可我并不愿意就此止步，能从这个再熟悉不过的话题中 “温故而知新”、查漏补缺，这是我写这篇博客的主要目标。让我们沿着这个思路继续探索，此时，我们会发现这里的过滤条件并不符合我们的预期：\nSwagger 中展示的分页查询接口\r具体来说，博主期待的查询参数应该是下面这种格式：\napi/LlmModel/paginate?pageIndex=1\u0026amp;pageSize=10\u0026amp;modelName=llama3\u0026amp;modelType=0 事实上，在真实的请求中，你将会看到的下面这种格式的查询参数：\napi/LlmModel/paginate?pageIndex=1\u0026amp;pageSize=10\u0026amp;filter.modelName=llama3\u0026amp;filter.modelType=0 在某个瞬间，你是否会对这个格式感到困惑呢？实际上，这两种格式都是合理的，前者被称为平铺格式，而后者则被称为嵌套格式。其中，平铺模式对前端友好，缺点是当参数较多时可能会造成混淆，无法区分出哪些是过滤条件；嵌套模式对后端友好，可以直接映射到 QueryParameter 参数的 Filter 属性上，缺点是可能与现有组件不兼容，需要前端在传参时做特殊处理。考虑到，无论是 HTTP 协议还是 RESTful 规范，在这一类问题均未做出强制性约束。因此，在实际工作中，你会看到五花八门的实现方式，如以下代码所示：\n// 使用 JSON 编码的查询参数 products?query={\u0026#34;page\u0026#34;:{\u0026#34;index\u0026#34;:1,\u0026#34;size\u0026#34;:10},\u0026#34;sort\u0026#34;:{\u0026#34;field\u0026#34;:\u0026#34;price\u0026#34;,\u0026#34;direction\u0026#34;:\u0026#34;desc\u0026#34;},\u0026#34;filter\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;laptop\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;electronics\u0026#34;}} // 使用组合式的过滤条件 products?page=1\u0026amp;size=10\u0026amp;sort=price\u0026amp;order=desc\u0026amp;filters=name:laptop;category:electronics 结合前文可知，OData 和 Gridify 都选择了组合式的过滤条件写法，而我们期望的是平铺格式的过滤条件写法。由此可见，单单是一个查询就有这么多种写法，秦始皇 “书同文”、“车同轨”、统一度量衡的意义在这一刻终于找到了答案，可是话说回来，正是这种灵活性让我们有了发挥的空间，不是吗？那么，该如何解决这个问题呢？这里的方案是实现一个自定义的 IModelBinder，从而改变 ASP.NET Core 模型绑定的默认行为，关键代码如下：\nTFilter BindFilter(ModelBindingContext bindingContext) { var filter = new TFilter(); var filterType = typeof(TFilter); var properties = filterType.GetProperties(); foreach (var property in properties) { var key = property.Name.ToLower(); if (!ReservedParameters.Contains(key)) { // 同时兼容平铺型以及嵌套型参数 // ?pageIndex=1\u0026amp;pageSize=10\u0026amp;name=xxx\u0026amp;age=xxx // ?pageIndex=1\u0026amp;pageSize=10\u0026amp;filter.name=xxx\u0026amp;filter.age=xxx var value = bindingContext.ValueProvider.GetValue(key).FirstValue; if (value == null) { value = bindingContext.ValueProvider.GetValue($\u0026#34;filter.{key}\u0026#34;).FirstValue; } if (value != null) { object convertedValue = null; var targetType = Nullable.GetUnderlyingType(property.PropertyType) ?? property.PropertyType; if (targetType.IsEnum) { convertedValue = Enum.Parse(targetType, value, true); } else if (targetType == typeof(bool) \u0026amp;\u0026amp; bool.TryParse(value, out var _)) { convertedValue = bool.Parse(value); } else if (targetType == typeof(DateTime) \u0026amp;\u0026amp; DateTime.TryParse(value, out var _)) { convertedValue = DateTime.Parse(value); } else { if (!string.IsNullOrEmpty(value.ToString())) convertedValue = Convert.ChangeType(value, targetType); } property.SetValue(filter, convertedValue); } } } return filter; } 可以注意到，本质上就是对两种类型的查询参数做了兼容，这样做的好处是，现在前后端都可以在各自的舒适区愉快工作，我认为这种效率的提升是有效提升，因为它真正地改善了生产关系，让前后端都能都能从中受益。相反，一味地提升效率而不去改善生产关系，永远都只会造成更多的内卷。这个方案是由我和 Claude 老师一起完成的，我觉得它还有进一步改善的空间，因为现在 IQueryableFilter 与 ISugarQueryable 强耦合，这显然不是我们希望看到的结果。当你使用 EntityFramework 时，你会希望这里变成 IQueryable；当你使用 MongoDB 时，你会希望这里变成 FilterDefinitionBuilder。所以，我认为这里还可以再引入一个中间层，你觉得呢？\n本文小结 回顾整个探索过程，我不禁莞尔，从在Gridify 中寻找灵感，到亲自动手设计查询方案，再到深入思考参数传递的细节，对我而言，这个过程中是在一次次的思考和实践中，逐渐接近问题的本质。说到底，我们讨论的并不仅仅是一个查询方案，而是如何在灵活性和规范性之间寻找平衡。这个问题，恐怕从软件开发诞生以来就一直存在，而今天我们依旧在探索。也许，正是这种永无止境的探索，构成了编程的真正魅力。AI 无疑会给程序员带来不小的冲击，可仔细想想，它不过是我们工具箱中的新成员，就像我们并不会因为有了电动螺丝刀，就忘记如何使用普通螺丝刀。作为程序员，我们不应该局限在代码生成这一个点，而是要着眼于解决更复杂的问题、改善生产关系。本文探索的通用查询方案，表面上看是一个技术问题，实则触及了软件工程中的普适性难题，即：灵活性与规范性的取舍。我承认，AI 是可以帮我们快速生成各种代码，可未来的软件开发，决不会仅仅是代码的简单堆砌，为什么我们的目标不能是更高层次的系统设计和问题解决？就像这篇文章里提出的方案并非完美无瑕，它依然有改进和优化的空间。所以，请不要放弃你的主观能动性，让 AI 辅助你思考，而不是替你思考。\n","date":"2024-09-23T12:52:10Z","image":"/posts/review-and-rethink-backend-universal-query-solutions/Sea-8948636_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/review-and-rethink-backend-universal-query-solutions/","slug":"Review-and-Rethink-Backend-Universal-Query-Solutions","tags":["查询","后端","思考","API","温故知新"],"title":"温故而知新：后端通用查询方案的再思考"},{"categories":["编程语言"],"content":"两个月前，我写过一篇题为为《关于 ChatGPT 的流式传输，你需要知道的一切》的文章。当时，我主要聚焦于 “流式传输” 这个概念。因此，Server-Sent Events、WebSocket 等技术，便顺理成章地成为了我的写作内容。然而，当我们将视野扩展到整个生成式 AI 领域时，我们会发现 “取消” 是一个非常普遍的业务场景。尽管我曾在这篇文章中提到了 AbortController 和 CancellationToken，但我并不认为我完全解决了当时的问题，即：如何让前、后端的取消动作真正地协同起来？言下之意，我希望前端的 AbortController 发起取消请求以后，后端的 CancellationToken 能够及时感知并响应这一变化。这一切就好比，AI 智能体固然可以通过 “观察” 来感知外部的变化，可当用户决定停止生成的时候，一切都应该戛然而止，无论你是不是为了节省那一点点 token。所以，当两个月前的子弹正中眉心时，我决定继续探讨这个话题。由此，便有了今天这篇稍显多余的博客。\n前后端协同取消 我必须承认，在推崇前后端分离的当下，我这个想法难免显得不合时宜。可什么是合时宜呢？在刚刚落幕的巴黎奥运会上，35 岁的 “龙队” 马龙，斩获了个人第 6 枚奥运金牌。对此，这个被誉为 “六边形战士” 的男人表示，“只要心怀热爱，永远都是当打之年”。这是否说明，一切的不合时宜都是自我设限，而年龄不过是个数字。在以往的工作中，我接触的主要是 “Fire and Forget” 这类场景。特别是当一个任务相对短暂时，有没有真正地取消从来都不会成为讨论的重点。直到最近做 Agent 的时候，我发觉这一切其实可以做得更好，即便我的原动力是为了省钱。\nasync Task Main() { Console.WriteLine(\u0026#34;[HeartBeat] 服务运行中，请按 Ctrl + C 键取消...\u0026#34;); var cts = new CancellationTokenSource(); Console.CancelKeyPress += (sender, e) =\u0026gt; { e.Cancel = true; cts.Cancel(); }; try { await HeartBeatAsync(cts.Token); } catch (OperationCanceledException) { Console.WriteLine(\u0026#34;[HeartBeat] 服务已停止.\u0026#34;); } } 首先，我们来考虑一般场景下的 CancellationToken 的使用，通常它需要搭配 CancellationTokenSource 来使用。当然，产生这一切的背景是 .NET 中提供了基于任务(Task)的异步编程模型 TAP，我们需要一种机制来处理任务的取消请求。考虑到线程本身只有一个 Abort() 方法，CancellationTokenSource 以及 CancellationToken 这种在多线程环境中传达取消请求、无需终止线程的技术应运而生。参考上述示例，CancellationTokenSource 在其中扮演着调度者的角色，它向 HeartBeatAsync() 这个方法传递了一个令牌，并在某个时间点通过 Cancel() 方法触发了取消动作。相对应地，在 HeartBeatAsync() 方法中，我们需要及时检查 CancellationToken 的 IsCancellationRequested 属性，以判断是否有取消请求产生，如以下代码片段所示：\nasync Task HeartBeatAsync(CancellationToken cancellationToken) { while (!cancellationToken.IsCancellationRequested) { Console.WriteLine($\u0026#34;[HeartBeat] {DateTime.Now.ToString(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;)}\u0026#34;); await Task.Delay(1000, cancellationToken); } } 可以注意到，async 关键字会造成对方法的异步化污染，类似地，CancellationToken 需要在所有的异步方法中传播下去，这可能会造成一定的副作用。关于这个问题，我们先暂时将其搁置在一边。现在，让我们来思考一个问题：在这个示例中取消请求由我们来发起，那么，在一个 Web 服务中，取消请求应该有谁来发起呢？你可能会说，你这个问题问得可真没水平，这肯定是客户端嘛！好，顺着这个思路继续深挖下去，客户端发起了取消请求以后，服务器端如何感知到这一变化的呢？在 ASP.NET Core 中，我们可以通过 HttpContext 中的 RequestAborted 属性来进行判断，它本身就是一个 CancellationToken。当客户端连接到服务器端时，ASP.NET Core 负责构建 HttpContext，其中的 RequestAborted 则来自于一个 CancellationTokenSource 实例；当客户端从服务器端断开连接时，框架将会调用这个 CancellationTokenSource 实例的 Cancel() 方法。下面是一个简化的流程示意图：\n服务器端如何感知客户端的取消请求\r为了验证这个猜想，我们可以通过下面的代码来验证。设想我们拥有一个经典的接口 Echo()，它可以像一个复读机一样重复我们的话语。在此，我们人为设置了 2s 的延迟，目的是便于观察 RequestAborted 属性：\n[HttpGet(\u0026#34;echo\u0026#34;)] public async Task\u0026lt;string\u0026gt; Echo(string text) { _logger.LogInformation(\u0026#34;[{0}] IsCancellationRequested: {1}\u0026#34;, DateTime.Now.ToString(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;), HttpContext.RequestAborted.IsCancellationRequested ); await Task.Delay(1000 * 2); _logger.LogInformation(\u0026#34;[{0}] IsCancellationRequested: {1}\u0026#34;, DateTime.Now.ToString(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;), HttpContext.RequestAborted.IsCancellationRequested ); return text; } 接下来，我们将编写客户端代码。运用前文中掌握的技巧，我们可以让 CancellationTokenSource 在 2s 以后触发取消请求。让我们来一起看看，这一变化将带来什么结果？\nvar cts = new CancellationTokenSource(); cts.CancelAfter(TimeSpan.FromSeconds(2)); try { using var httpClient = new HttpClient(); var response = await httpClient.GetStringAsync(\u0026#34;https://localhost:7261/api/Chat/Echo?text=你好\u0026#34;, cts.Token); } catch (OperationCanceledException) { Console.WriteLine(\u0026#34;请求已取消\u0026#34;); } 此时，我们可以注意到，RequestAborted 中的 IsCancellationRequested 中的属性，在 2s 前后发生了变化：\nHttpContext 中 RequestAborted 的变化(1) 作为对比，我们这里放一张正常请求的日志截图，相信大家可以都发现其中的差异：\nHttpContext 中 RequestAborted 的变化(2) 这说明什么呢？这表明，在 Web 服务中，我们可以利用 RequestAborted 来判断客户端是否发起了取消请求。实际上，我们还有更简单的做法，我们可以直接在控制器方法中添加一个 CancellationToken 类型的参数，ASP.NET Core 会自动将 RequestAborted 注入到其中，而这恰好解释了我们的疑问：我们都知道可以在 Controller 中使用这个参数，但我们从来没有想过这个参数从哪里来。对于《关于 ChatGPT 的流式传输，你需要知道的一切》这篇文章的困惑，此刻终于破云见日，这个当时没有解决的问题，现在看来竟然是如此的简单：\n[HttpGet(\u0026#34;streaming\u0026#34;)] [HttpPost(\u0026#34;streaming\u0026#34;)] public async Task GetStreamingAsync(CancellationToken cancellationToken) { try { cancellationToken.ThrowIfCancellationRequested(); var text = \u0026#34;天之道，损有余而补不足，是故虚胜实，不足胜有余。其意博，其理奥，其趣深，天地之象分，阴阳之候列，变化之由表，死生之兆彰，不谋而遗迹自同\u0026#34;; HttpContext.Response.ContentType = \u0026#34;text/event-stream\u0026#34;; foreach (var item in text) { var payload = JsonSerializer.Serialize(new { text = item.ToString() }); var message = $\u0026#34;data: {payload}\\n\\n\u0026#34;; await HttpContext.Response.WriteAsync(message, Encoding.UTF8, cancellationToken); await HttpContext.Response.Body.FlushAsync(cancellationToken); await Task.Delay(200); } await HttpContext.Response.WriteAsync(\u0026#34;data: [DONE]\u0026#34;, cancellationToken); await HttpContext.Response.Body.FlushAsync(cancellationToken); await HttpContext.Response.CompleteAsync(); } catch (OperationCanceledException ex) { _logger.LogInformation(\u0026#34;Operation is canceled.\u0026#34;); } } 此时，我们只需要在前端使用 fetch() 函数以及 AbortController，即可实现真正意义上的取消，如下图所示：\n前端发起取消请求\r日志表明，此刻后端成功了响应了前端发起的取消请求：\n后端响应取消请求\r当然，此前我用 SignalR 实现的双向取消机制，现在同样可以工作:\npublic async Task Generate(string requestId, string prompt) { var cts = new CancellationTokenSource(); _cancellationTokens[requestId] = cts; try { cts.Token.ThrowIfCancellationRequested(); await foreach (var item in _textGenerator.Generate(prompt, cts.Token)) { await Clients.Caller.SendAsync(\u0026#34;ReceiveChunks\u0026#34;, JsonSerializer.Serialize(new { text = item }), requestId, cts.Token); } } catch (OperationCanceledException ex) { _logger.LogInformation(\u0026#34;The task is canceled.\u0026#34;); } } public async Task Cancel(string requestId) { if (_cancellationTokens.TryGetValue(requestId, out var cts)) { await cts.CancelAsync(); await Clients.Caller.SendAsync(\u0026#34;GenerationCancelled\u0026#34;, requestId, cts.Token); _cancellationTokens.Remove(requestId); } } 这个时候，你可能会意识到，你的代码充被着大量的 CancellationToken 入侵了。例如，你会注意到，这里我必须将这个令牌传递给 ITextGenerator 这个接口，而如果该接口内部还有更多的依赖项，这无疑就会变成一种莫名的负担。我一直在考虑优化 Wikit 在 “停止生成” 功能上的代码，现在看来，“长痛不如短痛”，或许我应该尽快开始这块的工作，虽然我预感到这个过程可能会有些痛苦。如图所示，下面是一种解决 CancellationToken 侵入性问题的解决思路，即通过 IHttpContextAccessor 接口封装一个全新的 ICancellationTokenProvider 接口：\ninterface ICancellationTokenProvider { CancellationToken GetCancellationToken(); Task\u0026lt;CancellationToken\u0026gt; GetCancellationTokenAsync(); } 一个基本的实现如下。此时，我们只需要在应该 “取消” 的地方，注入该服务即可：\nclass CancellationTokenProvider : ICancellationTokenProvider { private readonly IHttpContextAccessor _httpContextAccessor; public CancellationTokenProvider(IHttpContextAccessor httpContextAccessor) { _httpContextAccessor = httpContextAccessor; } public CancellationToken GetCancellationToken() { return _httpContextAccessor?.HttpContext?.RequestAborted ?? CancellationToken.None; } public Task\u0026lt;CancellationToken\u0026gt; GetCancellationTokenAsync() { var cancellationToken = _httpContextAccessor?.HttpContext?.RequestAborted ?? CancellationToken.None; return Task.FromResult(cancellationToken); } } 参考资料 Recommended patterns for CancellationToken\n5 Recommended Patterns When Using Cancellation Token in .NET\nTask cancellation in C# and things you should know about it\nC# 中 CancellationToken 和 CancellationTokenSource 用法\nA .NET Programmer’s Guide to CancellationToken\nA Deep Dive into C#\u0026rsquo;s CancellationToken\nUsing CancellationTokens in ASP.NET Core MVC controllers\n","date":"2024-08-15T12:52:10Z","image":"/posts/cancellation-mechanism-cancellationtoken-cooperative-scene/cover.png","permalink":"https://qinyuanpei.github.io/posts/cancellation-mechanism-cancellationtoken-cooperative-scene/","slug":"Cancellation-Mechanism-CancellationToken-Cooperative-Scene","tags":["取消","AIGC","CTS","想法"],"title":"浅议 CancellationToken 在前后端协同取消场景中的应用"},{"categories":["编程语言"],"content":"《诗经》有言：七月流火，九月授衣，这句话常被用来描绘夏秋交替、天气由热转凉的季节变化。西安的雨季，自六月下旬悄然而至、连绵不绝，不由地令人感慨：古人诚不欺我。或许，七月注定是个多事之“秋”，前有萝卜快跑及其背后的无人驾驶引发热议，后有特朗普在宾夕法尼亚州竞选集会时遇刺，更遑论洞庭湖决口、西二环塌方。杨绛先生说，成长就是学会心平气和地去面对这世界的兵荒马乱，可真正的战争“俄乌冲突”至今已经持续800多天。有时候，我不免怀疑，历史可是被诅咒了的时间？两年前的此时此刻，日本前首相安倍晋三遇刺身亡，我专门写过一篇文章《杂感·七月寄望》 。现在，回想起两人长达19秒的史诗级握手画面，一时间居然有种“一笑泯恩仇”的错觉。因为，从某种意义上来说，他们似乎成为了共患难的“战友”。雍正之于万历，如同特朗普之于肯尼迪，虽时过境迁，而又似曾相识，大概世间万物总逃不出某种循环。最近一个月，从 RAG 到 Agent，再到微软 GraphRAG 的爆火，诸如 Graph、NER、知识图谱等知识点再次被激活。我突然觉得，我需要一篇文章来整理我当下的思绪。\n实现 Agent 以后 参照复旦大学的 RAG 综述论文实现 Advance RAG 以后，我开始将目标转向 Agent。一般来说，一个 Agent 至少应该具备三种基本能力：规划(Planning)、记忆(Memory)以及工具使用(Tool Use)，即：Agent = LLM + Planning + Memory + Tool Use。如果说，使用工具是人类文明的起点，那么，Agent 则标志着大模型从 “说话” 进化到 “做事”。目前的 Agent 或者是说智能体，本质上都是将大模型视作数字大脑，通过反思、CoT、ReAct 等提示工程模拟人类思考过程，再通过任务规划、工具使用来扩展其能力的边界，使其能够感知和连接真实世界。从早期的 AutoGPT 到全球首个 AI 程序员智能体 Devin，人们对于 AI 的期望值，正肉眼可见地一路水涨船高。\nAgent 的基本概念\r目前，市场上主流新能源汽车的智驾系统都大多处于 L2 或 L3 级别，萝卜快跑则率迈进 L4 级别。尽管我可以理解这一发展趋势的必然性，可当我意识到碳基生命自身的偶然性，我想知道，那些可能导致成千上万的人失业的失业的科技创新，是否是显得过于残酷和冰冷？在2024年的上半年，我接触到了多种 Agent 产品，例如 FastGPT、Coze、Dify 等等。这些产品基本都是基于工作流编排的思路，这实际上是一种对大型模型不稳定输出和多轮对话调用成本的妥协。受到过往工作经历影响，我对于工作流和低代码非常反感。因此，我坚信大模型动态地规划和执行任务的能力才是未来。在实现 Agent 的过程中，我参考 Semantic Kernel 的一个 PR 实现了一个支持 ReAct 模式的 Planner，这证明了我从去年开始接触大型模型时的种种想法，到目前为止基本上都是正确的。\n当下生成式 AI 的优化方向\r我主张采用小模型结合插件的方式，推进 AI 服务的本地化，因为一味地追求参数规模或上下文长度，只会陷入永无休止的百模大战。在技术和成本之间，你必须要找到一个平衡点。例如，最近大火的 GraphRAG，知识图谱结合大模型的理念虽好，但构建知识图谱的成本相对较高，运行一个简单示例的费用大约在5到10美元左右。在实现 Agent 的过程中，我发现，使用阿里的 Qwen2-7B 模型完全可以支持任务规划以及参数提取，唯一的问题是 Ollama 推理速度较慢，尤其是在纯 CPU 推理的情况下。此外，目前的 Agent 的反思功能大多依赖于多轮对话，其效果易受上下文长度的影响。即便使用 OpenAI、Moonshot 等厂商的服务，它们的 TPM/RPM 通常不会太高，导致公共 API 难以满足 Agent 的运行需求。如果增加接口调用间隔，无疑又会让屏幕前的用户失去耐心。因此，即便是在 token 价格越来越便宜的情况下，以任务为导向的 Agent，其 token 消耗量依然是一笔不小的开销。\nAgent 如何思考和观察\r一个名为爱因斯坦的智能体\r在调试 Agent 的过程中，博主曾先后将 OpenAI 和 Moonshot 用至“欠费”，最终不得不转向更为经济的 DeepSeek。最近，有朋友同我抱怨，“召回精度是提升了，可生成答案时间同样变长了”。这个问题在 Agent 中同样存在，当大模型观察到当前结果差强人意时，会尝试使用不同的工具来解决问题，可往往是花费了时间和金钱，最后还是没能得到满意答案。归根到底，最关键的推理能力来自模型本身，提示词不过是锦上添花，你是可以持续地“反思”和“观察”，可如果面对的是完全未知的事物，这一切又有什么意义呢？以电商业务为例，其特点是数据链路长、牵涉多个微服务，这使得我无法同时满足强一致性和低延迟。这一道理同样适用于 Agent，不论是大模型动态规划的工作流，还是人工编排的工作流，如果你想要一个逻辑上闭环的流程，就要接受其可能耗费大量时间的现实。对于类似 RAG 这样的检索型的任务，你必须要检索精度和响应时间之间找到平衡点。\nAgentic 比是不是 Agent 更重要\r在为大模型接入了日期/时间、天气预报、新闻报道、搜索引擎、网络爬虫等工具以后，我突然感到一切索然无味，甚至有时会觉得，它不再像原来那样“开朗”，甚至变得不苟言笑起来。最终，它变成了一个合格的、帮助我连接真实世界的“工具人”。可大模型应该被这些词汇修饰吗？或许，这只是我的一厢情愿。因为从某种角度来看，这一切的元凶，在于我的外部知识“污染”了它的先验知识，无论我怎么努力，它并不会比市面上的 AI 助手强大多少。\nText2SQL 实践 OK，现在让我们讨论Text2SQL Text2SQL，这是将大型模型与关系型数据库连接起来的一种尝试。实现 Agent 后，你会发现 RAG 实际上是 Agent 中的一个工具，并且广义上的 RAG 并不仅仅局限于向量数据库，它还可以扩展到搜索引擎、知识图谱、第三方 API、各种数据源(网页/SQL/文档)以及聊天的上下文。这样，我们便会意识到，如果大模型可以从数据库中读取信息，它便掌握了世界上使用最广泛的数据源。在 RAG 应用中，文档中的图像、表格等可能不适合进行向量检索，而数据库中存储的结构化数据对大型模型更为友好。唯一的问题是，关系型数据库通常使用 SQL 来进行查询。因此，如果大模型能生成 SQL 语句，那么，从大模型到数据库的链路便被打通了。从生成式 AI 的角度来看，SQL 和 Python、C# 等编程语言类似，均属于代码生成的范畴，甚至可以说 SQL 更简单，因为 SQL中使用中关键字和指令更少。在这种背景下，Text2SQL 这种从自然语言转化为 SQL 的技术便应运而生。\n一款以自然语言构建 SQL 的产品\r接下来，我们来探究 Text2SQL 的具体实现步骤。目前，AI 应用落地的表里是，Agent 做面子，提示工程做里子。因此，一个直接的方法是将用户需求和数据库的 Schema 一并提交给大型模型，由模型来生成相应的 SQL 语句。对于那些热衷于模型微调和训练的“炼丹术士”，可以关注下 Hrida-T2SQL-3B-V0.1 这个专门为 Text2SQL 设计的小模型。对于像我这样的 NLP 民科，自然不会去挑战这种高难度任务。按照这个思路，第一步是获取数据库的 Schema，即：了解数据库里哪些表、每张表里都有哪些字段以及这些字段具体是什么类型。这些信息对于大型模型生成准确的 SQL 语句至关重要。这里以 MySQL 数据库为例进行说明：\nSELECT t.TABLE_NAME, t.TABLE_COMMENT, c.COLUMN_NAME, c.COLUMN_COMMENT, c.DATA_TYPE, c.IS_NULLABLE FROM INFORMATION_SCHEMA.TABLES t LEFT JOIN INFORMATION_SCHEMA.COLUMNS c ON c.TABLE_NAME = t.TABLE_NAME WHERE t.TABLE_SCHEMA = \u0026#39;Chinook\u0026#39; 其中，Chinook 是数据库的名称，此时，我们就可以得到下面的结果：\n获取数据库的 Schema 信息\r接下来，我们只需按照第一列进行分组，便能获取每张表涵盖的字段。在此过程中，我同时提取了表和列的注释信息。因为在那些喜欢用拼音或是缩写命名的人面前，即使是大模型亦不免相形见绌。如果你的数据库表结构设计本就混乱不堪，不要说大模型，只怕是连神仙都难救。现在，让我们通过代码生成数据库的 Schema 信息：\n// 获取表描述信息 private async Task\u0026lt;IEnumerable\u0026lt;TableDescriptor\u0026gt;\u0026gt; GetTableDescriptorsAsync(string databaseName) { var sqlText = @\u0026#34;SELECT t.TABLE_NAME, t.TABLE_COMMENT, c.COLUMN_NAME, c.COLUMN_COMMENT, c.DATA_TYPE, c.IS_NULLABLE FROM INFORMATION_SCHEMA.TABLES t LEFT JOIN INFORMATION_SCHEMA.COLUMNS c ON c.TABLE_NAME = t.TABLE_NAME WHERE t.TABLE_SCHEMA = \u0026#39;{0}\u0026#39;\u0026#34;; using var sqlClient = new SqlSugarClient(_connectionConfig); var rows = await sqlClient.Ado.SqlQueryAsync\u0026lt;dynamic\u0026gt;(string.Format(sqlText, databaseName)); if (rows.Count == 0) return Enumerable.Empty\u0026lt;TableDescriptor\u0026gt;(); return rows.GroupBy(x =\u0026gt; x.TABLE_NAME).Select(g =\u0026gt; { return new TableDescriptor() { Name = g.ToList()[0].TABLE_NAME, Description = g.ToList()[0].TABLE_COMMENT, Columns = AsColumnDescriptors(g.ToList()) }; }).ToList(); } // 获取列描述信息 private IEnumerable\u0026lt;ColumnDescriptor\u0026gt; AsColumnDescriptors(List\u0026lt;dynamic\u0026gt; rows) { return rows.Select(x =\u0026gt; new ColumnDescriptor() { Name = x.COLUMN_NAME, DataType = x.DATA_TYPE, Description = x.COLUMN_COMMENT, IsNullable = x.IS_NULLABLE == \u0026#34;YES\u0026#34; }); } // 生成 Schema private string GeneratorDatabaseSchema(IEnumerable\u0026lt;TableDescriptor\u0026gt; tableDescriptors) { var stringBuilder = new StringBuilder(); foreach (var tableDescriptor in tableDescriptors) { stringBuilder.AppendLine($\u0026#34;{tableDescriptor.Name}, {tableDescriptor.Description}\u0026#34;); foreach (var columnDescriptor in tableDescriptor.Columns) { stringBuilder.AppendLine($\u0026#34; - {columnDescriptor.Name}, {columnDescriptor.DataType}, {columnDescriptor.Description}\u0026#34;); } stringBuilder.AppendLine(); } return stringBuilder.ToString(); } 最终，通过 GenerateDatabaseSchema() 方法，我们可以得到下面这样的结果：\n为数据库生成 Schema 信息\r接下来，就非常简单啦，我们只需要将这份 Schema 作为参数传入提示词模板即可，这份提示词可以在 这里 找到：\n[KernelFunction] [Description(\u0026#34;根据用户的输入生成和执行 SQL 并返回 Markdown 形式的表格数据\u0026#34;)] public async Task\u0026lt;string\u0026gt; QueryAsync([Description(\u0026#34;用户输入\u0026#34;)] string input, Kernel kernel) { var tableDescriptor = await GetTableDescriptorsAsync(\u0026#34;Chinook\u0026#34;); var databaseSchema = GeneratorDatabaseSchema(tableDescriptor); var promptTemplate = _promptTemplateService.LoadTemplate(\u0026#34;Text2SQL.txt\u0026#34;); promptTemplate.AddVariable(\u0026#34;input\u0026#34;, input); promptTemplate.AddVariable(\u0026#34;schema\u0026#34;, databaseSchema); var functionResult = await promptTemplate.InvokeAsync(kernel); var generatedSQL = functionResult.GetValue\u0026lt;string\u0026gt;().Replace(\u0026#34;```sql\u0026#34;, \u0026#34;\u0026#34;).Replace(\u0026#34;```\u0026#34;, \u0026#34;\u0026#34;); _logger.LogInformation(\u0026#34;Generated SQL: {0}\u0026#34;, generatedSQL); var queryResult = await ExecuteSQLAsync(generatedSQL); return queryResult; } 当我们拥有可以 “思考” 的 Agent 以后，可以非常容易地为其扩展出 Text2SQL 能力。如下图所示，当用户给定一个查询时，首先是通过大模型生成 SQL 语句，其次是执行 SQL 语句返回结果，最后是大模型观察结果生成最终答案：\nReAct 模式驱动下的 Text2SQL\r以表格的形式向用户呈现答案\r当然，这种方案本质上依赖于模型的推理能力。对于常规查询，大模型通常可以做到游刃有余。然而，对于复杂的查询，如子查询、窗口函数、LeetCode 等，大模型往往开始显得力不从心，经常出现下面这几类问题：\n生成的 SQL 有语法错误 使用了未定义的函数/变量 查询结果重复 我个人认为，总体而言，Text2SQL 技术虽有不足，但谓瑕不掩瑜，它让大模型有了连接关系型数据库的可能。当人工智能成为一种通用能力和基础设置，它便不应该再成为普通人使用和学习的门槛。从这个角度出发，我们应该让世间万物都和大模型连接在一起，未来它应该会变成得像水、电、天然气一样不可或缺，大家觉得呢？\n对效率的反思 作为程序员，或者说是技术人员，我们总是天然地追求着效率的最大化，甚至我们做过的每一个信息化的项目，都在加速着信息在互联网中的流动。可遗憾的是，我们是否掉进了由技术编织而成的名为“效率提升”的陷阱呢？正如我们有各种各样的聊天软件，可我们不见得就知道坐在你对面的那个人此刻在想些什么，甚至于我们在群组中被 @ 的时候，我们不见得有耐心读完、读懂全部信息，但我们还是要礼节性的打上一个:white_check_mark:。如今，无论天涯还是咫尺，都可以顺着网线来到你面前，这是“效率提升”这场运动带来的便利。可与此同时，在微博、小红书、抖音等社交媒体的影响下，这个世界亦开始变得不再那么真实。在一个程序员心目中，技术是没有立场、绝对正义的，可如果连正义都只是相对的，那么修饰正义的“绝对”就变得讽刺起来。于是，我们看到，在算法的驱使下，每个人被关进信息茧房，从此只能看到自己想看到的；于是，我们看到，在算法的算计下，外卖员的时间不断压缩，从此只能靠违章来争分夺秒；于是，我们看到，在敏捷的理论下，开发的周期越来越短，从此只能靠加班来卷赢同行……\n当无人驾驶走进现实\r我当然相信新的技术能带来新的机遇，但我们真正应该追求的效率，是那种可以让大多数人感到幸福的效率。百度发展 L4 级别无人驾驶，这件事情当然是正确的，甚至是非做不可的事情。可至少在当下这个时刻，提升效率并不是唯一正确的事情，就像生成式 AI 可以快速生成文本、图片甚至是视频，但这件事情并没有让我们从繁重的事务性工作中解脱出来，反而造成了人类世界的内卷，并且这种内卷完全没有赢的可能。以最新的 Claude 3.5 Sonnet 模型为例，它在某些情况下甚至可以媲美高级研究人员。这种效率提升对于普通智力水平的职场人来说，无异于降维打击。在当下这样一个充满危机感的职场环境下，像萝卜快跑这样的效率提升，除了断送掉“铁人三项”以外，我认为它并不会让更多的人感到幸福。一句话简单总结，事情是对的，但不合时宜。在职场中有一种潜规则，即：在工作中为员工设置各种障碍，从而使得每一个人都有事可做，这可以认为是职场中的不宣之秘。技术当然没有立场，有立场的从来都是人，如果一部分人坚持要提升效率、破旧立新，那只能说明在某个地方真的是有利可图。\n本文小结 我有一个不太好的习惯：当感觉心中的想法不足以填满一篇博客时，我总想释放出脑中的所有念头，并试图将它们全部塞进同一篇文章中。这样做的后果是，我写出的文章题目与文章内容间只有松散的联系。如果你读到这里，依旧感到困惑，请允许我在此表达歉意。可能是因为我每天都在接触大量碎片化信息，难以在短时间内整理出清晰的知识体系。特别是在我实现 StepwisePlanner 这段时间，我突然觉得 LLM Agent 索然无味。尽管，我每天都能在微信群里看到大量 AI 资讯，整个行业看起来一派繁荣，但现实中频频传来的失业潮、AI行业无的放矢的普遍现象，再加上西安持续下了半个多月的雨，绵绵不绝，让我深切感受到现实与理想之间的鸿沟。因此，当我开始写这篇博客时，我想表达的远不止 Text2SQL 这样简单，可一时间我只能找到这样一个话题；正如我想做的是 Agent，可是目前我只能做到这种程度。当我向同事演示 Text2SQL 时，他安慰我说：“这不是你能力问题，现在模型就这样”。我开始有些分辨不出这句话的真假，人类是如此地渴望情绪价值，可如果 AI 满足了你所有的情绪需求，你真的会感到满足吗？人类明明拥有足够的上下文信息来推导出答案，但人们不见得就会主动说出来，不是吗？\n","date":"2024-07-15T20:42:23Z","image":"/posts/semantic-kernel-driven-text2sql-practice/24e44881-f8b8-42b0-845f-09e16e674b47_1721116483631011226~tplv-a9rns2rl98-web-image.jpeg","permalink":"https://qinyuanpei.github.io/posts/semantic-kernel-driven-text2sql-practice/","slug":"Semantic-Kernel-Driven-Text2SQL-Practice","tags":["Agent","Text2SQL","AIGC","大模型"],"title":"Semantic Kernel 视角下的 Text2SQL 实践与思考"},{"categories":["生活感悟"],"content":"\r昨天，我去看了由胡歌和高圆圆主演的电影《走走停停》。当我踏入影院时，这部影片已上映了半月有余。在这个注意力稀缺的时代，观众总是被更新奇、更热闹的事物所吸引。这部自带文艺片底色的电影，让我在不经意间体验了一次 “私人影院”。当时，电影院里有且只有我一个观众。当电影中的桥段戳中我的笑点时，我总担心自己的笑声是否太过于放纵。事实上，我个人非常享受那一刻，甚至愿意将其视为 i 人的 “至爽时刻”。当思绪在戏里戏外游曳直至跳跃时，我的心情亦如同黑白照片、涓涓细流一般，于平淡之中泛起无穷回味。\n私人影院\r影片讲述了 “北漂” 中年人吴迪（胡歌饰），在职场和情场双双受挫后，选择回归故乡拍摄电影的故事。近年来，从 “逃离北上广” 到 “孔乙己的长衫” ，从 “35岁危机” 到 “Gap Year”，一个个热议话题此起彼伏。这些爆梗的流行背后，实则映射出人们心态上的种种变化。影片伊始，通过一系列闪回镜头，吴迪在故乡小镇的困窘扑面而来。按照 “三十而立” 的传统观念，吴迪妥妥是个大龄剩男，他不得不面对来自周围人的关于 “催婚”、“择业” 的各式嘈杂。在拒绝了父亲动用一切资源谋取的差事后，他选择再次回到自己的舒适区，甚至能在遇见老同学冯柳柳（高圆圆饰）时，坦然告诉对方自己正在 Gap 中、正在韬光养晦，哪怕彼时他正站在一群老年人中间排队领鸡蛋。\n韬光养晦\r如同平凡世界里的芸芸众生，吴迪在工作中颇具想法，自认有点小才华。当作品被人抄袭时，他并未未太过在意；当女友要和他分手时，他便打车送她离去。冯柳柳，一个向往大城市和大舞台的小记者，她决定为这位从北京归来的老同学拍摄纪录片。可镜头前的光鲜亮丽，实难掩饰吴迪内心的落寞，竟要靠小学时的奖杯来撑撑场面。冯柳柳觉得这样的纪录片不大真实，当她得知吴迪手上有个不错的剧本时，她鼓励吴迪将其拍出来，而她则负责记录整个电影拍摄的过程。自此，一系列啼笑皆非的故事，开始在吴家这间狭小的两居室内上演，哪怕这个剧组是个由业余爱好者组成的 “草台班子”，从导演、录音到女主角全都是新手不说，唯一的男主角则是个话剧团的冷门演员。\n冯柳柳看剧本\r“戏中戏” 是元电影的典型特征，在本片中你至少会看到四场戏。导演龙飞内心或许想拍摄严肃作品，影片中他曾向伍迪.艾伦和小津安二郎致敬，可面对市场他不得不将影片包装为戏剧电影，这构成第一层戏；吴迪眼中的《似是故人来》，是高雅艺术、是梦想，可在父亲眼中不过是老年人的 “出轨” 日常，是不可外扬的 “家丑”，这构成第二层戏；冯柳柳追求记录片的真实，可拍摄过程本就充满套路与技巧，特别是在经过上司的恶意剪辑后，真实与谎言的界限变得极为模糊，这构成第三层戏；吴迪的父母对彼此感情上的 “劣迹” 心知肚明，可是都没有互相戳破，依然在家人面前维持着这种体面，这构成第四层戏。生活或许如同《罗生门》一般，本就充满了表演的痕迹。\n“草台班子”\r从某种意义上说，吴迪一家人在人生观念上构成一条连续光谱。吴父站在光谱最左端，持悲观态度，认为人生变化无常，习惯成自然；吴母江美玲处于光谱中间地带，持中庸态度，她的人生虽有遗憾，可她愿意让儿子大胆尝试，坚定地支持儿子的想法；而吴迪站在光谱的末端，持乐观态度，外表或许平凡，内心却坚韧不拔，永不言败。在冯柳柳的镜头记录下，他曾装腔作势地表达道，“人不应该为了生活而毁掉生活的目的”。也许，他会偶尔记不起这句话到底出自哪位哲学家，是尼采还是海格尔？然而，生活总有拨云见日、柳暗花明的那一天，当困惑消散时，他总能在茶几上的《人生的智慧》中找到答案。电影拍摄成为母子间相互支持的桥梁，母亲以实际行动支持儿子的文艺创作，而吴迪则借助电影帮母亲实现艺术梦想。也许，去河边拍戏的那一天，正是她生命里最快乐的一天呢！\n现实中的岳红是个抗癌英雄\r不同于《水浒传》中逆来顺受的林冲，吴父在生活中常因情绪激动而掀桌子，尽管每次都被妻子和女儿提前预判。或许，母亲江美玲更像是这个家庭的粘合剂，以她的温柔和细腻将家庭成员紧密相连。吴迪母亲制作做绿豆汤场景，总让我联想到是枝裕和的电影《步履不停》，同样是一对母子，儿子埋怨母亲为什么不买点好的冰淇凌，母亲则微笑着拿出两支金属勺子……当吴迪的电影拍到一半的时，母亲江美玲因为心脏病突发意外去世，一切似乎都戛然而止。不知过了多久，当吴家父子看着电影镜头里的江美玲时，他们决定给《似是故人来》这部电影一个结局，因为他们都知道，江美玲最不喜欢半途而废。我认为，《走走停停》的精妙之处在于它的克制，它没有国产电影惯用的煽情和俗套，就是将一个故事故事娓娓道来。生活可不就是这样，哪来那么多戏剧性的反转呢？\n吴迪会一直开出租吗？\r直到故事落幕，吴迪和冯柳柳并没有因为所谓的 “套路” 而走到一起。当他们的车同时堵在高架上时，时而前行，时而驻足，时而超越他人，时而又被他人超越，这是走走停停的具象化。苏轼说：“人生如逆旅，我亦是行人”，在这条单程的人生路上，走走停停才是常态，远比一味向前更贴近真实。在我看来，一个成年人最大的自律是允许一切发生，允许别人是别人，允许自己做自己。虽然制造羁绊、构建回忆是人类的普遍选择，可在命运的际遇里，相遇和分别不过是人生的常态。从这个角度来看，遇见是缘分，不遇见亦是缘分，只要那相逢曾短暂地照亮过彼此，便不虚此行。影片的彩蛋部分，江美玲因《似是故人来》获得最佳新人奖，而饰演她的岳红则凭借《走走停停》获得最佳女配角，这无疑是演员与角色之间的相互成就。古语有云：“人生不相见，动如参与商”。回首向来，遗憾常用，成全难得。正如某档播客里说：结婚是为了幸福，离婚亦然，单身亦然。也许，唯有不执着，方能得始终。\n提及苏轼这位历史人物，我最近一直在读李一冰的《苏东坡新传》。站在中年的视角，重新去审视苏轼的生平。我意识到，以前从诗词里解读出来的苏轼，着实显得肤浅和虚幻。我甚至觉得，林语堂先生笔下的苏东坡，其幽默与达观，可能更多的是作者自身的心理投射。苏轼自二十岁离开故乡四川眉山赴京赶考，往后余生其足迹遍布东南，用 “走走停停” 来形容他再合适不过。越是深入了解历史，我越发觉得，李白的浪漫、白乐天的疯魔、苏东坡的豁达，或许都只是我们的一厢情愿。毕竟，我们根本无法想象，不到四十岁的苏轼在黄州狩猎时，开始自称老夫；那些意气风发的诗篇，如《赤壁赋》、《念奴娇·赤壁怀古》，偏偏是创作于他被贬黄州、内心苦闷忧郁之际。\n东坡足迹图\r读书时，我曾以 “苏轼集儒释道三家于一身” 之言， 语惊四座。可如今看来，苏轼本人或许并不乐于背负如此众多的信仰标签。作为一个四川眉山人，他的足迹遍布于天下，虽求佛问道，却难以寻觅到真正的答案，不得不以豁达、潇洒示人。时过境迁，无论是王安石还是司马光，都与苏轼渐行渐远。到了公元 1086 年，这两位因变法而水火不容的人物均已离世，只有苏轼还奔波在贬谪的路上。或许，像苏轼这样性情中人，秉持正义，本来就不该深陷宦海浮沉，可回顾苏轼在凤翔、杭州等地的政绩，可知他确实曾为民众福祉竭尽心力。苏轼政治生涯中的坎坷波折，宋慈欲澄清寰宇而不得的疲惫，大抵是殊途同归。如今，苏堤与西湖因苏轼而名声远扬，这何尝不是一种慰藉呢？\n","date":"2024-06-23T20:42:23Z","image":"/posts/g-for-gap/P2909066866.jpg","permalink":"https://qinyuanpei.github.io/posts/g-for-gap/","slug":"G-for-Gap","tags":["走走停停","苏轼","影评","感悟"],"title":"走走停停，允许一切发生"},{"categories":["编程语言"],"content":"当提及 ChatGPT 等生成式 AI 产品时，大家第一时间想到的是什么？对博主而言，印象最为深刻的是其流式输出效果，宛如打字机一般流畅。相信大家都注意到了，我给博客增加了 AI 摘要功能。虽然，这是一个非常“鸡肋”的功能，可是在光标闪烁的一刹那，我居然产生了一种“对方正在输入”的莫名期待。然而，此时此刻，理性会告诉我们：ChatGPT 的流式输出并不是为了让 AI 更“像”人类，它本质上是一种减少用户等待时长的优化策略。相比于人类的闪烁其词，心直口快或许更接近 AI 的真实想法。图灵测试，是一种用于判定机器是否具有智能的测试方法，其核心在于：如果程序表现出的行为与人类相似，我们便认为它具备了智能。当然，人机的不可区分性，同样带来了心理、伦理和法律上的问题。这便引出一个问题：人工智能，是否真的有必要像人类一样？有没有一种可能，让人工智能不那么地像人类，这反而是一种更加明智的做法？带着种种疑问，博主酝酿出了这篇文章，关于 ChatGPT 的流式传输，你需要知道的一切都在这里。从这一刻开始，“Attention Is All You Need”！\nServer-Sent Events 目前，在众多生成式 AI 产品中，对话框依然是最普遍的产品形态。因此，当你准备开发一款 AI 应用时，实现“流式传输”功能是基本要求。正如矛盾先生所言，“模仿是创造的第一步”，所以，让我们先来看看 ChatGPT 是如何实现这个功能的。ChatGPT 早期使用的是 Server-Sent Events 技术来实现流式传输。然而，截止到博主写作这篇文章时，ChatGPT 中流式传输的实现已升级为 WebSocket。不过，这个话题还是值得探讨一下的，因为市面上依然有大量的项目在使用这个技术，我们姑且将其理解为，一笔由 OpenAI 引领而产生的技术债务。关于 Server-Sent Events 的基本概念，大家可以参考博主以前的博客 基于 Server-Sent Events 实现服务端消息推送：\nServer-Sent Events 基本原理示意图\r下面，我们以 Kimi 为例来进行说明。通过观察浏览器的请求过程，足以一窥 Server-Sent Events 的个中奥妙。\nKimi 在浏览器中的请求过程 - A\r首先，Server-Sent Events 是基于 HTTP 协议的，其响应结果中的 Content-Type 取值为 text/event-stream。\nKimi 在浏览器中的请求过程 - B\r其次，Server-Sent Events 以事件流的形式向客户端返回数据，这些数据放在 Data 字段中。此时，客户端只需要从 Data 字段中提取内容，再将其显示到界面上即可，这样便可以快速地实现流式输出效果。按照这个思路，我们可以提供一个简单的实现，如下面的代码片段所示：\n[HttpGet(\u0026#34;streaming\u0026#34;)] public async Task StreamingAsync() { HttpContext.Response.ContentType = \u0026#34;text/event-stream\u0026#34;; foreach (var item in text) { var payload = JsonSerializer.Serialize(new { text = item.ToString() }); var message = $\u0026#34;data: {payload}\\n\\n\u0026#34;; await HttpContext.Response.WriteAsync(message, Encoding.UTF8); await HttpContext.Response.Body.FlushAsync(); await Task.Delay(200); } await HttpContext.Response.WriteAsync(\u0026#34;data: [DONE]\u0026#34;); await HttpContext.Response.Body.FlushAsync(); await HttpContext.Response.CompleteAsync(); } 接下来，我们只需要在客户端使用 EventSource API 即可，代码同样非常简单：\nconst eventSource = new EventSource(\u0026#39;https://localhost:7261/api/chat/streaming\u0026#39;); const messageList = document.querySelector(\u0026#39;#result\u0026#39;); messageList.innerText = \u0026#39;\u0026#39; eventSource.onmessage = (e) =\u0026gt; { if (e.data != \u0026#39;[DONE]\u0026#39;) { messageList.innerText += JSON.parse(e.data).text } }; eventSource.onerror = (e) =\u0026gt; { eventSource.close() } 由此，我们便可以实现下面的效果，这个效果和 ChatGPT 可以说是非常接近啦！\n通过 EventSource API 接入 SSE\r可惜，EventSource API 有一个致命的缺点：它仅支持通过 GET 方式发起请求，而在 ChatGPT 等聊天应用中通常需要以 POST 方式发起请求。如下面的代码片段所示，调用 OpenAI 的 Chat Completions 接口需要传递模型信息(model)、聊天历史信息(messages)、温度(temperature)等参数：\ncurl https://api.openai.com/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer $OPENAI_API_KEY\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;gpt-3.5-turbo-16k\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello!\u0026#34; } ] }\u0026#39; 此时，我们可以考虑使用 Fetch API 来解决这个问题。虽然微软提供了一个开箱即用的方案 Fetch Event Source，但是我建议大家最好还是亲自动手写一写。我承认，像 Axios 这样的第三方库固然非常好用，可一旦遇到这种不太常见的应用场景，你还是得回到 Fetch API 这种原生 API 上面，这再次印证了那句名言——没有银弹：\nconst response = await fetchWithTimeout(\u0026#39;https://localhost:7261/api/chat/streaming\u0026#39;, { timeout: 1000 * 60 * 10, method: \u0026#39;POST\u0026#39;, }); const reader = response.body.getReader(); const decoder = new TextDecoder(); let rawData = \u0026#39;\u0026#39;; while (true) { const { done, value } = await reader.read(); if (done) { break } rawData += decoder.decode(value); while (rawData.indexOf(\u0026#39;\\n\u0026#39;) != -1) { const lineIndex = rawData.indexOf(\u0026#39;\\n\u0026#39;); const message = rawData.slice(0, lineIndex); rawData = rawData.slice(lineIndex + 1); if (message.startsWith(\u0026#39;data: \u0026#39;)) { if (message.includes(\u0026#39;[DONE]\u0026#39;)) { break } const messageData = message.substring(5) messageList.innerText += JSON.parse(messageData).text } } } 这里使用了 Stream API 中的 ReadableStream 接口，它提供了一个getReader() 的方法，默认情况下，该方法返回一个 ReadableStreamDefaultReader 实例。这些术语听起来可能有些复杂，你可以将视为一种读取流数据的接口。每次调用 read() 方法时，就可以读取一部分数据。当返回的 done 属性为 true 时，表示读取完毕。由于读取出来的数据是二进制格式，我们还需要使用 TextDecoder 进行解码。此时，我们就可以突破 EventSource API 的限制，以 POST 的方式接入 SSE，具体示例见下图:\n通过过 Fetch API 接入 SSE\r或许，大家会好奇这里的 fetchWithTimeout() 方法到底是什么来路？事实上，它们仅仅是对超时做了处理的 fetch() 函数的封装。对于其中的 AbortController，请允许在下先卖个关子，谜底稍后便会揭开：\nasync function fetchWithTimeout(resource, options = {}) { const { timeout } = options; const controller = new AbortController(); setTimeout(() =\u0026gt; controller.abort(), timeout) const response = await fetch(resource, { ...options, signal: controller.signal }); return response; } IAsyncEnumerable 在上述示例中，使用 StreamingAsync() 方法返回数据时，我们采用了 foreach 语法顺序写入数据。这意味着我们必须在获取到全部数据之后，才能逐个发送给客户端。设想一下，如果我们是通过调用第三方 LLM 服务来向客户提供 AI 服务，那是不是要等所有数据都收集完毕再返回给客户端呢？根据以往的经验，这种做法是可行的，客户在调用你的服务时，必须考虑到调用第三方接口所需的时间。然而，在生成式 AI 的时代，这个想法就点不合时宜。此时，IAsyncEnumerable 这个接口便派上了用场。下面，让我们通过一个示例来做进一步的了解：\n// IEnumerable public static async Task\u0026lt;IEnumerable\u0026lt;int\u0026gt;\u0026gt; GetNumbersByEnumerable(int count) { var numbers = new List\u0026lt;int\u0026gt;(); for (var i = 0; i \u0026lt; count; i++) { await Task.Delay(200); numbers.Add(i); } return numbers; } // IAsyncEnumerable public static async IAsyncEnumerable\u0026lt;int\u0026gt; GetNumbersByAsyncEnumerable(int count) { for (var i = 0; i \u0026lt; count; i++) { await Task.Delay(200); yield return i; } } 其中，IAsyncEnumerable 是 C# 8.0 引入的一个接口，它允许以异步方式枚举集合。请看以下代码片段，你认为它的输出结果会是什么？毕竟这两段代码在功能上非常相似，都是遍历一个集合并输出其内容：\n// 遍历 IEnumerable 打印 Console.WriteLine(\u0026#34;IEnumerable: \u0026#34;); foreach (var item in await GetNumbersByEnumerable(10)) { Console.Write(item); } Console.WriteLine(); // 遍历 IAsyncEnumerable 打印 Console.WriteLine(\u0026#34;IAsyncEnumerable: \u0026#34;); await foreach (var item in GetNumbersByAsyncEnumerable(10)) { Console.Write(item); } 答案揭晓，结果非常的 Amazing 啊！尽管这两个方法都是异步的，可在打印时就可以明显地看出它们的差异：前者是一次性输出，而后者则是流式输出。我在不经意间完成了一次点题，更重要的是， 对于 IAsyncEnumerable 类型，你只需要为在其前面增加 async 关键词修饰即可，并不需要使用 Task 类型对其进行包装：\nEnumerable 与 AsyncEnumerable 对比\r可以说，这个类型天然地对 LLM 友好。例如，博主尝试通过 Ollama 接入 Mistral、Llama 3 等模型。条件受限，使用纯 CPU 进行推理。此时，如果调用非流式的接口无疑会花费大量时间，虽然我们可以在前端增加过渡动画，可这终究无法改变其生成慢的本质。此时，我们可以尝试下面的思路：首先调用流式接口，然后获取其响应流，再配合 IAsyncEnumerable 接口将其输出到 SSE 端点。下面是调用 Ollama API 接口的代码片段：\npublic async IAsyncEnumerable\u0026lt;string\u0026gt; ChatStreamAsync(OpenAIModel request) { using var httpClient = _httpClientFactory.CreateClient(); var httpRequest = new HttpRequestMessage(HttpMethod.Post, new Uri($\u0026#34;{_baseUrl}/v1/chat/completions\u0026#34;)); httpRequest.Content = new StringContent(JsonConvert.SerializeObject(request), Encoding.UTF8, \u0026#34;application/json\u0026#34;); var httpResponse = await httpClient.SendAsync(httpRequest, HttpCompletionOption.ResponseHeadersRead); httpResponse.EnsureSuccessStatusCode(); using var responseStream = await httpResponse.Content.ReadAsStreamAsync(); using var reader = new StreamReader(responseStream); var line = string.Empty; while ((line = await reader.ReadLineAsync()) != null) { if (string.IsNullOrEmpty(line)) continue; if (!line.StartsWith(\u0026#34;data: \u0026#34;)) continue; var data = line.Substring(\u0026#34;data: \u0026#34;.Length); if (data.IndexOf(\u0026#34;DONE\u0026#34;) == -1) yield return JObject.Parse(data)[\u0026#34;choices\u0026#34;][0][\u0026#34;delta\u0026#34;][\u0026#34;content\u0026#34;].Value\u0026lt;string\u0026gt;(); } } 现在，我们便可以像下面这样实现基于 Server-Sent Events 的流式传输接口：\nHttpContext.Response.ContentType = \u0026#34;text/event-stream\u0026#34;; await foreach (var item in ChatStreamAsync(new OpenAIModel(){ Model = \u0026#34;mistral:7b\u0026#34; })) { var payload = JsonSerializer.Serialize(new { text = item }); var message = $\u0026#34;data: {payload}\\n\\n\u0026#34;; await HttpContext.Response.WriteAsync(message, Encoding.UTF8); await HttpContext.Response.Body.FlushAsync(); await Task.Delay(200); } await HttpContext.Response.WriteAsync(\u0026#34;data: [DONE]\u0026#34;); await HttpContext.Response.Body.FlushAsync(); await HttpContext.Response.CompleteAsync(); 从某种意义上讲，你可能会觉得，大模型的流式接口都可以使用 IAsyncEnumerable 类型来进行承载？这种感觉是完全正确的。实际上，在微软的 Semantic Kernel 项目中，就可以看到 IAsyncEnumerable 的应用：\nSemantic Kernel 中的 IAsyncEnumerable\r事实上，JavaScript 中的 异步生成器，无论其概念还是语法，皆与 IAsyncEnumerable 非常相似：\nasync function* foo() { yield \u0026#39;a\u0026#39;; yield \u0026#39;b\u0026#39;; yield \u0026#39;c\u0026#39;; } let str = \u0026#39;\u0026#39;; async function generate() { for await (const val of foo()) { str = str + val; } console.log(str); } generate(); // \u0026#34;abc\u0026#34; 停止/取消生成 OK，对于流式传输这个话题，无论是前端还是后端部分，我想现在大家都可以做到得心应手啦。实际上，我们还有一个问题没有解决，那就是停止/取消生成。我发现，长时间处于舒适区，人们总不免会忽略某些问题。例如，在工作中习惯了 RESTful API 的我们，一旦需要通过 HttpClient 处理响应流，便可能感到困惑。对于前端开发者来说，调用 API 往往不会考虑取消操作，就像随意向天空开枪，结果嘛听天由命。可现在，这个问题在生成式 AI 中再次浮出水面，果然，“出来混还是要还的”。因此，接下来，我们将讨论在生成式 AI 里如何处理停止/取消生成的问题。\nasync function fetchWithTimeout(resource, options = {}) { const { timeout } = options; const controller = new AbortController(); setTimeout(() =\u0026gt; controller.abort(), timeout) const response = await fetch(resource, { ...options, signal: controller.signal }); return response; } 我们还是以这段代码为例来进行说明，这里实例化了一个 AbortController，并在 fetch() 函数中传入了 signal 参数，该参数的类型为 AbortSignal。可以注意到，这个 signal 参数值来自 AbortController 的 signal 属性。由此，我们可以了解到：fetch() 函数的取消机制，需要搭配 AbortController 来使用，正如 .NET 里 Task 的取消需要搭配 CancellationToken 使用一样。下面是一个更为直观的代码示例：\nconst controller = new AbortController(); const signal = controller.signal; fetch(url, { signal: controller.signal }); // AbortSignal 可以监听事件 signal.addEventListener(\u0026#39;abort\u0026#39;, () =\u0026gt; console.log(\u0026#39;abort!\u0026#39;)); controller.abort(); // abort() console.log(signal.aborted); // true 特别地，对于 fetch() 函数超时问题的处理，更推荐使用下面的方式，因为 AbortSignal 里提供了一个静态方法：\nconst res = await fetch(url, { signal: AbortSignal.timeout(5000) }); 至此，我们可以在前端页面实现对 Server-Sent Events 的取消/停止生成控制，如下图所示：\n在前端部分实现对 Server-Sent Events 的取消/停止生成控制\r可是，这个问题是否就此结束？对于当下的生成式 AI 而言，其核心任务是依据用户输入生成符合预期的答案。那么，当用户从前端页面触发了取消/停止生成动作以后，其真正期望的是不再继续为当前输入生成答案。例如，用户的预期是生成一篇 800 字左右的高考作文，但是在 AI 生成了 200 多个字以后，用户决定取消/停止生成后续的内容，我们是否能真正地让这一切停下来？当我在 Axios 中看到 CancelToken 的设计时，我不由自主地将其和 .NET 中的 CancellationToken 相比较。可遗憾的是，当前端通过 AbortController 发起取消请求时，后端服务器通常不会自动感知到这个取消动作，甚至 HTTP 协议本身都不支持取消请求的标准机制。\n基于 WebSocket 的双向通信机制实现取消/停止生成\r如图所示，博主设想了一种基于 WebSocket 的双向通信机制实现取消/停止生成的方案，当前端触发了取消/停止生成动作以后，后端调用 CancellationToken 的 Cancel() 方法，而这个令牌会被传递到文本生成流程中，当发现当前令牌被取消了以后便会引发异常，从而中止当前的文本生成流程。想法是挺美好的，代价是前端需要为每一次请求生成一个标识，并为每个标识映射一个 CancellationTokenSource。下面是一个基于 SignalR 的“拙劣”实现：\n// 生成文本 public async Task Generate(string requestId, string prompt) { var cts = new CancellationTokenSource(); _cancellationTokens[requestId] = cts; await foreach (var item in _textGenerator.Generate(prompt, cts.Token)) { var message = JsonSerializer.Serialize(new { text = item }) await Clients.Caller.SendAsync(\u0026#34;ReceiveChunks\u0026#34;, message, requestId, cts.Token); } } // 取消生成 public async Task Cancel(string requestId) { if (_cancellationTokens.TryGetValue(requestId, out var cts)) { await cts.CancelAsync(); await Clients.Caller.SendAsync(\u0026#34;GenerationCancelled\u0026#34;, requestId); _cancellationTokens.Remove(requestId); } } 本文小结 在与 Ollama 的 API 进行对接时，我发现使用流式 API 是一个不错的选择，因为结合着 IAsyncEnumerable 来使用的话，对于聊天体验的提升可以说是聊胜于无。在开发 AI 应用的过程中，我深刻体会到，当 AI 与人交流的时候，采用“流式传输”可以使人觉得 AI 真的有在思考，甚至有像人类一样组织着词句。然而，当 AI 与 API 交互时，“流式传输”可能会不太合适，特别是在 Function Calling 以及需要结构化输出的地方。Transformer 模型通过预测下一个词元来进行文本生成，其本质上与人类逐字发音、然后组成句子和段落的行为方式相似。唯一的区别在于，我们只有在写作的时候会按照某种结构去组织文本，除非我们需要带着讲稿在公众面前演讲，否则在任何情况下，我们基本上都是按照“流”的方式在传递信息。从这个视角来看，像 RESTful API 或者 RPC 那样按部就班的对话方式，可能并不是“正确”的选择。过去，我常被告知在群组里回复消息要及时，要用“好的”、“收到”这些词汇，现在回想起来，我只觉得风声中夹杂着荒诞呼啸而过。如你所见，在这篇博客里，我介绍了生成式 AI 里的基本功能——流式传输，从 Server-Sent Events 到 IAsyncEnumerable 再到 Fetch API 的取消机制，这些看似无关的想法被我串联在了一起，甚至写成了一篇文章。我不禁要怀疑，大模型的智能是否仅仅是文字的排列组合？而人类的知识是否仅仅是信息的高度压缩呢？\n","date":"2024-06-06T12:52:10Z","image":"/posts/everything-you-need-to-know-about-streaming-with-chatgpt/edoardo-bortoli-cPZutxQCeIc-unsplash.jpg","permalink":"https://qinyuanpei.github.io/posts/everything-you-need-to-know-about-streaming-with-chatgpt/","slug":"Everything-You-Need-to-Know-About-Streaming-with-ChatGPT","tags":["SSE","OpenAI","ChatGPT","流式传输"],"title":"关于 ChatGPT 的流式传输，你需要知道的一切"},{"categories":["生活感悟"],"content":" 昨晚，洗漱完毕后，我躺在床上刷着抖音。可在连续刷了十几条短视频以后，我突然对眼前的一切感到深深的绝望。或许是高德地图在抖音购买了推广，我的耳边一直充斥着同一条文案：我可以死在去远方的路上，但不能没去过远方。我心想，五一假期刚刚过去两周，资本真的需要在这个时间点，再次点燃人们“仗剑走天涯”的梦想吗？仔细一看，原来是高德地图新上线了一个“点亮城市”的功能，可以显示用户去过那些城市。Payne 说，一切能被拿来攀比的终将被拿来攀比。于是，在马头琴那深沉而悠扬的旋律衬托下，《边境》这首歌一旦响起，甚至比许巍的《蓝莲花》更具杀伤力。看着由大数据量产而来的、模式化的文案/配乐，我不免惶恐。究竟是从什么时候开始，数字幽灵夺舍碳基生命。一眼望去，互联网上充斥着各种电子压缩饼干，它们试图将日常生活高度提纯、浓缩、收敛，并使你相信那就是真实的生活。听起来像是硅基生命要统治地球？我想说的是，在 GPT4 以及未来的 AIGC 面前，这不过是一种迷人的 tricks。相比于被一小部分人类愚弄，我宁愿被 AI 愚弄，因为它真的要比我聪明许多。对此，我心悦诚服。\n如何杀掉一只羊？并生活在真实之中\r李诞有一期播客，标题叫做《如何杀掉一只羊？并生活在真实之中》，他以一个内蒙古人的视角，讲述了真实的内蒙古是如何杀掉一只羊，以及真实的内蒙古是什么样子。你或许听说过，下面这些关于内蒙古的刻板印象，比如：住在蒙古包里、高考射箭骑马、人均摔跤能手等等。如果现在给你两个选项，一个是“虚假但快乐”，一个是“真实且复杂”，你会怎么选呢？对此，李诞给出的答案是：让迪士尼成为迪士尼，让内蒙古成为内蒙古。我有时觉得，一生都特别“关键”的中国人，或许连整个人生都是打卡式的。君不见，小红书的打卡基本等同于去过一个地方，大家使用着别人使用过的文案，站在别人站过的打卡点，做着别人做过的事情，然后拍照发图。在种种确定性的背后，全然没有自由、随机的探索，更遑论深入理解。再比如大家喜闻乐见的爬山，现在全都是统一的“登山杖十八式”造型，我实在想不明白，曾经引领过非主流文化的90后，以追求个性、与众不同为荣，何以如今变得千人一面、毫无新意呢？香港演员梁家辉曾被人称为“千面影帝”，他在电影中饰演各种类型的角色，从严肃的历史人物到喜剧角色，从黑帮老大到普通市民，他都能精准地把握角色特点。可如今的朋友圈、微博、小红书，更像是某种角色扮演游戏，批量生产着紫萱/紫霞。假使换一张脸上去，坦白讲又有什么区别呢？只要你愿意，想你的风可以吹到月球甚至火星。\n虽然，我不明白这个姿势是什么意思，可大家都说帅，那就是真的帅\r在接触 ChatGPT 这类 AI 产品以后，一个最为直观的感受是，当 AI 完全掌握了小红书这类文案的写作套路以后，它完全可以取代人类写出更加“爆款”的文案。在这种前提下，在一堆充斥着 Emoji 的文字中间，自诩为高等智慧生物的人类，其实早已处在一种非常尴尬的境地。试想一下，如果让 AI 学会那些在流传在社交网络上的打卡地、拍照姿势、服装道具，那么，在这场名为模仿、盲从、跟风的竞赛中，人类到底还有多少胜算？也许，我们都不愿意承认，我们本质上就是一群习惯于攀比和跟风的乌合之众。可你只要仔细听一听，便会发现这座以“个性化推荐”为主要卖点的信息茧房，正在极速地降低我们的表达能力。整个平台/互联网的话语空间正在一点点塌陷，我们每天面对的词库严重同质、扁平和局限，越来越多的词语正在被压缩和稀释。每一天，大家都是面对着同一套网络流行语词库，越来越习惯于接收低信息密度的内容。这看似是大大降低了理解成本，可你回想一下，我们每天摄入的信息当中，真正有用的又有多少呢？ 《年会不能停》这个电影上映以后，我的同事特别喜欢讲“对齐颗粒度”，我不知道这是好事还是坏事。“家人们”，我唯一知道的事情是，我们渐渐患上了“失语症”，回忆一下，形容某种食物特别好吃，除了“咔咔炫”、“嘎嘎香”，你还能想起什么？如果是我，我会讲“唇齿留香”、“大快朵颐”等等。正如你会更喜欢我叫你朋友，而不是“老铁”或是“亲”，扪心自问，你果真喜欢这种小红书式的表达方式吗？\n一个擦得锃亮的鱼缸，虽然看起来漂亮精致，可惜触摸不到真实和丰富\r就像每一个短视频，我现在听到的都是视频制作者的声嘶力竭和急功近利，大致的含义都是在表达：如果你不 XXX，那么你就会 YYY。这里的 XXX 和 YYY 大家可以自行代入具体场景。我想说的是，即便是为了刺激流量，其途径并非只有贩卖焦虑这一条路。人都想看到自己想看到的一切，这当然无可厚非。不过，在我心目中“真”显得更为重要，就像刘亦菲饰演的花木兰，只有在卸下伪装时、以女性真实面貌出现时，她才会意识到自己有多么强大。比如,在发朋友圈的问题上，有人觉得应该要多发朋友圈，因为朋友圈的访客是未来的自己；有人觉得应该要少发朋友圈，因为成熟的人不会有那么多情绪写在脸上。我的观点是：想发就发，不想发就不发。在这一期播客的评论区，有人将当下的互联网，比喻为“一个擦得锃亮的鱼缸”，虽然看起来漂亮精致，可惜触摸不到真实和丰富。我有个朋友，经常开着 N 倍速看 B 站，我不免怀疑这是否和打卡式旅游如出一辙。或许，现在的 AI 可以在数秒内阅读完一篇文章或者一个网页，可这些知识永远都属于 AI 而不属于你。出去旅行的时候，我总想着避开网红店的纷扰，可大众点评和小红书总是会将你带去那些地方。曾经引以为傲的推荐算法，可以做到个性化推荐、千人千面，何以如今大家的兴趣爱好、言行举止越来越趋向雷同？到底是我们在通过消费行为驯化数据，还是我们心甘情愿被资本和消费主义驯化？\n虽然楚门离开了，可 “楚门的世界” 一直都在\r某时某刻，我意识到，这个世界是立体的、复杂的，而社交媒体则试图将其平面化、简单化。我承认，信息在经过浓缩化、碎片化、符号化以后会更利于传播。但是，一个丰富、多元、复杂的世界，是不是愿意被简化、该不该被简化呢？我想，这是一个非常深刻的问题。按照小红书上的理论，旅行的第一个阶段是穿梭于各大热门城市之间，可时间久了以后便觉得哪里都一样。此时，便进入第二个阶段：徒步、雪山、潜水、无人区、极限运动等等，在将以上种种都体验过以后，突然开始怀念城市里的人来人往、花开花落。此时，便进入第三个阶段：学习文化知识、欣赏古建筑、了解城市历史/变迁等等。再往后，第四个阶段便是走出国门、看世界。虽然，这只是其中一个版本，但是这足以说明问题。那就是，我们固执地认为这个世界看一次就够了，有多少人会在旅行过后故地重游呢？以一种符号式的、标签式的方式认知这个世界是否可行呢？自然是可以的，只不过这多少有些肤浅！那么，如何摆脱信息茧房的诅咒呢？我个人认为，首先，是从被动接收转变为主动订阅，比如使用 RSS 来代替微信公众号；其次，尽可能读英文的或者第一手的资料，比如官方文档、论文，而不是经过咀嚼或者裁剪等二次加工的产物；然后，定期调整个人行为/使用习惯，避免落入推荐算法的陷阱。大模型是可以生成更多的信息，可如果我们失去了高密度信息的读取能力，这一切又有什么意义呢，难道我们一定要让自己活成 GDP 和点击率吗？\n","date":"2024-05-16T11:17:23Z","image":"/posts/seeing-the-world-through-tiktok/water-8200502_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/seeing-the-world-through-tiktok/","slug":"Seeing-The-World-Through-TikTok","tags":["随笔","感悟","真实","参差"],"title":"从抖音看见世界的参差"},{"categories":["生活感悟"],"content":"五一假期首日，我突发奇想，决定前往某个南方城市。因为，在我屈指可数的旅行经历中，唯一去过的南方城市有且只有上海。离西安最近的“南方”自然是四川，因此，我规划了一条“广元—阆中”的旅行路线。或许，在大多数人眼中，只有成都能代表四川，可为了避开节假日的人潮汹涌，我选择探索这些小城市。阆中吸引我的地方，在于它是中国四大古城之一。自从年初游览过平遥古城以后，我对不同流派和风格的古建筑便产生了浓厚兴趣。阆中是一座极具三国特色的城市，刘备曾派遣张飞镇守此地达七年之久。对于历史爱好者来说，这简直是一种朝圣。若论广元的名胜古迹，剑阁无疑是最有名的地方。在各种短视频里，一句“老将军，可知天水姜伯约？”，让姜维和天水这座城市紧紧联系在一起，甚至连天水的有轨电车都以“伯约号”命名。然而，真正让姜维这个人物屹立起来，巍巍乎四百年大汉最后的脊梁，这个地方一定是剑阁。面对刘禅的投降，姜维曾悲愤地说：“臣等正欲死战，陛下何故先降?”。可冷静下来以后，他在密信中向刘禅表示：“愿陛下忍数日之辱，臣欲使社稷危而复安，日月幽而复明”。从天水到剑阁，你无法想象这份“孤忠”来自一个有羌人血统的异乡人。遗憾的是，五一假期并不浪漫，正当我沉浸在对历史的喟叹中无法自拔时，H 君告知我，阆中回西安的车票已售罄，这迫使我改变原来的计划。最终，我选择了“广元-汉中”的路线。幸运的是，这两个地方在某种程度上相互呼应着彼此，串联起从秦朝到唐朝近八百年的历史。\n《三国演义》中的姜维形象\r广元篇-故垒西边 广元的首站是皇泽寺，这是中国唯一一座供奉女皇帝武则天的祠庙。沿着依山而建的石阶向上攀登，从高处俯瞰，嘉陵江穿城而过的景象尽收眼底。或许是觉得三国年代久远、无从考据，广元将更多的曝光度留给了唐朝的武则天。因此在市区内，你可以看到则天路、则天路小学、则天小区等以武则天命名的地方。关于武则天的出生地，可谓是众说纷纭，莫衷一是。多年前我曾游览过洛阳的明堂天堂，如今再次听闻这位女皇帝的事迹，心中感慨不已。\n黄泽寺 \u0026amp;amp; 大佛楼\r黄泽寺石窟\r提及武则天及其建立的武周政权，历史上评价褒贬不一。若单单从治国能力来看，武则天堪称“巾帼不让须眉”：在军事上，她破吐蕃、败突厥，设立北庭都护府，维护了国家统一与稳定；文化领域，她继承并发展科举制度，首创“殿试”，打破了关陇集团近百年来的垄断地位；经济上，她重视农业生产，实施屯田和奖励农桑政策，促进了唐朝经济的繁荣。后世对武则天的批评，主要集中在任用酷吏、政治清洗以及豢养面首。由此可见，在一个由男性主导的社会结构内，女性的政治参与及其自主和力量的表现，常常被视为“逾矩”。当无法否定一名女性在政治、社会和文化领域的贡献时，批评往往转向道德层面。例如，《资质通鉴》的作者司马光，便是从李唐正统和程朱理学的视角，对武则天以女性身份称帝进行批判。\n皇泽寺上的嘉陵江\r嘉陵江夜景\r无可否认的是，武则天对打破封建礼教束缚以及解放女性方面产生了深远影响。如果以现代视角去审视武则天的一生，她无疑是在挑战传统史学中“红颜祸水”刻板印象。可有趣的地方在于，武则天并不符合女权主义者的定义，因为在她统治期间更多的是体现个人的政治成就，而非女性整体权力的提升。在武则天执政期间，她从未提倡或实施过女性平等的观念，其次行为和决策更符合当时的男性皇帝的行为模式。在广元的首日，细雨绵绵。当时，“胖猫”事件正成为网络热点，联想到江西那位“睁眼看世界”的觉醒姐，我忽然有种雾里看花的感觉。或许，如今的女性主义/女权/女拳，早已变得如这段历史般扑朔迷离。那些高呼着“Girls Help Girls”口号的女性们，在得知武则天做出还政李唐的决定后，是否会数落起这位女皇帝的不是，指责她为什么不让儿子李显改姓武？\n千佛崖石窟\r千佛崖上的嘉陵江\r听起来似乎荒诞不经，可荒诞之处又何至于此呢？在沸沸扬扬的“胖猫”事件中，不管是当事人还是旁观者，都以一种近乎癫狂的姿态参与其中。陈奕迅在《六月飞霜》中唱道：“最可笑的，喊亦正常；最悲壮的，笑亦正常”，这让我不禁思考，“哪一个可，发育正常”。广元的第二站是昭化古城，如果你对这个名字感到陌生，那么，葭萌关这个名字或许能瞬间唤起你的三国记忆，《三国演义》中著名的“张飞挑灯夜战马超”，正是发生在这里。再往前追溯五百多年，秦惠文王嬴驷派张仪、司马错伐蜀，同样在葭萌关发生过激战。现存的昭化古城是明代重建的，当我漫步在狭窄的古城街道上，我不断地将眼前的景象与心中的三国景象相比较。往事越千年，一切早已面目全非，昔日的年金戈铁马早已无处寻觅。这座在今天看来颇为逼仄的城池，何以能成为历史上兵家必争的战略要地？\n昭化古城-葭萌\r昭化古城-云雾缭绕\r葭萌关在传统意义上指的是昭化古城的西门，即临清门。临清门附近有敬候祠，那里安葬着蜀汉的第三位丞相费祎。诸葛亮在《出师表》中称赞他“志虑忠纯”。站在祠前远望，群山环抱，刘备曾评价此地为“弹丸之城，金汤之固”。从地理上看，广元位于四川最北端，是汉中通往蜀地的必经之路，葭萌关恰好处于广元与剑门关之间，是进入剑门关的咽喉要道，难怪张鲁要派马超来攻打葭萌关。一千多年后，我们从陕西进入四川，基本上与当年钟会、邓艾灭蜀的路线吻合。遥想当年，姜维面对钟会、邓艾、诸葛绪三路大军的攻势，以神来之笔巧妙夺取阴平桥头，与钟会大军对峙于剑阁。这一神级操作，或许只有后来的教员四渡赤水出奇兵能与之相媲美。\n昭化古城-敬候祠-费祎墓步道\r昭化古城-费祎墓碑刻与姜维塑像\r剑门关因李白的《蜀道难》而声名显赫，其地势险要，有“一夫当关，万夫莫开”的说法，这是姜维可以凭借 5 万兵马成功阻挠钟会的 15 万大军的关键。我相信，当你亲临剑门关前，你会深刻体会到其浑然天成的壮观。剑门关处于崇山峻岭之间，是从汉中到达成都的重要通道。两侧山峰耸立，地形上易守难攻。剑门关的地理优势，在一定程度上弥补了姜维兵力上的不足，在那个“兵马未动、粮草先行”的战争年代，姜维的策略是坚守到对方粮草消耗殆尽。可惜历史自有其进程，正当姜维和钟会在剑阁对峙的时候，邓艾偷渡阴平、翻越摩天岭、袭江油、破绵竹，剑锋直指成都。直到后主刘禅投降时，钟会依然没能突破剑阁防线。那一刻，我和伯约的心境无异，“臣等正欲死战，陛下何故先降?”。由于天气和时间限制，此次未能亲临剑门关，这无疑是一大遗憾。\n剑门关关楼\r汉中篇-克复中原 北方人常说，大海是他们的执念。于我而言，我更愿随遇而安：遇山则登山，遇海则观海。因为人一旦有了执念，便注定要变得痛苦。幸运的是，这次旅行遇见了两条江。嘉陵江之于广元，如同汉江之于汉中。它们各自从这两座城市中川流而过，连同那些绵延千年的历史和传说，一起汇入浩浩荡荡的长江。汉江，又称汉水，每当提及这个名字，我脑海中浮现的总是《三国演义》第七十一回的故事：占对山黄忠逸待劳，据汉水赵云寡胜众。我常常在想，赵云是否从韩信的“背水一战”中获得了灵感。或许，历史本就是循环往复的，正如“背水一战”的策略，同一时期的马谡和徐晃都曾经用过，可结果确是大相径庭。冥冥之中，好像有什么东西在牵动着我的情绪，即使如今的汉水边早已不复当年汉中之战的鼓角争鸣。这让我不禁想到了定军山、祁山、五丈原……这是我第一次感受到这种超越地理和时间的奇妙联系。《三国演义》里的故事从未像现在这样鲜活生动，我想，这或许就是历史的魅力所在。\n天下武侯祠展览馆\r马超墓祠\r汉中之行，首站是勉县的诸葛古镇，那里坐落着武侯祠和马超墓祠。此前，我只知道成都有武侯祠，得知全国现存七座武侯祠，我颇感意外。可想到丞相在国人心目中的地位，我觉得这一切又相当的合理。勉县武侯祠，始建于蜀汉景耀六年(263年)，是所有武侯祠中建祠最早、且唯一由皇帝下昭修建的祠庙。武侯祠内红墙青瓦，漫步其间，每当凉风拂背而过，周围树叶便开始梭梭作响，不由得令人想起《三国演义》里武侯定军山显圣的故事。蜀汉建兴十二年(234年)，诸葛亮是在在北伐途中病逝五丈原(今陕西省宝鸡市岐山县)，后主按照丞相遗愿将其葬于定军山下。武侯墓距武侯祠车程大约 10 分钟车程，位于汉水之南、定军山麓。遥想当年，老将黄忠就是在定军山斩杀夏侯渊，彻底扭转了刘备在汉中之战中的不利局面。战略上，汉中的重要性丝毫不亚于广元，是通往关中地区的地理要冲。诸葛亮六出祁山，最接近长安的一次是陈仓(今陕西省宝鸡市)，“悠悠苍天，何薄于我”。或许，英雄注定是要壮志难酬的，翻开史书，里面写满了意难平。\n武侯祠-天下第一流\r武侯北伐示意图\r武侯墓前的匾额上书“双桂流芳”四字，相传，墓前两棵千年桂树皆为三国时期所植。北周庾信在《枯树赋》中写道，“昔年移柳，依依汉南，今看摇落，凄怆江潭，树犹如此，人何以堪”。此情此景，如通年轻时的子弹正中眉心，我在心里不断重复着《兰亭集序》里的一句话，“俯仰之间，已为陈迹”。固然，武侯墓游客络绎不绝、门庭若市，而马超和费祎的坟墓则显得冷清寂寥。也许，人们都还记得那个白袍银甲的锦马超，曾在潼关前杀得曹操割须弃袍。可没有人知道，马超从归顺刘备到病逝，只有短短八年，享年四十七岁。苏东坡赤壁泛舟时曾感慨道：“酾酒临江，横槊赋诗，固一世之雄也，而今安在哉？”。作家当年明月在某次采访中提到，“所谓百年功名、千秋霸业、万古流芳，与一件事情相比，其实算不了什么。这件事情就是——用你喜欢的方式度过一生。”。我并非主张历史虚无主义，而是想说，那些你认为难以释怀的瞬间，在人类文明的长河中，甚至都不如一粒尘埃。人是可以放下某些执念，继续前行的，生命中并没有那么多非如此不可的选择。\n武侯墓前\r夜幕降临，灯火阑珊，我登上天汉楼，俯瞰天汉湿地公园。微风拂过，轻轻拨动屋檐下的铃铛，不时传来悦耳的声响。相传，当年曹操与刘备争夺汉中时，两军在汉水两岸对峙，曹操驻扎北岸，刘备则驻扎在南岸，双方僵持多日。后来，诸葛亮命人在曹军睡着以后，擂鼓呐喊、虚张声势，令本就多疑的曹操更加不安，担心刘备伺机偷袭。最终，曹操不得不退兵三十里以避其锋芒。可惜，辛弃疾笔下“天下英雄谁敌手”的曹刘，终究还是抵不过大浪淘沙，唯有脚下这条汉水，逝者如斯，不分昼夜。第二天，我游览了古汉台和拜将台。古汉台是刘邦被封为汉王时在汉中的行宫遗址，而拜将台则是韩信被刘邦拜为大将军时所筑的高台。世事浮沉，沧海桑田，汉家的旌旗不再随风飘扬。可在现场演员们的演绎下，刘邦与项羽“楚汉争霸”、韩信“明修栈道，暗度陈仓”、张骞“凿空西域”、诸葛亮“六出祁山”，一个个生动形象的历史人物，仿佛正在唤醒这片大地上沉睡的往事。\n古汉台与拜将台\r《汉颂》情景剧\r李白曾言“蜀道之难，难于上青天”。古蜀道两千年来基本维持“北四南三”的格局。蜀道的北段主要集中在汉中，包括陈仓道、褒斜道、傥骆道和子午道。魏延的“子午谷奇谋”便是计划通过子午道奇袭长安。事实证明，这是一个非常冒险的军事行动计划。公元 232 年，魏国大将军曹真从子午道出兵伐蜀，适逢雨季，栈道为雨水冲刷断绝，行军一个月，结果只走了一半的路程，无功而返；东晋恒温两次经子午道出兵，一次伐蜀，一次北伐前秦，结果第一次重蹈曹真覆辙，第二次被前秦军队包围；明朝末年，高迎祥带领 5 万大军用时 15 天通过子午谷，结果在谷口遇到了孙传庭的 2 万伏兵；1936 年西安事变后，国民革命军第 51 师师长王耀武，打算从子午道进军西安勤王护驾，结果在子午谷中走了三天，因为水源和补给问题，最终无功而返，在谷中三天后撤回。历史的讽刺之处在于，同一计谋一旦被使用，便永远成阳谋。四个人、五次用兵，皆以失败而告终，唯独杨玉环“日啖荔枝三百颗”，取涪州荔枝，经子午谷至长安，三日可达，畅通无阻。\n马伯庸的小说《长安的荔枝》即描绘了这一历史轶事，“一骑红尘妃子笑，无人知是荔枝来”，在李白眼中比登天还要难的蜀道，结果在给杨贵妃运输荔枝这件事情上“易如反掌”，不知李白闻此会作何感想呢？\n古蜀道地形示意图\r天汉楼夜景\r结语 夜色深沉，汉水寂静，历史的回声在涟漪中渐渐消散。楚河汉界间的龙争虎斗、韩信的智勇双全、诸葛亮的克复中原、姜维的“一计害三贤”……这些曾经激荡人心的故事，如今都化作了风中的一缕缕低语，轻轻拂过这片古老的土地。或许，生活在这片古老土地上的我们，是通过倾听来了解过去和书写未来，我们倾听风中传来的古人的智慧，我们倾听大地诉说岁月的沉淀，我们是见证者、更是参与者，就像历史长河中的一滴水，虽然渺小、短暂，但我们亦是河流本身。诚然，一切都会尘归尘、土归土。可正是尘埃，构成了我们脚下这片土地的厚重与深邃，不是吗？\n","date":"2024-05-10T11:17:23Z","image":"/posts/in-the-twinkling-of-an-eye/Seagulls-8583786_1280.png","permalink":"https://qinyuanpei.github.io/posts/in-the-twinkling-of-an-eye/","slug":"In-The-Twinkling-Of-An-Eye","tags":["游记","随笔","历史","感悟"],"title":"俯仰之间：五一小长假出行记"},{"categories":["编程语言"],"content":"有时候，我觉得人类还真是种擅长画地为牢的动物，因为突然发现，当人们以文化/理念的名义形成团体/圈子的时候，其结局都不可避免地走向了筛选和区分的道路。或许，大家都不约而同地笃信，在成年人的世界里，那条不成文的社交潜规则——“只筛选不教育，只选择不改变”。与千百年前百家争鸣不同，团体/圈子间并不热衷于交流，倒像是一种标签化的分类方式，甚至是一种非黑即白的二元分类方式。比如，通常人们认为男性不能讨论女性主义，可我经常在女性主义视角下看到对男性的讨论。女性朋友们一致认为，女性种种不幸完全是由男性以及男性背后的父权造成的。于是，在小红书上打着不被定义的标签的女性们，自顾自地定义着别人。亦或者，在这个内卷的世界里，人们被互相定义、被资本定义、被用户画像定义、被美颜相机定义……这种种的定义，最终会成为我们所有人的宿命。鲁迅先生说，中国人的性情是喜欢调和折中的，对此我表示怀疑。因为，以如今的现状而言，中国人或许更喜欢玉石俱焚。在我看来，标签是定义、是附和、是选择，无论我们是否知晓，那条路是否能代表未来。\n是非善恶 最近，Meta 发布了 Llama3，一时风光无二。微软不甘示弱，紧随其后发布了 Phi-3。曾经，我认为在小红书上检索信息比百度更高效，可当我批评完百度的竞价排名后，我发现小红书上的广告问题更严重，特别是 AI 的加入让这一问题愈发严重。回到 AI 话题，最近人们对于大模型的态度大致可以总结为：对 Llama3 和 Phi-3 寄予厚望，认为它们接近 GPT-4 的水平，而对 OpenAI 以及 GPT-5 的前景则持续看衰。我不太关心这些预期，我在意的是新模型发布以后，各路牛鬼蛇神都可以活跃起来。小红书上有一篇帖子提到，Llama3 的发布使得本地化 RAG 更有意义，并分享了一个使用 LlamaIndex 实现 RAG 的案例，随后是小红书上经典的套路：私信、拉群、发链接。我对帖子中的观点保留态度，因为 Llama3 作为大型模型，主要解决的是推理问题；而 RAG 是检索 + 生成的方案，其核心在于提高检索的召回率，即：问题与文本块之间的相关性。显然，无论 Llama3 是否发布，RAG 都能正常落地。大型模型的推理能力，影响的是最终的生成结果，而非检索的召回率。\n最简单的 RAG 范式\r故事的结局是我遭到了反驳，对方质疑我对 RAG 的理解，并建议我阅读她主页的某个帖子，据说是 RAG 论文作者在斯坦福的讲课内容。我原本是打算去学习的，可戏剧性的是，我被对方拉黑了。我还能再说什么呢？当然选择原谅对方。为了证明我对 RAG 的理解没有偏差，我决定分享我最近对于 Rewrite 和 Rerank 的体悟。我想明确指出的是，无需使用 Llama3，只要提升检索部分的召回率，RAG 方案完全可以实施。实际上，我们甚至都不需要 GPT-4 级别的模型，选择一个合适的小模型足矣。我意识到，我最大的错误在于，试图在一个以信息差为生意的人面前打破信息壁垒，帮助他人摆脱知识的诅咒。正如我之前所述，某些团体或圈子的目的并非促进信息流通和交流，而是为了向特定的人群提供通行证，以便在来来往往的人群中筛选和区分同类。或许，你会认为你已经筛选出你想要的人，但从更广阔的视角来看，这不过是另一种傲慢与偏见。当然，你们权利忽视这些问题，就像我不在乎周围环境如何一样。作为一个崇尚科学的人，我只关心真理，除非你的真理更为真实。\n实现 Rewrite 在 RAG 的语境中，Rewrite 是重写或者改写的意思。此时，诸位或许会困惑，为什么需要对用户输入的问题进行二次加工呢？在程序员群体中，有一本非常经典的书 ——《提问的智慧》，其核心观点是：在技术的世界里，当你提出一个问题时，最终能否得到有用的答案，往往取决于你提问和追问的方式。以此作为类比，众所周知，人类的输入通常随性而模糊，特别是在使用自然语言作为交互媒介的时候。在这种情况下，大语言模型难以准确理解人类的真实意图。因此，就需要对用户的原始查询进行改写，通过生成多个语义相似但是表述不同的问题，来提高或增强检索的多样性和覆盖面。由于重写后的查询会变得更为具体，故而，Rewrite 在缩小检索范围、提高检索相关性方面有一定的优势。例如，下面的提示词实现了对用户输入的改写：\n通过提示词实现 Rewrite 实际效果如何呢？我们可以分别在 Kimi 和 ChatGPT 中进行测试。如下图所示，左边为 Kimi，右边为 ChatGPT：\nKimi(左)/ChatGPT(右) 测试结果对比\r可以发现，Kimi 在改写的过程中，补充了更多的上下文，比如 “金庸”、“射雕英雄传”，而 ChatGPT 给出的答案则相对保守，甚至出现了 “奥义” 这种动漫作品中的元素。我承认，模型的推理能力是会影响到答案的生成，可这一切与 RAG 中的 Retrieval 无关，因为在那篇帖子里，对方分享的是最简单的 RAG 范式，自然就不包含 Rewrite 与 Rerank。所以，即便 Llama3 的推理能力得到了大幅度提升，它并不会对 RAG 有任何实质性的影响。不知这位女性朋友在评论区大杀四方的时候，是否想明白了这个道理？\n# 对问题进行重写的提示词 rewrite_prompt_template = \u0026#39;\u0026#39;\u0026#39; 你是一个帮助用户完成信息检索的智能助理，你的职责是将用户输入的问题，转化为若干个相似的问题，从而帮助用户检索到更多有用的信息。 此外，你还需要遵守下列约定： 1、生成的问题必须与原问题存在一定的相关性，至少 \u0026gt;= 50% 2、生成的问题必须与原问题相似或相近，不得改变用户原有的意图 3、生成的问题以 JSON 格式返回，示例如下： ``` {{ \u0026#34;input\u0026#34;: \u0026#34;《越女剑》这部小说主要讲了什么样的一个故事\u0026#34;, \u0026#34;output\u0026#34;: [\u0026#34;《越女剑》这部小说主要情节是什么\u0026#34;,\u0026#34;《越女剑》这部小说的故事梗概是什么\u0026#34;] }} ``` 4、每次最多产生 5 个相似的问题 现在，我的问题是：{question} \u0026#39;\u0026#39;\u0026#39; 好了，下面我们结合 LangChain 来做具体的工程实践。继续沿用上面的提示词模板，唯一需要注意的是，这里的 JSON 示例需要转义，因为它与变量 question 前后的花括号存在冲突，解决方案是使用两对花括号。方便起见，这里使用 LLMChain 来调用月之暗面的 API 接口，尽可能避免给大家增加心智负担：\nllm = ChatOpenAI( model_name=\u0026#39;moonshot-v1-8k\u0026#39;, temperature=0.75, openai_api_base=\u0026#39;https://api.moonshot.cn/v1\u0026#39;, openai_api_key=\u0026#39;\u0026lt;Moonshot API_KEY\u0026gt;\u0026#39;, streaming=False, ) rewrite_chain = LLMChain( llm=llm, prompt=PromptTemplate(template=rewrite_prompt_template, input_variables=[\u0026#34;question\u0026#34;]), ) 此时，问题就变得非常简单啦，下面是一个基本的实现思路：\ndef rewrite(question): try: result = rewrite_chain.invoke({\u0026#39;question\u0026#39;: question}) text = result[\u0026#39;text\u0026#39;].replace(\u0026#39;```json\u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39;```\u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39; \u0026#39;,\u0026#39;\u0026#39;) return json.loads(text) except: return {\u0026#39;input\u0026#39;: question, \u0026#39;output\u0026#39;:[]} 因为，大模型返回的 JSON 是类似 Markdown 的语法格式，即：使用一对 ``` 符号包裹着，并且前面还有 “json” 字眼。所以，我们只需要对字符串做简单地替换即可。博主特别想吐槽的是，Python 内置的 json 模块在反序列化的时候，居然不允许里面有空格、换行符这种形式的字符存在。当然，LangChain 中提供了 输出解析器 来解决这个问题。这里，我们使用 PydanticOutputParser 即可处理 JSON 形式的返回值：\nfrom langchain.output_parsers import PydanticOutputParser from langchain_core.pydantic_v1 import BaseModel, Field from typing import List class RewriteResult(BaseModel): input: str = Field(), output: List[str] = Field(), RewriteResultParser = PydanticOutputParser(pydantic_object=RewriteResult) 那么，如何使用这个解析器呢？此时，你有两种选择，我将其称之为，自动挡和手动挡：\n# 自动挡：在构造 LLMChain 时直接指定 output_parser 参数 # 此时，invoke()方法的返回值中, text 属性是一个对象 rewrite_chain = LLMChain( llm=llm, prompt=PromptTemplate(template=rewrite_prompt_template, input_variables=[\u0026#34;question\u0026#34;]), output_parser=RewriteResultParser ) # 手动挡：在 invoke() 方法调用后，调用 RewriteResultParser 的 parse() 方法 # 此时，处理方式与当前方案基本一致 result = rewrite_chain.invoke({\u0026#39;question\u0026#39;: question}) RewriteResultParser.parse(result[\u0026#39;text\u0026#39;]) 至此，我们就在工程层面上实现了 Rewrite，当我们输入一个问题后，大模型可以帮我们生成 5 个相似的问题：\nRewrite 在 LangChain 中的实现\r实现 Rerank OK，现在我们来考虑一下 Rerank。在 RAG 的语境下，Rerank 表示对结果进行重排，由此便引出第一个问题：为什么需要重排？如果你熟悉 LangChain 中的 VectorStore 类，便会知道它实际上是提供了 similarity_search() 和 similarity_search_with_score() 这样两个方法。继续深究下去，后者本就可以度量出问题与文本间的相关性或者相似度，那么，我们不禁要再次发问，真的需要重排？如果想要回答这两个问题，就不得不提到目前在向量检索中普遍使用的 HNSW 算法。这个算法有什么问题呢？原来，为了实现快速检索，HNSW 算法会存在一点随机性，特别是在创建和维护图结构时，这就导致 Top-K 最邻近搜索中可能得到不准确的结果。此前，博主尝试使用金庸先生的 15 部武侠小说做知识库。当时遇到的问题是，通过向量数据库检索出来的内容与问题本身并不强相关。比如，郭靖这个人物主要登场于射雕三部曲，可是在检索的过程中，不可避免地出现了《雪山飞狐》、《连城诀》等小说的身影，这显然与事实不符。我们需要一种评估问题与文本间相关性的方案，Rerank 应运而生。\nRerank 流程示意图\r目前，推荐使用的 Rerank 模型主要有：闭源的 CohereAI，以及开源的 bge-reranker-large 和 bge-reranker-base。下面，我们以 BAAI/bge-reranker-large 这个模型为例进行说明，我个人推荐的、最简单的方案是使用 FlagEmbedding 这个库：\npython -m pip install -U FlagEmbedding from FlagEmbedding import FlagReranker # 构造一个 FlagReranker 实例，设置 use_fp16 为 true 可以加快计算速度 reranker = FlagReranker(\u0026#39;BAAI/bge-reranker-large\u0026#39;, use_fp16=True) # 计算两个文本间的相关性评分 score = reranker.compute_score([\u0026#39;夏天到了\u0026#39;, \u0026#39;四五月的夏天热到爆炸\u0026#39;]) # 计算多对文本间的相关性评分 scores = reranker.compute_score([ [\u0026#39;你好\u0026#39;, \u0026#39;How are you\u0026#39;], [\u0026#39;你好\u0026#39;, \u0026#39;Hello\u0026#39;] ]) 查阅 FlagReranker 的源代码，你会发现它本质上还是 transformer 库的封装。所以，当你运行上面这些示例代码的时候，你大概率还是会遇到从 HuggingFace 上下载模型失败这种沉疴宿疾。这里，请允许博主隆重介绍 ModelScope，一个由阿里维护的大模型社区，中文名称叫做魔搭社区，从这里下载模型要更为方便一点，并且它完全兼容 transformer 库。以上面的模型为例，我们需要将代码修改成下面这样：\nimport modelscope reranker_model_dir = modelscope.snapshot_download(\u0026#39;Xorbits/bge-reranker-base\u0026#39;, revision=\u0026#39;master\u0026#39;) reranker = FlagReranker(reranker_model_dir, use_fp16=True) 可以注意到，ModelScope 上的模型，基本上都是从 HuggingFace 上 fork 过来的，因此模型的名称可能会有差异，使用前建议先到社区里搜索一下，如果一定要找一个最具说服力的理由，那就是这里面的模型对中文更友好一点，相当于是别人重新训练或者微调过的模型，我个人使用起来倒是感觉还好。如下图所示，我们得到每一组文本的相关性评分。我想知道，这个结果是否符合你的心理预期呢？\n使用 bge-reranker-base 模型评估文本相关性-A\r事实上，我们可以将 compute_score() 方法中的 normalize 参数设置为 true，使其结果归一化:\n使用 bge-reranker-base 模型评估文本相关性-B\r效果展示 在实现了 Rewrite 和 Reranker 以后，我们就可以按照 RAG 常规的做法，将其串联起来，如下面的代码所示：\n# 对用户输入的问题进行改写 rewrite_result = rewrite(question) rewrite_questions = rewrite_result[\u0026#39;output\u0026#39;] # 合并问题 \u0026amp; 检索文档 questions = [question] if len(rewrite_questions) \u0026gt; 0: questions.extend(rewrite_questions) documents = retrieve(questions) # 对文档重新排序,取前十条文本作为上下文 documents = rerank(question, documents) documents = list(sorted(documents, key=lambda x:x[\u0026#39;score\u0026#39;], reverse=True))[:10] documents = list(map(lambda x:x[\u0026#39;document\u0026#39;], documents)) contents = list(map(lambda x:x.page_content, documents)) context = \u0026#39;\\n\u0026#39;.join(contents) # 生成答案 answer = generate(question, context) 下面是博主在本地调试时实际生成的答案，站在我的角度，这个结果相比于从前，可谓是云泥之别：\n使用 Rewrite 和 Reranker 以后的实际效果\r源代码参考：https://github.com/Regularly-Archive/2024/blob/main/KnowledgeBase/others/rewrite_rerank.py\n本文小结 当写完这篇文章的时候，我终于意识到，相比于剖析抽象的社科文化，我果然还是喜欢更喜欢逻辑推理。正如文科生更侧重于表达感受，而理科生更倾向于陈述事实。思维方式上的差异，最终造成了双方在行动上的差异。所以，在文科生那里你听到的都是批判、质疑，而在理科生这里你听到的则是论证、实验。前者强调人文关怀，比如人类的情感、价值观、道德感、历史等等；而后者强调逻辑思维，比如可观测的事实、可量化的数据等等。当然，对我个人而言，无论是否发生此次小红书事件，我都会去尝试将 Rewrite 和 Rerank 这两个步骤整合进 RAG 里，真正令我感到沮丧的，是社交媒体上对第一性原理的无视。如你所见，通过实际的 Python 代码示例以及 LangChain 库的应用，我们展示了如何实现 Rewrite 和 Rerank。最终，我们得到的结论是：这两个步骤对于提高检索的多样性、准确性，以及生成答案的相关性至关重要。当然，如果单单从信息检索的角度来看，RAG 里其实并没有多少创新的东西，只不过在搭上 AI 这辆顺风车以后，一切都显得瞬间高级了起来。但是，希望大家能够明白一件事情，那就是：RAG 的核心永远都是 Retrieval，本文完。\n","date":"2024-04-26T09:29:47Z","image":"/posts/the-true-or-false-rewrite-rerank-of-rag/four-white-llamas.jpg","permalink":"https://qinyuanpei.github.io/posts/the-true-or-false-rewrite-rerank-of-rag/","slug":"The-True-Or-False-Rewrite-Rerank-Of-RAG","tags":["RAG","Rewrite","Rerank","思考","感悟"],"title":"RAG 的是与非、Rewrite 和 Rerank"},{"categories":["生活感悟"],"content":" 对于宫崎骏老爷子的告别之作 ——《你想活出怎样的人生》，我内心果然还是充满了期待，即便这部电影自打上映以来便争议不断。那些或明或暗的隐喻、洗白军国主义的嫌疑、以及偏意识流的表达方式，让这部作品在大荧幕前有了更多悬念。作为一个不大迷信豆瓣评分的叛逆中年，我还是决定亲自去电影院看看。和五年前一个人去看《千与千寻》的重映一样，我选择了一个安静的周末，带着一杯咖啡，提前坐在座位上等待电影开始。我想，正如张驰在《飞驰人生2》里所说的那样，“有些事情是需要一个终点的”，这部电影可能是宫崎骏的最后一次创作。与雷军将造车视为最后一次创业不同，这个多次声称要 “封笔” 的老人，已经年过八十。故而，这件事情本身就充满一种 “烈士暮年” 的沧桑感， 不由得令人感慨。我想说的是，对我而言，这个终点是学会与自我和解。\n如果单单从视觉观感上进行评价，《你想活出怎样的人生》是一部 100% 的宫崎骏式动画电影。其基本叙事结构都可以概括为：主人公因为某种原因来到了某个地方，然后因为某种机缘进入了异世界，主人公在异世界中冒险、成长，最终回归现实，一切戛然而止。这个故事听起来非常熟悉，没错，这正是《千与千寻》这部作品的主要情节。因此，当你再次审视《你想活出怎样的人生》这部作品时，你会发现两部电影的剧情走向完全一致。电影讲述了少年牧真人的母亲葬身火海后，他随父亲与继母组成新家庭。深陷于悲伤的真人阴内心封闭，难以融入新环境。一次意外，他跟随一只会说话的苍鹭闯入废弃的神秘塔楼，却不料进入了奇幻的 “亡灵世界”，自此开始了一场不可思议的冒险故事。那天，电影院里只坐了十来个人，如夜空中零散点缀着的寒星，戏里戏外，冒险各自开始。\n电影中的苍鹭作为少年真人的引路人\r在宫崎骏的诸多作品中，这部电影在表达上极其个人化。因而，网络上有一种声音，说这部电影并不是为观众而拍，它更像是宫崎骏对自己人生的一种总结。众所周知，宫崎骏对飞机有着特别的喜欢，这一点在《红猪》、《风之谷》、《天空之城》、《哈尔的移动城堡》等作品中均有所体现。了解过宫崎骏生平以后，你会发现他的家族经营着一家飞机工厂，生产用于战争的零式战斗机部件。宫崎骏从小对飞行器充满向往，可当他意识到战争带来的破坏时，他开始为家族的过去感到羞愧。所以，宫崎骏在作品中融入飞行元素的同时，一直倡导反对战争、尊重自然的理念。这种矛盾的心理，终于在《起风了》这部作品中被推向了一个高潮。如果说堀越二郎是宫崎骏飞行理想的化身，那么，这部电影或许更接近宫崎骏的自传。尽管电影中真人父亲的军工厂是以远景形式出现，但结合故事背景，可以推断出真人实际上是童年时期的宫崎骏。\n火元素是动漫作品中经久不衰的存在\r男主的名字牧真人寓意着 “成为一个真正的人”。然而，在母亲葬身火海以后，他陷入了前所未有的身份认同危机。他无法接受继母作为自己的母亲，在学校受排挤时，他甚至不惜自残使自己生一场大病。熟悉日本近代历史的人都知道，从 “黑船事件” 到 “明治维新” 再到 “屈原” ，日本同样经历过一系列的 “迷茫期”，即：我是谁？我在哪？我要干什么？姜文导演的《邪不压正》，主角李天然的行为可以概括为三个字：“找爸爸”，这同样是在探讨身份认同这一主题。在这样的背景下，一只会讲话的苍鹭出现在真人的视野中，并一步步地引导他走向了塔楼内的 “亡灵世界”。按照电影中的设定，真人的母亲虽然是死于战火，但她为了能再见儿子一面，只身前往 “亡灵世界” 并化身为元气少女 “火美” —— 一个可以驾驭火的巫女形象。在真人即将被鹦鹉士兵杀死并吃掉的时候，她如同《鬼灭之刃》中的炎柱炼狱杏寿郎一般照亮世界，救下了这个在异世界中跌跌撞撞的迷茫少年。该不该说，这一幕与《千与千寻》中的白龙照顾初来乍到的千寻极其相似？\n如爱因斯坦一般的舅公形象设定\r有人认为，故事中的舅公，代表了日本皇室等统治阶级，因为他们强调血统。电影中要成为舅公的继承人，必须拥有他的血脉。。鹦鹉代表军国主义/法西斯主义，其服饰具有普鲁士风格，鹦鹉大王的 “冲动” 导致异世界的崩塌，讽刺了那些为了宏大叙事而让这个世界频频陷入战火的野心家。电影中，一只鹈鹕在临死前向男主坦白，他们之所以要吃哇啦哇啦，是因为异世界中没有其他食物来源，这反映的是那些被裹挟进战争中的无辜者们。从古至今，帝王将天地视为棋盘，相互争斗，最终都是 “一将功成万骨枯”、“兴，百姓苦；亡，百姓苦”。异世界由积木搭建，积木来源于一块黑色的石头，结合舅公当时所处的年代，不难联想到这个 “黑色的石头”，本质上就是从西方驶来的 “黑船”。今天，我们说 “世界是个草台班子”，如果用积木来解构世界，或许就能理解这句话，理解 “万物为虚”。毕竟，人类自身的文明，何尝不是像电影中的积木一样脆弱不堪呢？可即便如此，人类依然在努力地为哇啦哇啦这样的新生命，争取一个更加光明的未来。换句话说，每一个人其实都应该成为自身文明的守护者。\n鹦鹉大王的这两撇胡子非常得普鲁士\r“亡灵世界” 是一个因战争而受诅咒的平行宇宙，其秩序被人类世界的战争扭曲。舅公，一个接触过西方思想的人，希望在这里建立一个没有战争的乌托邦。可随着时间的推移，这座理想国失去了平衡。占据着塔楼的鹦鹉群体，过着衣食无忧、战备充足的生活；而生活在地狱苦海中的鹈鹕则备受煎熬、食不果腹，不得不以哇啦哇啦为食。等到真人和火美找到舅公的时候，鹦鹉大王的武装势力已经威胁到了 “亡灵世界” 的统治。历史上，皇室被权臣架空的例子屡见不鲜，与其说是堆石成塔的积木被 “污染”，不如说鹦鹉们的内心早已被欲望和权利填满。故事接近尾声的时候，崇尚武力的鹦鹉大王一剑砍翻了石塔，导致异世界的坍塌，这是否呼应了新海诚作品中的 “伤痕文化”？在日本文化中，“侘寂” 一词代表不完美和残缺。在废墟上不断建立起新的家园，是人类千百年来往复循环着的宿命。\n有人说，这一幕像极了波妞与宗介\r最后，舅公给真人交代了一个任务：用13个尚未被污染的石块积木搭建一座牢固的房子，用善心创造一个没有恶意的世界。可当真人触摸到自己头上的那块伤疤时，他意识到，无论是被学校里的同学排挤，还是他近乎自残的行为，恶意在现实世界中一直存在。这一切就好像，“火美” 明明知道等待她的是炼狱、是死亡，可她必须回到那个世界，因为只有这样才能迎来真人的诞生。同样地，舅公曾用心勾画过的理想国固然是黄粱一梦，“到头来，宫阙万间都做了土”。这一切虽然是一场不折不扣的悲剧，可如果腐朽的旧秩序不曾毁灭，孕育希望的新世界又从何处诞生呢？异世界的斑驳陆离与现实世界的残酷荒诞相互映射，就像旅行归来后回到熟悉的地方。这或许是宫崎骏本人的 “心灵奇旅” 和 “寻梦环游记”，可它同样是宫崎骏留给我们的最后一道谜题，当你足够了解这个世界真实的面目，是否还有勇气继续生活？\n异世界的本源——黑色石头\r你想活出怎样的人生？这个问题对于真人而言，是身处乱世，如何学会当下痛苦和对亲人的执念，以及如何学会面对生命中的无常缺憾与生离死别。可对于屏幕前的你我来说，则是：如何去拥抱这个并不完美而又真实存在着的世界，学会与自我和解。宫崎骏身上体现了许多对立和矛盾的特质：他喜欢飞行技术，却难以接受家族参与战争的事实；他是一个反战主义者，却对军事和飞机充满兴趣；他是一个日本人，却对欧式建筑情有独钟；他讽刺统治者的剥削，可他本人却是出了名的工作狂和暴君。在内心深处的隐痛和分裂下，他一生都在纠结是 “承认” 还是 “怀疑”。从怀疑出发，试图建立乌托邦；最终接受现实，承认不完美。可以说，这宫崎骏最为私人、最深情的一次表达，是一次全面而系统的 “答记者问”。遗憾的是，或许是因为想要表达的东西太多，老爷子大开大合、恣意挥霍的叙事风格，无处不在的隐喻，都让整个故事显得晦涩而沉重，这个年逾八旬的老者，其表达地多少有点语焉不详。\n抽象吗？确实听抽象的\r请记住：你唯一所拥有的这段人生，唯一的选择便是抓住它，认真过下去。正如真人必须接受母亲的去世，接纳继母，才能迎来 “重生”。生命变幻无常，残酷而荒诞。有的人渐行渐远，而原本熟悉的开始变得陌生起来；有些东西一旦失去，就永远不会再回来，无论你有多么不舍或不甘；裂痕一旦出现，就永远无法再复原，无论你怀念的是过去还是此刻。回首往事，那些遥远而模糊的回忆，时常与错综复杂的现实纠缠在一起，令人真伪难辨。人生或许迷茫，历史或许重演，世界或许变质，可这世上只有一种英雄主义，那就是在认清生命的真相后，依然热爱生活。不要纠结、不要自证，接受一个事实并非耻辱。日本在二战中战败是事实，战后背靠美国获得经济飞速发展，这同样是事实。人类的一切建筑都是建立在某种废墟上面，而废墟同样作为新世界的地基。只要接受真实，回归本真，我们就能继续向前、过好现在的生活。起风了，唯有努力生存。走出电影院的那一刻，我下意识地摸了摸口袋里的电影票根，从某种意义上看，它难道不是我从电影的异世界里带出的积木或是护身符？虽然，从更宏大的人生尺度而言，我或许根本带不走任何东西。\n","date":"2024-04-18T09:29:47Z","image":"/posts/the-boy-the-heron-the-self-reconciliation/P2896704315.jpg","permalink":"https://qinyuanpei.github.io/posts/the-boy-the-heron-the-self-reconciliation/","slug":"The-Boy-The-Heron-The-Self-Reconciliation","tags":["影评","宫崎骏","自我和解","感悟"],"title":"《你想活出怎样的人生》与宫崎骏的自我和解"},{"categories":["编程语言"],"content":"随着 ChatGPT 的兴起及其背后的 AIGC 产业不断升温，向量数据库已成为备受业界瞩目的领域。FAISS、Milvus、Pinecone、Chroma、Qdrant 等产品层出不穷。市场调研公司 MarketsandMarkets 的数据显示，全球向量数据库市场规模预计将从 2020 年的 3.2 亿美元增长至 2025 年的 10.5 亿美元，年均复合增长率高达 26.8%。这表明向量数据库正从最初的不温不火逐步演变为大模型的 \u0026ldquo;超级大脑\u0026rdquo;。向量数据库，不仅解决了大模型在 \u0026ldquo;事实性\u0026rdquo; 和 \u0026ldquo;实时性\u0026rdquo; 方面的固有缺陷，还为企业重新定义了知识库管理方式。此外，与传统关系型数据库相比，向量数据库在处理大规模高维数据方面具有更高的查询效率和更强的处理能力。因此，向量数据库被认为是未来极具潜力的数据库产品。然而，面对非结构化数据的挑战，传统的关系型/非关系型数据库并未坐以待毙，开始支持向量数据库的特性，PostgrelSQL 就是其中的佼佼者。本文探讨的主题是：如何利用 PostgreSQL 实现向量检索以及全文检索。\n从大模型的内卷说起 截止目前，OpenAI 官方支持的上下文长度上限为 128K，即 128000 个 token，这意味着它最多可支持约 64000 个汉字的内容。当然，如果考虑到输入、输出两部分的 token 消耗数量，这 64000 个汉字多少要大打折扣。除此以外，国外的 Claude 2、国内的 Moonshot AI，先后将上下文长度提升到 200K 量级，这似乎预示着大模型正在朝着 “更多参数” 和 “更长上下文” 两个方向“内卷”。众所周知的是，现阶段大模型的训练往往需要成百上千的显卡，不论是“更多参数”还是“更长上下文”，本质上都意味着成本增加，这一点，从 Kimi 近期的宕机事件就可以看出。\nAI 眼中的显卡集群\r所以，为什么说 RAG(Retrieval-Augmented Generation) 是目前最为经济的 AI 应用开发方向呢？因为它在通过外挂知识库 “丰富” 大模型的同时，能更好地适应当前 “上下文长度受限” 这一背景。诚然，如果有一天，随着技术的不断发展，芯片的价格可以变得低廉起来，大模型可以天然地支持更长的上下文长度，或许大家就不需要 RAG 了。可至少在 2024 年这个时间节点下，不管是企业还是个人，如果你更看重知识库私有化和数据安全，RAG 始终是绕不过去的一个点。同济大学在 Retrieval-Augmented Generation for Large Language Models: A Survey 这篇论文中提出了 RAG 的三种不同范式，如下图所示：\n三种 RAG 范式的对比\r实现向量检索 PostgreSQL，可以说是目前世界上功能最强大的数据库系统之一。针对这个观点，请你先不要急着反驳我。因为，你可以利用这个时间来阅读下面这篇文章《技术极简主义：一切皆用 Postgres》。更不必说，这篇文章里的内容，对于整个 PostgreSQL 生态而言，不过是沧海一粟。单单是向量检索这个话题，你可以看到诸如 pase、pgvector、pg_embedding、pg_vectorize 等解决方案。这里，博主以 pgvector 这个插件为例来进行说明。\npgvector 基本使用 CREATE EXTENSION IF NOT EXISTS vector; 首先，我们使用上面的 SQL 语句来启用 pgvector 插件。此时，我们可以创建一张表来存储向量数据：\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3)); 接下来，准备若干条数据进行查询测试，可以注意到，这里的向量为三维向量：\nINSERT INTO items (embedding) VALUES (\u0026#39;[1,2,3]\u0026#39;), (\u0026#39;[4,5,6]\u0026#39;), (\u0026#39;[7,8,9]\u0026#39;); 现在，假设我们有一个向量为：[3,2,1]，如何查询距离该向量最近的数据呢？\n# L2/欧式距离 SELECT *, embedding \u0026lt;-\u0026gt; \u0026#39;[3,2,1]\u0026#39; AS distance FROM items ORDER BY distance ASC; # 向量内积 SELECT *, (embedding \u0026lt;#\u0026gt; \u0026#39;[3,2,1]\u0026#39;) * -1 AS distance FROM items ORDER BY distance ASC; # 余弦相似 SELECT *, (1 - (embedding \u0026lt;=\u0026gt; \u0026#39;[3,1,2]\u0026#39;)) AS distance FROM items ORDER BY distance ASC; 注意到，这里我们有三种表示距离的方法，即：欧式距离、向量内积和余弦相似，下面是对应的查询结果：\nL2/欧式距离\r向量内积\r余弦相似\r可以注意到，不管是哪一种方案，距离 [3,2,1] 最近的向量始终都是 [1,2,3]，这完全符合我们的预期。在 RAG 的场景中，向量通常由 Embedding 模型来生成，其维度可能会达到 1024 甚至更高。考虑到，pgvector 最多支持 16000 个维度的向量，所以，当你准备开发一款 AI 应用时，PostgreSQL 可以兼顾关系型数据库和向量数据库。对于高维度的向量计算，你可以使用索引来加快查询速度，pgvector 支持 HNSW 和 IVFFlat 两种索引：\n索引 HNSW IVFFlat 特点 查询性能较好；构建时间慢、占用内存多 查询性能较差；构建时间快、占用内存少 原理 多层图查询 将向量划分为列表，搜索距离最近的子列表 支持类型 vector, halfvec、bit、sparsevec vector 这里要注意的是，虽然 pgvector 最多支持 16000 个维度的向量，但不管是 HNSW 还是 IVFFlat 索引，它们最多支持 2000 个维度的向量。此外，halfvec、bit、sparsevec 这三种类型目前都还是 unreleased 状态，所以，两种索引算是平分秋色。下面是创建索引的 SQL 语句语法说明：\n# HNSW 索引 ## L2/欧式距离索引 CREATE INDEX ON items USING hnsw (embedding vector_l2_ops); ## 向量内积索引 CREATE INDEX ON items USING hnsw (embedding vector_ip_ops); ## 余弦相似索引 CREATE INDEX ON items USING hnsw (embedding vector_cosine_ops); # IVFFlat 索引 ## L2/欧式距离索引 CREATE INDEX ON items USING ivfflat (embedding vector_l2_ops) WITH (lists = 100); ## 向量内积索引 CREATE INDEX ON items USING ivfflat (embedding vector_ip_ops) WITH (lists = 100); ## 余弦相似索引 CREATE INDEX ON items USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100); pgvector 与 EFCore 集成 对于像博主这样使用 C#/.NET 进行开发的朋友，我们可以使用 pgvector-dotnet 这个项目，这里以 Entity Framework Core 为例：\ndotnet add package Pgvector.EntityFrameworkCore 为了继续沿用上面的例子，为此，我们定义下面的实体类。一个非常实用的小技巧是：如果你不确定向量的维数，可以不用写这个 [Column] 特性。当然，整张表中的向量维数应该是相同的，就像两个矩阵在相乘时应该满足特定的条件一样，你还记得是什么样的条件吗?:smile:\nclass Item { public int Id { get; set; } [Column(TypeName = \u0026#34;vector(3)\u0026#34;)] public Vector? Embedding { get; set; } } 此时，我们就可以像平时一样向数据库中插入一个向量：\nctx.Items.Add(new Item { Embedding = new Vector(new float[] { 1, 2, 3 }) }); ctx.SaveChanges(); 当然，查询会稍微不同，因为 LINQ 中没有计算距离相关的表达式：\nvar embedding = new Vector(new float[] { 1, 1, 1 }); var items = await ctx.Items .OrderBy(x =\u0026gt; x.Embedding!.L2Distance(embedding)) .Take(5) .ToListAsync(); 除了上面的 L2Distance，我们还可以使用 MaxInnerProduct 和 CosineDistance 两个函数，它们都属于 Vector 类型的扩展方法，这里不再详细展开说明。\n// HNSW 索引 modelBuilder.Entity\u0026lt;Item\u0026gt;() .HasIndex(i =\u0026gt; i.Embedding) .HasMethod(\u0026#34;hnsw\u0026#34;) .HasOperators(\u0026#34;vector_l2_ops\u0026#34;) .HasStorageParameter(\u0026#34;m\u0026#34;, 16) .HasStorageParameter(\u0026#34;ef_construction\u0026#34;, 64); // IVFFlat 索引 modelBuilder.Entity\u0026lt;Item\u0026gt;() .HasIndex(i =\u0026gt; i.Embedding) .HasMethod(\u0026#34;ivfflat\u0026#34;) .HasOperators(\u0026#34;vector_l2_ops\u0026#34;) .HasStorageParameter(\u0026#34;lists\u0026#34;, 100); 可以注意到，我们依然可以使用 HNSW 和 IVFFlat 这两种索引，并且其参数与 pgvector 完全一致。实际上，如果你使用过 LangChain 或者 Semantic Kernel 这类 LLM 框架，你就会发现没有银弹，它们正在做的事情，无非就是屏幕前的你和我，想要去努力搞清楚的东西。如下图所示，LangChain 和 Semantic Kernel 均支持使用 PostgreSQL 作为其向量数据库：\nPostgreSQL 在 LangChain 中的应用\rPostgreSQL 在 Semantic Kernel 中的应用\r此时此刻，想来你应该明白了 RAG 的工作原理。当我们输入一个问题后，首先会由 Embedding 模型将其转化为一个向量，然后我们从向量数据库中找出距离该向量最近的若干条记录，并将这些记录对应的文本信息作为 LLM 的上下文。此时，LLM 就会整合这些信息并输出最终答案。\n利用向量检索查询金庸武侠小说\r可是这样就足够了吗？我想，或许还不太够。因为如果你按照这个思路实践下来，你会发现通过向量检索出来的内容，其相关性或许并不强，所以，现在业界主要的精力都放在了 Retrieval 上，提出了诸如 Rerank、Rewrite 的方案，例如针对相关性排序、对输入的问题进行重写或者同时生成多个相似问题等。\n实现全文检索 在开发基于 Semantic Kernel 的 AI 应用时，其实我对于 PostgreSQL 的认知完全是渐进式的，在熟悉了 pgvector 插件的使用以后，我开始尝试去了解 PostgreSQL 中的类型。最终，它们促使我实现了某些 Semantic Kernel 中没有的功能。此前关注 FastGPT 这个项目的时候，我注意到，它除了支持常规的向量检索外，还支持全文检索。因此，我想知道基于全文检索的检索方案，相比于向量检索的检索方案是否更具有性价比。毕竟，通过 LLM 生成向量需要消耗 token 以及时间，并且当文件内容发生变化时更新向量很麻烦。所以，下面我们来聊一聊如何利用 PostgreSQL 实现全文检索，即：在对输入的问题进行分词处理后，直接去检索含有该关键字的内容。\nPostgreSQL 对全文检索的支持 与 pg_vector 不同，PostgreSQL 天然支持全文检索特性，唯一不同的地方在于分词器。比如，中文和英文的分词规则显然不同。此时，我们就可以使用 zhparser、pg_jieba、nlpbamboo、SCWS 等支持中文分词的插件。不用这些插件是否可以呢？经过博主测试，官方自带的分词器，在处理类似 “西施”、“勾践” 的关键字时，无法检索到相关的内容，可见分词器的影响还是很大的。可惜，插件丰富的 PostgreSQL 和 Nginx 一样，安装第三方模块总避免不了折腾一番。下面，博主将以 pg_jieba 为例进行说明：\nPostgreSQL 自带的分词器\r如图所示，PostgreSQL 自带了一个分词器 default ，你可以使用下面的 SQL 语句进行查询：\nselect * from pg_ts_parser 在此基础上，PostgreSQL 创建一组用于全文检索的配置，使用下面的 SQL 语句进行查询：\nselect * from pg_ts_config 此时，你会得到看到下面的结果：\nPostgreSQL 自带的全文检索配置\r其中：3722 是 default 这个分词器的唯一标识，并且这里面目前没有针对中文的配置。PostgreSQL 主要提供三类函数来支持全文检索，它们分别是文档解析函数、查询解析函数以及排序函数：\n文档解析函数：即 to_tsvector() 函数，负责对指定的字符进行分词。例如，当输入下列命令时，将返回字符串中的每个单词及其序号： SELECT to_tsvector(\u0026#39;simple\u0026#39;,\u0026#39;nothing is true everything is permitted\u0026#39;) to_tsvector() 函数输出\r查询解析函数：即 to_tsquery()、plainto_tsquery()、phraseto_tsquery() 和 websearch_to_tsquery() 这四个函数，它们负责将指定的字符串转化为表达式。例如，以下命令表示，包含 nothing 或者同时包含 is 和 true： SELECT to_tsquery(\u0026#39;simple\u0026#39;,\u0026#39;nothing | (is \u0026amp; true)\u0026#39;) to_tsquery() 函数输出\r更多的细节，请参考官方文档：http://www.postgres.cn/docs/12/textsearch-controls.html\n排序函数：即 ts_rank() 和 ts_rank_cd()，它们负责计算给定文档与特定查询中的相关性。例如，下面的例子展示了内容与关键词间的相关性： select \u0026#39;nothing is true everything is permitted\u0026#39; as content, \u0026#39;nothing | true\u0026#39; as keywords, ts_rank_cd( to_tsvector(\u0026#39;english\u0026#39;,\u0026#39;nothing is true everything is permitted\u0026#39;), to_tsquery(\u0026#39;english\u0026#39;,\u0026#39;nothing | true\u0026#39;) ) as relevance ts_rank_cd() 函数输出\r现在，结合以上知识，假设我们希望从数据中查询符合特定关键字的内容，我们可以像下面这样编写 SQL 语句：\nSELECT t.*, ts_rank_cd( to_tsvector(\u0026#39;english\u0026#39;, t.content), to_tsquery(\u0026#39;english\u0026#39;, \u0026#39;范蠡 \u0026amp; 阿青 | 夫差\u0026#39;) ) AS relevance FROM \u0026#34;sk-default\u0026#34; t WHERE t.content @@ to_tsquery(\u0026#39;english\u0026#39;, \u0026#39;范蠡 \u0026amp; 阿青 | 夫差\u0026#39;) ORDER BY relevance DESC; 其中， @@ 操作符的作用是将待检索的字段与检索条件连接起来。此时，结果如下图所示：\nPostgreSQL 全文检索结果展示\r结果显示，我们给定的关键词与内容之间的相关性为：0.025，通过这个信息，理论上我们就可以找出我们真正需要的信息。而这便是目前 PostgreSQL 在全文检索方面能达到的程度。对此，大家是否感到满意呢？\n使用 pg_jieba 增强中文检索 《越女剑》是金庸先生创作的短篇武侠小说，以春秋末期的吴越争霸作为历史背景，讲述了阿青被范蠡引荐到宫中教授士兵剑术，最终帮助越王勾践复仇雪耻的故事。在教授剑术的过程中，阿青暗自喜欢上了范蠡，而范蠡则喜欢西施，内心深感不忿的阿青找上范蠡和西施，在相互争执的过程中，阿青竹棒上的劲力伤及西施，自此后世留下了 “西子捧心” 的传奇佳话。从这个故事中，我们不难发现，范蠡、阿青、夫差等历史人物悉数登场。但是，我们通过全文检索仅仅得到了一条记录，这是为什么呢？答案便在于上面提到的分词器。所以，下面我们尝试使用 pg_jieba 这个插件来强化检索效果，关于这个插件的安装请参考官方文档，博主这里提供一个开箱即用的方案供大家参考：\nhttps://github.com/Regularly-Archive/2024/tree/main/PostgreSQL.Embedding/Docker\r接下来，我们首先需要启用 pg_jieba 插件：\nCREATE EXTENSION IF NOT EXISTS pg_jieba; 现在，如果我们再次查询分词器信息：\nselect * from pg_ts_parser 你会注意到，这里多了几个分词器，这些便是 pg_jieba 插件提供的分词器：\npg_jieba 插件提供的分词器\r如果你继续查询全文检索的配置信息，你将会由类似的发现：\nselect * from pg_ts_config 此时，你会看到下面的结果：\npg_jieba 插件提供的全文检索配置\r现在，我们就可以直接使用 jiebacfg 这个配置来进行查询：\nSELECT t.*, ts_rank_cd( to_tsvector(\u0026#39;jiebacfg\u0026#39;, t.content), to_tsquery(\u0026#39;jiebacfg\u0026#39;, \u0026#39;范蠡 \u0026amp; 阿青 | 夫差\u0026#39;) ) AS relevance FROM \u0026#34;sk-default\u0026#34; t WHERE t.content @@ to_tsquery(\u0026#39;jiebacfg\u0026#39;, \u0026#39;范蠡 \u0026amp; 阿青 | 夫差\u0026#39;) ORDER BY relevance DESC; 有时候，你可能希望创建自己的全文检索配置，此时，你可以使用下面的 SQL 语句：\n# 拷贝一个现有配置 CREATE TEXT SEARCH CONFIGURATION my_english (copy = english); # 创建一个全新的配置 CREATE TEXT SEARCH CONFIGURATION my_jieba (parser = \u0026#39;jieba\u0026#39;); 当然，你还可以通过创建索引来提升全文检索的效率，使用下面的 SQL 语句即可：\nCREATE INDEX idx_full_text_search ON \u0026#34;sk-default\u0026#34; USING GIN(to_tsvector(\u0026#39;jiebacfg\u0026#39;, content)); 最终，博主采用全文检索 + 模糊匹配(LIKE) 的方式完成了整个设想，因为其中包含相关性及检索数目的控制，所以，我个人感觉这个方案比单纯的向量检索要更为经济一点，毕竟，通过 Embedding 模型生成向量需要时间和金钱，除非你使用一个本地的离线模型来完成这个工作。到目前为止，知识库可以使用向量检索或者全文检索，我认为，在实际场景中完全可以将这两种方案结合起来使用。如下图所示，当博主向知识库提问的时候，它已经可以给出非常接近事实的答案，虽然这个答案依然令人捧腹，哈哈！\n使用全文检索后的知识库输出展示\r至此，基于 PostgreSQL 的全文检索方案完成落地，大家对这个方案的表现还满意吗？\n本文小结 或许，我每次写博客的时候，都没有办法做到 100% 的完全遵从写作计划。就像这篇文章，我一开始的规划仅仅是写 pgvector，但我总感觉这些零零散散的内容，不值得专门去写一篇文章。直到拖延了将近半个月以后，我发觉基于 PostgreSQL 的全文检索方案可以作为 Semantic Kernel 的一个改进点，经过一番权衡和上下求索，总算是完成了这篇文章的写作。本文的核心要点其实只有两个，其一是选择 PostgreSQL 作为当下 AI 应用开发中的向量数据库，其二是利用 PostgreSQL 的全文搜索特性强化知识库检索。其实，采用向量还是文本进行检索，本质上只是一种查询媒介的选择，真正有价值的是在检索过程中的种种思考。比如，如何对搜索结果进行排序、如何根据用户的输入产生相似的问题等等，从表面上看，这是一个 AI 应用开发的问题，然而，从更深刻地角度来看，这其实是一个信息检索的问题。作为李彦宏先生曾经的粉丝，我非常希望百度可以在搜索引擎中引入 AI 技术，毕竟，RAG 这个方向的 AI 应用，其着眼点还是在信息的检索上。虽然大家都在质疑 Kimi 的 200K 上下文到底是不是真的 200K，可如果一件事情能通过 RAG 这种更经济、更环保的方式实现，还能让用户感到心悦诚服的话，何乐而不为呢？最后，请允许我推荐一个最近正在使用的 AI 搜索引擎：秘塔AI搜索，真的比百度好用! 🫡\n","date":"2024-03-15T21:34:36Z","image":"/posts/use-efcore-with-postgresql-for-vector-storage-and-retrieval/library-898333_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/use-efcore-with-postgresql-for-vector-storage-and-retrieval/","slug":"Use-EFCore-With-PostgreSQL-For-Vector-Storage-And-Retrieval","tags":["PostgreSQL","pgvector","pg_jieba","全文检索","向量"],"title":"使用 EFCore 和 PostgreSQL 实现向量存储及检索"},{"categories":["编程语言"],"content":"有时候，我难免不由地感慨，真实的人类世界，本就是一个巨大的娱乐圈，即使是在英雄辈出的 IT 行业。数日前，Google 正式对外发布了 Gemini 1.5 Pro，一个建立在 Transformer 和 MoE 架构上的多模态模型。可惜，这个被 Google 寄予厚望的产品并未激起多少水花，因为就在同一天 OpenAI 发布了 Sora，一个支持从文字生成视频的模型，可谓是一时风光无二。有人说，OpenAI 站在 Google 的肩膀上，用 Google 的技术疯狂刷屏。此中曲直，远非我等外人所能预也。我们唯一能确定的事情是，通用人工智能，即：AGI（Artificial General Intelligence）的实现，正在以肉眼可见的速度被缩短，以前在科幻电影中看到的种种场景，或许会比我们想象中来得更快一些。不过，等待 AGI 来临前的黑夜注定是漫长而孤寂的。在此期间，我们继续来探索 AI 应用落地的最佳实践，即：在成功部署本地 AI 大模型后，如何通过外挂知识库的方式为其 “注入” 新的知识。\n从 RAG \u0026amp; GPTs 开始 在上一期博客中，博主曾经有一个困惑，那就是当前阶段 AI 应用的最佳实践到底是什么？站在 2023 年的时间节点上，博主曾经以为未来属于提示词工程(Prompt Engineering)，而站在 2024 年的时间节点上，博主认为 RAG \u0026amp; GPTs 在实践方面或许要略胜一筹。在过去的一年里，我们陆陆续续看到像 Prompt Heroes、PromptBase、AI Short\u0026hellip;等等这样的提示词网站出现，甚至提示词可以像商品一样进行交易。与此同时，随着 OpenAI GPT Store 的发布，我们仿佛可以看到一种 AI 应用商店的雏形。什么是 GPTs 呢？通常是指可以让使用者量身定做 AI 助理的工具。譬如，它允许用户上传资料来丰富 ChatGPT 的知识库，允许用户使用个性化的提示词来指导 ChatGPT 的行为，允许用户整合各项技能(搜索引擎、Web API、Function Calling)\u0026hellip;等等。我们在上一期博客中提到人工智能的 “安卓时刻”，一个重要的契机是目前产生了类似应用商店的 GPT Store，如下图所示：\nOpenAI 推出 GPT Store\r如果你觉得 OpenAI 的 GPT Store 离我们还稍微有点距离的话，不妨了解一下 FastGPT 这个项目，它以更加直观的方式展示了一个 GPTs 是如何被创造出来的。如图所示，博主利用我的博客作为知识库创建了一个博客助手，而这一切只需要选模型、编写提示词、上传资料三个步骤即可。感兴趣的朋友可以从 这里 进行体验：\n通过 FastGPT 创建 AI 应用\r由此，我们就可以得出一个结论，目前 AI 应用落地主要还是围绕大模型微调(Fine Tuning)、提示词工程(Prompt Engineering) 以及知识增强展开，并且 GPTs 里依然有提示词参与，两者并不冲突。考虑到，大模型微调这条线存在一定的门槛，我们暂且将其放在一旁。此时，提示词工程和知识增强就成为了 AI 应用落地的关键。知识增强，专业术语为检索增强生成，即：Retrieval-Augmented Generation，RAG，其基本思路就是将大语言模型和知识库结合起来，通过外挂知识库的方式来增强大模型的生成能力。比如微软的 New Bing 是 GPT-4 + 搜索引擎的方案，而更一般的方案则是 LLM + 向量数据库的思路，下图展示了 RAG 运作的基本原理：\nRAG 运作的基本原理\r从这个角度来看，LangChain 及其衍生项目 AutoChain、Embedchain，甚至 FastGPT 等项目解决的本质都是 RAG 和 Agent 的问题。其中，Agent 不在本文的讨论范围内，这里博主不打算详细展开。接下来的内容，博主会按照这个思路进行阐述，并且以 LangChain 为例来对其中的细节进行说明。\n知识库构建 如你所见，RAG 由 LLM 和 知识库两部分组成。首先，我们来构建知识库，通常，这个过程可以划分为下面四个步骤，即：载入文档(Loader)、拆分文本(Splitter)、文本向量化(Embeddings)、向量存储(VectorStore)。\n构建知识库的步骤\rLoader 你会注意到，博主在文章中加粗显示了这四个步骤的英文描述，事实上，这代表了 LangChain 中的一部分概念，以 Loader 为例，它负责从各种文档中载入内容，下面展示了从文本文件、PDF 文件以及网页中载入内容：\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, WebBaseLoader # TextLoader # 指定编码 loader = TextLoader(\u0026#34;./input/金庸武侠小说全集/射雕英雄传.txt\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) loader.load() # 自动推断 # python -m pip install chardet loader = TextLoader(\u0026#34;./input/金庸武侠小说全集/射雕英雄传.txt\u0026#34;, autodetect_encoding=True) loader.load() # PyPDFLoader # python -m pip install pypdf loader = PyPDFLoader(\u0026#34;./input/文学作品/追风筝的人.pdf\u0026#34;) loader.load() # WebBaseLoader # python -m pip install beautifulsoup4 loader = WebBaseLoader(web_paths=(\u0026#39;https://blog.yuanpei.me\u0026#39;,), bs_kwargs={}) loader.load() 当然，现实中通常会有很多文档，此时，我们可以使用 DirectoryLoader 来一次性载入多个文档：\nfrom langchain_community.document_loaders import DirectoryLoader loader = DirectoryLoader(\u0026#34;./posts/\u0026#34;, glob=\u0026#34;*.md\u0026#34;, loader_kwargs={}, show_progress=True, silent_errors=True) 默认情况下，DirectoryLoader 使用 UnstructuredFileLoader 这个通用的 Loader 来兼容各种格式的文件，不过，你依然可以使用 loader_cls 参数来指定 Loader 类型：\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader loader = DirectoryLoader(\u0026#34;./posts/\u0026#34;, glob=\u0026#34;*.md\u0026#34;, loader_cls=TextLoader, loader_kwargs={}, show_progress=True, silent_errors=True) Splitter 调用 Loader 的 load() 方法，返回的是一组 Document 的集合。此时，我们可以将这些 Document 交给 TextSplitter 来对分本内容的分割，因为我们最终需要对文本块做向量化处理：\nfrom langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter text_splitter = CharacterTextSplitter(separator = \u0026#34;\\n\\n\u0026#34;, chunk_size = 600, chunk_overlap = 100, length_function = len) text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) 这里，博主先后使用了 CharacterTextSplitter 和 RecursiveCharacterTextSplitter 两种 Splitter，两者的区别是：CharacterTextSplitter 会将文本按照单个字符进行分割，而 RecursiveCharacterTextSplitter 则会将文本按照连续的多个字符进行分割。不过，经过博主的测试，两种分割方式最终对 LLM 的影响微乎其微，大家可以参照官方文档中的说明，选择符合个人预期的分割方法：\nLangChain 中不同的 TextSplitter\r按照一般的流程，我们只需要按下面的方式，即可完成文档的分割：\ndocuments = loader.load()。 text_splitter.split_documents(documents) 不过，实际操作中更推荐使用 load_and_Split() 方法，两步合并为一步，更简洁一点：\ndocuments = loader.load_and_split(text_splitter) Embeddings 经过 Splitter 处理以后，我们将得到一系列文本块，它依然是一组 Document 集合。此时，便轮到 Embeddings 出场，它将负责将文本块向量化。这里，博主使用的是 HuggingFaceEmbeddings，首次运行它会自动下载模型文件。当然，我天朝上国自有国情在此，从 Hugging Face 下载模型的问题在 AI 的道路上可谓是阴魂不散。因此，下面演示的是，通过镜像站来下载模型的方法。考虑到，默认的 sentence-transformers/all-mpnet-base-v2 模型是英文模型，我们引入一个对中文更友好的 GanymedeNil/text2vec-large-chinese 模型:\nimport os from langchain_community.embeddings import HuggingFaceEmbeddings os.environ[\u0026#34;HF_ENDPOINT\u0026#34;] = \u0026#34;https://hf-mirror.com\u0026#34; embeddings = HuggingFaceEmbeddings(model_name=\u0026#34;GanymedeNil/text2vec-large-chinese\u0026#34;) # 单个句子 print(embeddings.embed_query(\u0026#34;人生若只如初见，何事秋风悲画扇\u0026#34;)) # 多个句子 print(embeddings.embed_documents([\u0026#34;人生若只如初见，何事秋风悲画扇\u0026#34;,\u0026#34;等闲变却故人心，却道故人心易变\u0026#34;])) 当然，从最终向量化以后的结果来看，前者的向量维数只有 768，而后者则可以达到 1024。至此，我们就完成文本内容的向量化。一旦所有的文本信息都被转换为向量数据，此时，信息检索就完完全全地变成了一道数学题，由向量的余弦公式，我们可以非常容易地计算出两个向量间的夹角，这个夹角越小，则表明两个向量越相近。这就是向量相似性检索的基本原理，在此前的文章《视频是不能 P 的系列：使用 Milvus 实现海量人脸快速检索》一文中，博主曾经尝试利用这种思路来优化人脸识别效率，不知道大家是否还留下印象🙃\u0026hellip;\n视频是不能 P 的系列：使用 Milvus 实现海量人脸快速检索\rVectorStore OK，当我们将文本信息转化为向量以后，就需要考虑如何储存这些向量信息，如上文中给出的图片所示，LangChain 中支持诸如 FAISS、Milvus、Pinecone、Chroma\u0026hellip;等多种向量数据库，以及像 Elasticsearch、PostgreSQL 这样的具备向量存储能力的传统数据库，我们可以根据自己的需求选择合适的存储方案。为了方便演示，这里我们使用 Facebook 出品的 FAISS，它不需要像 Milvus 那样准备额外的环境，使用体验上更接近 SQLite 这种嵌入式的数据库。当然，如果你比较中意于 Chroma，一切行止由心，LangChain 在自由度上拉满：\nfrom langchain_community.vectorstores.faiss import FAISS import pickle # python -m pip install faiss-cpu vector_store = FAISS.from_documents(documents, embeddings) with open(\u0026#39;./output/\u0026lt;Your-Persistence-Path\u0026gt;\u0026#39;, \u0026#34;wb\u0026#34;) as f: pickle.dump(vector_store, f) FastGPT 中创建知识库的过程，原理完全相同，它支持从文档或者网址构建知识库，最终将向量化数据存储在 MonggoDB 中，目前，这个项目使用的 FastAI 4K 模型来源不明，而 Embedding-2 模型则是来自于 OpenAI。所以，FastGPT 这个网站最大的资金消耗主要来自 FastAI，目测是一个独立训练出来的私有模型。下图展示了博主在本地构建知识库的过程，如你所见，博主使用了 263 篇博客的内容来构建这个知识库：\n本地知识库构建过程\rLLaMA 的再度整合 坦白讲，纯 CPU 环境下的知识库非常花时间，譬如，博主使用 1024 个维度的向量来储存金庸先生的 15 部小说，经序列化后的文件体积高达 1.2G。果然，大模型相关的一切事物都非常庞大，这或许能帮大家理解类似 7B、13B、60B\u0026hellip;等等的大模型的本质，因为它们同样是由大量的高精度的、浮点型的向量数据构成。可偏偏就是这些对人类来说毫无意义、宛如天书一般的数字，能够从你的只言片语中“理解”你的意图，回应你的各种输入，难道你还能笃定，这一切不是某种魔法，而提示词不过是施展魔法时吟唱的咒语？从这个角度来看，RAG 是什么呢？RAG 更像是你在吟唱咒语时随手拿一本魔法书在手里，你随时可以从这本魔法书里查询资料完善咒语。\nRAG 如何工作\rRAG 与 Prompt 现在，我们顺着这个思路，将 LLaMA 和 LangChain 结合起来，这里的 LLaMA 指的是 llama.cpp + Qwen-1_8B-Chat，这个方案我们在上一期博客已经介绍过了，这里不再赘述。我们先来看看 Retrieve 的过程，即：如何从向量数据库中找到问题相关的内容，显然，这是 RAG 里的第一步：\nwith open(\u0026#34;./output/个人博客.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: vectorstore = pickle.load(f) query = \u0026#39;Envoy 在微服务中的应用场景有哪些\u0026#39; topK = 3 # 普通的相似性检索 result = vectorstore.similarity_search(query=query, k=topK) # 带有分数的相似性检索 result = vectorstore.similarity_search_with_relevance_scores(query=query, k=topK) # 最大边界相关性检索 result = vectorstore.max_marginal_relevance_search(query=query, k=topK) 这里，博主例举出了常用的向量相似性检索的方法，大家可以参考 官方文档 选择合适的方法，通常情况下，使用 similarity_search() 方法即可；可有时候向量数据库会检索出与问题相关性不强的内容，此时，就需要使用 similarity_search_with_relevance_scores() 配合某个阈值进行处理；第三种方法，我个人感觉，其返回结果的相关性不如前两种方法。还是那句话，目前博主的 AI 道路还在摸索阶段，大家酌情使用即可。\nfrom langchain.prompts.prompt import PromptTemplate # 定义提示词模板 PROMPT_TEMPLATE = \u0026#34;\u0026#34;\u0026#34; You are a helpful AI bot. Your name is {name}. Please answer the question only based on the following context: {context} If the question is about your identity or role or name, answer \u0026#39;{name}\u0026#39; directly, no need to refer to the context. If the context is not enough to support the generation of an answer, Please return \u0026#34;I\u0026#39;m sorry, I can\u0026#39;t answer your question.\u0026#34; immediately. You have an opportunity to refine the existing answer (only if needed) with current context. You must always answer the question in Chinese. Queation: {question} \u0026#34;\u0026#34;\u0026#34; # 根据问题做相似性检索 question = \u0026#34;Envoy在微服务中都有哪些应用场景\u0026#34; documents = vector_store.similarity_search(question, k=3) context = \u0026#39;\\n\\n\u0026#39;.join([document.page_content for document in documents]) # 从模板创建提示词并填充模板 prompt = PromptTemplate.from_template(PROMPT_TEMPLATE) query = prompt.format(question=question, name=\u0026#34;ChatGPT\u0026#34;, context=context) RAG 中的 A 是指 Augmented，表示增强的，这个增强体现在哪里呢？答案是提示词。如图所示，在这段提示词里面，博主试图让大语言模型假扮 ChatGPT，同时告诉它按照给定的上下文来回答问题，并且要求它必须使用中文进行回答。通过这段代码，我们就可以产生符合我们预期的输入，而接下来，我们只需要将其传递给大语言模型即可。除了这种方式以外，我们还可以按 OpenAI 的规范，使用 ChatPromptTemplate 来组织提示词：\nfrom langchain_core.prompts import ChatPromptTemplate # 定义提示词模板 PROMPT_TEMPLATE = \u0026#34;\u0026#34;\u0026#34; You are a helpful AI bot. Your name is {name}. Please answer the question only based on the following context: {context} If the question is about your identity or role or name, answer \u0026#39;{name}\u0026#39; directly, no need to refer to the context. If the context is not enough to support the generation of an answer, Please return \u0026#34;I\u0026#39;m sorry, I can\u0026#39;t answer your question.\u0026#34; immediately. You have an opportunity to refine the existing answer (only if needed) with current context. You must always answer the question in Chinese. \u0026#34;\u0026#34;\u0026#34; # 根据问题做相似性检索 question = \u0026#34;Envoy在微服务中都有哪些应用场景\u0026#34; documents = vector_store.similarity_search(question, k=3) context = \u0026#39;\\n\\n\u0026#39;.join([document.page_content for document in documents]) # 从模板创建提示词并填充模板 prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, PROMPT_TEMPLATE), (\u0026#34;user\u0026#34;, \u0026#34;{question}\u0026#34;), ]) query = prompt.format(question=question, name=\u0026#34;ChatGPT\u0026#34;, context=context) 显而易见，第二种更好一点，因为它可以将问题和上下文显著地区分开来。当然，为了让 LLM 确信它就是 ChatGPT，博主在提示词工程上可没少下功夫。这其实佐证了博主一开始的观点：2023 年的提示词工程，2024 年的 GPTs，这两者并不冲突，而是一种相辅相成的关系。RAG 中的 Retrieve 和 Augmented 现在都已先后登场， 最后这个 G 自然是指 Generation，理论上剩下的工作交给 LLM 即可。可是如何你仔细想想，就会发现这里隐含着两个问题，其一是从向量数据库中检索到的信息，有一定的可能性超过 LLM 的上下文长度；其二是对上下文的约束越严格，LLM 就越显得 “弱智”，你必须要在发散和收敛中做出选择。以一言蔽之，LLM 有幻觉固然不好，可如果缺乏想象力，一切只怕是会变得更糟糕！不知大家是如何考虑这个问题的呢？\nLangChain 中的 Chain 在 LangChain 的诸多概念中，Chain 或许是最抽象、最重要的哪一个，因为它就像一个管道一样，可以讲我们这篇文章中提到的各种组件串联起来。譬如， LLMChain 可以将一个LLM 和 Prompt 串联起来，RetrievalQA 可以配合 LLM 和知识库实现简单的 Q\u0026amp;A，ConversationalRetrievalChain 可以配合 LLM、知识库和聊天历史实现对话式检索。起初，我对于 RetrievalQA 这个类是极其排斥和反感的，因为它不论是看起来还是用起来，都像极了一个 Chain，可它就偏偏不是一个 Chain，真是奇哉怪也！\nfrom langchain.chains import LLMChain from langchain_community.llms import OpenAI from langchain_core.prompts import PromptTemplate prompt_template = \u0026#34;Tell me a {adjective} joke\u0026#34; prompt = PromptTemplate( input_variables=[\u0026#34;adjective\u0026#34;], template=prompt_template ) llm = LLMChain(llm=OpenAI(), prompt=prompt) 刚刚提到，我们大体上有两种方式来将 LLM 和 LangChain 结合起来，一种是基本的 Q\u0026amp;A，一种是对话式检索。其中，streaming 参数用于支持流式返回，return_source_documents 参与用于控制是否返回引用的文档信息，ConversationBufferMemory 组件用于处理对话历史。下面给出具体的实现代码：\n# 基本的 Q\u0026amp;A def get_basic_qa_chain(baseUrl=\u0026#39;\u0026#39;, apiKey=\u0026#39;\u0026#39;, storeFilePath=\u0026#39;\u0026#39;): llm = ChatOpenAI( model_name=\u0026#34;gpt-3.5-turbo\u0026#34;, temperature=0.75, openai_api_base=baseUrl, openai_api_key=apiKey, streaming=True ) retriever = load_retriever(storeFilePath) chain = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;stuff\u0026#34;, retriever=retriever, return_source_documents=True, ) return chain # 对话式检索 def get_conversational_retrieval_chain(baseUrl=\u0026#39;\u0026#39;, apiKey=\u0026#39;\u0026#39;, storeFilePath=\u0026#39;\u0026#39;): llm = ChatOpenAI( model_name=\u0026#34;gpt-3.5-turbo\u0026#34;, temperature=0.75, openai_api_base=baseUrl, openai_api_key=apiKey, streaming=True ) retriever = load_retriever(storeFilePath) memory = ConversationBufferMemory( memory_key=\u0026#34;chat_history\u0026#34;, return_messages=True, input_key=\u0026#34;question\u0026#34;, output_key=\u0026#34;source_documents\u0026#34; ) chain = ConversationalRetrievalChain.from_llm( llm=llm, retriever=retriever, memory=memory, return_source_documents=True, ) return chain 现在，我们就可以调用 Chain 来和大语言模型及知识库进行交互：\n# 构造 Chain，使用本地模型 chain = get_conversational_retrieval_chain_(\u0026#34;http://localhost:8080/v1/\u0026#34;, \u0026#39;sk-1234567890\u0026#39;, \u0026#39;./output/个人博客.pkl\u0026#39;) # 根据问题做相似性检索 question = \u0026#34;Envoy在微服务中都有哪些应用场景\u0026#34; documents = vector_store.similarity_search(question, k=3) context = \u0026#39;\\n\\n\u0026#39;.join([document.page_content for document in documents]) # 从模板创建提示词并填充模板 prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, PROMPT_TEMPLATE), (\u0026#34;user\u0026#34;, \u0026#34;{question}\u0026#34;), ]) query = prompt.format(question=question, name=\u0026#34;ChatGPT\u0026#34;, context=context) # 方式一： 同步调用 result = chain.invoke(query) # 方式二： 流式调用 result = chain.stream(query) 如下图所示，在经过反复地调试和优化以后，我们可以针对博客中的内容进行提问，并且程序会返回与问题相关的文档信息，这个结果整体而言还是挺不错的。当然，距离 FastGPT 这种相对完善的产品还是挺遥远的，使用 CPU 进行推理的 llama.cpp 除了生成速度慢以外，还会出现缓存 kv cache 不足的问题。庆幸的是，在折腾 LangChain 的过程中，逐渐理解了 RAG 以及 LangChain 的整体思路。在 LangChain 的 官方文档 中，官方提供了使用 LangChain 落地 AI 应用的示例，这篇文章主要参考了 RAG 这一篇，大家可以特别关注一下。\n本地 AI 知识库运行结果展示\rLangChain 除了概念多以外，API 变动非常频繁，经常出现破坏性的变更，而且官方现在主要在推 LangChain Expression Language(LCEL)，所以，网络上的资料经常都是过期的，建议大家有时间还是去看纯英文的官方文档。当然，大家有兴趣的话，可以考虑下 LangChain 的替代品，譬如 AutoChain 和 Embedchain，它们的 API 毫无疑问都比 LangChain 简单，实际使用情况，只有靠各位自己去体验啦，哈哈！\n本文小结 在探索 AI 的过程中，一个令人难过的事实是：为了创作这篇大约1.5万字的文章，博主先后花了一周左右的时间来做实验，然后花了两天的时间来形成文字，而在将这些博客内容 “投喂” 给 AI 以后，它可以迅速 “描摹” 出我的轮廓。你不得不承认，这个世界的信息实在多到爆炸，以致于我们绝无可能构建出一个无穷大的知识库。AI 更像是这个复杂时代地观察者，可以帮助我们更快、更好地去理解这个世界，虽然这个代价是 AI 可能会产生更多噪音，这有一点像什么呢？就像你在看视频时开启倍速播放，或许你节省了时间、提升了节奏，可终究失去了某种身体力行的体验感，正如在这篇文章里，我使用 LLaMA 和 LangChain 实现 RAG 的过程，于屏幕前的你而言，不过是互联网海洋里鼠标轻轻掠起的浪花。对我而言，写作的整个过程，与 RAG 更是何其的相似，我在输出这篇文章的同时，查阅大量的资料，最终，它们成为了我认知、感悟以及价值观的一部分，而这，便是自我的增强。\n","date":"2024-02-29T10:30:47Z","image":"/posts/基于-LLaMA-和-LangChain-实践本地-AI-知识库/cover.png","permalink":"https://qinyuanpei.github.io/posts/practice-local-ai-knowledg-base-based-on-llama-and-langchain/","slug":"Practice-Local-AI-Knowledg-Base-Based-On-LLaMA-And-LangChain","tags":["LLaMA","LangChain","RAG","GPTs"],"title":"基于 LLaMA 和 LangChain 实践本地 AI 知识库"},{"categories":["编程语言"],"content":"对于刚刚落下帷幕的2023年，人们曾经给予其高度评价——AIGC元年。随着 ChatGPT 的火爆出圈，大语言模型、AI 生成内容、多模态、提示词、量化\u0026hellip;等等名词开始相继频频出现在人们的视野当中，而在这场足以引发第四次工业革命的技术浪潮里，人们对于人工智能的态度，正从一开始的惊喜慢慢地变成担忧。因为 AI 在生成文字、代码、图像、音频和视频等方面的能力越来越强大，强大到需要 “冷门歌手” 孙燕姿亲自发文回应，强大到连山姆·奥特曼都被 OpenAI 解雇。在经历过 OpenAI 套壳、New Bing、GitHub Copilot 以及各式 AI 应用、各类大语言模型的持续轰炸后，我们终于迎来了人工智能的 “安卓时刻”，即除了 ChatGPT、Gemini 等专有模型以外，我们现在有更多的开源大模型可以选择。可这难免会让我们感到困惑，人工智能的尽头到底是什么呢？2013年的时候，我以为未来属于提示词工程(Prompt Engineering)，可后来好像是 RAG 以及 GPTs 更受欢迎？\n从哪里开始 在经历过早期调用 OpenAI API 各种障碍后，我觉得大语言模型，最终还是需要回归到私有化部署这条路上来。毕竟，连最近新上市的手机都开始内置大语言模型了，我先后在手机上体验了有大语言模型加持的小爱同学，以及抖音的豆包，不能说体验有多好，可终归是聊胜于无。目前，整个人工智能领域大致可以分为三个层次，即：算力、模型和应用。其中，算力，本质上就是芯片，对大模型来说特指高性能显卡；模型，现在在 Hugging Face 可以找到各种开源的模型，即便可以节省训练模型的成本，可对这些模型的微调和改进依然是 “最后一公里” 的痛点；应用，目前 GPTs 极大地推动了各类 AI 应用的落地，而像 Poe 这类聚合式的 AI 应用功能要更强大一点。最终，我决定先在 CPU 环境下利用 llama.cpp 部署一个 AI 大模型，等打通上下游关节后，再考虑使用 GPU 环境实现最终落地。从头开始训练一个模型是不大现实的，可如果通过 LangChain 这类框架接入本地知识库还是有希望的。\n编译 llama.cpp llama.cpp 是一个纯 C/C++ 实现的 LLaMA 模型推理工具，由于其具有极高的性能，因此，它可以同时在 GPU 和 CPU 环境下运行，这给了像博主这种寻常百姓可操作的空间。在 Meta 半开源了其 LLaMA 模型以后，斯坦福大学发布了其基于 LLaMA-7B 模型微调而来的模型 Alpaca，在开源社区的积极响应下，在 Hugging Face 上面相继衍生出了更多的基于 LLaMA 模型的模型，这意味着这些由 LLaMA 衍生而来的模型，都可以交给 llama.cpp 这个项目来进行推理。对硬件要求低、可供选择的模型多，这是博主选择 llama.cpp 的主要原因。在这篇文章里，博主使用的是一台搭配 i7-1360P 处理器、32G 内存的笔记本，按照 LLaMA 的性能要求，运行 GGML 格式的 7B 模型至少需要 13G 内存，而运行 GGML 格式的 13B 模型至少需要 24G 内存，大家可以根据自身配置选择合适的模型，个人建议选择 7B 即可，因为 13B 运行时间一长以后还是会感到吃力，哎😰。\nllama.cpp 在不同尺寸模型下对内存的要求\r准备工作 在正式开始前，请确保你可以熟练使用 Git，以及具备科学上网的条件，因为我们需要从 Hugging Face 上下载模型。此外，你还需要下载并安装以下软件：\nPython: 官方网站、华为镜像，建议选择 3.9 及其以上版本 w64devkit：便携式 C/C++ 编译环境，集成了 gcc、make 等常见的工具 OpenBLAS(可选): 可以提供 CPU 加速的高性能矩阵计算库，建议安装 w64devkit 和 OpenBLAS 下载下来都是压缩包，直接解压即可，建议将 w64devkit 解压在一个不含空格和中文的路径下，例如：C:\\w64devkit。接下来，我们还需要 OpenBLAS 的库文件和头文件，请将其 include 目录下的内容，全部复制到 C:\\w64devkit\\x86_64-w64-mingw32\\include 目录下；请将其 lib 目录下的 libopenblas.a 文件复制到 C:\\w64devkit\\x86_64-w64-mingw32\\lib 目录下。保险起见，个人建议将 C:\\w64devkit 目录添加到 Path 环境变量中，如下图所示：\n请检查你的系统环境变量\r至此，我们就完成了全部的准备工作。需要说明的是，这里是以 Windows + Make + OpenBLAS 为例进行演示和写作。如果你是 Mac 或者 Linux 系统用户，或者你想 CMake 或者 CUDA，请参考官方文档：https://github.com/ggerganov/llama.cpp，虽然这份文档是纯英文的，但是我相信这应该难不倒屏幕前的各位程序员朋友，哈哈😄。\n编译过程 好的，对于 llama.cpp 而言，其实官方提供了预编译的可执行程序，具体请参考这里：https://github.com/ggerganov/llama.cpp/releases。通常情况下，普通的 Windows 用户只需要选择类似 llama-b2084-bin-win-openblas-x64.zip 这样的发行版本即可。如果你拥有高性能显卡，可以选择类似 llama-b2084-bin-win-cublas-cu12.2.0-x64.zip 这样的发行版即可，其中的 cu 表示 CUDA，这是由显卡厂商 Nvdia 推出的运算平台。什么样的显卡算高性能显卡呢？就我朴实无华的游戏史观点而言，只要能流畅运行育碧旗下的《刺客信条：大革命》及其后续作品的，都可以算得上高性能显卡。这里，我们选择手动编译，因为通读整个文档你就会发现，llama.cpp 里面提供了大量的编译参数，这些参数或多或少地会影响到你编译的产物。所以，如果你喜欢折腾，品味独特，我还是建议你手动编译 llama.cpp，平时使用 Visual Studio 编译程序多少有点温水煮青蛙啦🐸。\n一张由 AI 应用 Poe 生成的刺客信条图片\r首先，我们使用 Git 克隆一份 llama.cpp 的源代码：\ngit clone https://github.com/ggerganov/llama.cpp 接下来，打开一开始准备好的 w64devkit.exe 工具，在这里其路径为：C:\\w64devkit\\w64devkit.exe，它将打开一个单独的命令行窗口。此时，我们需要进入上一步中克隆下来的 llama.cpp 源代码目录。在本文示例中，其路径为： D:\\Projects\\llama.cpp。下面是对应的命令行语句：\ncd d:\\ cd projects/llama.cpp 至此，我们就进入了 llama.cpp 的源代码目录，还记得在准备工作阶段，我们提到过的 OpenBLAS 及其 CPU 加速功能吗？如果你不需要 CPU 加速功能，那么，你可以直接输入 make 命令，否则你需要输入 make LLAMA_OPENBLAS=1。博主这里选择的是后者，没有 GPU 当然要精打细算地过日子，你说对不对？\nmake LLAMA_OPENBLAS=1 此时，如果编译成功的话，你会在当前目录下看到编译出来的各种可执行文件：\nllama.cpp 编译结果展示\r如果没有的话，请按照下面的方式尝试重新生成，直至编译成功：\nmake clean make LLAMA_OPENBLAS=1 坦白地讲，本来我一开始是打算从 llama-cpp-python 这个项目着手的，可惜通过 pip 安装的时候终于还是遇到了各种 C/C++ 的问题，最终决定还是返璞归真从 llama.cpp 本体入手。个人感觉 w64devkit 这个懒人包还是挺不错的，以前在 Windows 上想用 Make 或者 CMake 编译程序何其痛苦啊，现在总算是舒服了不少啊😏\n大模型尝鲜 OK，llamp.cpp 编译成功以后，我们就可以用它来跑各种各样的 AI 大模型了，它支持哪些模型呢？在官方文档里作者给出了说明，这里仅仅截取了其中一部分，而对对于我们接下来的探索已经完全足够啦：\nllama.cpp 支持哪些 AI 大模型？\r在当下的这场 AI 大模型追逐赛中，国内的大模型大多在模仿由 Meta 开源的 LLaMA 架构，无非是有些选择了从头开始训练，而有些选择了用中文语料做各种微调优化。如果一定追本溯源的话，LLaMA 参考了 GPT，GPT 参考了 Transformer，而 Transformer 则最早出现在 Google 于 2017 年发表的论文 Attention Is All You Need。说来有一点戏剧性，Google 在这场追逐赛中起了个大早，可目前看起来好像是投资了 OpenAI 的微软占据了上风，特别是 OpenAI 前 CEO 山姆·奥特曼已经加入微软，并且微软的 Copilot 在浏览器、桌面以及 Github 上都有着挺不错的表现。当然，历来代表着技术先进性的 Google 当然不会束手待毙，虽然目前的 Gemini Pro 还不及 ChatGPT，但终归可以讲一句未来可期。只有 OpenAI 一家独大，不管是对人工智能领域还是普通用户而言，绝对不是一件好事。\nllama.cpp 使用大模型的基本流程\r转换与量化 在接触大模型的过程中，除了提示词以外，你可能还会经常听到一个词：量化，下面我们就来聊一聊量化。如图所示，llama.cpp 使用大模型的基本流程，主要分为两步，分别是转化和量化。首先，来考虑第一个问题，为什么需要转化？前面提到过，现阶段 AI 大模型的起源都是 Transformer 模型，而 llama.cpp 使用的则是 GGML 模型，所以，当我们从 Hugging Face 上下载了某个大模型以后，第一件事情就是将其转化为 GGML 模型，这样，llama.cpp 便可以正确读取并使用这些模型进行推理。当然，更深层次的原因是，GGML 是和 llama.cpp 一起被设计出来的，其目的就是为了在 CPU 上运行量化后的模型，事实上，它们都出自同一个作者 Georgi Gerganov，GGML 由此得名，你可以将其理解为一个使用 C/C++ 实现的机器学习库。\nGeorgi Gerganov 大佬的 Github 主页\r接下来，我们来考虑第二个问题，什么是量化？根据知乎上的相关话题，ChatGPT-3 的模型参数最多高达 1750亿，而 ChatGPT-4 的模型参数则将近20000亿，你或许无法理解这些参数具体是什么，如果我们换一种方式来表示呢？根据估算，ChatGPT-3 的磁盘占用大约为 332G，而 ChatGPT-4 的参数规模大概是 ChatGPT-3 的 10 倍以上，这到底会给未来留下多少想象空间，我们始终无法得知，但如此庞大的规模哪怕对 GPU 来说依然是种负担。回想一下平时你玩过的那些 3A 大作的游戏目录你就知道了，这样一种体量的模型文件对人类来说，是完全不亚于奥特曼这般庞然大物的存在。至此，量化这种可以显著缩小模型体积的技术应运而生。\n模型量化的类比展示\r该如何解释这种技术呢？我们知道，一般的机器学习模型都使用单精度浮点型数值来表示，当我们使用一个有符号整型数值来近似替代的时候，虽然它会丢失部分精度，但是好处是这串数字的长度变短了。大模型量化采用了类似的原理，它会将模型中的权重和激活值表示为低精度的数据类型，这样，便可以减少模型的储存空间、能耗和计算时间，从而使其可以在资源受限的设备上高效运行。如上图所示，同一张图片在不同位数的颜色值下形态各异，可这副画的基本轮廓还是能看清楚的，这就是量化过程的形象化展示。显然，这是一种牺牲精度来换取性能的方案。\nllama.cpp 不同量化方法下的模型文件体积与推理速度(已过期)\r模型下载 现在，我们来聊聊模型下载问题，因为目前 Hugging Face 在国内基本处于不可访问的状态，但如果我们打算学习或者研究人工智能的话，这个网站始终是一个绕不过去的点，所以，这里来专门介绍下如何从 Hugging Face 上下载 AI 大模型。这里介绍三种方法：脚本下载、手动下载 以及 Git 下载。\n脚本方式 如果你经常光顾 Hugging Face ，那么，你会在许多模型文件的 README 里见到下面的代码片段：\nimport torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained(\u0026#34;Qwen/Qwen-7B-Chat\u0026#34;, trust_remote_code=False) model = AutoModelForCausalLM.from_pretrained(\u0026#34;Qwen/Qwen-7B-Chat\u0026#34;, torch_dtype=torch.float16, trust_remote_code=False) response, history = model.chat(tokenizer, \u0026#34;你好\u0026#34;, history=None) print(response) 这其实是一个用来测试预训练模型的脚本，不过它会缓存模型文件到本地，所以，这个方案一定程度上可行的，不过，经过我的测试，它一直在报 SSL 错误，即使挂着代理一样无法解决这个问题。在官方文档中一番折腾后，我发现了 hf_hub_download() 这个函数：\nfrom huggingface_hub import hf_hub_download hf_hub_download(repo_id=\u0026#34;Qwen/Qwen-7B-Chat\u0026#34;, filename=\u0026#34;config.json\u0026#34;) 非常遗憾，这个方法还是行不通，还是提示 SSL 错误，而且它一次只能下载一个文件，如果你想下载整个仓库，你需要使用 snapshot_download()。你知道在中国做程序员最痛苦的是什么吗？就是你对着官方文档一顿疯狂输出，结果最后输给了源、代理、加速器、镜像网站🙄\n手动方式 手动下载这个方案虽然非常的低效，可你不得不承认，这居然是最靠谱的方案。如图所示，我们只需要依次点击图中的下载按钮即可。建议下载时单独建一个文件夹，将同一个仓库内的文件都放在不起，这样，更利于你管理不同的模型文件。当然，这些模型文件都挺大的，所以，需要你的磁盘足够大，网络足够好，同时要有足够的耐心：\n从 Hugging Face 上手动下载模型\r我才不会告诉你，除了通义千问是用 Git 下载的，剩下的模型都是我手动下载的😕\nGit 方式 作为程序员，Git 不单单是一个工具，更是一种信仰。这里博主重点点名表扬阿里的通义千问：\n# Make sure you have git-lfs installed (https://git-lfs.com) git lfs install git clone git@hf.co:Qwen/Qwen-7B-Chat # if you want to clone without large files – just their pointers # prepend your git clone with the following env var: GIT_LFS_SKIP_SMUDGE=1 当然，还有从镜像网站 https://hf-mirror.com/ 下载模型的方法，这里就不再详细展开讲啦！此处的模型统一放在 llama.cpp 的 models 子目录下面，每个文件夹是一个模型，后续展开同样遵循这个原则。为了公平起见，我们使用同一个问题来向这些不同的模型进行提问，“昨天的当天是明天的什么”，看看 AI 大模型会作何反应。\nChinese-Llama-2-7B 体验 Step-1：完成模型的转换和量化：\npython convert.py models/chinese-llama-2-7b/ --outfile models/chinese-llama-2-7b/chinese-llama-2-7b-ggml.bin ./quantize models/chinese-llama-2-7b/chinese-llama-2-7b-ggml.bin models/chinese-llama-2-7b/chinese-llama-2-7b-ggml-model-q4_0.bin q4_0 Step-2：以命令行交互的方式运行模型:\n./main -m models/chinese-llama-2-7b/chinese-llama-2-7b-ggml-model-q4_0.bin -n 256 --repeat_penalty 1.0 --color -i -r \u0026#34;User:\u0026#34; -f prompts/chat-with-bob.txt 此时，运行结果如下： Chinese-Llama-2-7B 运行效果展示\r或许是这个问题太难，AI 被直接问出了母语，哈哈🤣\nChinese-Llama-2-13B 体验 Step-1：完成模型的转换和量化：\npython convert.py models/chinese-llama-2-13b/ --outfile models/chinese-llama-2-13b/chinese-llama-2-13b-ggml.bin ./quantize models/chinese-llama-2-13b/chinese-llama-2-13b-ggml.bin models/chinese-llama-2-13b/chinese-llama-2-13b-ggml-model-q4_0.bin q4_0 Step-2：以命令行交互的方式运行模型:\n./main -m models/chinese-llama-2-13b/chinese-llama-2-13b-ggml-model-q4_0.bin -n 256 --repeat_penalty 1.0 --color -i -r \u0026#34;User:\u0026#34; -f prompts/chat-with-bob.txt 此时，运行结果如下： Chinese-Llama-2-13B 运行效果展示\r果然，13B 要比 7B 聪明一点，可这句话完全不通顺啊🤣\nChinese-Alpaca-2-7B 体验 Step-1：完成模型的转换和量化：\npython convert.py models/chinese-alpaca-2-7b/ --outfile models/chinese-alpaca-2-7b/chinese-alpaca-2-7b-ggml.bin ./quantize models/chinese-alpaca-2-7b/chinese-alpaca-2-7b-ggml.bin models/chinese-alpaca-2-7b/chinese-alpaca-2-7b-ggml-model-q4_0.bin q4_0 Step-2：以命令行交互的方式运行模型:\n./main -m models/chinese-alpaca-2-7b/chinese-alpaca-2-7b-ggml-model-q4_0.bin -n 256 --repeat_penalty 1.0 --color -i -r \u0026#34;User:\u0026#34; -f prompts/chat-with-bob.txt 此时，运行结果如下： Chinese-Alpaca-2-7B 运行效果展示\r嗯，Alpaca 不愧是 LLaMA 的微调版本，这回答没有任何问题，甚至带着点哲学家的意味🤔\nQwen-1_8B-Chat 体验 Step-1：完成模型的转换和量化：\npython convert-hf-to-gguf.py models/Qwen-1_8B-Chat/ ./quantize models/Qwen-1_8B-Chat/ggml-model-f16.gguf models/Qwen-1_8B-Chat/ggml-model-q5_k_m.gguf q5_k_m Step-2：以命令行交互的方式运行模型:\n./main -m models/Qwen-1_8B-Chat/ggml-model-q5_k_m.gguf -n 256 --repeat_penalty 1.0 --color -i -r \u0026#34;User:\u0026#34; -f prompts/chat-with-bob.txt 此时，运行结果如下： Qwen-1_8B-Chat 运行效果展示\r还是你最诚实，一不留神就暴漏了训练数据集的时间范围，整体对比下来，感觉还是阿里的 Qwen-1_8B-Chat 模型效果更好一点，实际如何呢，我们可以通过下面的命令来评估模型的好坏。其中，wikitext 是一个用来测试的数据集，可以从网上免费获得。这里以 Qwen-1_8B-Chat 模型为例，跑一次评估大概需要 2 个小时左右 ：\n./perplexity -m models/Qwen-1_8B-Chat/ggml-model-q5_k_m.gguf -f wikitext-2-raw/wiki.test.raw 下面是 Qwen-1_8B-Chat 的评估结果：\nQwen-1_8B-Chat 评估结果\r对接 ChatGPT-Next-Web 除了以上面这种命令行交互的方式运行模型，我们还可以使用下面的命令在本地运行一个 OpenAI 服务：\n./server -m models/Qwen-1_8B-Chat/ggml-model-q5_k_m.gguf -c 2048 默认端口为 8080，llama.cpp 提供了一个相对简陋的聊天界面:\nllama.cpp 默认提供的聊天界面\r此外，llama.cpp 提供了完全与 OpenAI API 兼容的 API 接口，因此，我们可以使用 Postman 或者 Apifox 来请求本地的 AI 接口。当然，因为是使用 CPU 进行推理，所以，目前生成文本的速度非常感人：\nllama.cpp 提供的 API 接口\r既然现在有了与 OpenAI API 完全兼容的接口，那么，我们就可以考虑将其接入支持 OpenAI API 的前端页面。这里，博主选择的是市面上最为流行的 ChatGPT-Next-Web。方便起见，这里我们直接使用 Docker 来进行演示：\n# OPENAI_API_KEY 和 CODE 两个环境变量随便写即可 # 因为我们并不需要去调用真正的 OpenAI API docker run -d -p 3000:3000 -e OPENAI_API_KEY=xxxx -e CODE=1qaz2wsx3edc yidadaa/chatgpt-next-web 接下来，我们在设置里维护一下接口地址，因为我们要使用自定义的接口，虽然文档里提到可以用 BASE_URL 以及 PROXY_URL 两个变量来配置，但我一直没有尝试成功，程序员每天起起落落的人生啊！\n配置本地 AI 接口\r一切就绪以后，接下来，就是见证奇迹的时刻：\n使用本地 AI 接口\r至此，我们成功地将 ChatGPT-Next-Web 和 llama.cpp 两个项目结合起来，初步达成了在本地部署 AI 大模型的小目标。按照这个思路，理论上你只要有台服务器，就可以构建出一个自己的 AI 聊天工具。当然，目前这个模型里的知识都来自阿里通义千问，如果你希望它更贴近自己的上下文，就可以考虑对现有模型进行微调或者使用 LangChain 这类框架接入本地知识库，因为 llama.cpp 里同样提供了 Embeddings 等功能的 API ，并且它与 OpenAI 的 API 完全兼容，这意味着它完全可以利用 OpenAI 周边的生态。显然，这是下一个阶段的规划啦！\n本文小结 本文旨在尝试使用 llama.cpp 在本地部署 AI 大模型，随着人工智能的快速发展，我们逐渐认识到私有化部署的重要性和潜力。在此背景下，llama.cpp 作为一个纯 C/C++ 实现的 LLaMA 模型推理工具，提供了在本地环境下高性能的 AI 推理能力。在这篇文章中，我们可以了解到 llama.cpp 具有在 GPU 和 CPU 环境下运行的灵活性，满足私有化部署的需求。文章详细介绍了 llama.cpp 编译和部署的过程，为读者提供了一份在本地部署 AI 大模型的教程。私有化部署的 AI 大模型，相比于 ChatGPT 这类通用大模型，更注重数据隐私和安全性，对云服务的依赖更少，可以做到更好的本地化控制。虽然编译 llama.cpp 有一定的复杂性，AI 大模型的下载、转化、量化需要一定的耐心，可当本地的 AI 应用运行起来的那一刻，博主觉得这一切完全值得，以上就是本文的全部内容，下期再见！\n","date":"2024-02-04T12:30:47Z","image":"/posts/an-attempt-to-deploy-a-large-ai-model-locally-using-llama.cpp/cover.png","permalink":"https://qinyuanpei.github.io/posts/an-attempt-to-deploy-a-large-ai-model-locally-using-llama.cpp/","slug":"An-Attempt-To-Deploy-A-Large-AI-Model-Locally-Using-llama.cpp","tags":["llama.cpp","ChatGPT","Qwen-7B-Chat","LLaMA"],"title":"使用 llama.cpp 在本地部署 AI 大模型的一次尝试"},{"categories":["编程语言"],"content":"在电视剧《繁花》里有这样一个情节，汪小姐和宝总在一起时喜欢吃排骨年糕，后来两人分道扬镳，汪小姐用 “从此想，排骨是排骨，年糕是年糕” 这句对白来概括两个人的关系。不得不说，这句伤感中带着点文艺的台词，在受到剧粉及书迷追捧的同时，更是戳中了无数吃货的心。排骨年糕好不好吃，我不晓得。我唯一知道的事情是，人们需要亲密关系，可人们同样需要界限和距离感，排骨和年糕，就像是工作和生活，当我们意识到 “工作是工作，生活是生活” 的时候，或许我们就能达到真正的 “Work-Life Balance”。那么，对于程序员来说，工作和生活的界限在哪里呢？我想，这一切或许可以从为 Git 配置多个 SSH Key 说起。\n相信大家都会遇到这种场景，即一台电脑上同时存在多个 Git 账号的情况。譬如，公司的项目使用 Gitlab 托管，而个人的项目使用 Github 托管，更不必说，云效、Gitee、码云、Coding 等形形色色的平台。在这种情况下，你需要为每个代码托管平台生成 SSH Key，然后将其对应的公钥复制到指定的位置。所以，如何让这些不同托管平台的 SSH Key 和平共处、互不影响呢？这就是今天这篇文章想要分享的冷知识。当然，对博主个人而言，最主要的目的，还是希望能将公司和个人两个身份区分开来，所以，下面以 Github 和 Gitlab 为例来展示具体的配置过程。\n生成 SSH Key 首先，为两个平台生成各自的 SSH Key，使用 ssh-keygen 命令即可:\nssh-keygen -t rsa -C \u0026#34;\u0026lt;公司邮箱\u0026gt;\u0026#34; -f ~/.ssh/company-ssh ssh-keygen -t rsa -C \u0026#34;\u0026lt;个人邮箱\u0026gt;\u0026#34; -f ~/.ssh/personal-ssh 考虑到安全性问题，现在更推荐使用 Ed2519 加密算法，此时，你只需要替换上述命令中的 rsa 为 ed2519 即可。\n配置 Config 接下来，我们需要为本地的 SSH 配置上个步骤中生成的两个 SSH Key。通常，这个配置文件存在于以下路径：\nLinux: ~/.ssh/config Windows: C:\\Users\\\u0026lt;Your-User\u0026gt;\\.ssh\\config 如果在 Windows 系统下找不到该文件，我们直接创建一个无扩展名的文本文件即可：\nHost gitlab.com\rHostName gitlab.com\rPreferredAuthentications publickey\rIdentityFile ~/.ssh/company-ssh\rHost github.com\rHostName ssh.github.com\rPreferredAuthentications publickey\rIdentityFile ~/.ssh/personal-ssh 其中，Host 这一行表示该配置项的唯一标识，HostName 表示需要连接的目标服务器，IdentityFile 表示私钥文件的路径。可以注意到，我们这里共有两个 SSH Key 的配置，它们分别指向了上个步骤中生成的 SSH Key 私钥文件。至此，我们就完成了 SSH Key 的配置。\n添加公钥 可以注意到，在上述配置文件中，PreferredAuthentications 这一项的值为 publickey，这表示平台将使用公钥来进行认证。所以，接下来，我们需要在 Github 以及 Gitlab 上添加对应的公钥：\n为 Github 添加 SSH 公钥\r为 Gitlab 添加 SSH 公钥\r这一点，在各个平台上基本上是相似的，通常都可以在设置中找到相应的选项，这里不再赘述。\n身份验证 OK，现在我们可以来验证下 SSH Key 是否生效，以 Github 为例：\nssh -T git@github.com 如果你可以看到类似下面这样的回应，就证明这个 SSH Key 已添加成功，Gitlab 同理：\n为 Github 添加 SSH 公钥\r使用 SourceTree 到这一步，其实我们的目的就达到了，Git 客户端会根据项目里 Git 仓库的地址，来决定要使用哪一个 SSH Key。博主平时习惯使用 SourceTree，所以，这里我还想再补充一个点，那就是邮箱。众所周知，Git 里的配置有全局配置和本地配置两种，通常情况下，我们会使用下面的命令来配置全局用户：\ngit config --global user.name \u0026#34;\u0026lt;Your-Name\u0026gt;\u0026#34; git config --global user.email \u0026lt;Your-Email\u0026gt; 而 SourceTree，默认使用全局用户，这无疑会令我们在工作和生活上的身份产生混淆。对此，我的建议是选取工作邮箱来配置全局用户，然后在个人项目里使用个人邮箱来配置本地用户，这样就不会在个人项目中泄露或者掺杂公司邮箱，这样可以有效地杜绝信息安全或者是知识产权方面的纠纷。我现在养成的一个习惯是，在 SourceTree 里提交代码的时候，会稍微留意一下下面的用户信息，这算是在本地配置了多个 SSH Key 以后的一个副作用啦！\n本文完！\n","date":"2024-01-30T12:30:47Z","image":"/posts/how-to-configure-multiple-ssh-keys-for-git/GIT-Branch-and-its-Operations.png","permalink":"https://qinyuanpei.github.io/posts/how-to-configure-multiple-ssh-keys-for-git/","slug":"How-To-Configure-Multiple-SSH-Keys-For-Git","tags":["Git","SSH-Key","备忘","Bash"],"title":"如何为 Git 配置多个 SSH Key"},{"categories":["编程语言"],"content":"国庆节回来后的工作内容，基本都在围绕着各种各样的硬件展开，这无疑让本就漫长的 “七天班” ，更加平添了三分枯燥，我甚至在不知不觉中学会了，如何给打印机装上不同尺寸的纸张。华为的 Mate 60 发布以后，人群中此起彼伏地传出 “遥遥领先” 的声音，大概人类总是热衷于评价那些不甚了解的事物。这个现象到了工作中就会变成，总有某些人觉得某件事情特别简单。其实。一切你认为“简单”的东西，背后一定有无数的人们上下求索、苦心孤诣，就像计算机从早期的埃尼阿克(ENIAC)发展到今天的智能手机，你能使用它并不代表它就“简单”，人还是应该对为止的领域保持敬畏和谦逊。回到这篇文章，今天我想和大家聊一聊，我为了解决那些“简单”的问题而做出的尝试。本期的故事主角是我们最熟悉不过的 USB 设备，有道是 “千古兴亡多少事”，且听我娓娓道来。\n故事是这样的，基于某些不可抗因素上的考虑，博主需要在程序中集成某厂商的硬件。我猜测，人们觉得这件事情“简单”，或许是看到这个设备有一条 USB 连接线，因为在人们的固有印象中，只要把它接到电脑上就可以正常工作了。事实的确如此，因为你只要考虑串口(SerialPort)、USB 以及这两者间的相互转换即可。当然，这世上的事情圆满者少，遗憾者多，博主在使用过程中发现，厂商的提供的 SDK 存在 Bug，当设备从电脑上拔出后，其 SDK 的初始化函数依然正常返回了，这意味着我们无法在使用设备前“正确”地检测出硬件状态。考虑厂商愿不愿意修复这个 Bug 还是个未知数，博主不得不尝试另辟蹊径。\nWindows 中的设备与打印机\r相信这张图片大家都见过无数次啦，在这里你可以看到操作系统接入的各种设备。以鼠标为例，通过下面这个对话框，我们可以获得这个设备的各种属性信息：\n查阅鼠标的硬件信息\r在各种属性信息中，硬件 Id 是最为关键的一组信息，我们可以看到鼠标这个设备的 VID 为 0000，PID 为 3825。其中，VID 是指 Vender ID，即：供应商识别码；PID 是指 Product ID，即：产品识别码。事实上，所有的 USB 设备都有 VID 和 PID，VID 由供应商向 USB-IF 申请获得，而 PID 则由供应商自行指定，计算机正是 VID、PID 以及设备的版本号来决定加载或者安装相应的驱动程序。因此，如果想要判断计算机是否连接了某个 USB 设备，我们可以使用下面的方案：\nbool HasUsbDevice(string vid, string pid) { var query = $\u0026#34;SELECT * FROM Win32_PnPEntity WHERE DeviceID LIKE \u0026#39;USB%VID_{vid}\u0026amp;PID_{pid}%\u0026#39;\u0026#34;; var searcher = new ManagementObjectSearcher(query); var devices = searcher.Get(); return devices.Count \u0026gt; 0; } 需要说明的是，这是通过古老的 WMI 来查询 USB 设备信息，还记得我们前面收集到的 VID 以及 PID 吗？此时，我们需要简单调用一下即可：\nif (HasUsbDevice2(\u0026#34;0000\u0026#34;, \u0026#34;3825\u0026#34;) { Console.WriteLine(\u0026#34;[WMI]设备已连接\u0026#34;); } else { Console.WriteLine(\u0026#34;[WMI]设备未连接\u0026#34;); } 当然，在 .NET 8.0 发布以后，依然固执地抱着这些 Windows 平台的 API 不放，多少有点食古不化的意味。所以，实际工作中我会推荐本文题目中的 LibUsbDotNet 库，除了跨平台方面的考量，这个库的功能要更强大一点，可以做到向 USB 设备发送数据或者从 USB 设备接收数据。下面由我来对这个库的使用进行说明，目前，我们可以从 Github 以及 SourceForge 上下载对应的项目，两者的区别是 Github 上的项目更新一点：\nhttps://github.com/LibUsbDotNet/LibUsbDotNet https://sourceforge.net/projects/libusbdotnet/ 下载后是一个可执行文件，我们点击安装即可，它会安装好相关的库以及驱动文件，默认的安装目录为：C:\\Program Files\\LibUsbDotNet。在安装完成后，它会提示我们进入下面的对话框，这一步的目的是给特定的设备安装 libusb 驱动，因为只有安装了驱动的情况下，接下来的一切才会发生，除非 LibUsbDotNet 会隔空取物。\n为设备安装 libusb 驱动-1\r这里，我们还是选择鼠标这个硬件，你需要重点关注 PID 以及 VID 两个参数，因为这是唯一能区分不同 USB 设备的标识：\n为设备安装 libusb 驱动-2\r最后，点击 “Install” 按钮即可为当前设备安装 libusb 驱动。接下来的事情就变得非常简单啦，我们只需要通过 NuGet 安装 LibUsbDotNet 即可：\nbool HasUsbDevice(short vid, short pid) { var useDeviceFinder = new UsbDeviceFinder(vid, pid); var usbDevice = UsbDevice.OpenUsbDevice(useDeviceFinder); return usbDevice != null; } 可以注意到，LibUsbDotNet 需要的 VID 以及 PID 都是 short 类型的，所以，相比于 WMI 的方案，它在调用上会存在一点差异：\nvar verdorId = Convert.ToInt16(\u0026#34;0x0000\u0026#34;, 16); var productId = Convert.ToInt16(\u0026#34;0x3825\u0026#34;, 16); if (HasUsbDevice(verdorId, productId)) { Console.WriteLine(\u0026#34;[LibUsbDotNet]设备已连接\u0026#34;); } else { Console.WriteLine(\u0026#34;[LibUsbDotNet]设备未连接\u0026#34;); } 显然，你会注意到，我在原本的 “0000” 和 “3825” 前面都补了 “0x” 这样的字符，这是因为 VID 和 PID 都是 16 位的二进制数，它们都可以简写为 4 位十六进制数，所以，不管是在 Windows 上还是 LibUsbDotNet 提供的软件中，它都是以 4 位十六进制数的简写形式存在的。因此，这里就需要进行先补 “0x” 再做转换的处理。\n两种 USB 设备检测方案效果演示\r除了判断 USB 设备是否存在，有时候我们还需要关注 USB 设备的状态变化。例如，插入 USB 设备或者拔出 USB 设备。古人云：世上本无事，庸人自扰之。可这个世界上还就真的有这般无聊的人，动辄喜欢搞拔设备、拔网线这种所谓的深度测试。所以，下面我们来考虑如何处理这种极端的场景。从一开始，博主选择 LibUsbDotNet 这个库，就是看到它提供了 DeviceNotifier 这个类型。不过，在博主后续的尝试中发现，截止到 2.2.29 版本，这个类型已然无迹可寻，而 3.X 版本目前依然出于预发行状态，并且 API 与现在的版本不兼容，所以，这个念头不得不就此作罢。\n事件查看器 \u0026amp;amp; USB 设备\r当然，在觉醒了 WMI 的远古记忆以后，我们会意识到 Windows 下存在着一个大型的数据库，理论上我们只需要查询这个数据库，就可以监听到 USB 设备的状态变化。如图所示，我们会注意到每个硬件的对话框里有一个 “事件” 选项卡，而这些事件最终会在事件查看器里面汇合。在 ChatGPT 以及 wbemtest 的帮助下，我们找到了两个重要的重要的类名：__InstanceCreationEvent、__InstanceDeletionEvent。此时，我们可以编写出下面的代码：\nvoid MonitorUsbDevice() { // 监听 USB 设备插入 var queryInsert = new WqlEventQuery(\u0026#34;SELECT * FROM __InstanceCreationEvent WITHIN 1 WHERE TargetInstance ISA \u0026#39;Win32_USBControllerDevice\u0026#39;\u0026#34;); var watcherInsert = new ManagementEventWatcher(queryInsert); watcherInsert.EventArrived += (sender, e) =\u0026gt; { // 被插入的逻辑处理 var targetInstance = (ManagementBaseObject)e.NewEvent[\u0026#34;TargetInstance\u0026#34;]; // \\\\SNOWFLY-PC\\root\\cimv2:Win32_PnPEntity.DeviceID=\u0026#34;HID\\\\VID_0000\u0026amp;PID_3825\\\\6\u0026amp;2BE8ADFA\u0026amp;0\u0026amp;0000\u0026#34; var deviceId = targetInstance.Properties[\u0026#34;Dependent\u0026#34;].Value.ToString(); var device = new ManagementObject(deviceId); var args = new DeviceNotifierEventArgs(); // Win32_PnPEntity.DeviceID=\u0026#34;HID\\\\VID_0000\u0026amp;PID_3825\\\\6\u0026amp;2BE8ADFA\u0026amp;0\u0026amp;0000\u0026#34; args.DeviceId = device.Path.RelativePath.Split(\u0026#34;=\u0026#34;)[1].Replace(\u0026#34;\\\u0026#34;\u0026#34;, \u0026#34;\u0026#34;); args.DevicePath = device.Path.ToString(); args.Pid = \u0026#34;0x\u0026#34; + deviceId.Split(new char[] { \u0026#39;\u0026amp;\u0026#39;, \u0026#39;\\\\\u0026#39; }).FirstOrDefault(x =\u0026gt; x.StartsWith(\u0026#34;PID_\u0026#34;)).Replace(\u0026#34;PID_\u0026#34;, \u0026#34;\u0026#34;); args.Vid = \u0026#34;0x\u0026#34; + deviceId.Split(new char[] { \u0026#39;\u0026amp;\u0026#39;, \u0026#39;\\\\\u0026#39; }).FirstOrDefault(x =\u0026gt; x.StartsWith(\u0026#34;VID_\u0026#34;)).Replace(\u0026#34;VID_\u0026#34;, \u0026#34;\u0026#34;); if (!args.DeviceId.StartsWith(\u0026#34;USB\u0026#34;)) return; Console.WriteLine($\u0026#34;设备已插入 =\u0026gt; {JsonConvert.SerializeObject(args)}\u0026#34;); }; watcherInsert.Start(); var queryDelete = new WqlEventQuery(\u0026#34;SELECT * FROM __InstanceDeletionEvent WITHIN 1 WHERE TargetInstance ISA \u0026#39;Win32_USBControllerDevice\u0026#39;\u0026#34;); var watcherDelete = new ManagementEventWatcher(queryDelete); watcherDelete.EventArrived += (sender, e) =\u0026gt; { // 被拔出的逻辑处理 var targetInstance = (ManagementBaseObject)e.NewEvent[\u0026#34;TargetInstance\u0026#34;]; // \\\\SNOWFLY-PC\\root\\cimv2:Win32_PnPEntity.DeviceID=\u0026#34;HID\\\\VID_0000\u0026amp;PID_3825\\\\6\u0026amp;2BE8ADFA\u0026amp;0\u0026amp;0000\u0026#34; var deviceId = targetInstance.Properties[\u0026#34;Dependent\u0026#34;].Value.ToString(); var device = new ManagementObject(deviceId); var args = new DeviceNotifierEventArgs(); // Win32_PnPEntity.DeviceID=\u0026#34;HID\\\\VID_0000\u0026amp;PID_3825\\\\6\u0026amp;2BE8ADFA\u0026amp;0\u0026amp;0000\u0026#34; args.DeviceId = device.Path.RelativePath.Split(\u0026#34;=\u0026#34;)[1].Replace(\u0026#34;\\\u0026#34;\u0026#34;, \u0026#34;\u0026#34;); args.DevicePath = device.Path.ToString(); args.Pid = \u0026#34;0x\u0026#34; + deviceId.Split(new char[] { \u0026#39;\u0026amp;\u0026#39;, \u0026#39;\\\\\u0026#39; }).FirstOrDefault(x =\u0026gt; x.StartsWith(\u0026#34;PID_\u0026#34;)).Replace(\u0026#34;PID_\u0026#34;, \u0026#34;\u0026#34;); args.Vid = \u0026#34;0x\u0026#34; + deviceId.Split(new char[] { \u0026#39;\u0026amp;\u0026#39;, \u0026#39;\\\\\u0026#39; }).FirstOrDefault(x =\u0026gt; x.StartsWith(\u0026#34;VID_\u0026#34;)).Replace(\u0026#34;VID_\u0026#34;, \u0026#34;\u0026#34;); if (!args.DeviceId.StartsWith(\u0026#34;USB\u0026#34;)) return; Console.WriteLine($\u0026#34;设备已拔出 =\u0026gt; {JsonConvert.SerializeObject(args)}\u0026#34;); }; watcherDelete.Start(); } 理解这段代码基本上没有任何难度，唯一需要说明的地方是，插入或者拔出一个 USB 设备实际上会产生两条消息，它们分别表示的是设备实例与接口实例的创建。这个话听起来或许有些晦涩，可能连微软都不知道它自己在说什么。具体到博主的这个示例中，其规律是两者的 DeviceID 格式不同，一次是 HID ，一个是 USB，因此，我们只需要过滤掉 HID 的那条消息即可。最终，博主实现的效果如下图所示：\n通过 WMI 监听 USB 设备插入或者拔出\r有了这个思路，我们就可以在程序启动时对 USB 设备进行监控，一旦发现某个重要的设备被移除，程序就可以及时地做出响应或处理，而不用等到真正要用设备的时候引发异常，我越来越觉得，编程本质就是一群聪明人在千方百计地照顾一个“巨婴”，每次测试同事都说这里或者那里要加一个提示，可即使增加了提示，人们依然无止无休地问你为什么，错误信息不过是程序员自我安慰剂，除了程序员以外没有人会在乎它具体是什么。如果你对此怀疑表示怀疑的话，不妨回去翻翻你写的代码，有多少行是真正的、有用的代码，又有多少代码是为了防呆呢？好了，以上就是这篇博客的全部内容啦，本文完。\n","date":"2023-10-18T12:30:47Z","image":"/posts/csharp-uses-libusbdotnet-to-implement-usb-device-detection/immo-wegmann-HtsVneqf5Fg-unsplash.jpg","permalink":"https://qinyuanpei.github.io/posts/csharp-uses-libusbdotnet-to-implement-usb-device-detection/","slug":"CSharp-Uses-LibUsbDotNet-To-Implement-USB-Device-Detection","tags":["硬件","USB","WMI","LibUsbDotNet"],"title":"C# 使用 LibUsbDotNet 实现 USB 设备检测"},{"categories":["编程语言"],"content":"对于八月份的印象，我发现大部分都留给了出差。而九月初出差回来，我便立马投入了新项目的研发工作。因此，无论是中秋节还是国庆节，在这一连串忙碌的日子里，无不充满着仓促的气息。王北洛说，“活着不就是仓促，哪里由得了你我”。最近，我一直在忙着搞打印，我时常怀疑在“数字化转型”这件事情上，人们的口号大于实质，否则，人们便不会如此热衷于打印单据，虽然时间已过去许多年，可有些事情似乎从未改变过，无论是过去的 FastReport、FineReport，还是如今的 PrintDocument 以及基于 Web 的打印方案，它们只是形式在变化而已，真正的本质并未改变，就像业务可以从线下转移到线上一样，可人们试图控制和聚合信息流的意愿从未消退。在变与不变这两者间，我们总强调“适应” 和 “向前看”，可每个人都在有意无意地，试图向别人兜售某种在“舒适圈”浸染已久的概念，这一刻，我觉得还是应该多一点变化。所以，我想以 “样式与数据分离的打印方案” 为主题，探索一种 “新” 的玩法。\n从 PrintDocument 说起 一切的故事都有一个起点，而对于 C# 或者 .NET 来说，PrintDocument 始终是打印绕不过去的一个点。虽然，在别人的眼里，打印无非是调用系统 API 向打印机发送指令，可如果考虑到针式、喷墨、激光、热敏\u0026hellip;等等不一而足的打印机种类，以及各种尺寸的打印纸、三联单/五联单、小票纸，我觉得这个问题还是蛮复杂的。考虑到篇幅，我不打算在这里科普这些 API 的使用方法，下面这张思维导图展示了 PrintDocument 所具备的关于 “打印” 的能力。从这个角度来看，打印需要考虑的事情何其纷扰耶，甚至你还要考虑打印机缺/卡纸、切刀打印机是否正确地切割了纸张\u0026hellip;等等的问题。此前，网络上流传着一个段子，大意是有人问如何解决打印时产生的空白页。此时，在职场打拼多年的前辈会语重心长地告诉你，只需要将其打印出来然后丢掉其中的空白页😺。\nPrintDocument 思维导图\r相信大家都见过类似下面这样的单据或者小票：\n某公司公路出库单及华润万家购物小票\r通常情况下，如果使用 C# 中的 PrintDocument 来实现打印，其基本思路是构造一个 PrintDocument 实例，同时注册 PrintPage 事件，而在该事件中，我们可以利用 Graphics 来绘制线条、文字、图片等元素：\nvar printDocument = new PrintDocument(); printDocument.PrintController = new StandardPrintController(); // 设置打印机名称 printDocument.DefaultPageSettings.PrinterSettings.PrinterName = \u0026#34;HP LaserJet Pro MFP M126nw\u0026#34;; // 设置纸张大小为 A5 foreach (PaperSize paperSize in printDocument.DefaultPageSettings.PrinterSettings.PaperSizes) { if (paperSize.PaperName == \u0026#34;A5\u0026#34;) { printDocument.DefaultPageSettings.PaperSize = paperSize; break; } } // 注册 PrintPage 事件 printDocument.PrintPage += async (s, e) =\u0026gt; { // ... // 绘制一个二维码 var qrCodeWidth = 100; var qrCodeName = Guid.NewGuid().ToString(\u0026#34;N\u0026#34;); var qrCodePath = PathSourcecs.CaptureFace + @\u0026#34;\\\u0026#34; + qrCodeName + \u0026#34;.png\u0026#34;; QRCodeByZxingNet.NewQRCodeByZxingNet(qrCodePath, orderSaHwVo.saRecordId, qrCodeWidth, qrCodeWidth, ImageFormat.Png, BarcodeFormat.QR_CODE); var image = System.Drawing.Image.FromFile(qrCodePath); args.Graphics.DrawImage(image, marginLeft, totalHeight); // ... }; 当然，你还可以利用 BeginPrint 和 EndPrint 这组事件来处理打印开始和打印结束的逻辑，这里我们按下不表，下面是打印以及打印预览的代码实现，可以发现，这一切在微软 API 的加持下非常简单：\n// 打印 printDocument.Print() // 打印预览 var printPreviewDialog = new PrintPreviewDialog(); printPreviewDialog.Document = printDocument; printPreviewDialog.TopLevel = true; printPreviewDialog.ShowDialog(); 如下图所示，下面是通过 PrintPreviewDialog 组件实现的打印预览效果：\n通过 PrintPreviewDialog 实现打印预览\r如果从这个角度来审视 PrintDocument，它毫无疑问是一个非常完美的解决方案！\n样式与数据分离的尝试 历史经验告诉我们，凡事没有绝对，使用这个方案来打印最大的问题在于，样式和数据没有分离开来，甚至严重耦合在一起。这就导致每次只要更换打印格式，整个代码基本上等于全部重写。时过境迁，一个项目里存在着各种版本的 PrintPage 代码更是家常便饭。作为一名程序员，我一直呼吁大家努力去抓住那些不变的东西，可对于人生而言，适应变化、拥抱变化、创造变化的心态显然更具有普适性。所以，这世上是否会有一种 “以不变应万变” 的方案来解决这个问题呢？所以，下面来探索打印样式与数据的分离问题。\n通过 HTML 模板实现打印\r作为一个前/后端都写的伪・全栈工程师，我有时候甚至觉得，人类或许是是一遍遍地重复循环着自身，从原生到 Web 的演化过程中，我看到的是人们周而复始地在用新技术 “重制” 过去的旧业务。譬如，前端同样有单据打印的需求，通常可以使用 vue-print-nb 或者 vue3-print-nb 来实现。诚然，前端的打印方案自始至终都摆脱不了浏览器自身的特性限制，可我们还是能从中找到某种共性。如图所示，在此前的前端项目中，我使用 EJS 这个模板引擎来编写和渲染 HTML模板，再通过 vue-print-nb 将其打印出来。所以，在这里我想继续沿用这个方案，下面是整体的实现思路：\n实现样式与数据分离的打印方案\r如图所示，我们的思路是利用此前博主介绍过的 Liquid 来渲染 HTML 模板。此时，打印样式可以通过前端三件套搞定，我们只需要在模板文件中完成字段绑定即可，这样就可以实现数据和样式的分离。当然，这一切还不足以传递给 PrintDocument 来使用，所以，还需要将其进一步转化为图片或者 PDF 文件。在 IE 浏览器还没有寿终正寝的时间线里，你可以使用 WebBrowser 来实现图片的转化，可如果 2023 年我们还固执地着 WebBrowser 不愿放手，这何尝不是一种莫名的执念呢？下面采用全新的 WebView2 方案的一种实现：\n// 确保 WebView2 内核可用 await webView.EnsureCoreWebView2Async(); // 加载并渲染模板 var htmlContent = File.ReadAllText(\u0026#34;HtmlTemplate.html\u0026#34;); var template = DotLiquid.Template.Parse(htmlContent); htmlContent = template.Render(Hash.FromAnonymousObject(new { Remark = \u0026#34;这是通过打印模板渲染的内容\u0026#34; }) // 加载网页并截图 webView.Reload(); this.webView.NavigateToString(htmlContent); using var fileStream = File.OpenWrite(\u0026#34;snapshot.jpg\u0026#34;) await webView.CoreWebView2.CapturePreviewAsync(CoreWebView2CapturePreviewImageFormat.Jpeg, fileStream); 此时，我们只需要为 PrintDocument 注册 PrintPage 事件即可：\nprintDocument.PrintPage += async (s, e) =\u0026gt; { // 以下两行代码可以显著提升打印效果 e.Graphics.InterpolationMode = System.Drawing.Drawing2D.InterpolationMode.NearestNeighbor; e.Graphics.PixelOffsetMode = System.Drawing.Drawing2D.PixelOffsetMode.Half; // 按实际打印区域缩放、绘制图片 // 当然，这里会牵涉到像素、毫米、页边距等等的问题，还需要做进一步的研究，我这里表达的是一种可行性 var image = System.Drawing.Image.FromFile(\u0026#34;snapshot.jpg\u0026#34;); var printableArea = printDocument.DefaultPageSettings.PrintableArea; var ratio = image.Width / image.Height; e.Graphics.DrawImage(image, new System.Drawing.RectangleF(printableArea.Left, printableArea.Top, printableArea.Width, printableArea.Width / ratio)); }; 实际上，打印通常会牵扯到页边距、分页、纸张大小等等的问题，而采用前端三件套来渲染内容，自然不可避免地牵连出诸如像素、毫米、英寸、DPI \u0026hellip;等等一堆的名词。我不得不承认，这一切非常复杂，即便在普通用户眼中，打印就像变魔术一般，只需要轻轻地点击一下鼠标。如果考虑到图片放缩导致的变形问题，理论上 HTML 模板的宽高比应该与实际打印纸张的宽高比相同，可事实是每次处理打印问题总不免要花点时间来做调试。我个人觉得，如果使用 PDF 作为打印的载体，效果应该会比图片稍微好一点。\n使用浏览器自带的打印方案\r考虑 PDF 的理由主要有两个方面，其一是基于 Webkit 内核的浏览器天然地对 PDF 格式友好，其二是可以复用浏览器自身的打印能力。如图所示，我们可以利用全新的 WebView2 组件去调用浏览器自带的打印对话框。从某种意义上来讲，这和前端常用的 vue-print-nb 或者 vue3-print-nb 插件并没有任何区别，本文的一切碎碎念似乎都在这一刻流向了同一个地方。可这种方案的缺陷在于，它无法跳过打印浏览器自带的打印对话框，甚至你连用户点击了打印还是取消都无从判断，更不必说要去判断打印机是否打印完成。实际上，即便是 PrintDocument，它同样无法“准确”地获得打印进度。一旦人们提出静默打印的诉求，这一切的一切终将重新回到 PrintDocument 的方案。\nRDLC 报表设计器\r事实上，微软还提供了一种 RDLC 报表的方案，这种方案更贴近传统的报表类业务，它可以通过定义实体类、创建数据集、添加数据源、设计模板等一系列流程完成报表设计，如果你使用过 FastReport 这类产品，自然会觉得这一切似曾相识，甚至连 DataSet、DataTable 这种偏底层的 API 都会倍感亲切。微软的 RDLC 以及 FastReport 的报表模板，本质上都是一个 XML 文件，其底层应该都是利用了 PrintDocument 这套 API。如果你看到相关的代码片段，就会明白一件事情，即：太阳底下没有新鲜事，无外乎是将每一页渲染为图片，再通过 DrawImage() 方法绘制出来。当一个人越来越接近本质，就会天然地厌倦外在的装饰或者形式，可惜生活中好像到处都是这样的事情。\n本文小结 在无数次纠结下，我终于写完了这篇没什么技术含量的文章。首先，打印这个话题非常零散，这些难以形成体系的内容，属实无法达到一篇文章的篇幅。其次，打印在业务中的价值非常低，有或者没有并不会影响主线流程，更多的情况下是一种聊胜于无的点缀。从这两个角度来看的话，我这篇文章甚至都没有什么价值，因为在人们的印象中，打印终归是一件非常简单的事情，哪怕有的人连装纸这件事情都能搞砸，可这丝毫不会影响人们心目中对它的定位，人们唯一能记住的就是接上电源、按下开关、点击鼠标。如果我永远改变不了这一点，我唯一能做的就是将这些碎碎念记录下来，无论是殊途同归还是独辟蹊径，我在意的是此时此刻坐在电脑前的我的感受，仅此而已。\n","date":"2023-09-20T12:30:47Z","image":"/posts/a-printing-scheme-for-separating-style-and-data-based-on-csharp/pexels-suzy-hazelwood-1999352.jpg","permalink":"https://qinyuanpei.github.io/posts/a-printing-scheme-for-separating-style-and-data-based-on-csharp/","slug":"A-Printing-Scheme-For-Separating-Style-and-Data-Based-on-CSharp","tags":["打印","PrintDocument","WebView2","模板引擎"],"title":"基于 C# 实现样式与数据分离的打印方案"},{"categories":["编程语言"],"content":"不知道从什么时候起，人们开始喜欢上数字大屏这种“花里胡哨”的东西，仿佛只要用上“科技蓝”这样神奇的色调，就可以让一家公司焕然一新，瞬间变得科技感满满。不管数字大屏的实际意义，是用来帮助企业监控和决策，还是为了方便领导参观和视察，抑或是为了向外界展示和宣传。总之，自从数字大屏诞生之后，它始终就没能摆脱其前任“中国式报表”那种大而全的宿命。追随着 ECharts、Superset、FineBI、DataEase 等数据可视化产品的身影一路走来，你会发现人们在追求“花里胡哨”这件事情上永无止境。如今的数据大屏，元素多(表格、视频、2D/2.5D/3D地图)、种类多(图表、报表、流程图)、媒介多(PC、平板、电视、LED)，主打的就是一个眼花缭乱。\n某数据中心设备运行监控示意图\r当数字大屏的这股时尚潮流涌向物联网和工业互联网领域以后，就不可避免地催生出像上面这样的“数字大屏”需求，请原谅我使用如此模糊的措辞，因为我实在难以给它一个准确的定义，工艺流程图、设备运行监控图、组态图、SCADA\u0026hellip;。也许，这些名称不见得都能做到全面概括，可这些东西的确具备了数字大屏的特征，哪怕这些设备元件、管道阀门在科技蓝配色下违和感十足。作为一位低调的程序员，我一向不喜欢这种粉饰太平的面子工程，所以，当设计师同事带着设计图来找我时，我当时内心是拒绝的：\n基于 HTML5 图片热区特性实现交互的思路\r也许，此时你的内心深处会闪过一丝蔑视，认为这有什么难度呢？我只需要在图片上叠加若干个透明的 div，这样不就可以实现图片特定区域的交互逻辑啦！我承认，这是一个非常好的思路，但是在实践过程中你就会发现，div 的交互区域通常都是一个标准的矩形，而设计师同事常常使用圆角矩形和不规则图形来增强设计感。因此，在交互方面可能会存在一些缺陷，尤其是在 2.5D 的图片设计稿中，交互区域实际上是一个多边形。接下来，我将介绍一种基于 HTML5 图片热区特性来实现交互的思路：\n\u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;Demo-01.jpg\u0026#34; usemap=\u0026#34;#imageMap\u0026#34; style=\u0026#34;width: 600px; height: 315px\u0026#34;\u0026gt; \u0026lt;map name=\u0026#34;imageMap\u0026#34;\u0026gt;\u0026lt;/map\u0026gt; \u0026lt;/div\u0026gt; 首先，准备一张图片以及一个 map 标签，并且这个 map 标签通过 usemap 属性与这张图片进行了关联。参照上面的示意图，我们定义了两个可交互的区域。其中，区域1是矩形区域，区域2是圆形区域：\nconst areas = [{ key: \u0026#39;半泽直树\u0026#39;, shape: \u0026#39;rect\u0026#39;, coords: [0, 0, 308.5, 315] }, { key: \u0026#39;大和田\u0026#39;, shape: \u0026#39;circle\u0026#39;, coords: [418, 134, 157.5] }] 因为 area 标签需要搭配 map 标签来使用，所以，我们将通过下面的代码来动态地创建区域，同时为每个区域绑定相应的事件：\nconst popup = document.getElementById(\u0026#39;popup\u0026#39;); const imageMap = document.getElementsByName(\u0026#39;imageMap\u0026#39;)[0]; areas.forEach(area =\u0026gt; { // 创建区域 let ele = document.createElement(\u0026#39;area\u0026#39;); ele.shape = area.shape; ele.coords = area.coords.join(\u0026#39;,\u0026#39;) ele.setAttribute(\u0026#39;data-key\u0026#39;, area.key); // 绑定事件 ele.onclick = function (e) { alert(e.target.dataset.key); }; ele.onmousemove = function (e) { popup.innerHTML = `\u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt;${e.target.dataset.key}\u0026lt;/div\u0026gt;`; popup.style.left = `${e.x - 75}px`; popup.style.top = `${e.y - 45}px`; popup.style.display = \u0026#39;block\u0026#39; }; ele.onmouseover = function (e) { popup.style.display = \u0026#39;display\u0026#39;; }; // 添加到map标签 imageMap.appendChild(ele); }) 此时，当我们鼠标移动到指定的区域时，就可以触发对应的气泡提示，如下图所示：\nSee the Pen Marker-Clusterer by qinyuanpei (@qinyuanpei)\ron CodePen.\r这个方案相对于纯 div 标签的思路要稍微好上一点点，因为 area 标签里的 shape 属性支持多边形，这意味着不规则区域的交互可以继续进行下去。可这种方案，本质上并没有摆脱“手工标注”，你不得不为每一个区域标注好坐标，这对于没有设计感的程序员来说可能是一场折磨，更重要的是，一旦这个方案运用到数字大屏上面，你总要去解决屏幕尺寸变化、全屏/非全屏等一系列问题，显然，这个时候这些区域的坐标都需要重新计算。这个时候，博主就想到了 SVG 这种可缩放的矢量图形，这是一种自描述的标记语言，无论怎么缩放都不会失真。下面是一个简单的 SVG 图片示例，我们可以大致了解到其结构是一个 XML 文件：\nSee the Pen Marker-Clusterer by qinyuanpei (@qinyuanpei)\ron CodePen.\r既然 SVG 中本身就自带着描述位置的坐标信息，那么，我们是不是可以基于 SVG 来实现相应的交互逻辑呢？下面我们以一张中国地图为例来验证这个想法：\nOK，我们希望实现什么样的交互效果呢？当鼠标移动到指定的省份时，该省份会变成红色高亮状态，并且会在鼠标位置触发气泡提示。具体怎么做呢？首先，我们来准备下面的 HTML 结构：\n\u0026lt;div id=\u0026#34;popup\u0026#34; class=\u0026#34;rectangle\u0026#34; style=\u0026#34;display: none;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 接下来，我们通过脚本来加载 SVG 图片，同时为其绑定相关事件：\nconst popup = document.getElementById(\u0026#39;popup\u0026#39;); fetch(\u0026#39;China.svg\u0026#39;) .then(res =\u0026gt; res.text()) .then(text =\u0026gt; { const container = document.getElementsByClassName(\u0026#39;container\u0026#39;)[0]; container.innerHTML = text; // 允许SVG交互 const svg = document.getElementsByTagName(\u0026#39;svg\u0026#39;)[0]; svg.setAttribute(\u0026#39;pointer-events\u0026#39;, \u0026#39;cursor\u0026#39;); // 为每一个路径绑定事件 const paths = svg.childNodes[0].childNodes; for (var i = 0; i \u0026lt; paths.length; i++) { paths[i].onclick = function (e) { alert(`${e.target.id}`) }; paths[i].onmouseover = function (e) { e.target.setAttribute(\u0026#39;fill\u0026#39;, \u0026#39;red\u0026#39;); popup.innerHTML = `\u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt;${e.target.id}\u0026lt;/div\u0026gt;` popup.style.left = `${e.x - 75}px`; popup.style.top = `${e.y - 45}px`; popup.style.display = \u0026#39;block\u0026#39; }; paths[i].onmouseout = function (e) { e.target.setAttribute(\u0026#39;fill\u0026#39;, \u0026#39;#eee\u0026#39;); popup.style.display = \u0026#39;none\u0026#39; }; } }) 在这个示例中，每一个省份对应着 SVG 中的一个 path 节点，因此，我们只需要在加载完 SVG 以后再去遍历这些节点即可。可能大家会有疑问，为什么这里不用 img、embed 或者 object 这些标签来承载一个 SVG 图形呢？因为这样我们是没有办法访问 svg 标签及其子节点的。现在，我们就可以看到下面的效果：\nSee the Pen Marker-Clusterer by qinyuanpei (@qinyuanpei)\ron CodePen.\r这两个方案，你更倾向于哪一个呢？从一个程序员的角度来看，我更喜欢使用 SVG，因为 XML 这种格式不管对人还是机器来说都非常友好。实际上，到目前为止，这篇博客里对方案可行性的探索业已完成，而在现实中，更多的挑战往往来自非技术因素。譬如，如何让设计师同事适应使用相对普通、朴素的矢量图格式。当然，从这篇文章的思路延伸出去，无论是复杂的数据大屏，还是布局编辑器/低代码、地图、流程图、工作流等问题，我们都无法摆脱 DOM、Canvas、WebGL、SVG 等知识体系。特别是当我们需要处理拖拽/平移、缩放、旋转等常规操作时，这些都是最具挑战性的部分，不是吗？\n","date":"2023-08-20T12:30:47Z","image":"/posts/practice-of-svg-based-graphic-interaction-solution/Demo-02.jpg","permalink":"https://qinyuanpei.github.io/posts/practice-of-svg-based-graphic-interaction-solution/","slug":"Practice-Of-SVG-Based-Graphic-Interaction-Solution","tags":["前端","SVG","交互","探索"],"title":"基于 SVG 的图形交互方案实践"},{"categories":["编程语言"],"content":"转眼间，2023 年已进入下半场，在这样一个时间节点下，长视频平台如爱奇艺、优酷、腾讯视频等，以及短视频平台如抖音、快手等，对大家来说早已是司空见惯的事物。然而，在我们追剧、刷弹幕的时候，很少有人会去深入思考这些平台背后的技术奥秘。直到最近，我需要在前端项目中实现视频播放时，我终于意识到，这些视频不仅在格式上存在着差异，甚至连播放形式都各有不同。举个例子，当下最为火热的 “直播”，通常是指实时的视频播放。相对应地，非实时的视频播放则被称为 “点播”。如果你有接触过 Netflix，你或许还听说过 “流媒体” 这个词汇。为了更好地理解这些概念，我利用空闲时间整理了一个相对完整的技术体系，并以此为基础撰写了今天这篇文章。\n从 HTML5 说起 好了，现在让我们从最简单的视频播放方案开始说起。在 HTML5 标准发布前，主流的视频播放方案是使用 Adobe 的 Flash Player 插件，国内的优酷、土豆等视频网站创立初期都经历过这个阶段。后来，随着乔布斯那封 “关于 Flash 的思考” 的公开信的发表，某种意义上加速了整个 Flash 技术的 “消亡”。再后来，随着 HTML5 标准发布，我们可以使用 video 或者 audio 标签在网页中呈现音/视频内容。如图所示，下面是 video 标签的基本用法：\n\u0026lt;video controls\u0026gt; \u0026lt;source src=\u0026#34;myVideo.mp4\u0026#34; type=\u0026#34;video/mp4\u0026#34;\u0026gt; \u0026lt;source src=\u0026#34;myVideo.webm\u0026#34; type=\u0026#34;video/webm\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Your browser doesn\u0026#39;t support HTML5 video. Here is a \u0026lt;a href=\u0026#34;myVideo.mp4\u0026#34;\u0026gt;link to the video\u0026lt;/a\u0026gt; instead.\u0026lt;/p\u0026gt; \u0026lt;/video\u0026gt; 具体来讲，这个 video 标签可以支持 Ogg、MPEG4 和 WebM 三种视频格式。可惜，并不是所有的浏览器都支持这些格式，因此，你可以在 video 标签内指定多个视频源，并且当这些视频源都不被支持的时候，你可以使用一个自定义的 HTML 结构来进行降级处理。需要注意的是，MPEG-4 即 MP4 格式，实际上是一组格式，因为在视频处理中通常还涉及到编码器的问题。可不幸的是，浏览器目前唯一支持的编码器是 H.264，在这种情况下，可选择的视频格式将会变得非常有限。\nMPEG-4 视频格式在火狐浏览器的支持情况\r综上所述，这种方案的缺点主要体现在：可选择的视频格式、以及这些视频格式的兼容性问题上。在大多数情况下，你可以使用一个静态文件服务来承载这些视频文件，这意味着每次浏览器都会下载完整的视频文件。\n[HttpGet(\u0026#34;download\u0026#34;)] public ActionResult Download([FromQuery] string fileName) { var filePath = Path.Combine(_hostEnvironment.ContentRootPath, \u0026#34;Assets\u0026#34;, fileName); if (!System.IO.File.Exists(filePath)) return NotFound(); var contentType = GetContentType(filePath); var fileStream = System.IO.File.OpenRead(filePath); return File(fileStream, contentType, enableRangeProcessing: true); } 事实上，你还可以使用带 Range 请求头的请求方式，来实现类似断点续传或者分段下载的效果。如图所示，可以注意到，分段下载返回的状态码为 206，客户端通过 Range字段告知服务端自己希望下载文件的哪一部分，而服务器则通过 Content-Range 字段告知客户端当前返回是文件的哪一部分，以及整个文件的大小：\n通过 ASP.NET Core 实现文件的分段下载\rRTMP 还是 RTSP? 虽然，从易用性的角度来看，HTML5 中的 video 标签显得平易近人，可遗憾的是，我们到现在为止仅仅看到是视频播放领域的冰山一角。举一个简单的例子，当我们通过电视收看电视台的节目时，有时会遇到现场直播的场景。在此场景下，人们对视频播放有了实时性的要求。如下图所示，常见的流媒体协议，大体上可以分为三类：\n传统的视频流协议：以 RTMP 和 RTSP 为代表 基于 HTTP 的自适应协议：以 HTTP-FLV、HLS、MPEG-DASH 为代表 新技术：以 WebRTC 和 SRT 为代表 常见的流媒体协议有哪些？\r在通常的认知中，直播使用 RTMP 或者 RTSP，点播使用 HTTP。当然，工程上的问题没有绝对，随着近年来直播行业的持续火爆，理论上上述协议中的任意一种都可以实现直播。不过，在此之前，我们最好还是来了解一下，目前应用最为广泛的 RTMP 和 RTSP 。在这个过程中，你或许会深刻地感受到技术更迭带来的那种沧桑感。\nRTMP，即：Real Time Messaging Protocol，是一种以 TCP 协议作为底层协议的实时消息协议，采用了 H.264 视频编解码器 以及 ACC 音频编解码器。时间追溯到 2005 年，Macromedia 这家公司制定并开发了 RTMP 协议，其目的是为了在 RTMP 服务器和用户设备上的 Flash 播放器之间传输流式数据。后来，Adobe 收购了 Macromedia，Flash 技术得到了进一步的发展，相继诞生了 ActionScript、Adobe AIR 等产品。再后来，乔布斯那封 “关于 Flash 的思考” 的公开信，像一篇檄文一样直指 Flash 的种种缺点，冥冥中 “加速” 了 Flash 的消亡。当然，从 “事后诸葛亮” 的角度来看，我们可以说那是乔帮主的又一次的成功预言。可这世上的事情总是难以捉摸，就像 Flash 早已于 2020 年宣布死亡，可 RTMP 如今还活着，孰是孰非，有时候真的有没有那么重要吗？\nRTMP 协议基本流程演示\r如图所示，是一个 RTMP 的基本流程。可以注意到，其核心在于 “推流” 和 “拉流”，简单来说，我们可以通过 FFmpeg 或者 OBS 来向一个 RTMP 服务器推送视频流。此时，运行在用户设备上的 Flash 播放器或者流媒体播放器就可以从 RTMP 服务器上拉取视频流，进而实现视频播放的效果。相较于 HTTP 这种 “平民化” 的协议，RTMP 这种视频流协议明显要更复杂一点。幸运的是，我们可以借助 Nginx 及其第三方模块 nginx-http-flv-module、nginx-rtmp-module 快速搭建一个 RTMP 服务器，这里以 nginx-http-flv-module 为例来进行说明：\nrtmp { server { listen 1935; application pushLive { live on; gop_cache on; } } } 首先，通过 rtmp 节点下的一系列配置，我们让 Nginx 摇身一变成为一个 RTMP 服务器，如前文所述，RTMP 服务器使用 1935 端口。在此基础上，我们增加一个名为 pushLive 的应用，这一步的目的是对外暴露一个推送视频流的路由。一个 RTMP 服务器下可以同时配置多个应用，其中，live on; 指令表示开启直播，gop_cache on; 指令表示开启 GOP 的缓存。那么，GOP 又是什么呢？通常情况下，它是视频流中两个关键帧之间的距离。考虑到解码器必须拿到 GOP 才能开始解码、播放，因此，缓存 GOP 一定程度上可以降低延迟。\nhttp { server { location /pullLive { flv_live on; chunked_transfer_encoding on; add_header Access-Control-Allow-Credentials true; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Headers X-Requested-With; add_header Access-Control-Allow-Methods GET,POST,OPTIONS; add_header Cache-Control no-cache; } } } 接下来，我们需要在 Nginx 服务器里配置一个拉取视频流的路由，这里拉取的是 FLV 格式的视频流。考虑到视频流的推送方和拉取方通常位于不同的域，因此，还需要为 Nginx 配置跨域所需的头部字段。至此，我们搭建出了一个简单的 RTMP 服务器。\n通过 OBS 向 RTMP 服务器推流-A\r通过 VLC 从 RTMP 服务器拉流-A\r现在，我们只需要分别在 OBS 和 VLC 配置推流、拉流的地址。这里的推流码以及 stream 字段，可以理解为直播的频道或者房间号。只要双方都在一个频道或者房间里，即可实现直播效果：\n通过搭建 RTMP 服务器实现直播效果\r好的，介绍完 RTMP，现在再来介绍一下 RTSP，它表示的是实时流协议，即：Real Time Streaming Protocol。该协议于 1996 年问世，由 RealNetworks、Netscape和哥伦比亚大学的专家共同开发。RTSP 使用 TCP 和 UDP 作为底层协议，其中，TCP 负责传输控制指令，如播放和停止，而 UDP 负责传输音频、视频和其他数据。与 RTMP 类似，RTSP 采用了 ACC 作为音频编码器，采用 H.265 作为视频编码器。考虑到，RTMP 是Adobe 的专有协议，而 RTSP 是一种公共协议。因此，在传统的安防、监控等垂直领域中，人们更倾向于使用 RTSP ，通常在集成这类产品时，都需要集成厂商提供的 SDK 或安装相应的浏览器插件。\nffmpeg -re -rtsp_transport tcp -i \u0026lt;rtsp地址\u0026gt; -vcodec libx264 -vprofile baseline -acodec aac -ar 44100 -strict -2 -ac 1 -f flv -s 1280x720 -q 10 \u0026lt;rtmp推流地址\u0026gt; 我个人认为，RTSP 唯一的优势在于更低的延迟，如果让我来选的话，我还是会选择 RTMP。数学上有种重要的思维，将一个从未解决过的问题，转化为一个解决过的问题。此时，我们就可以使用上面的命令将一个 RTSP 视频流转换为 RTMP 视频流，这样，我们就可以完全不用理会厂商提供的 SDK 或者浏览器插件。毕竟，ActiveX、OLE 控件早就该扫进历史的垃圾堆啦。当然，除了 FFmpeg，你还可以使用 VLC 播放器进行推流转换，大家有兴趣的话可以自行尝试。\nFlash 的折戟沉沙 对于喜欢怀旧的人来说，回顾历史时总不免一阵唏嘘。因为一同被扫进历史垃圾堆的，不单单有的 ActiveX、OLE 控件、Sliverlight、MFC、CGI、SOAP\u0026hellip; ，还有一度被 Adobe 寄予厚望的 Flash。2020 年 12 月以后，Adobe Flash Player 将不再受到浏览器的支持，这大抵正式宣告了 Flash 的死亡。有趣的是，当年微软为了对抗 Flash 而推出的 Sliverlight，如今早已是 “王图霸业，血海深仇，尽归尘土”，唯有 HTML5 “桃花依旧笑春风”，这一幕，与金庸先生《天龙八部》里逍遥三老的经历何其相似耶！无独有偶，微软的 IE 浏览器与谷歌的 Chrome 浏览器相爱相杀多年，直到去年 6 月 IE 浏览器正式退役，微软打不过就加入的做法，不免让人担忧，“屠龙少年终成恶龙” 这一幕会再次上演。如果注定要这样的话，那么，我只能选择 Firefox🤦。\nIE 浏览器的墓碑及墓志铭\r言归正传，Flash 的折戟沉沙，直接导致 RTMP 和 RTSP 这对难兄难弟失去了赖以生存的 “土壤”。对于 RTMP 来说，其缺点主要是 FLV 与 HTML5 的兼容性问题、带宽问题，人们依然可以使用那些支持 FLV 格式的播放器软件，例如 PotPlayer、KMPlayer、VLC 等等；可对于 RTSP 来说，其本身与 HTTP 不兼容，必须要借助第三方插件，例如 NPAPI/PPAPI、ActiveX 等等。事实上，这些插件在不同浏览上的兼容性并不能得到充分的保证，这样就造成了一个相对混乱的局面。我以为，最重要的原因是，浏览器或者说前端生态圈，缺少像 FFmpeg 一样强大的视频编/解码工具。不过，随着 WebAssembly 等技术的成熟，我相信这一切终有一天会得到改善！\nffmpeg.wasm - Github\rgithub.com/ffmpegwasm/ffmpeg.wasm\r苹果公司在成功 “驱逐” Flash 以后，转头就开始开发自家的流媒体协议，这便是后来的 HLS 协议，其全称为：HTTP Live Streaming，是一种基于 HTTP 的流媒体传输协议，主要用于实时音/视频流的传输。其基本原理是：首先，它会将整个流切割为无数个可以通过 HTTP 下载的小文件。然后，它会向客户端提供一个配套的媒体文件列表。此时，客户端只需要按照顺序拉取和播放媒体文件，即可实现看起来像是在播放一条完整流媒体的效果。也许，你此前从来没有听说过这些内容，可当你在移动设备上播放 m3u8 格式的视频时，我想告诉你，你已经同 HLS 见过面啦！实际上，这个说法并不太严谨，因为 m3u8 本身就只是一个播放列表，真正的媒体文件其实是那些 ts文件。考虑到整个传输层是标准的 HTTP 协议，因此，这个方案可以非常方便地利用 CDN 进行分发、加速。\nFLV 还是 HLS? 感谢苹果公司，感谢乔布斯，现在我们有了新的选择。类似地，我们可以使用 Nginx 及其第三方模块 nginx-rtmp-module 来搭建一个支持 HLS 协议的流媒体服务器。其基本思路是：通过 OBS 或者 FFmpeg 向 RTMP 服务器推送视频流，服务器端生成对应的 m3u8 以及 ts 文件，此时，只需要像访问静态资源一样访问 m3u8 文件即可。除此之外，你同样可以将事先生成好的 m3u8 以及 ts 文件放置在指定的目录。首先，我们来定义一个用于推送视频流的路由：\nrtmp { server { listen 1935; application pushLive { live on; hls on; hls_path /tmp/hls; hls_fragment 15s; meta off; } } } 接下来，我们需要定义用于访问 m3u8 资源的路由。其中：alias 指令需要和上文中的 hls_path 指令保持一致。否则，Nginx 可能会找不到这些 m3u8 资源：\nhttp { server { location /hls { types { application/vnd.apple.mpegurl m3u8; } alias /tmp/hls; add_header Access-Control-Allow-Credentials true; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Headers X-Requested-With; add_header Access-Control-Allow-Methods GET,POST,OPTIONS; add_header Cache-Control no-cache; } } } 显然，接下来的事情就顺理成章啦！我们通过 OBS 推流，再通过 VLC 播放即可，如下图所示。此时此刻，屏幕前的你，是不是觉得刘海柱更像奇异博士了呢？😺\n通过 OBS 向 RTMP 服务器推流-B\r通过 VLC 从 RTMP 服务器拉流-B\r为了证明我前面讲过的理论，这里放一张容器内 /tmp/hls/ 目录下的内容截图，可以注意到，它的确生成了相应的 m3u8 以及 ts文件：\nHLS 协议三剑客：HTTP、m3u8、ts\r事实上，关于 ts 文件，其正式的格式名称为 MPEG-TS，视频编码采用了 H.264 格式，音频编码则采用了 AAC、MP3、AC-3 或者 EC-3 格式，这意味着它同样可以支持纯音频格式，即 MPEG 格式的基本音频文件。整体而言，HLS 和上文中提到过的 MPEG-DASH 非常相似，均采用了这种 “切片播放” 的思想，由于每个切片文件都非常小，因此，它在客户端产生的缓存更小、起播更快、响应更快。HLS 最初主要在苹果的 iOS 中流行，后来，逐渐获得安卓平台系统层面的支持，整体来看，在移动端的兼容性相当不错。\n曾几何时，乔帮主炮轰 Flash，主要的观点是：Flash 在移动设备上性能差影响电池续航；Flash 是为个人电脑和鼠标设计的，并不适用于触屏和手指。多年以后，回头来再看这段，当年乔布斯的确做了一个正确的决定，越来越多的原生应用被 Web 技术代替，React Native、Flutter、Electron、Taro\u0026hellip;等技术的流行，大抵都在证明 Web/HTML5 是大势所趋、是不可违逆的历史潮流。如今，当我们用前端技术去构建 App 和小程序时，这些曾经游走在原生应用中的历史遗留问题，终于要再次浮出水面。\nflv.js - Github\rgithub.com/Bilibili/flv.js/\r幸运的是，我们生活在一个技术高度发展的年代，作为 Flash 时代的遗产，浏览器本身是不支持 FLV 格式的，直到盛行二次元文化的 B 站开源了 flv.js，这让在前端播放 FLV 格式的视频成为可能，我们继续使用前面 RTMP 直播的示例来演示这部分功能。首先，我们准备一个简单的 HTML 结构，同时在页面中引入 flv.js ：\n\u0026lt;script src=\u0026#34;https://cdn.bootcdn.net/ajax/libs/flv.js/1.5.0/flv.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;video id=\u0026#34;videoElement\u0026#34; controls autoplay width=\u0026#34;100%\u0026#34; height=\u0026#34;100%\u0026#34;\u0026gt;\u0026lt;/video\u0026gt; 感谢 flv.js，我们只需要下面几行代码就可以播放一个来自 RTMP 视频流：\n\u0026lt;script\u0026gt; if (flvjs.isSupported()) { var videoElement = document.getElementById(\u0026#39;videoElement\u0026#39;); var flvPlayer = flvjs.createPlayer({ type: \u0026#39;flv\u0026#39;, isLive:true, url: \u0026#39;http://127.0.0.1:50001/pullLive?port=1935\u0026amp;app=pushLive\u0026amp;stream=test\u0026#39; }); flvPlayer.attachMediaElement(videoElement); flvPlayer.load(); flvPlayer.play(); } \u0026lt;/script\u0026gt; 视频流在网页中的播放效果，如下图所示，经过博主测试，延迟大概在 15 秒左右：\n通过 flv.js 播放 RTMP 视频流\r类似地，对于 HLS 协议的视频流，我们可以使用 hls.js 从前端承载一个实时的视频流:\n\u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/hls.js@latest\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;video id=\u0026#34;video\u0026#34;\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;script\u0026gt; if (Hls.isSupported()) { var video = document.getElementById(\u0026#39;video\u0026#39;); hls.attachMedia(video); hls.on(Hls.Events.MEDIA_ATTACHED, function () { hls.loadSource(\u0026#39;http://127.0.0.1:50002/hls/test.m3u8\u0026#39;); hls.on(Hls.Events.MANIFEST_PARSED, function (event, data) { console.log(\u0026#39;manifest loaded, found \u0026#39; + data.levels.length + \u0026#39; quality level\u0026#39;); }); }); } \u0026lt;/script\u0026gt; 此时，视频流在网页中的播放效果，如下图所示，测试发现，延迟大概在 15 秒左右：\n通过 hls.js 播放 HLS 视频流\r理论上，HLS 比 FLV 的延迟要高一点，从目前这个结果来看，两者似乎平分秋色，所以，这道题该怎么选呢？我的想法是：在 PC 端选 FLV，在移动端选 HLS。因为国内主流的视频网站，基本上都在用 FLV。诚然，这有些人云亦云，可工程上的解决方案，有哪一个不是一点点摸索出来的，最重要的是适合自己！\nWebRTC 的期冀 行文至此，关于流媒体的前世今生，其实已经基本厘清，甚至我们还能从尘埃往事中读出历史惊人的相似性，正如苹果公司开发 HLS 协议是为了挣脱 Flash 的摆布，可即便有乔布斯那样极具远见的战略眼光，依然阻挡不了 HTML5 那如潮水一般涌来的 “大势所趋”。也许，后人有一天会像嫌弃 Flash 一样嫌弃 HLS。技术的进步，始终是一件掺杂着期许和遗憾的事情，甚至于连同此时此刻都永远不是终点。所以，接下来我想聊一聊实时音/视频通信的未来——WebRTC。\nSRS\rSimple Realtime Server\r回想过去三年的疫情，催生出大量网课/在线教育、视频会议、远程办公……等方面的需求，而 WebRTC 这样一个支持实时音/视频通信的开放标准，天然地拟合着这些业务方向，如前文所述，虽然利用 Nginx 搭建一个流媒体服务器并不算太难，可如果想要在当下的网络环境中做到低延迟，还是需要投入大量的精力去做进一步的优化。关于 WebRTC 的细节，大家可以自行检索，这里推荐一个更成熟的流媒体服务器 SRS，它可以支持本文中提及的所有协议，最最最重要的是它支持 WebRTC 协议。所以，下面我们利用 SRS 来快速搭建一个 WebRTC 的示例，可以让大家提前感受一下 WebRTC 的魅力：\ndocker run --rm --env CANDIDATE=\u0026lt;Your_IP\u0026gt; -p 1935:1935 -p 8080:8080 -p 1985:1985 -p 8000:8000/udp registry.cn-hangzhou.aliyuncs.com/ossrs/srs:4 objs/srs -c conf/rtmp2rtc.conf 首先，参照官方文档，我们将运行一个支持从 RTMP 转换到 WebRTC 的 SRS 容器实例。顾名思义，它可以将接收到的 RTMP 视频流，以 WebRTC 的形式发布出来。此时，我们只需要在浏览器中建立 WebRTC 连接即可。\ndocker run --rm -it registry.cn-hangzhou.aliyuncs.com/ossrs/srs:encoder ffmpeg -stream_loop -1 -re -i doc/source.flv -c copy -f flv rtmp://host.docker.internal/live/livestream 接下来，我们同样参照官方示例，运行一个 FFmpeg 的容器示例，它负责将内置的一个 FLV 格式的视频文件推送到 RTMP 服务器上。这里的 RTMP 服务器，其实就是 SRS 容器实例。可以注意到，FFmpeg 确实指向了一个 rtmp:// 开头的地址。当然，在掌握了这些原理以后，你可以继续使用 OBS 推送视频流。对程序员来说，不会/记不住命令行这件小事，实在是无伤大雅。因为在真理面前，一切不过都是工具罢了，甚至连你我都是，不是吗？\n通过 SRS 实现 RTMP 转 WebRTC 效果演示\r此时，我们打开浏览器，就可以在 WebRTC 播放器中看到对应的视频。因为主流的浏览器基本都支持 WebRTC ，所以，你不需要再像以前一样借助第三方库或者插件，就可以直接播放视频。从实际的效果来看，除了画质有点模糊以外，整体表现要比 FLV 和 HLS 出色。博主认为，前端值得探索的方向还有更多，譬如 WebAssembly、WebGL、IoT、跨平台/跨端技术等等，如果始终局限在写页面，则永远是没有出路的。最近这几年 “前端已死” 的声音不绝于耳，不知道屏幕前的诸位作何感想呢？\n写文章写到凌晨的时候，我的疲倦早已所剩无几，起初选择这个方向的时候，我更多的是关注前端、关注我需要解决的问题。可等我梳理清这些知识脉络以后，我发现它的知识点远比我想象中的密集、庞杂。因此，整篇博客基本上就是在查资料和做实验交替进行的情况下写出来的。本文先后介绍了 HTML5 中的视频播放技术、RTMP/RTSP这类传统的视频流协议、HTTP-FLV/HLS这类基于 HTTP 的自适应协议，以及属于未来的新技术 WebRTC。在这个过程中，博主提到了 Flash 的衰落、HTML5 的崛起，即便你对于技术话题百分百无感，这些互联网世界里的奇闻佚事，或许还有机会成为一种怀念。而对于我说，如何在专业和活泼两者间完成取舍，同样是一件值得思考的事情，就像视频播放总要在带宽、画质、延迟、流量等因素上综合权衡一样。\n参考文章 实时传输 Web 音频与视频 秒懂流媒体协议 RTMP 与 RTSP 流媒体协议 HLS SRS - 介绍 有了 WebRTC ，直播可以这样玩！ 用 WebRTC 和 Node.js 开发实时视频聊天应用 ","date":"2023-07-15T13:32:47Z","image":"/posts/overview-of-front-end-video-playback-technology/pexels-terje-sollie-320617.jpg","permalink":"https://qinyuanpei.github.io/posts/overview-of-front-end-video-playback-technology/","slug":"Overview-Of-Front-End-Video-Playback-Technology","tags":["流媒体","RTMP","FLV","HLS"],"title":"前端视频播放技术概览"},{"categories":["生活感悟"],"content":"\r豆瓣上的观影记录映入眼帘的那一刻，我突然意识到，看 《疾速追杀4》 居然是五月份的事情啦，而六月份到目前为止仅仅看过《千寻小姐》 这部电影，甚至今年连去电影院的次数都屈指可数，我想，这大概能证明我此刻忙碌与困顿交织在一起的心境。在某一趟下班回家的地铁上，在被各种电子设备围剿的某时某刻，我果然还是决定要再写点什么。从某种意义上来讲，工作日的白天本不属于自己，而现在，甚至连夜晚都不属于自己，那种彻底地放空大脑、任由思绪汪洋恣肆的境界，不知不觉间竟变成了一件奢侈品。或许，这一切需要经过一场雨水的冲刷方能如愿，当我独自游走在雨声淅淅的街头时，我不由得地问自己，我该用什么样的语言来描摹这一刻，正如薛定谔的那只处于死猫和活猫叠加状态的猫一样，当一种情感难以言说的时候，我还是习惯将它写下来。\n其实，一直不太明白有村架纯这个“村花”称号的来由，相比之下，莫妮卡·贝鲁奇之于“球花”、陈坤之于“厂花”，总算是有迹可循。到今天为止，我只看过三部有村架纯主演的电影，第一部是《浪客剑心》 真人版里的雪代巴，第二部是《花束般的恋爱》的八谷娟，而第三部显然是《千寻小姐》里的古泽绫。有人说，演员大体上分为两类，一类是千人千面，演什么像什么，一类是千人一面，演什么都像自己。看《花束般的恋爱》的时候，配合着编剧坂元裕二那刻意的、日记式的独白，你总能从村花那张清纯可爱的“大脸”，抑或者是那双亮晶晶的眼睛里，读出某种糖分超标的“甜美”来。\n《花束般的恋爱》里的八谷娟\r也许，你会觉得她就像雅人叔吐槽过的“晨间剧女主”一样，永远只会演这种“恋爱脑晚期”的美少女，但我想说的是，从《浪客剑心》里清冷胜雪的雪代巴，再到《千寻小姐》里若即若离的“千寻”，她的表演其实是完成了从外放到内敛的转换，如果说，从前的她是站在舞台中心、聚光灯下的主角，现在的她则是远离人群、貌合神离的配角。单论豆瓣评分的话，观众大概更喜欢《花束般的恋爱》，可对我而言，这部电影更像是她寻求演技突破的一部作品，即使她此前已经凭借《花束般的恋爱》拿了最佳女主角的奖项。\n《浪客剑心》里的雪代巴\r单论《千寻小姐》这个名字的话，似乎更容易联想到宫崎骏老爷子的经典作品《千与千寻》。在这部电影中，有一个非常隐晦的细节时，千寻在和汤婆婆签订契约时使用的是“获野千寻”而非“荻野千寻”，而这其实是千寻一家能回到现实世界的重要原因。同样的，在电影《千寻小姐》里，千寻这个名字同样是一个假名，女主本来的名字叫做古泽绫，此前是风俗店的小姐，在辞职后来到一座小岛上，化名千寻在便利店上班。可奇怪的是，千寻从来不对这段经历有所掩饰，甚至能坦然地接受顾客的“调侃”，嘻嘻一笑，“谢谢夸奖”。久而久之，千寻居然变成了这座小岛上的“名人”。\n《千寻小姐》剧照：名人的名场面\r可偏偏就是这样一个“名人”，她喜欢跪坐在地上和流浪猫聊天，她喜欢独自一人荡秋千、去海边散步，甚至是送便当给流浪老人、陪老人吃饭、帮老人洗澡、送老人衣服。有时候，她会陪高中生冈地、小学生小诚一起玩，在她的影响下，冈地认识了来自“同一颗星球”的朋友，而小诚则学会了如何向妈妈表达爱意……在一切事物都向着好的方向发展、当被千寻“治愈”的人们围坐在一起其乐融融的时候，随着镜头的推移旋转，千寻却不知在何时悄然离去，也许，有人知道呢？可谁又知道呢？\n《千寻小姐》剧照：我能在你旁边站一会儿吗？\r在女高中生冈地的眼里，千寻是整座岛上最酷的人，她会远远地拍下千寻荡秋千时的画面，会在千寻独自站在海边的时候，默默现在她的身边，可没人知道，千寻其实是整座岛上最孤独的人。某一天，当千寻发现流浪老人死在某个角落里，她默默地挖坑掩埋老人的尸体，一如后来她掩埋那只死在码头上的海鸟，即使是后来接到弟弟的电话，被告知母亲去世的消息时，她依然像往常一样平静地说，自己不会去参加葬礼。原来，千寻一直坚信，“我们都是住在人类身体里，来自不同星球的外星人，而来自同一个星球的人，总有一天会遇见他们”，而这，居然是千寻的一位客人曾经告诉她的。\n《千寻小姐》剧照：人类的悲欢并不相通\r正如鲁迅先生所言，“人类的悲观并不相通”，人类始终无法理解彼此，即使是家人、爱人、朋友。千寻深知，“你无法独占一个人的心”，而她也不希望有人这样对待她，所以，即使是面对母亲的离世，她依然表现得没有悲伤、没有遗憾、没有孤独。虽然从整体上看起来，这是一个边缘人物治愈主流人物的故事，甚至主角身上还有一点回避型依恋和边缘人格的特征，可有句话是这样说的，“一个被别人温柔对待过的人，更懂得什么是温柔”，千寻就像一个全身都散发着炽烈光芒的天使，独自温暖着身边的每一个人，可她更像一个幽灵般的孤独过客，随风漂泊，不知所终\u0026hellip;\n《千寻小姐》剧照：今天不拍照吗？\r近些年来，原生家庭这个词汇被频频提起，而从千寻对母亲的态度来看，大概率没有得到父母的关爱和呵护。电影中穿插了一段女主童年时的回忆，彼时的女主曾经遇到过一个名为千寻的风俗女，但对方并没有以一种成年人的视角同女主相处，而是以一种平视的目光将女主视为一个独立个体，更重要的是，她没有因为自身风俗女的身份妄自菲薄，始终用她的善良和耐心温暖着年幼的女主。两人告别时，对方留下了自己的名片，这便是女主千寻这个名字的由来。后来，从风俗行业辞职的女主，在一个雨夜认识了前便利店店员多惠阿姨，在得知女主的经历后她并没有表现出特别的诧异，甚至主动提出为女主的便当“加餐”，直到后来多惠阿姨因为眼疾而不得不住院，女主决定接替多惠阿姨在便利店的工作。\n《千寻小姐》剧照：我超想复刻的炒面\r如果说，在《千与千寻》所描绘的那个神隐世界中，失去原来的名字就意味着再无法回到原来的世界，那么，在《千寻小姐》这部电影里，千寻这个名字就变成了某种符号。对女主而言，不管是童年时期遇到的真·千寻——风俗女千寻，还是雨夜中遇到的“千寻小姐”——便利店店员多惠阿姨，或许都是真正的“千寻小姐”，因为她们都丝毫不吝于向陌生人施以温柔，甚至连女主自己最后都变成了“千寻小姐”，她去不同的地方、换不同的工作、体验不同的生活，即便有时候感觉自己会像金鱼一样沉入水底，她始终都能忠于自我、自由而热烈地活着。在电影结尾，女主出现在一家农场，在给奶牛投喂饲料的时候，你说，有没有一种可能，她在用自己的方式饲养着那份遗世独立、不可方物的孤独呢？\n《千寻小姐》剧照：死去的海鸟\r正如多惠阿姨说得那样，“你这个人啊，不管走到哪里，始终都有一种孤独感”，所以，当多惠阿姨发现女主悄悄离开以后，她立刻打电话给女主，劝她留下来，“你不用这样一直漂泊下去的”，可女主就像一叶孤舟顺水漂流，遇合尽兴，枯荣随缘，她可以和身边所有人相处融洽，却似乎总是与这个世界显得格格不入。也许，你觉得这样子显得太过孤独，可孤独才是这个世界最真实的体验，它几乎无处不在，并非只有你我才有，是一种普遍而深刻的人类经验，从刀耕火种的远古时期到如今高楼耸立的科技时代，划破黑夜的那束火光或许在不断地变换着形式，可人类的孤独自始至终从未消失过。曾经有学生提问姜文导演，“您的电影人物为什么到最后总是一个人？为什么要把人放在这么孤独的处境？”，一个熟悉的声音回应道，“你以为你最后不是一个人吗？”，所谓孤独，大抵如此罢！\n","date":"2023-06-10T12:30:47Z","image":"/posts/call-me-chihiro/P2888413690.png","permalink":"https://qinyuanpei.github.io/posts/call-me-chihiro/","slug":"Call-Me-Chihiro","tags":["千寻小姐","有村架纯","影评","随笔"],"title":"你好，千寻小姐"},{"categories":["编程语言"],"content":"多年前，我曾写过一篇关于 Python 动态导入的文章，当时想要解决的问题是，如何通过动态导入 Python 脚本来实现插件机制，即整个应用程序由主程序和插件两部分组成，主程序通过 importlib 模块中的 import_module 方法动态地导入一个 Python 脚本，最终通过 getattr、setattr 等方法实现反射调用。时过境迁，代码还是那些代码，江湖故人早已不知所踪。我向来都是一个喜欢怀旧的人，我怀念的是那些遗忘在寒江孤影里的江湖故人，我怀念的是那些湮灭在时光尘埃里的代码片段。或许，在屏幕前的你看来，一个每天都在经历着“更新换代”的技术人员，更应该对这一切的消逝习以为常。可正如这世界上的风、沙、星辰等流动的事物一样，无论我们愿意与否，时间总会在不经意间将那些熟悉而珍贵的东西一一带走，不放弃对过去的回忆和珍视，这便是我在世事变幻的洪流中追求的安宁与平静。正所谓“温故而知新”，今天我想要怀旧的话题是 Python 里的动态导入。\n众所周知，这段时间我一直在开发基于 ChatGPT 的人工智能管家 Jarvis，在整个探索过程中，类似语音识别、语音合成这些领域，博主先后考察了微软、百度、腾讯\u0026hellip;这些大厂的方案，这可以说是非常符合我作为 Python “调包侠” 的人设啦！以语音识别为例，最终，你可能会得到类似下面这样的代码：\nclass ASREngineFactory: @staticmethod def create(config, type): if type == ASREngineProvider.Baidu: return BaiduASR(config[\u0026#39;BAIDU_APP_ID\u0026#39;], config[\u0026#39;BAIDU_API_KEY\u0026#39;], config[\u0026#39;BAIDU_SECRET_KEY\u0026#39;]) elif type == ASREngineProvider.PaddleSpeech: return PaddleSpeechASR() elif type == ASREngineProvider.OpenAIWhisper: return WhisperASR() 没错，非常经典的简单工厂模式，你只需要告诉工厂类，你需要使用哪种语音识别引擎，它就可以自动帮你创建出对应的示例，如下图所示，这看起来非常合理，对吧？\nconfig = load_config_from_env(env_file) engine = ASREngineFactory.create(config, ASREngineProvider.PaddleSpeech) 这里，其实有一段小插曲，博主最近开始尝试使用 virtualenv 来管理不同的 Python 版本，这样做的好处是，我只需要在不同的工作场所拉取代码、激活环境，就可以享受到完全一样的开发环境。当然，这一切都只是理论上的，实际使用下来的感受是，它并不能完全抹平环境上的差异。譬如，当我试图在个人电脑上安装 PaddleSpeech 和 Rasa 这两个库时，依然免不了遇到各种错误，即使是在同一个 Python 环境下。\n此时，你会发现一个非常尴尬的问题，即使我不使用 PaddleSpeech 来作为 Jarvis 的语音识别引擎，它依然无法正常工作，原因是我环境中没有安装 PaddleSpeech，我不得不注释掉项目中所有和 PaddleSpeech 有关的代码，而这一切的根源其实是，我们在代码中使用了静态导入的方式，如下图所示：\n# baidu-aip from aip import AipSpeech # paddlespeech from paddlespeech.cli.asr.infer import ASRExecutor # openai-whisper import whisper 我相信，这个代码在通常情况下是没有任何问题的，可凡事都有例外，有没有一种可能，我们可以像使用 C# 里的 #if DEBUG、#if NET40 等预处理指令一样，让它按照不同的条件去导入不同的模块呢？比如，当我使用 Whisper 时，我希望它只导入 whisper 模块，而当我使用 PaddleSpeech 时，我希望它只导入 paddlespeech.cli.asr.infer 模块下的 ASRExecutor 类。换言之，我希望实现两个目的，其一是按需导入，只导入需要的模块。其二是延迟导入，使用的时候再导入。\n延时摄影技术纪录下的星空\r好了，既然一切问题的根源是静态导入，那么，我们的思路就是将其调整为动态导入，此时，我们需要祭出大杀器 importlib，这里以 baidu-aip 这个包为例：\nclass BaiduASR: def __init__(self, APP_ID, API_KEY, SECRET_KEY): aip = None try: aip = importlib.import_module(\u0026#39;aip\u0026#39;) except ImportError as e: print(\u0026#34;baidu-aip is required, run \u0026#39;pip install baidu-aip\u0026#39; first\u0026#34;) self.client = aip.AipSpeech(APP_ID, API_KEY, SECRET_KEY) self.recoginzer = sr.Recognizer() # ...... 可以注意到，主要的改动在第 5 行，因为 AipSpeech 是 aip 这个模块中的一个类，所以，我们可以在动态导入 aip 模块后直接使用该类型。当然，这个方案会损失一点点编程体验，因为 IDE 的智能提示可能会失效。考虑到使用者不一定安装了这个库，我们可以在异常处理中提醒对方安装这个库，这是我从开源社区学会的一个小技巧😀。当然，你还需要删除此前静态导入部分的代码片段:\nfrom aip import AipSpeech 现在，当我需要使用某一个语音识别引擎时，我只需要给 ASREngineFactory 传入一个类型，它将会在创建实例的时候动态导入对应的模块。这样，即使我没有安装 PaddleSpeech，它丝毫不影响我使用 baidu-aip 或者 openai-whisper 这两个库，这样听起来更合理一点，不是吗？\nimport sys plugins = [\u0026#39;plugin1\u0026#39;, \u0026#39;plugin2\u0026#39;, \u0026#39;plugin3\u0026#39;] for plugin in plugins: __import__(plugin) sys.modules[plugin].run() 除了这种方式以外，对于此前博主讨论过的插件化问题，我们还可以使用 __import__方式，它同样可以实现类似的效果。如图所示，假设我们有三个插件 plugin1、plugin2、plugin3，它们各自拥有一个叫做 run() 的方法，此时，我们可以通过 __import__ 这个内置的函数来动态地导入插件。按照 Python 模块的缓存机制，每个模块只会被导入一次。首先，它会去检查 sys.modules 中是否存在该模块，只有当该模块不存在的时候，它才会去检索和导入该模块。因此，我们可以从 sys.modules 调用该模块的 run() 方法。当然，这个方案最大的问题是，需要手动处理模块名称字符串，特别是当你为插件引入模块或者是包的概念的时候。\n","date":"2023-05-29T20:49:47Z","image":"/posts/discussing-dynamic-import-in-python-again/pedro-lopes-PcoZLIoLAOM-unsplash.jpg","permalink":"https://qinyuanpei.github.io/posts/discussing-dynamic-import-in-python-again/","slug":"Discussing-Dynamic-Import-In-Python-Again","tags":["Python","动态导入","奇技淫巧","importlib","温故知新"],"title":"温故而知新，再话 Python 动态导入"},{"categories":["编程语言"],"content":"在刘慈欣老师的《三体》小说中，整个故事是以杨冬的死亡线索展开的，而她自杀的原因是物理学不存在了。随着 GPT-4 的发布，『NLP已死』和『NLP不存在了』的声音开始不绝于耳。如果说杨冬认为物理学被颠覆源于智子的“欺骗”，那么，现在的大型语言模型对于 NLP 的冲击，实际上改变了AI与最终用户互动的方式。传统的 NLP 技术方向涵盖了信息抽取、文本挖掘、机器翻译、语音合成、语音识别、语义理解、句法分析，这些都被视为自然语言处理的中间任务。因此，传统的 NLP 模式是在每个领域中提供各种不同的工具。当需要对自然语言进行处理时，你不得不将这些不同的工具结合起来，就像博主曾经使用过的结巴分词、SnowNLP、词袋模型、情感分析、TF-IDF一样。然而，现在的大型语言模型更像是一个直接面对最终任务的“智者”，跳过了这些中间任务。因为我们最终的目的就是要让程序产生智能，并让人类相信它能够“理解”他们的意图。显然，在这一点上，ChatGPT 和 Midjourney 都做到了。作为非科班的程序员，我无法科学、客观地评判 LLM 和 NLP 的优劣。但是我想分享一下我在开发 “贾维斯” 过程中获得的一点心得，希望对大家有所启发。\nHey Jarvis 在将小爱同学接入 ChatGPT 以后，我开始思考怎么样在智能和功能上取得平衡，尽管 ChatGPT 提升了小爱同学在聊天方面的智能，可这同时破坏了当下智能音箱普遍使用的“指令集”，你无法运用 ChatGPT 的这种“聪慧”让它真正地帮你做点事情。所以，我大概率要再次发明“智能音箱”，可我想知道，有了 ChatGPT 的加持以后，它到底能达到什么样的智能水平？带着这样一个想法，我开始从头编写贾维斯，一个基于 ChatGPT 的人工智能管家，其灵感来源于钢铁侠的人工智能管家 Jarvis。目前，它可以查询日期/时间、检索信息、播放音乐、控制米家/电脑、打开应用、讲笑话、查询天气、编程。以下是我在 Bilibili 上发布的视频演示：\n你可能会好奇这些功能都是如何实现的？请允许我做一个简单说明，下面是“贾维斯”的整体设计思路：\n贾维斯整体设计思路说明\r从整体而言，整个程序并不算特别复杂，因为语音唤醒、语音识别、语音合成这些均已有非常成熟的方案。在接入 ChatGPT 以后，我开始尝试为它扩展更多的功能。这个时候，我了解到自然语言理解(NLU)的这个方向，并且它依然属于自然语言处理(NLP)这个范畴，更具体的关键词则是意图识别或者意图提取。以查询日期这个功能为例，我只需要在某个函数上添加“路由”，即可为贾维斯设计不同的“技能”：\n@trigger.route(keywords=[\u0026#39;查询日期\u0026#39;,\u0026#39;询问日期\u0026#39;,\u0026#39;日期查询\u0026#39;]) def report_date(input): now = datetime.datetime.now() week_list = [\u0026#39;星期一\u0026#39;,\u0026#39;星期二\u0026#39;,\u0026#39;星期三\u0026#39;,\u0026#39;星期四\u0026#39;,\u0026#39;星期五\u0026#39;,\u0026#39;星期六\u0026#39;,\u0026#39;星期日\u0026#39;] formated = now.strftime(\u0026#39;%Y年%m月%d日\u0026#39;) week_day = week_list[now.weekday()] history = today_on_history() if history != None: return f\u0026#39;今天是{formated}，{week_day}。{history}。\u0026#39; else: return f\u0026#39;今天是{formated}，{week_day}。\u0026#39; 此时，问题就被转化为如何识别或者提取一句话中的真实意图。坦白地讲，面对人类这种口是心非的动物，想要了解其真实意图是非常困难的。譬如，人类都希望别人能够“懂我”，可没有人会喜欢被别人一眼看穿。因此，我们这里讨论的意图，特指那些动宾结构的指令型用语，例如：打开客厅的灯、查询明天的天气等等。不论屏幕前的你对 AI 持何种态度，我想说的是，AI 已然参与到我日常的编程和写作中，这正是我开发贾维斯的动力所在，我希望它能参与到更多的事项中去，甚至想让它取代家里的小爱同学。\n基于 ChatGPT 实现意图提取 对笔者而言，在接触到 ChatGPT 以后，编程语言就仿佛进入了一个新的时代——魔法时代。因为无论你需要大语言模型帮你做什么，你要做的第一件事情就是编写 Prompt。而关于这件事情本身，则像极了魔法师在施法前吟唱咒语。所以，你说未来会出现那种被称之为“魔法吟唱师”的职业吗？当然，更有意思的点在于，如果说目前的编程语言都是过程式的，那么，在进入魔法时代以后，这一切就会变成描述式的。关于意图提取这个话题，你打算怎么样向 AI 解释它呢？下面开始我的尝试：\n我想让你扮演我的自然语言处理工具，当我告诉你一句话的时候，你可以对它进行分词、句法分析、词性分析、上下文分析、主题建模/抽取，你可以自由地使用结巴分词、nltk 或者是 SnowNLP 这些工具，你需要猜测我这句话中的意图，并用下面的形式表示出来：\n{ \u0026#34;query\u0026#34;: \u0026#34;查询许嵩的资料\u0026#34;, \u0026#34;intent\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;查询\u0026#34;, \u0026#34;confidence\u0026#34;: 0.95 }, \u0026#34;entities\u0026#34;: [ { \u0026#34;entity\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;许嵩\u0026#34;, \u0026#34;start\u0026#34;: 2, \u0026#34;end\u0026#34;: 4, \u0026#34;confidence\u0026#34;: 0.98 }] } 其中，query 字段表示原始查询文本，intent 字段表示查询意图，entities 字段表示本次查询中提取的实体信息。在这个示例中，意图为查询，置信度为0.95，实体为人名，值为“许嵩”，起始位置为2，结束位置为4，置信度为0.98。你只需要返回给我这样一段 JSON，不需要任何冗余的信息。需要注意的是，你要对相似的意图进行归类和合并，使用一个统一的意图进行描述。我的问题是：\n如你所见，这是一段非常复杂的提示词，采用了经典的 Markdown 格式，借此告诉 ChatGPT 它需要做什么样的事情、以及期望它返回的数据格式。通常情况下，我们可以使用 requests 库来调用 ChatGPT 的 API 接口：\n# 一个小技巧，使用三个单引号来处理多行文本 prompt = \u0026#39;\u0026#39;\u0026#39;在这里填入你的提示词\u0026#39;\u0026#39;\u0026#39; # 定义请求头 headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer \u0026lt;Your OpenAI API KEY\u0026gt;\u0026#34;, } # 定义请求体 query = \u0026#39;播放许嵩的断桥残雪\u0026#39; query = prompt + query payload = { \u0026#34;model\u0026#34;: \u0026#34;gpt-3.5-turbo\u0026#34;, \u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: query}] } # 调用 ChatGPT API r = requests.post(\u0026#39;https://api.openai.com/v1/completions\u0026#39;, headers=headers, json=data) data = r.json() content = data.get(\u0026#34;choices\u0026#34;)[0][\u0026#34;message\u0026#34;][\u0026#34;content\u0026#34;] # 由于返回的是 JSON，需要再做一次反序列化 result = json.loads(content) 在将我的指令和提示词一起发送给 ChatGPT 后，我实际上获得了一个具备意图提取能力的 API 接口。虽然我没有使用传统 NLP 工具或技术，但我确实实现了类似的功能。我测试过 ChatGPT 的英文翻译效果，发现它的翻译效果甚至比主流的翻译工具要好很多。因此，无论传统 NLP 如何努力，似乎都难以与之匹敌，这大抵解释了为什么人们会发出“NLP已死”的声音。你还记得刘慈欣老师书中的“降维打击”这一概念吗？传统 NLP 一直致力于优化模型和算法，即使 GPT 使用的技术并不算先进。然而，在海量数据和大规模算力面前，GPT 最终呈现出来的效果比这些方案要好得多，这确实是一种奇迹。以下是博主通过 ChatGPT 提取出来的意图：\n通过 ChatGPT 提取意图\r那么，这个方案是否就完美无缺呢？答案显然是否定的，一个最为明显的问题是，ChatGPT 对意图的分类非常随意，例如，查询日期和日期查询，是同一种含义的不同表达，属于近义词，可是 ChatGPT 并不会主动合并这些相似的结果，即使你在提示词中已经提及了这一点。那么，该如何优化这个问题呢？一种可行的方案是在提示词中告诉它你期望的意图分类，如信息查询、天气查询、设备控制\u0026hellip;等等。除此之外，这段提示词的长度大概为1000个字符，大概会消耗 100 至 200 个左右的 Token，虽然支持多模态的 GPT-4 变得更强大了，可它的价格是 GPT-3.5 的 15 到 60 倍左右，因此，从经济成本考虑，这个基于 ChatGPT 的意图提取方案，在未来产生的费用可能会越来越高。当然，如果你能训练出比 GPT-3.5 更强大的大语言模型，这个问题就不再是一个问题啦！\n基于 Rasa NLU 实现意图提取 也许，你在视频注意到了，当我向贾维斯下达控制智能家居的指令时，有时候它并不能正确地理解我地意图。于是，我开始尝试回归到传统 NLP 的思路。在这个过程中，我了解到了 Rasa NLU 这个项目，这是一个致力于构建 AI 助手的开源框架，你可以提供它提供的各种能力训练出一个聊天机器人。当然，我最感兴趣的还是它提供的自然语言理解(NLU)能力，我打算借助于这一框架，训练出一个可以离线使用的意图提取器。因为只有这样，你才能深刻地理解 LLM 和 NLP 的区别在哪里。参照官方教程，在正式开始训练前，你至少应该做好以下准备：\n一个 Python 3.9 的环境，建议使用 virtualenv 来管理你的虚拟环境 安装 CMake 环境，建议将其添加至操作系统环境变量中 下载中文词向量模型文件 total_word_feature_extractor_zh.dat，提取码：g54u 接下来，安装必要的依赖项。其中，MITIE 建议通过源代码来安装：\npython -m pip install rasa jieba scikit-learn # 从源代码安装 MITIE，直接安装失败率非常高 python -m pip install git+https://github.com/mit-nlp/MITIE.git Rasa 的训练主要分为 Core 和 NLU 两个部分。其中，Core 这部分主要用于聊天机器人的训练，换句话说，只有当你需要实现一个功能完备的聊天机器人的时候，你才需要针对这部分进行训练。考虑到，我们这里仅仅是为了使用 Rasa 提供的 NLU 能力。因此，我们只需要训练 NLU 这部分即可。下面是笔者参照官方文档编写的训练数据， 在 Rasa 1.0 以及 Rasa 2.0 版本中，通常使用 JSON 和 Markdown 格式来编写训练数据，而在 Rasa 3.0 中，我们有了新的选择——YAML，下面是一个简单的示例：\nversion: \u0026#34;3.1\u0026#34; nlu: - intent: greet examples: | - 你好 - 哈喽 - 嗨 - 嘿 - 早上好 - 晚上好 - 早安 - 晚安 - intent: query_weather examples: | - [今天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}的天气怎么样 - [明天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}的天气怎么样 - [后天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}的天气怎么样 - [今天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}下雨吗 - [明天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}下雨吗 - [后天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}下雨吗 - [今天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}的气温是多少 - [明天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}的气温是多少 - [后天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}的气温是多少 - 帮我查询一下[今天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}的天气 - 帮我查询一下[明天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}的天气 - 帮我查询一下[后天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}的天气 - intent: query_time examples: | - 几点了 - 现在几点 - 告诉我[时间]{\u0026#34;entity\u0026#34;: \u0026#34;time\u0026#34;} - 查询[时间]{\u0026#34;entity\u0026#34;: \u0026#34;time\u0026#34;} - intent: query_date examples: | - [今天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}几号 - [明天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}几号 - [后天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}几号 - [今天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}周几 - [明天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}周几 - [后天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}周几 - [今天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}星期几 - [明天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}星期几 - [后天]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;}星期几 - 查询[日期]{\u0026#34;entity\u0026#34;: \u0026#34;date\u0026#34;} - intent: search_info examples: | - 搜索[周杰伦]{\u0026#34;entity\u0026#34;: \u0026#34;name\u0026#34;}的信息 - 查询[刺客信条]{\u0026#34;entity\u0026#34;: \u0026#34;name\u0026#34;}的资料 - 检索[许嵩]{\u0026#34;entity\u0026#34;: \u0026#34;name\u0026#34;}的情况 完整的训练数据可以在 这里 找到，你可以注意到，在训练数据中博主手动标注出了一部分实体，你必须确保这些词语能被分词组件正确地切分。这一点如何保证呢？事实上，Rasa 采用管道的形式来组织各种各样的组件，例如，在 Rasa 中你可以使用 JiebaTokenizer 或者 MitieTokenizer 对训练数据进行分词。编写 Rasa 配置文件，本质上就是决定选择哪些组件来处理你的训练数据。为了方便大家理解，我做了一个简单的表格供大家参考：\n分类 组件 描述 NLP MitieNLP、SpacyNLP 在管道中加载一个预训练的词向量模型 Tokenizers WhitespaceTokenizer、JiebaTokenizer、MitieTokenizer、SpacyTokenizer 在管道中使用分词组件，对训练数据进行分词 Featurizers MitieFeaturizer、SpacyFeaturizer、ConveRTFeaturizer、LanguageModelFeaturizer、RegexFeaturizer、CountVectorsFeaturizer、LexicalSyntacticFeaturizer 在管道中使用特征化组件，对文本特征进行分类 Intent Classifiers MitieIntentClassifier、LogisticRegressionClassifier、SklearnIntentClassifier、KeywordIntentClassifier、DIETClassifier、FallbackClassifier 在管道中使用意图分类器，识别输入内容的意图 Entity Extractors MitieEntityExtractor、SpacyEntityExtractor、SpacyEntityExtractor、DucklingEntityExtractor、RegexEntityExtractor、EntitySynonymMapper 在管道中使用实体提取器，提取输入内容中的实体 经过反反复复的训练和测试，博主找到了下面这套最合适的配置文件：\nlanguage: \u0026#34;zh\u0026#34; assistant_id: JarvisBot pipeline: - name: \u0026#34;MitieNLP\u0026#34; model: \u0026#34;data/total_word_feature_extractor_zh.dat\u0026#34; - name: JiebaTokenizer dictionary_path: \u0026#34;data/jieba/\u0026#34; \u0026#34;intent_tokenization_flag\u0026#34;: false \u0026#34;intent_split_symbol\u0026#34;: \u0026#34;_\u0026#34; \u0026#34;token_pattern\u0026#34;: None - name: \u0026#34;CountVectorsFeaturizer\u0026#34; \u0026#34;analyzer\u0026#34;: \u0026#34;word\u0026#34; \u0026#34;min_ngram\u0026#34;: 1 \u0026#34;max_ngram\u0026#34;: 1 \u0026#34;OOV_token\u0026#34;: \u0026#34;_oov_\u0026#34; \u0026#34;use_shared_vocab\u0026#34;: false - name: LexicalSyntacticFeaturizer \u0026#34;features\u0026#34;: [[\u0026#34;low\u0026#34;, \u0026#34;title\u0026#34;, \u0026#34;upper\u0026#34;], [\u0026#34;BOS\u0026#34;, \u0026#34;EOS\u0026#34;, \u0026#34;low\u0026#34;, \u0026#34;upper\u0026#34;, \u0026#34;title\u0026#34;, \u0026#34;digit\u0026#34;], [\u0026#34;low\u0026#34;, \u0026#34;title\u0026#34;, \u0026#34;upper\u0026#34;]] - name: LogisticRegressionClassifier max_iter: 100 solver: lbfgs tol: 0.0001 random_state: 42 ranking_length: 10 - name: \u0026#34;MitieIntentClassifier\u0026#34; - name: \u0026#34;MitieEntityExtractor\u0026#34; - name: \u0026#34;EntitySynonymMapper\u0026#34; 现在，我们只需要通过下面的脚本训练模型即可：\npython -m rasa train -c sample_configs/config_jieba_mitie_sklearn.yml --data data/examples/rasa/jarvis_rasa_zh.yml nlu 其中：-c 参数用于指定配置文件、--data参数用于指定训练数据。经博主测试，在普通的 CPU 环境下，完成一次训练大概需要 4 个小时，每次训练后它会在 models 目录生成一个模型文件，在不指定模型名称的情况下，这个模型文件的名称通常是随机生成。当然，你可以使用 --fixed-model-name参数使其固定下来。如下图所示，这是博主“炼丹”过程中保存的截图，希望屏幕前的、比我更懂机器学习的各位，帮我分析一下这个训练是否还有优化空间：\n堪比 “炼丹” 的训练过程\r当模型训练好以后，我们就可以开始测试啦！此时，我们需要执行下面的脚本：\npython -m rasa run --enable-api -m models\\nlu-20230515-183633-creative-bucket.tar.gz 该命令会在本地运行一个 NLU Server，端口默认为 5005，通常，你可以像下面这样访问它提供的 API 接口：\ncurl localhost:5005/model/parse -d \u0026#39;{\u0026#34;text\u0026#34;:\u0026#34;hello\u0026#34;}\u0026#39; 下面是博主实际测试的结果，个人对这个结果还是挺满意的，如果一定要说缺陷的话，首先，它会把 “王菲的传奇” 这句话切分为 “王菲” 和 “的传奇”，当然，这大概率是没有排除某些停用词的缘故；其次，它不能提取出我们期望的实体，在这个例子中，“孙燕姿”这个实体固然重要，可更重要的应该是“遇见”，这将是接下来优化的一个重点。\n测试 Rasa 提供的 NLU 接口\r当然！幸运的是，我们至少解决了当时 ChatGPT 没有解决的问题，因为这些意图的分类都是我们在训练数据中人为规定的，所以，它不会出现意料之外的意图分类，或者是相似的意图分类，就像本文一开始给出的代码示例一样，“查询日期”、“询问日期”、“日期查询” 本质上都是在描述同一件事情，可由于这些分类都是由 ChatGPT 产生的，所以，基于 ChatGPT 的这个方案多少是有点不确定性在里面的，而对于一个 API 来说，相同的输入应该得到相同的结果，从这一点上来看，或许是 Rasa 更胜一筹！\n结论 ChatGPT 发布以来，关于被 AI 替代以致于失业的焦虑声音不断涌现，即便是传统的 NLP 亦不能幸免。在开发贾维斯的过程中，博主需要解决意图识别的问题，本文分享了两种解决问题的思路，它们分别是以 ChatGPT 为代表的大语言模型、以 Rasa 为代表的传统 NLP。对于 “NLP 已死”、“NLP 不存在了”这样的观点，笔者的看法是，这实际代表了人工智能的两种方向：通用智能和专业智能。虽然像 ChatGPT 这样的通用型 LLM 表现不俗，但是对于 NLP 的一系列问题，永远依赖于调用一个外部模型注定不可行。在这种情况下，你不可能像 OpenAI 一样投入大量人力、物力去训练一个 LLM。此时，选择 LLaMa 或者 Alpaca 这样的“小”模型就不失为一种明智的选择。此外，现阶段 LLM 的微调、“投喂”数据依然需要 NLP 的知识。也许，正如杨冬所言，并不是物理学不存在了，而是我们认为的物理学不存在了。同样的，像传统 NLP 那样细分的、垂直的子任务会越来越少，而多模态/跨模态的场景和应用会越来越多。如果用一句 100% 正确的废话来描述就是，这是一个机遇与挑战并存的时代！\n","date":"2023-05-12T20:49:47Z","image":"/posts/in-the-post-gpt-era-nlp-no-longer-exists/n_125_76_177_10@20230515164443_80139.jpg","permalink":"https://qinyuanpei.github.io/posts/in-the-post-gpt-era-nlp-no-longer-exists/","slug":"In-The-Post-GPT-Era-NLP-No-Longer-Exists","tags":["GPT","NLP","Rasa","贾维斯"],"title":"后 GPT 时代，NLP 不存在了？"},{"categories":["编程语言"],"content":"最近我一直在优化一个人脸识别项目，这个过程令我深感科学的尽头永远都是殊途同归。一年前，我使用 dlib 实现人脸识别时遇到了两个悬而未决的问题：一是因为人脸样本数目增加导致性能下降问题；二是如何快速地判断目标人脸是否在人脸样本中。然而，在经过虹软人脸识别 SDK 的折磨后，我意识到这两个问题实际上从未消失。它们总会在某个合适的时机突然跳出来，然后开始无声无息地敲打你的灵魂。果然，“出来混还是要还的”。现在重新审视这两个问题，我认为，它们本质上是1：1 和 1：N 的问题。在使用虹软人脸识别 SDK 的过程中，我遇到了一个非常棘手的难题，即：当目标人脸在人脸数据库中时，识别过程非常流畅；可当目标人脸不在人脸数据库中时，识别过程就异常卡顿。结合使用 dlib 做人脸识别的经验，我猜测魁祸首可能是频繁的特征对比。相比于输出一个枯燥的结论，我更喜欢梳理解决问题的思路。因此，这篇博客的主题是，利用 Milvus 实现海量人脸快速检索的实现过程。\n从人脸识别到向量 故事应该从哪里讲起呢？我想，可以从人脸数据库这个角度来切入。当我们把人脸特征存储到 CSV 或者数据库中时，本质上是将 1:N 问题转化为 1:1 问题。因此，我们不得不遍历人脸数据库的每个样本，然后选取与目标人脸最相似或最匹配的那个。这意味着，人脸识别的效率将受到到样本数量和相似度/距离计算方法等因素的影响。以虹软人脸识别 SDK 为例，其免费版提供了 1:1 人脸特征对比的接口，付费版提供了 1:N 人脸特征对比的接口。当然，据热心网友透露，官方这个 1:N 其实还是通过 1:1 循环来实现的。可即便如此，在相同的时间复杂度下，想要写好这样一个循环，这件事情本身并不容易。所以，影响人脸识别效率的因素里，还应该考虑到人的因素。在这个硬件性能过剩的时代，“锱铢必较”大抵会成为一种难能可贵的品质。谁能想到，如今训练模型的门槛变成了一块显卡呢？\n通过 one-hot 编码实现的文本向量化表示示意图 如果我们从另一个角度思考这个问题，就会发现向量作为全新的数据类型，是所有这些问题的根源。无论是通过 CSV 还是关系型数据库进行数据处理，对向量数据进行过滤和筛选都是不可直接实现的。这迫使我们需要在内存中加载所有的人脸特征数据，再通过逐个计算和对比的方式来查找目标数据。当目标人脸在数据库中不存在时，这项工作就会变得困难和耗时。这实际上代表着数据从结构化到非结构化的转变趋势。例如，在 NLP 领域，计算文本相似度的理论依据就是向量的余弦公式。而在最近最火热的 ChatGPT 中，Embeddings 模型同样是基于文本的向量化表示。如果你有学习过机器学习的相关知识，就会更加深刻地认识到向量的重要性。正如刘慈欣在《三体》中所描述的那样，高维文明可以对低维文明实施降维打击。如果我们把向量看作是一种将高维度信息压缩为低维度信息的技术，那么，时下这场 AI 革命是不是可以同样视为降维打击呢？试想一下，那些如同咒语一般的提示词(Prompt)背后，不正是由无数个超出人类认知范围的多维向量在参与着复杂计算吗？\nMilvus 向量数据库 正如我们所看到的，AIGC 改变了我们对这个世界的编程方式，即从 DSL/GPL 逐步地转向自然语言。在 OpenAI 的 GPT4 以及百度的文心一言中，我们会注意到这些大语言模型(LLM)开始支持图片。也许，以后还会支持音频、视频、文件……等等不同的形式，而这其实就是我们经常听到“多模态”的概念。可以预见的是，未来会有更多的非结构化数据加入其中，传统的关系型数据库将不再适合 AI 时代。譬如，最为典型的“以图搜图”功能，传统的模糊查询已经无法满足复杂的匹配需求。从这个角度来说，向量数据库将会是未来 AI 应用不可或缺的基础设施，就像此刻的关系型数据库对于 CRUD 一样重要。目前，向量数据库主要有 Facebook 的 Faiss、Pinecone、Vespa、国内创业公司 Zilliz 的 Milvus，以及京东的 Varch 等等，笔者这里以 Milvus 为例来展示向量数据库的核心功能——相似度检索。\nMilvus 的安装与使用 作为一家国内公司出品的产品，Milvus 的文档写得非常详细，更重要的是它支持 Python，这是我选择 Milvus 的其中一个理由。在阅读文档的过程中，博主发现，官方非常贴心地准备了 docker-compose.yml 文件，我是无论如何都不能拒绝这份善意的，因此，我们直接通过 Docker 启动即可：\ndocker-compose up -d 接下来，我们通过一个简单的示例来演示 Milvus 的用法。首先，我们假设可以用一个三维向量 (a, b, c) 来表示一个人的身高、体重、年龄。此时，我们有如下图所示的人物信息：\nId Name Metrics 1 小明 (1.8, 75, 25) 2 小月 (1.75, 70, 24) 3 小王 (1.8, 80, 28) 4 小李 (1.78, 78, 30) 5 小张 (1.75, 70, 23) 6 小赵 (1.8, 76, 29) 好了，为了将这些数据存入 Milvus，我们需要创建一个对应的集合，它相当于关系型数据库中的一张表，这里定义三个字段 id、name 和 metrics，它们分别表示主键Id、人物名称和人物指标。其中，人物指标是一个三维向量：\n# pip install pymilvus==2.2.7 from pymilvus import connections, CollectionSchema, FieldSchema, DataType, Collection, utility # 连接数据库 connections.connect( alias=\u0026#34;default\u0026#34;, user=\u0026#39;minioadmin\u0026#39;, password=\u0026#39;minioadmin\u0026#39;, host=\u0026#39;localhost\u0026#39;, port=\u0026#39;19530\u0026#39; ) # 创建集合 people_id = FieldSchema( name=\u0026#39;id\u0026#39;, dtype=DataType.INT64, is_primary=True ) people_name = FieldSchema( name=\u0026#39;name\u0026#39;, dtype=DataType.VARCHAR, max_length=200 ) people_metrics = FieldSchema( name=\u0026#39;metrics\u0026#39;, dtype=DataType.FLOAT_VECTOR, dim=3 ) schema = CollectionSchema(fields=[people_id, people_name, people_metrics]) collection = Collection(name=\u0026#39;people\u0026#39;, schema=schema, using=\u0026#39;default\u0026#39;, shards_num=2) 接下来，我们将这几个人物的信息写入 Milvus，需要注意的是，Python 中的向量其实使用数组来表示的，并且在组织写入数据的时候，我们应该按照列的方式来整理这些数据：\n# 插入数据 data = [ [1, 2, 3, 4, 5, 6], [\u0026#39;小明\u0026#39;,\u0026#39;小月\u0026#39;,\u0026#39;小王\u0026#39;,\u0026#39;小李\u0026#39;,\u0026#39;小张\u0026#39;,\u0026#39;小赵\u0026#39;], [[1.8, 75, 25],[1.75, 70, 24],[1.8, 80, 28],[1.78, 78, 30],[1.75, 70, 23],[1.8, 76, 29]] ] mr = collection.insert(data) collection.flush() 那么，现在数据已经存储到 Milvus 中，我们该如何进行相似度检索呢？为此，我们需要创建一个对应的索引：\n# 创建索引 index_params = { \u0026#34;metric_type\u0026#34;:\u0026#34;L2\u0026#34;, # L2:欧式距离, IP:向量内积 \u0026#34;index_type\u0026#34;:\u0026#34;FLAT\u0026#34;, \u0026#34;params\u0026#34;:{ } } collection.create_index( field_name=\u0026#39;metrics\u0026#39;, index_params=index_params ) 接下来，我们来编写查询条件，你可以选择使用欧式距离(L2)或者向量的点积(IP)来表示相似度，博主这里还是最经典的欧式距离，更多的细节可以参考官方文档。如下图所示，你觉得和目标人物最接近的是哪一位呢？\ncollection.load() # 查询数据 search_params = {\u0026#34;metric_type\u0026#34;: \u0026#34;L2\u0026#34;, \u0026#34;params\u0026#34;: {\u0026#34;nprobe\u0026#34;: 10}} results = collection.search( data=[[1.75, 72, 24]], # 查询一个身高 1.75 米、体重 72 公斤、年龄 24 岁的人物 anns_field=\u0026#34;metrics\u0026#34;, param=search_params, limit=10, expr=None, output_fields=[\u0026#39;id\u0026#39;,\u0026#39;name\u0026#39;], consistency_level=\u0026#34;Strong\u0026#34; ) 答案揭晓，是小月，你猜对了吗？这说明 Milvus 返回的结果是符合我们的预期的。同理，你可以得到距离小王、小李、小张“最近”的人物，而这就是 Milvus 在向量相似性检索上的一个简单应用。\nMilvus 向量相似性检索结果展示\r其实，我并不喜欢这种单调的评价体系，因为人类自始至终都是一种复杂的生物，可我们这个世界，好像还是更喜欢用金钱这个指标来评价一个人，或许是因为一维向量更简单一点？可你不得不承认，在将一切都转化为向量以后，你就可以从定性分析转变为定量分析，人类的一切情绪波动在 AI 的眼中，不过是单调、重复的数学计算。\nMilvus 在人脸识别上的应用 现在，让我们尝试将 Milvus 运用到人脸识别上面。我们知道，在 dlib 中人脸特征值可以用一个 128 维的向量来表示。所以，这是一个非常合理的联想。此时，Milvus 会帮我们完成相似度计算的工作，这不正是我们希望看到的结果吗？坦白地讲，当我看到 Milvus 需要将数据加载到内存中时，我是有一点失望的，因为我担心这一切只是某种循环结构的掩饰。Anyway，现在可以通过 Milvus 找到相似度最高的人脸，依然不失为一种新的思路。有了前面的基础，我们可以非常容易地写出下面的代码：\n# 提取人脸特征并写入向量数据库 def extract_features_to_milvus(faces_dir): collection = None if not utility.has_collection(\u0026#39;faces\u0026#39;): collection = create_collection(\u0026#39;faces\u0026#39;) create_index(collection, \u0026#39;person_face\u0026#39;) else: collection = Collection(\u0026#39;faces\u0026#39;) mean_features_list = list(get_mean_features_of_face(faces_dir)) person_names = list(map(lambda x:x[1], mean_features_list)) person_faces = list(map(lambda x:x[0].tolist(), mean_features_list)) person_ids = [i+1 for i in range(len(mean_features_list))] mr = collection.insert([person_ids, person_names, person_faces]) collection.flush() # 从向量数据库中查询最相似的人脸 def search_face(collection, feature): search_params = {\u0026#34;metric_type\u0026#34;: \u0026#34;IP\u0026#34;, \u0026#34;params\u0026#34;: {\u0026#34;nprobe\u0026#34;: 10}} return collection.search( data=[np.array(feature).tolist()], anns_field=\u0026#34;person_face\u0026#34;, param=search_params, limit=20, expr=None, output_fields=[\u0026#39;person_id\u0026#39;,\u0026#39;person_name\u0026#39;], consistency_level=\u0026#34;Strong\u0026#34; ) 你可能会问，实际的识别效果如何？对此，我想说的是，你不能依靠 Milvus 返回相似度最高的一张人脸。经过博主的测试，人脸识别的准确度和你每次查询数据多少有关，你至少应该返回几十个可能的结果，然后再从这些结果中选取一条匹配程度最高的数据。我个人的看法是，现阶段的向量数据库可能都逃脱不了循环的宿命，在样本数目不大的情况下，其优势并不显著。那么，从这个角度来说，这篇文章的尝试和探索是不是就宣告失败了呢？我只能说，人有时候要试着放下对结果的执念，因为你不可能每次尝试都能得到好的结果。曾经有前辈把机器学习比作炼丹，时下流行的各种 LLM，无一不经历了反复的训练和优化，所以，你可以说，我至少了解了一种可用于图片检索的方案啊，对吧？\n本文小结 这篇文章主要介绍了将向量数据库引入人脸识别中的思路，并针对真实项目中的性能优化问题进行了探讨。传统的关系型数据库无法处理非结构化的人脸特征数据，因此在人脸识别场景下，遍历整个人脸库并逐一对比特征值是一种非常常见的做法。现在，随着 ChatGPT 和其他大型语言模型的出现，以及类似于 Embedding 的概念的频繁出现，人们对向量的概念有了更深入的了解。其实，不管是以前的词袋模型还是现在的 Word2Vec，向量的概念一直都存在，无非是现在有更好的契机去了解这些知识。借助于向量这个桥梁，我们可以将大量的非结构化数据转换为结构化的数据。从某种意义上来讲，向量是一种从高维度走向低维度的信息压缩技术。而在人脸识别的场景中，向量数据库可以帮助我们完成相似度索引的工作，即使最终的识别效果并不理想，可我们终究还是在这个过程中有所收获。好了，以上就是这篇博客的全部内容，欢迎大家在评论区留下你的建议或者意见。\n","date":"2023-04-24T20:49:47Z","image":"/posts/use-milvus-to-quickly-retrieve-massive-faces/eye-g11309d339_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/use-milvus-to-quickly-retrieve-massive-faces/","slug":"Use-Milvus-To-Quickly-Retrieve-Massive-Faces","tags":["人脸识别","Python","Milvus","向量"],"title":"视频是不能 P 的系列：使用 Milvus 实现海量人脸快速检索"},{"categories":["编程语言"],"content":"在某个瞬间，我忽然发觉，三体或是AI，本质上是非常相近的事物，甚至在面对任何未知领域的时候，人类总会不自觉地划分为降临派、拯救派和幸存派。姑且不论马斯克等人叫停 GPT-5 的真实动机如何，当大语言模型(LLM)裹挟着 AIGC 的浪潮气势汹汹地袭来时，你是否会像很多人一样，担心有一天会被机器取代以致于失业呢？此前，我曾自嘲般地提到过，我是一名 YAML 工程师 、Markdown 工程师、Dockerfile 工程师……，甚至以后还会变成一名 Prompt 工程师，而这背后的因果关系，本质上我们对这个世界的编程方式，正在逐步地从 DSL 转向自然语言。我个人认为，任何低端、重复的工作最终都会被机器取代，而诸如情感、艺术、心理、创意……等非理性领域，则可能会成为人类最后的防线。两年前，柯洁以 0:3 的比分输给 AlphaGo，一度在棋盘前情绪失控，我想，那一刻他大概不会想到两年后还会出现 ChatGPT。在《蜘蛛侠：英雄无归》 电影里面，彼得·帕克对奇异博士说，“你知道比魔法更神奇的东西是什么吗？是数学”。我个人非常喜欢这句话，因为在绝对的理性面前，一切的技巧都是徒然，更重要的是，如此深刻的哲理，居然是来自生活中一个真实案例。\n电子签章与数学 好的，虽然我们说那些低端、重复的工作最终都会被机器取代，但是真正残酷的现实是，我们并没有那么多需要创造力的工作，就像我们并不需要那么多架构师一样。毕竟，你想象不到，一个人在五年前和五年后做的工作毫无差别，特别是企业级应用中非常普遍的打印。过去这些年，企业数字化转型的口号一直在喊，可到头来我们并没有等来真正的无纸化，企业依然对打印单据这件事情乐此不疲，仿佛没有这一张纸业务就没法开展一样。在这个过程中，企业会希望你能在单据上加盖公司的印章，这就产生了所谓的“电子签章”的需求。当然，我们这里不考虑电子签章的申请、加/解密、防伪等实际的流程，我们只是考虑将其通过 GDI+ 绘制出来即可。考虑到印章有圆形和椭圆形两种形制，所以，我们下面来进行分类讨论。\n圆形印章 可以注意到，圆形印章通常由四部分组成，分别是顶部文字、中心部分的五角星、中下部分文字和底部文字。\n通过程序绘制的印章样例\r其中，顶部文字表示印章所属的公司/组织/机构，底部文字表示14位印章编号，这两部分文字均呈圆弧状分布。具体该如何实现呢？我们来一起看一下。首先，圆形印章的轮廓是一个标准的圆形，这个绘制非常容易：\n// 从位图创建一个画布 var bitmap = new Bitmap(width, height, PixelFormat.Format32bppArgb); var g = Graphics.FromImage(bitmap); // 绘制圆形边框 var rect = new RectangleF(x, y, radius, radius); var Pen pen = new Pen(Color.Red, 3.0f); g.DrawEllipse(pen, rect); 而对于中心部分的五角星，我们使用一个路径填充即可。此时，问题的关键是在圆上找出五角星的五个顶点。显然，五角星的顶点满足下面的几何关系：\n小学二年级就学过的五角星几何关系\r利用三角函数的知识，我们可以非常容易地写出对应代码，请注意，计算机中使用的坐标系 Y 轴正方向向下：\nvar Radius = rect.Width / 2 * 0.45; var Center = new PointF(rect.X + rect.Width / 2, rect.Y + rect.Height / 2); PointF[] points = new PointF[] { // P0 new PointF(Center.X, (float)(Center.Y - Radius)), // P1 new PointF( (float)(Center.X + Radius * Math.Sin(72 * Math.PI / 180)), (float)(Center.Y - Radius * Math.Cos(72 * Math.PI / 180)) ), // P2 new PointF( (float)(Center.X + Radius * Math.Sin(36 * Math.PI / 180)), (float)(Center.Y + Radius * Math.Cos(36* Math.PI / 180)) ), // P3 new PointF( (float)(Center.X - Radius * Math.Sin(36 * Math.PI / 180)), (float)( Center.Y + Radius * Math.Cos(36 * Math.PI / 180)) ), // P4 new PointF( (float)(Center.X - Radius * Math.Sin(72 * Math.PI / 180)), (float)(Center.Y - Radius * Math.Cos(72 * Math.PI / 180)) ), }; // 根据五个点生成一个封闭路径 var path = new GraphicsPath(FillMode.Winding); path.AddLine(points[0], points[2]); path.AddLine(points[2], points[4]); path.AddLine(points[4], points[1]); path.AddLine(points[1], points[3]); path.AddLine(points[3], points[0]); path.CloseFigure(); // 填充路径 g.RotateTransform(0); g.FillPath(new SolidBrush(Color.Red), path); 接下来，我们来考虑如何绘制这两段呈圆弧状分布的文字，这里需要用到的数学知识是圆的参数方程以及三角函数。其基本思路是：选定一个起始角度，再根据总角度和文字数目计算一个步长，进而确定每一个文字对应的角度。譬如，这里假定上半部分圆弧总角度为300 度，下半部分圆弧总角度为 60 度：\nvar center = new PointF(rect.X + rect.Width / 2.0f, rect.Y + rect.Height / 2.0f); var fontToFit = new Font(\u0026#34;宋体\u0026#34;, 13, FontStyle.Bold, GraphicsUnit.Pixel); var totalAngle = Math.PI * 5 / 3; var stepAngle = totalAngle / (text1.Length + 1); var startAngle = Math.PI * 4 / 3; for (int i = 0; i \u0026lt; text.Length; i++) { float angle = (float)(startAngle - (i + 1) * stepAngle); if (angle \u0026lt; 0) angle += (float)Math.PI * 2; var point = new PointF( center.X + radius * (float)Math.Cos(angle), center.Y - radius * (float)Math.Sin(angle) ); g.DrawString(text1[i].ToString(), fontToFit1, brush, point.x, point.y); } 我们知道，在三角函数定义中，逆时针方向为正方向，所以，对于上半部分的弧形文字，只需要用起始角度依次减去对应的步长数。为了让后续计算角度的时候更方便一点，这里会将负角统一转换为正角。接下来的事情就顺理成章啦，因为你可以利用三角函数计算出对应的坐标，此时，我们只需要在指定的位置调用 DrawString 函数将每个字符绘制出来即可：\n通过程序绘制的印章样例_v1\r不过，你很快会发现一个问题，那就是这些文字的方向并没有像一般印章那样，始终“正对”着你的实现。此时，你会用到第三个数学知识，即：当一个点变为原点以后，它与 X 轴正方向的夹角会如何变化？你为什么需要这个知识呢？因为我们需要对每个字符做一次平移变换、一次旋转变换，这样才能达到我们的目的，即：无论你从哪一个方向去看这些文字，它对你来说都是“正”的，其代码实现如下：\nvar center = new PointF(rect.X + rect.Width / 2.0f, rect.Y + rect.Height / 2.0f); var fontToFit = new Font(\u0026#34;宋体\u0026#34;, 13, FontStyle.Bold, GraphicsUnit.Pixel); var totalAngle = Math.PI * 5 / 3; var stepAngle = totalAngle / (text1.Length + 1); var startAngle = Math.PI * 4 / 3; for (int i = 0; i \u0026lt; text.Length; i++) { float angle = (float)(startAngle - (i + 1) * stepAngle); if (angle \u0026lt; 0) angle += (float)Math.PI * 2; var point = new PointF( center.X + radius * (float)Math.Cos(angle), center.Y - radius * (float)Math.Sin(angle) ); g.TranslateTransform(point.X, point.Y); var transformAngle = (float)(angle * 180 / Math.PI + 90); if (transformAngle \u0026gt; 360) transformAngle -= 360; // 注意：RotateTransform() 方法旋转方向是顺时针方向，所以，要用 360 度减去当前角度 // 印章上方的文字需要正对着外侧，所以，要再加上 180 度 transformAngle = 360 - transformAngle + 180; g.RotateTransform(transformAngle); g.DrawString(text[i].ToString(), fontToFit, brush, 0, 0, format); g.ResetTransform(); } 这里的关键是 TranslateTransform() 和 RotateTransform() 两个方法，这就是我们上文中提到的平移变换和旋转变换。为什么要这样处理呢？因为我们希望的看到是，文字旋转到“正确”的方向且保持位置不变，如果没有平移转换的话，它会就变成点 A 围绕 点 B 旋转，这样显示不符合我们的预期。总之，当一切都在向着预期的方向发展的时候，我们就可以利用这个技巧“照葫芦画瓢”，需要注意的是，底部弧形文字的“正方向”是向下的，因此，在做旋转变换的时候，两者会相差 180 度。对此，我想说，数学真的好用：\n通过程序绘制的印章样例_v2\r相比之下，绘制中下部分文字就非常简单啦，因为它是在一个矩形范围上绘制，你唯一需要的就只有一个勾股定理。 理论上，你只要将以上代码片段整合起来，就可以绘制出一个相对完美的印章，我个人踩坑的体会是：真正的困难常常是印章的实际尺寸、打印尺寸、DPI、分辨率这些客观因素。\n椭圆形印章 好了，接下来再说椭圆形印章的绘制，原理基本相同，对应的数学知识是椭圆的参数方程。不知道大家是否还记得圆锥曲线、离心率、焦点等概念，博主在写这篇博客的时候，的确是回过头再次重温了这些知识。作为一名程序员，平时没少被别人“教育”业务的重要性，可对我来说，业务大概就是朝三暮四、朝秦暮楚的代名词，相比这些人为想象和构筑的东西，我更喜欢风、沙、星辰这些接近自然和宇宙的东西，对我来说，数学便是如此。好了，下面是绘制弧形文字的代码片段，供大家参考：\nvar a = rect.Width / 2 * 0.8f; var b = rect.Height / 2 * 0.8f; var center = new PointF(rect.X + rect.Width / 2.0f, rect.Y + rect.Height / 2.0f); var fontToFit = new Font(\u0026#34;宋体\u0026#34;, 13, FontStyle.Bold, GraphicsUnit.Pixel); var totalAngle = Math.PI * 5 / 3 var stepAngle = totalAngle / (text1.Length + 1); var startAngle = Math.PI * 4 / 3 for (int i = 0; i \u0026lt; text.Length; i++) { float angle = (float)(startAngle - (i + 1) * stepAngle); if (angle \u0026lt; 0) angle += (float)Math.PI * 2; // 利用椭圆参数方程计算坐标 var point = new PointF( center.X + a * (float)Math.Cos(angle), center.Y - b * (float)Math.Sin(angle) ); g.TranslateTransform(point.X, point.Y); var transformAngle = (float)(angle * 180 / Math.PI + 90); if (transformAngle \u0026gt; 360) transformAngle -= 360; // 注意：RotateTransform() 方法旋转方向是顺时针方向，所以，要用 360 度减去当前角度 // 印章上方的文字需要正对着外侧，所以，要再加上 180 度 transformAngle = 360 - transformAngle + 180; g.RotateTransform(transformAngle); g.DrawString(text[i].ToString(), fontToFit1, brush, 0, 0, format); g.ResetTransform(); } 可以看出，本质上只需要用半长轴 a 和半短轴 b 替换半径即可。考虑到圆是椭圆的特殊形式，这种从特殊到一般的认知方式，难道不显得有趣吗？同样地，这里提供一个椭圆形印章的效果图：\n通过程序绘制的印章样例_v3\r至此，关于如何通过 GDI+ 绘制印章，笔者可谓是倾囊相授啦，在这个过程中，我最享受的环节，恰恰是那些再寻常不过的数学知识，当你觉得 AI 有一天一定会取代人类的时候，我以为，这代表着绝对理性的胜利，就像令柯洁落泪的 AlphaGo 一样，没有任何技巧，它只是在经历无数次计算以后得出的必然结果，从某种意义上来讲，数学反而是宇宙间最简单的学问，不是吗？如果说人工智能里还有哪些更接近“玄学”的东西，我以为，大概是我们自始至终都没能解释清楚，模仿神经元的神经网络到底是怎样一步步地产生了“意识”？而这一切就好比，你的确知道 ChatGPT 给了你一个满意的答案，但你始终不知道的是，它到底是在经过了一个什么样的思考过程以后，才能给出一个如此契合你心理预期的答案？恍惚之间，我会觉得它符合人类眼中的“高情商”标准，即使它对你一无所知，可它还是能讲出令你“舒服”的话语，如果语言本身就充满了这种迷惑性，那么人类最看重的情感到底又算什么？\n字体自适应方案 好了，现在让我们来考虑得更长远一点，当我们实现了一个相对通用的印章绘制算法以后，我们会希望通过配置这些文字来生成不同的印章。此时，一个新的问题产生了：在印章尺寸固定(有相关标准)的情况下，如何能兼容不同长度的文字？一个容易想到的方案是修改字体大小。譬如，当文字较多的时候就缩小字体，当文字较少时就放大字体。所以，下面我想分享的是动态调整字体大小实现字体自适应。\n基于宽高 第一种方案基于宽高，即字体是绘制在一个矩形区域内，印章的中下部分通常用来表示印章的用途，此时，我们可以利用 MeasureString 这个方法来测量整个字符串的宽度，并将其和当前矩形的宽度进行比较。如果实际宽度大于当前矩形的宽度，则需要减小字号；如果实际宽度小于当前矩形的宽度，则需要增加字号。当然，缩小的时候可以给一个最小字号，因为你要确保别人能看清印章上的文字；放大的时候需要考虑矩形的高度，因为你要确保印章上的元素不会相互重叠。下面是一个基本实现：\nfloat ScaleFontSizeByContainerSize(Graphics g, string text, Font font, SizeF size) { var fontSize = font.Size; // 对字体缩小时需要考虑最小的字体大小 var measuredSize = g.MeasureString(text, new Font(font.FontFamily, fontSize)); while (measuredSize.Width \u0026gt; size.Width) { fontSize -= 0.5f; if (fontSize \u0026lt;= MIN_FONT_SIZE) break; measuredSize = g.MeasureString(text, new Font(font.FontFamily, fontSize)); } // 对字体放大时需要考虑高度的问题 measuredSize = g.MeasureString(text, new Font(font.FontFamily, fontSize)); while (measuredSize.Width \u0026lt; size.Width \u0026amp;\u0026amp; measuredSize.Height \u0026lt; size.Height) { fontSize += 0.5f; measuredSize = g.MeasureString(text, new Font(font.FontFamily, fontSize)); } return fontSize; } 如图所示，下图展示了相同尺寸下，文字根据字数的多少按不同的字号动态进行缩放：\n基于宽高的字体大小自适应方案\r基于周长 第二种方案基于周长，主要是针对印章中呈圆弧状分布的这些文字，此时，宽度和高度不足以评估文字能否“恰当”地分布在印章上，所以，我们就可以尝试用周长来进行比较。按照微积分的思想，我们可以粗略地认为，每个字符的宽度累加起来，其总和就约等于对应的这段孤线的长度。在这种情况下，我们可以考虑使用弧长公式和椭圆的周长公式。当文字宽度小于周长时，表明字体还可以放大一点；当文字宽度大于周长时，表明字体还可以缩小一点。类似地，这里给出一个基本实现：\nfloat ScaleFontSizeByPerimeter(Graphics g, string text, Font font, float radius, float angle) { var fontSize = font.Size; // 圆形周长公式 var perimeter = angle * Math.PI * radius / 180; // // 椭圆周长公式 // var h = Math.Pow((a - b) / (a + b), 2); // var c = Math.PI * (a + b) * (1 + (3 * h / (10 + (4 - 3 * h)))); // var perimeter = c * angle / 360; // 对字体缩小时需要考虑最小的字体大小 var measuredSize = g.MeasureString(text, new Font(font.FontFamily, fontSize)); while (measuredSize.Width \u0026gt; perimeter) { fontSize -= 0.1f; if (fontSize \u0026lt;= MIN_FONT_SIZE) break; measuredSize = g.MeasureString(text, new Font(font.FontFamily, fontSize)); } // 对字体放大时需要考虑高度的问题 measuredSize = g.MeasureString(text, new Font(font.FontFamily, fontSize)); while (measuredSize.Width \u0026lt; perimeter) { fontSize += 0.1f; if (fontSize \u0026gt;= MAX_FONT_SIZE) break; measuredSize = g.MeasureString(text, new Font(font.FontFamily, fontSize)); } return fontSize; } 如图所示，下图展示了相同尺寸下，文字根据字数的多少按不同的字号动态进行缩放：\n基于周长的字体大小自适应方案\r本文小结 写这篇博客时，其实是在一个多事之秋。如果是指新海诚的新电影《铃芽之旅》，我大概会写遗憾以及自我和解；如果是指 ChatGPT，我大概会写我对于人工智能的看法；如果是指景甜和张继科这对分手了的男女朋友，我大概会写我对于人类情感的理解；如果是指中国电科的离职事件，我大概会写我对生产力/生产关系的想法……可当很多东西纠缠在一起的时候，任何一种情绪注定都无法独自存活下去，所以，我还是决定写一点可以掌控的事物。如彼得·帕克所言，“比魔法更神奇的东西是数学”，在一个不确定性远大于确定性的时代，能够再次感受到数学世界的美好实在是一种幸运，因为一切的公式/定理都会引导你通往某个确定的地方，正如当我画完圆形印章以后，我可以快速地画出椭圆形印章，它不会增加一丝一毫的心智上的负担，而人类热衷于构建的业务流程，则永远都存在着这样那样的缺陷。也许，直到这一刻，我才能够明白科学家们渴望用一个公式描述这个世界的偏执。因为，在一个不确定的世界里寻找确定，这件事情本身就足够迷人，就像你输入到 AI 模型里的一个个提示词，它们本身或许是随机的、不确定的，可你需要的可能是一个确定的结果。毕竟，这个世界早就复杂到连选择本身都是一件困难重重的事情，对吧？\n","date":"2023-04-05T15:49:47Z","image":"/posts/exploration-of-font-size-adaptation-scheme-under-gdi+/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/exploration-of-font-size-adaptation-scheme-under-gdi+/","slug":"Exploration-Of-Font-Size-Adaptation-Scheme-Under-GDI+","tags":["图形学","GDI+","数学","打印"],"title":"GDI+下字体大小自适应方案初探"},{"categories":["编程语言"],"content":"2023年三月对于金融和科技领域来说，可谓是“冰火两重天”。硅谷银行倒闭事件像一枚深水炸弹一样在金融领域扩散开来，而 OpenAI 则凭借 ChatGPT 这款产品一路“狂飙”，成为当下最负盛名的爆款话题。就在百度推出同类产品“文心一言”的前夕，OpenAI 正式发布了 GPT-4，直至微软高调宣布在 Office 全家桶中集成了 GPT-4，将这场技术狂欢推向高潮。作为一个关注聊天机器人的人，我从大学时期就开始通过 AIML 标记语言构建语料库，并逐渐接触 NLP 领域的知识。我认为这一波人工智能的热度代表了 OpenAI 主张的大语言模型(LLM)的胜利。ChatGPT 虽然始于聊天机器人，但绝不会止于聊天机器人。它的最终形态或许会是钢铁侠的智能管家“贾维斯”，抑或是《流浪地球》里超级人工智能 MOSS。事实上，我日常会用 ChatGPT 写程序原型、翻译文本、提取主题/关键词，这段时间更是尝鲜了智能家居。因此，我想和大家分享一下小爱音箱集成 ChatGPT 的过程。\n基本原理 如果你像博主一样是一名智能家居新手玩家，那么在正式接触智能家居之前，你应该至少听说过 WIFI、ZigBee、BLE 这些名词。这些是指智能家居中的通信协议，例如小爱音箱可以作为蓝牙 Mesh 网关去连接那些使用蓝牙通信的设备，而 ZigBee 则是一种短距离、低功耗、支持自组网的无线通信协议。虽然 ZigBee 对外宣称是一个开放标准，但不同的厂商出于利益考虑，并不完全兼容彼此的设备，离真正的万物互联始终还有一段距离。因此，你会发现米家有类似多模网关这样的产品，现阶段的智能家居是一个多种协议混合使用的局面，2C 市场更青睐蓝牙和 WIFI 方案，2B 市场更青睐 ZigBee 方案。为了让更多的设备加入整个智能家居生态，开源的智能家居方案 HomeAssistant 就此诞生。其中的 IFTTT 组件可以扩展出更多的智能玩法；为了让设备加入苹果公司的 HomeKit 生态，HomeBridge 这样一个“曲线救国”的方案就此诞生。可以说，现阶段智能家居的高阶玩法，基本都是围绕这两个平台展开。作为一名普通的消费者，你并没有机会去选择使用哪种协议，更多的是去选择使用哪一个平台。\nSmart Home Protocols: WiFi vs Bluetooth vs ZigBee vs Z-Wave\r前面提到 ZigBee 的自组网具有离线可用的特性。与 WIFI 不同，WIFI 需要接入互联网，一旦断网就无法对设备进行有效控制，而蓝牙和 ZigBee 就没有这种烦恼。唯一的问题是它们都需要对应的网关。目前，米家的设备控制主要有远程控制和本地控制两种方式。远程控制需要发送指令到米家的服务器，这种方式对小米来说更有利，唯独不利于实现“万物互联”这一伟大远景。本地控制至少需要一个智能家庭屏或中枢网关，其好处是延迟低、离线可用、保障隐私。从某种角度来说，这与人们开始使用 NAS 搭建私有云的初衷一致，都是为了更好地保护隐私和数据安全。由于博主不具备本地控制的条件，所以，我们还是采用远程控制的方案，即通过向米家的服务器发送指令来达到控制设备的目的。在这个过程中，接入 ChatGPT 的 API，再控制小爱音箱将其响应内容朗读出来。这个方案可以实现远程控制的同时，利用 ChatGPT 弥补小爱同学“智能”上的不足。如图所示，下面是一个简单的示意图：\n米家远程控制及 ChatGPT 接入示意图\r如何控制米家 接下来，我们来探讨如何控制米家设备。米家通过 HTTP 协议实现远程控制，为此，我们推荐使用两个非常实用的库：MiService 和 python-miio。这两个库各有优缺点。如果你对米家物联网协议更感兴趣，我们建议使用 python-miio；如果你希望更快、更容易地上手米家智能家居，我们则推荐使用 MiService。从使用方式上来看，MiService 使用 DID 来区分不同的设备，而 python-miio 使用 Token 和 IP 来区分不同的设备。下面，我们以 MiService 为例来演示如何通过编程控制米家设备。\n网关在整个智能家居中的地位\r在开始前，请确保你安装了 MiService , 你可以选择下面两种方式之一进行安装：\n# 从包管理器安装 python -m pip install MiService # 从源代码安装 git clone git@github.com:Yonsm/MiService.git cd MiService python -m pip install . 安装后，你会在 Python 主目录下的 Scipts 目录看到一个叫做 micli 的命令行工具。如果你的 Scripts 目录在全局环境变量中 Path 中，那么，恭喜你，你可以直接在终端中使用该命令。为了向该工具表明身份，我们需要在终端中导出下面两个环境变量：\nexport MI_USER=\u0026lt;小米账号\u0026gt; export MI_PASS=\u0026lt;小米密码\u0026gt; 此时，我们在终端中输入命令 micli list，我们就可以得到当前账号下所有的米家设备信息：\n[{ \u0026#34;name\u0026#34;: \u0026#34;客厅主灯\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;yeelink.light.ceil31\u0026#34;, \u0026#34;did\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;token\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;小爱同学\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;xiaomi.wifispeaker.x08e\u0026#34;, \u0026#34;did\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;token\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;温湿度计\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;miaomiaoce.sensor_ht.t2\u0026#34;, \u0026#34;did\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;token\u0026#34;: \u0026#34;\u0026#34; }] 可以注意到，在这个列表中有我们想要的小爱同学，在获得关键信息 DID 后，我们将其在终端中导出：\nexport MI_DID=\u0026lt;请替换为你的DID\u0026gt; 通过上述信息，我们了解到小爱音箱的型号为 X08E，你可以在音箱底部找到这个信息。接下来，我们需要访问 https://home.miot-spec.com/ 这个网站，以获取更多控制小爱音箱的信息。在该网站中，输入当前型号，你将会看到如下页面：\n米家产品规范截图-1\r点击页面中标有链接的部分，你将看到小爱音箱拥有各种控制指令，如下图所示：\n米家产品规范截图-2\r针对我们最关心的语音部分，你会发现它整体上分为三个级别：分类、属性(Properties) 和动作(Actions)。每个分类都有唯一的 SIID，每个属性都有唯一的 PIID，每个动作都有唯一的 AIID。参考 MiService 的文档，我们可以轻松编写以下指令：\n# 获取小爱音箱播放状态 1:Playing, 0:Stop, 2:Pause micli 3-1 # 播放文本内容 micli 7-3 Hello # 执行文本指令 micli 7-4 为我点亮世界 # 播放 micli action \u0026#39;{\u0026#34;did\u0026#34;:\u0026#34;\u0026lt;你的DID\u0026gt;\u0026#34;,\u0026#34;siid\u0026#34;:3,\u0026#34;aiid\u0026#34;:2,\u0026#34;in\u0026#34;:[]}\u0026#39; # 暂停 micli action \u0026#39;{\u0026#34;did\u0026#34;:\u0026#34;\u0026lt;你的DID\u0026gt;\u0026#34;,\u0026#34;siid\u0026#34;:3,\u0026#34;aiid\u0026#34;:3,\u0026#34;in\u0026#34;:[]}\u0026#39; # 停止 micli action \u0026#39;{\u0026#34;did\u0026#34;:\u0026#34;\u0026lt;你的DID\u0026gt;\u0026#34;,\u0026#34;siid\u0026#34;:3,\u0026#34;aiid\u0026#34;:4,\u0026#34;in\u0026#34;:[]}\u0026#39; # 设置音量 micli 2-1=#60 以上演示以小爱音箱为例，事实上，只要掌握了这种方法，大多数米家设备都可以通过编程实现控制。例如，智能门锁和门窗传感器可以通过程序获取电池耗电量的信息，以便提醒及时更换电池。目前，第三方物联网平台如 巴法云 采用发布/订阅模式来控制单片机，这使得像 ESP32 这样的开发板可以快速接入米家生态。我个人认为，这些技术都非常有趣，值得我们做进一步的深入探索。\n如何接入 ChatGPT 对于 Python，接入 ChatGPT 非常方便，因为 OpenAI 官方提供了 SDK。首先，我们需要使用 pip 安装 openai 包：\npython -m pip install openai 接下来，我们只需导入 openai 模块，并设置 API 密钥：\nimport openai openai.api_key = \u0026#34;\u0026lt;YOUR_API_KEY\u0026gt;\u0026#34; 现在，我们可以调用 OpenAI 的 API，输入对话文本，获取机器生成的回复，如下所示：\ncompletion = openai.ChatCompletion.create( model=\u0026#34;gpt-3.5-turbo\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;:\u0026#34;Hi，ChatGPT!\u0026#34;}] ) 当然，鉴于 OpenAI 在国内的可访问性问题，笔者通过 Vercel 搭建了一个代理服务。此时，你可以使用下面的 API 接口，如下所示。虽然这只是对官方 SDK 进行了二次封装，但是考虑到国情如此，我们只能说聊胜于无：\ncurl -X POST \u0026#39;https://openai-proxy.yuanpei.me/openai/v1/completions\u0026#39; \\\r-H \u0026#39;Authorization: Bearer \u0026lt;Your-OpenAI-API-KEY\u0026gt;\u0026#39; \\\r-H \u0026#39;Content-Type: application/json\u0026#39; \\\r-d \u0026#39;{\r\u0026#34;model\u0026#34;: \u0026#34;gpt-3.5-turbo\u0026#34;,\r\u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;:\u0026#34;Hello ChatGPT!\u0026#34;}]\r}\u0026#39; 更多的细节，大家可以参考这里：https://github.com/qinyuanpei/openai-proxy。\n风云合璧 到目前为止，整个思路非常清晰。当我们使用米家 App 时，可以注意到米家存储了我们和小爱同学的对话内容。因此，我们的思路是通过接口获取对话记录，每一组对话中的第一句话，就是我们对小爱同学的提问。此时，我们可以用这个问题去询问 ChatGPT，等待 ChatGPT 给出回应后，再利用上述技巧将文本转化为语音输出。然而，这个方案最大的弊端是，无法完全屏蔽小爱同学自身的交互逻辑。因此，在下面的演示视频中，我们不得不通过程序来强行打断小爱同学的发言，这其实是一种无奈的做法：\n当然，我们还有另一种思路，即通过蓝牙连接到小爱音箱。在这种情况下，小爱同学就完全成为一个输出设备，甚至无需介入米家。但这样做会失去与小爱同学互动的乐趣，除非你想创建类似于“贾维斯”的人工智能程序，否则请不要轻易尝试这条注定艰辛的道路。下面是一个针对这种方案的简化实现：\nimport speech_recognition as sr import pyttsx3 # 初始化语音识别 \u0026amp; 语音合成 r = sr.Recognizer() engine = pyttsx3.init() # 语音识别函数 def recognize_speech(): with sr.Microphone() as source: print(\u0026#39;Please speak:\u0026#39;) audio = r.listen(source) try: text = r.recognize_google(audio, language=\u0026#39;zh-CN\u0026#39;) print(\u0026#39;You said:\u0026#39;, text) return text except Exception as e: print(\u0026#39;Error:\u0026#39;, e) return \u0026#39;\u0026#39; # 语音合成函数 def speak(text): engine.say(text) engine.runAndWait() # 主程序 speak(\u0026#39;你好，请问有什么可以帮助您的？\u0026#39;) while True: text = recognize_speech() if text == \u0026#39;退出\u0026#39;: break elif text.startswith(\u0026#39;帮我\u0026#39;) # todo：在这里调用 ChatGPT pass else: speak(\u0026#39;对不起，我不明白您的意思，请您再说一遍。\u0026#39;) 视频中的演示程序源自 Github 上的一个开源项目 https://github.com/yihong0618/xiaogpt。在此基础上，我做了一些微不足道的工作。首先，将对小爱的控制方式完全替换为上文中的指令，因为我感觉 MiNAService 这个类不是特别稳定；其次，将 OpenAI 的 API 替换为我自己架设的代理接口，这样就解决了科学上网的烦恼；最后，将 MI_USER、MI_PASS、MI_DID、OPENAI_API_KEY 四个参数放入 .env 文件中，简化了程序启动时的参数。我已将这些工作提交到了我的 Github 上，对此感兴趣的朋友可以自行下载：https://github.com/Regularly-Archive/2023/tree/main/XiaoiGPT。\n本文小结 在这个过程中，我最大的收获是学会了如何用编程控制米家设备。尽管现在智能家居的交互场景还相对有限，但就像 ChatGPT 一样，当你意识到 AI 正式进入一个新阶段的时候，一切都显得为时已晚。百度的文心一言发布后，陆续有人拿到了内测资格，一时间好像没有人再去讨论 ChatGPT 了，因为大家的精力都放在戏谑文心一言生成的图片上面。我经常思考未来是否会出现一种职业叫做“魔法吟唱师”，负责为各种 AI 模型输入提示词。去年 Stable Diffusion 开始流行时，我就意识到提示词对结果产生影响。遗憾的是，此时此刻，人类仍然为比 AI 多懂几个成语、多吃过几道菜而沾沾自喜。作为一名技术人员，我对科技的未来一直抱有乐观的态度，因为我认为技术日新月异，真正拖累人心的始终是那些非技术因素。人们总以为 AI 能帮助解决各种各样的问题，但问题是我们是否能清晰地表达出我们的想法呢？我们总是太过痴迷于华丽的言辞，而忽略了简单直接的表达方式。人们实在太享受口是心非的感觉，就像今天这篇博客，其实是由我和 ChatGPT 一起完成的，但你是否能分清其中的真假吗？\n","date":"2023-03-20T15:49:47Z","image":"/posts/the-xiaoai-speaker-integrates-an-incomplete-tutorial-on-chatgpt/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/the-xiaoai-speaker-integrates-an-incomplete-tutorial-on-chatgpt/","slug":"The-Xiaoai-Speaker-Integrates-An-Incomplete-Tutorial-On-ChatGPT","tags":["人工智能","ChatGPT","智能家居","小爱同学"],"title":"小爱音箱集成 ChatGPT 的不完全教程"},{"categories":["生活感悟"],"content":"新年的第一篇博客迟迟没有落笔，我想，大抵是因为过去一年并没有一个“圆满”的结局，而这一切就好像，你总是要写完一个句号，方能心安理得地另起一行。我猛然意识到，这场疫情无形中放大了故乡在时间和空间上的距离感。因此，面对阔别久矣的故乡，我甚至选择了早上的第一趟高铁。当我在列车上看着身后的事物一点点变小直至消失，这实在像极了我每一次出门远行。彼时彼刻，一个念头我在脑海中盘桓辗转：见到家人的那一刻，一定要给对方一个坚实的拥抱。可奇怪的是，越是面对熟悉的人，我们反而越是拙于表达。当我推开家门看到再熟悉不过的一切时，我内心突然获得了某种安宁，即使我们一直期待的事情是，每次相逢都能有不同的体验。我说我不大喜欢过年走亲戚这类活动，老大哥面无表情地回一句：看三体。\n三体问题，其实可以看作是 N 体问题的一个特例，而所谓 N 体问题，则是指 N 个参照值相对一致的天体，仅在万有引力的作用下会呈现出什么样的运动规律。该问题最早由天才数学家希尔伯特教授，在 1990 年的全球数学大会上提出，是目前公认的人类科学界最难的 23 道数学题之一。如果你无法理解这个难度，请允许我找一个更有名的问题——费马大定理作为参照。事实上，早在 1687 年近代物理学之父牛顿，第一次提出了三体问题。此后的三百余年里，三体问题更是串联起无数如雷贯耳的名字：欧拉、拉格朗日、庞加莱、希尔伯特、开普勒\u0026hellip;等等。\n三体运动 “混沌” 模型示意图\r从某种意义上来讲，三体问题贯穿了整个物理学的发展历程。具体来讲，当 N = 1 时，单个质点的运动轨迹是匀速直线运动，这对应的是牛顿第一定律；当 N = 2 时，两个质点的相对位置始终在一条圆锥曲线上，这对应的是开普勒定律；当 N = 3 时，这就是著名的三体问题，此时，这三个质点的运动轨迹是不可预测的，而从数学的角度来看，三体问题没有解析通解，即：我们没有办法用一个通用的公式来描述其运动轨迹。一个最浅显的例子是，在三维空间中描述位置需要 x、y、z 三个参数，因此，每一个质点可以写出 3 个二阶微分方程或者 6 个 一阶微分方程。那么，当有三个这样的质点的时候，最多的时候会得到 18 个一阶微分方程。可以预见，求解三体问题，等价于求解这样 18 个一阶微分方程构成的方程组，其难度可想而知。\n三体运动 “三角形” 模型示意图\r不过，虽然三体问题没有解析通解，我们总是可以找到某个特定解。譬如，当三颗天体在一条直线上时，两侧的天体会围绕中心天体呈椭圆形的运动轨迹。我们最为熟悉的太阳系就是这种情形；如果三颗天体呈三角形分布，则它们会一起围绕三角形中心点旋转；除此以外，还可以是同一侧的两颗天体围绕第三颗天体呈椭圆形的运动轨迹，其可能性可以说是不一而足。从这个角度来讲，或许我们以后还会发现更多的特定解，但我们始终没有办法用一个完美的公式来描述所有可能的场景，这就是三体问题的复杂性所在。\n三体运动 “8字形” 模型示意图\r回到刘慈欣老师的小说设定，三体星球是一个围绕半人马座 α 三合星系统公转的一个行星，由于这颗星球同时受到三个 “太阳” 的影响，使其呈现出一种在“恒纪元”和“乱纪元”交替往复的状态，为了在这样的环境中生存下去，三体人不得不通过 “脱水” 和 “浸泡” 的方式来维持生命。事实上，半人马座 α 三合星是真实存在的，甚至 2016 年科学家还在这个星系中发现了行星 “比邻星 b” 的踪迹。读到这里，我们不免会怀疑《三体》其实是一本纪实小说，难道这颗比邻星上真的有外星人吗？不同于小说中设定的三个太阳，真实的 “三体” 星球面临的最大威胁是 X 射线，据不完全统计，这颗比邻星上面承受的 X 射线是地球的四百倍。所以，就算真的有外星人，在如此强度的 X 射线面前，胜负还真的挺难说呢？\n半人马座 α 三合星示意图\r我最早接触的科幻小说，当属法国作家凡尔纳的《海底两万里》，可我并不是一个硬科幻爱好者，大概是因为高中时期的我对数学更感兴趣一点。因此，当我以一名程序员的视角来看三体这部作品的时候，我更多的是关注计算机和哲学两个维度。比如史强和汪淼进入三体游戏这一段，表面是在讲三体人如何 “给文明以岁月”，可我看到的其实是人类从古至今的文明演化历程，周文王、孔子、墨子、冯·诺依曼、牛顿、爱因斯坦\u0026hellip;..人类正是依靠着知识和技艺的传承，一步步地走到了今天。譬如，《三体》中的关于 “人列计算机” 的构想可谓是天马行空，可仔细想来，这不过是早期晶体管计算机的具象化，人列计算机中的每一个士兵，本质上就是一个可以输出 0 和 1 的晶体管，正是这些 0 和 1 组成了各种各样的逻辑门。众所周知，逻辑门是构成计算机最基础的组件，常见的加法器、计数器、累加器等都和逻辑门有着千丝万缕的联系。\n三体人列计算机中的逻辑门\r纵观古今，人类更关注如双子星一般耀眼、一时瑜亮的事物，以文学史度之，譬如盛唐有李杜、德国有歌德和席勒、法国有加缪和萨特、俄国有列夫·托尔斯泰和陀思妥耶夫斯基。我想，这或许和我们对宇宙的认知有关，在过去几千年的时间长河里，我们长期观察得天体只有太阳和月亮。因此，“三日凌空”、“三合星系统” 这种异象对我们来说极具诱惑。在关注三体问题得这段时间里，我正好在读一本关于分布式系统的书籍，我马上意识到，程序员世界里的三体问题其实就是 CAP 定理。因为它同时受到一致性( Consistency)、可用性(Availability) 和 分区容忍性(Partition Tolerance) 三个因素的影响，初始条件的选取会影响整体架构的设计和落地。当然，实际操作中它要比三体问题更简单一点，考虑 P 是客观存在的因素，我们其实只有 CP 和 AP 两种选择。\n从三体问题到分布式系统\r小说中设定三体人到达地球需要 400 年的时间，如果考虑到信息的传递同样需要时间，比如古代皇帝驾崩、颁布新政令的时候可能出现延迟，此时就会产生信息差。对应到分布式系统中，在处理主从切换的时候，可能会出现因为延迟而导致的不一致问题，试想一下，一个国家两个皇帝或者是一个系统中有两个 Master 节点，这并不比 “三日凌空” 好到哪里去。三体人为什么能实现真正的 “人列计算机” 呢？按照小说的设定，三体人具有思维透明的特点，这意味着三体人之间信息的交换是无损的、低延迟的。或许，我们会认为人类有城府、懂计谋，是因为人类懂得隐藏自身真实的想法。可是，你说有没有一种可能，三体人只是不擅说谎，他们完全懂得运用计谋，甚至这种计谋是浮在表面的 “阳谋” 。如果按照黑暗森林法则，整个宇宙本质上就是一款大型 “吃鸡” 游戏，三体人害怕地球人吗？确实怕，可令他们感到害怕的绝非人类本身，而是害怕人类将其坐标暴露给更高维度的文明。从这个角度来看，三体人的确没有说谎，甚至在有意无意的暗示人类。\n高效通信 vs 战略性\r三体人的 “脱水” 和 “浸泡” ，以程序员的视角来看，大约等同于序列化和反序列化。按照原著设定，当恒纪元到来的时候，三体人会通过 “浸泡” 的方式复活；当乱纪元到来的时候，三体人会通过 “脱水” 的方式冬眠。我认为，这里有一个非常深刻的话题，即探讨人的意识和肉体的关系。参照一般的理论，意识不但会认识到精神上的 “我” ，而且能认识到肉体上的 “我” ，并且还会把它们统一起来，构成这个世界上的 “我” 。那么，经历过 “脱水” 和 “浸泡” 这两道工序的三体人，是否还是原来的那个三体人呢？三体人经过 “脱水” 以后变成干纤维，肉体不复存在，意识又该何去何从？我们分辨一个人，到底是通过容貌、举止这些肉体上的特征，还是通过体验、感受这些意识上的特征？扩展到更大的层面上，公司和社会又分别通过什么来区分你？有时候，我们不得不接受来自自己或者时别人身上的变化，可能我们彼此的容貌没有任何变化，但我们从心理和情感上早已不再是当初的那个人，对此，你又将作何感想呢？我只知道，反序列化出的这个对象实例，肯定不是当初序列化时的那个对象实例。\n奥日系列游戏中设想的黑暗森林\r","date":"2023-02-18T15:49:47Z","image":"/posts/random-thoughts-of-three-body-world-as-programmers/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/random-thoughts-of-three-body-world-as-programmers/","slug":"Random-Thoughts-Of-Three-Body-World-As-Programmers","tags":["三体","程序员","科幻小说","技术思考"],"title":"程序员视角下的三体世界随想"},{"categories":["编程语言"],"content":"作为一名软件工程师，不，或许应该叫做 YAML 工程师、Markdown 工程师、Dockerfile 工程师……等等，这绝非自谦，更多的是一种自嘲。毕竟，从入行的那一天开始，追求配置上的动态灵活，就如同思想一般刻进每个程序员的 DNA 里。可当你意识到，在这个世界上，提出主张的人和解决问题的人，并不是同一群人时，你或许会心头一紧，接着便是直呼上当，我甚至不能理解，为什么程序员提交完代码，还要像运维一样折腾各种配置文件。特别是在 DevOps 的理念流行开以后，程序员们简直就是在通过各种配置文件互相折磨对方。如果程序员不能通过程序变得懒惰，那是不是说明，我们早已忘记了当初学习编程时的初心？我们都以为代码可以不用修改，可有哪一次代码能逃过面目全非的结局？每当这个时候，我就特别想怼那些主张配置文件的人，要不您来？言归正传，今天我想聊聊容器、配置文件和环境变量，为什么称为渐进式思考呢？因为它更像是一种不同人生阶段的回顾。\n从何说起 故老相传，鸿蒙初开，天地混沌。上帝说，要有光。于是，盘古抄起那把传说中的开天神斧，对着虚空世界就是一通输出。那一刻，这位创世神周围就像发生了奇点大爆炸一样迅速扩张。最终，它的身体化作了世间万物，推动这个世界从无到有的进化历程。屏幕前的你，无需纠结这段融合了东/西方神话、现代物理学的表述是否严谨，因为我想说的是，在一个事物发展的初期，一定是朴素而且原始的。相信大家开始写 Dockerfile 的时候，一定没少写过下面这样的脚本：\nCOPY /config/nginx.conf /etc/nginx/nginx.conf 如你所见，该命令会复制主机上的配置文件到容器的指定目录，而这其实是符合我们一开始对容器的预期的，即：我们只需要将程序打包到镜像里，就可以快速地完成程序的部署。可是，我们显然忽略了一个问题，当程序部署到不同的环境中时，它需要的配置文件自然是不同的。此时，你可能会采用下面的做法：\ndocker exec -it \u0026lt;容器Id\u0026gt; sh vim /etc/nginx/nginx.conf 环境变量 果然，大道至简，没有任何技巧，简直真诚到极致。常言道：智者不入爱河，这个做法辛不辛苦姑且不论，关键是容器一旦重启，你连慨叹镜花水月的时间都没有啦。所以，这个方案可谓是劳心劳力，为我所不取也！再后来，你发现容器里可以使用环境变量，于是你就灵机一动，为什么不能让这个配置文件支持动态配置呢？于是，你尝试使用下面的做法：\nserver { listen ${NGINX_PORT}; listen [::]:${NGINX_PORT}; server_name ${NGINX_HOST}; location / { root /usr/share/nginx/html; index index.html index.htm; } } 此时，我们只需要在 .env 文件或者 docker-compose.yml 文件里指定这些环境变量即可。对于这个思路，我们可以使用 envsubst 这个工具来实现：\nexport NGINX_PORT=80 export NGINX_HOST=xyz.com apt-get update \u0026amp;\u0026amp; apt-get install -y gettext-base envsubst \u0026lt; /config/nginx.conf \u0026gt; /etc/nginx/nginx.conf 此时，我们会发现，它可以实现环境变量的“注入”：\n环境变量的“注入”\r当然，如果这段脚本是写在 RUN 指令后面，那么，这个改进是非常有限的。因为如果你希望更新配置，你必须要重新构建一个镜像，一个更好的做法是，将这段脚本放到 CMD 或者 ENTRYPOINT 指令里。这样，我们更新配置时只需要重启容器即可，这是不是就符合配置上的动态灵活了呢？事实上，这正是博主公司一直采用的做法。不过，运维同事大概率是没听说过 envsubst 这个工具，他使用的是更朴素的 sed 命令：\necho -e \u0026#34;\\nReading all environment variables...\u0026#34; for line in $(printenv); do varname=$(echo $line | tr -dc \u0026#39;[:alnum:]=_\u0026#39; | cut -d\u0026#39;=\u0026#39; -f1) if [ \u0026#34;$varname\u0026#34; != \u0026#34;\u0026#34; ] then envval=$(eval \u0026#34;echo \\\u0026#34;\\$$varname\\\u0026#34;\u0026#34;) if [ \u0026#34;$envval\u0026#34; != \u0026#34;\u0026#34; ] then echo -e \u0026#34;Filling $varname into config templates...\u0026#34; escaped_envval=$(printf \u0026#39;%s\\n\u0026#39; \u0026#34;$envval\u0026#34; | sed -e \u0026#39;s/[\\/\u0026amp;]/\\\\\u0026amp;/g\u0026#39;) sed -i \u0026#34;s/{{ $varname }}/$escaped_envval/g\u0026#34; ./config-templates/* fi fi done 当然，我们最终还是放弃了这个方案，因为它增加了我们的维护成本，在开发和测试各有一套配置的前提下，再增加一套配置模板，同时还要保证这 3 套配置上的一致性，其难度可想而知。更重要的是，这套基于字符串替换的方案，支持的绑定语法非常有限，譬如不支持默认值或者是参数计算。如果某一个环境变量忘记配置，那么容器大概率是无法正常启动的。在放弃了配置模板的方案以后，我们不得不开始学习下面这套配置语法：\nexport Position__Title=Environment_Editor export Position__Name=Environment_Rick export Logging__0__Name ToEmail export Logging__0__Level Critical export Logging__0__Args__FromAddress MySystem@example.com export Logging__0__Args__ToAddress SRE@example.com export Logging__1__Name ToConsole export Logging__1__Level Information 这份由环境变量组成的配置信息，其等价表示为：\n{ \u0026#34;Position\u0026#34;: { \u0026#34;Title\u0026#34;: \u0026#34;Environment_Editor\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;Environment_Rick\u0026#34; }, \u0026#34;Logging\u0026#34;: [{ \u0026#34;Name\u0026#34;: \u0026#34;ToEmail\u0026#34;, \u0026#34;Level\u0026#34;: \u0026#34;Critical\u0026#34;, \u0026#34;Args\u0026#34;: { \u0026#34;FromAddress\u0026#34;: \u0026#34;MySystem@example.com\u0026#34;, \u0026#34;ToAddress\u0026#34;: \u0026#34;SRE@example.com\u0026#34; } }, { \u0026#34;Name\u0026#34;: \u0026#34;ToConsole\u0026#34;, \u0026#34;Level\u0026#34;: \u0026#34;Information\u0026#34; }] } 再后来，随着知识体系的不断完善，你发现容器内的配置文件可以挂载到主机上。此时，修改主机上的配置文件，就可以更新容器内的配置文件，其基本用法如下：\ndocker run -v \u0026lt;host-dir\u0026gt;:\u0026lt;container-dir\u0026gt;:\u0026lt;rw|wo\u0026gt; 或者，你可以使用 docker-compose.yml 来进行服务编排，然后通过 volumes 字段来挂载一个目录:\nversion: \u0026#39;3.5\u0026#39; services: nextcloud_web: build: ./ ports: - ${NEXTCLOUD_SERVER_PORT_HTTP}:80 - ${NEXTCLOUD_SERVER_PORT_HTTPS}:443 volumes: - \u0026#34;./data/nextcloud/config:/var/www/html/data/config\u0026#34; 那么，在这种模式下，是否就完美无瑕了呢？我个人觉得，这个方案对人的要求变得更高了一点，因为此前你只需要关注特定的环境变量即可，可现在你面对是一份完整的配置文件，你不得不了解每一种配置文件的细节，甚至于连 YAML 文件里的缩进都要关心，博主已经不止一次地帮别人处理开发环境 Envoy 配置文件的问题。除此之外，如果配置文件的结构频繁地发生变动，确保这两份配置文件步调一致，就再次变成了一个新的问题。\nDocker Config \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Hello Docker\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello {{ env \u0026#34;HELLO\u0026#34; }}! I\u0026#39;m service {{ .Service.Name }}.\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 事实上，在 Docker Swarm 的模式下，官方提供了一种机制来管理配置文件，它最大的优点是：不用挂载目录或者注入环境变量。如上图所示，假设我们有一个名为 index.html.tmpl 的模板文件。接下来，我们只需要创建一个配置并使用这个配置即可：\n# 创建一个名为 index-config 的配置 docker config create --template-driver golang index-config index.html.tmpl # 使用 index-config 配置启动容器 docker service create \\ --name hello-world \\ --env HELLO=\u0026#34;Docker\u0026#34; \\ --config source=index-config,target=/usr/share/nginx/html/index.html \\ --publish published=3000,target=80 \\ nginx:alpine K8S 与 ConfigMap 我不想评价 Docker Swarm 和 Kubernetes 孰优孰劣，我只知道，博主公司最终还是从 Docker Swarm 退回到了 Docker Compose 。对博主而言，现在主要使用 Docker Compose，我一直不愿意触碰 Kubernetes 的原因是，我不想再多学习一种配置文件。有人说， Kubernetes 是目前容器编排的事实标准，没有什么东西能在这个领域超越它。那么， Kubernetes 是如何管理容器内的配置的呢？答案是 ConfigMap：\napiVersion: v1 kind: ConfigMap metadata: name: app-info data: baseUrl: \u0026#34;http://xyz.com:8080 timeout: 86400 如图所示，这是一个最基本的 ConfigMap 示例，我们只需要通过下面的命令就可以创建一个 ConfigMap：\nkubectl create -f configmap.yaml 当然，它还有下面这些重载形式，可以从不同的数据源创建 ConfigMap ：\n# 从单个文件创建 kubectl create configmap *** --from-file=file1 # 从多个文件创建 kubectl create configmap *** --from-file=file1 --from-file=file2 # 从键值对创建 kubectl create configmap *** --from-literal=value1=123 --from-literal=value2=234 # 从环境变量创建 kubectl create configmap *** --from-env-file=.env 最终，这些 ConfigMap 可以作为 Pod 级别的卷或者是在环境变量中被引用。请注意，博主并不打算动手去写一个 K8S 示例，因为比做一件事情更重要的事情是，知道为什么要做这件事情。你也许完全想象不到，我当初对环境变量这个方案是有多抗拒，因为我觉得明明配置中心更好用啊，这些配置文件你全都放到配置中心里，届时容器只需要从配置中心里拉取配置即可。我承认，这个设想非常美好，可惜对于技术选型这些事情来说，有时候，它并不是一道选择题，而是一道填空题。\n本文小结 圣人有云：温故而知新，可以为师矣！通过这一系列的梳理，我们可以得出一个结论，即：容器内的配置管理，唯一正确的方向就是让配置和容器分离。不管你是用上面这些方案中的哪一种，其关键就是让配置动态地在容器内生效，而非固化到容器中。可有意思的是，K8S 里的 ConfigMap 是可以设置为 Immutable 的，这就引申出一个关乎平衡的哲学命题，变与不变该如何去抉择，一个人想要在这个世界上安身立命，到底是要遵从本性、初心不改？还是要世故圆滑、适者生存？也许，到底还是古人更聪明一点，讲究一个外圆内方。从这个角度来看的话，古代的钱币被铸成外圆内方的形制，大抵有刻意为之的嫌疑。毕竟啊，无论是在哪个朝代，一心只想搞钱这种想法都显得真实、不做作，这同样是我对于技术的态度。好了，以上就是这篇文章的全部内容，欢迎大家在评论区交换想法！\n","date":"2022-12-01T12:30:47Z","image":"/posts/progressive-thinking-about-docker-container-configuration-information/spiral-g6a266c317_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/progressive-thinking-about-docker-container-configuration-information/","slug":"Progressive-Thinking-About-Docker-Container-Configuration-Information","tags":["容器","配置","Docker","Bash"],"title":"关于 Docker 容器配置信息的渐进式思考"},{"categories":["编程语言"],"content":"有时候，我们需要在容器内执行某种定时任务。譬如，Kerberos 客户端从 KDC 中获取到的 TGT 默认有效期为 10 个小时，一旦这个票据失效，我们将无法使用单点登录功能。此时，我们就需要一个定时任务来定时刷新票据。此前，博主为大家介绍过 Quartz 和 Hangfire 这样的定时任务系统，而对于 Linux 来说，其内置的 crontab 是比以上两种方案更加轻量级的一种方案，它可以定时地去执行 Linux 中的命令或者是脚本。对应到 Kerberos 的这个例子里面，从 KDC 申请一个新的票据，我们只需要使用 kinit 这个命令即可。因此，在今天这篇博客里，我想和大家分享一下，如何在 Docker 容器内集成 Crontab 定时任务，姑且算是在探索 Kerberos 过程中的无心插柳，Kerberos 认证这个话题博主还需要再消化一下，请大家拭目以待，哈哈！\nCrontab 基础知识 众所周知，Linux 中的所有内容都是以文件的形式保存和管理的，即：一切皆为文件。那么，自然而然的地，Linux 中的定时任务同样遵循这套丛林法则，因此，当我们谈论到在 Linux 中执行定时任务这个话题的时候，本质上依然是在谈论某种特定格式的文件。事实上，这类文件通常被称为 crontab 文件，这是一个来源于希腊语 chronos 的词汇，其含义是时间。Linux 会定时(每分钟)读取 crontab 文件中的指令，检查是否有预定任务需要执行。下面是一个 crontab 文件的示例：\n# 每分钟执行一次 ls 命令 * * * * * /bin/ls # 周一到周五的下午5点发邮件 0 17 * * 1-5 mail -s \u0026#34;hi\u0026#34; alex@162.com # 每月1号和15号执行脚本 0 0 1,15 * * /var/www/newbee/check.sh # 00:20、02:20、04:20...执行 echo 命令 20 0-23/2 * * * echo \u0026#34;hello\u0026#34; 可以注意到，crontab 文件就像是一份写给 Linux 的日程表，它会告诉 Linux 每个时刻应该做什么样的事情。虽然这些事情看起来都显得琐碎繁复，甚至都精确到了每一分钟，可如果我们观察得足够仔细的话，就会发现这些定时任务都可以用下面的形式来表示，即：\n* * * * * \u0026lt;program\u0026gt; 其中，\u0026lt;program\u0026gt; 可以是一个命令或者脚本，而对于 \u0026lt;program\u0026gt; 前面的这部分，我们通常将其称为 cron 表达式，其含义定义如下：\n* * * * * - - - - - | | | | | | | | | +----- 星期(0 - 6) | | | +---------- 月份(1 - 12) | | +--------------- 日期(1 - 31) | +-------------------- 小时(0 - 23) +------------------------- 分钟(0 - 23) 例如，49 19 24 11 * 这串神秘代码表示的是：11 月 24 日 19 时 49 分，希望历史可以记住这一天。在此基础上，当第一位为 * 时，表示每分钟都执行 \u0026lt;program\u0026gt; ；当第一位为 a-b 时，表示在 a 分钟到 b 分钟这段时间内执行 \u0026lt;program\u0026gt; ；当第一位为 */n 时，表示每隔 n 分钟执行一次 \u0026lt;program\u0026gt; ；当第一位为 a,b,c\u0026hellip; 时表示第 a、b、c\u0026hellip;分钟时执行一次 \u0026lt;program\u0026gt; 。依次类推，我们可以在不同的时间单位上使用这些表达式。例如：\n# 每周六晚上00:00 0 0 * * 6 # 每周六和周天晚上00:00 0 0 * * 0,6 # 每周六和周天晚上00:00 ~ 04:00 0 0-4 * * 0,6 # 每周六和周天晚上00:00 和 01:00 执行 0 0,1 * * 0,6 # 每周六和周天晚上每隔两个小时执行一次 0 */2 * * 0,6 现在，回到我们一开始的问题，如何通过定时任务来解决 Kerberos 票据过期的问题呢？首先，我们准备一个名为 renew.sh 的脚本，这是一个非常简单的脚本，无论你是否接触过 Kerberos，是否了解 Principal 或是 SPN 这些概念，你都可以轻而易举地上手：\necho [$(date)] \u0026#39;request a new ticket for HTTP/web.your-domain.com@YOUR-DOMAIN.COM\u0026#39; kinit -kt /etc/apache2/krb-container.keytab HTTP/web.your-domain.com@YOUR-DOMAIN.COM 接下来，我们只需要在真正的 crontab 文件里引用这个脚本即可，从上面的表达式定义出发，我们可以知道，这个定时任务每隔 10 分钟执行一次：\ncronfile=\u0026#39;/usr/crontab/cron.conf\u0026#39; renewTicket=\u0026#39;*/10 * * * * /usr/crontab/renew.sh\u0026#39; echo \u0026#34;$renewTicket\u0026#34; | tee -a $cronfile crontab $cronfile /etc/init.d/cron reload /etc/init.d/cron restart 这里运用到的第一个技巧是 crontab 命令，它可以生成、编辑、删除、列举定时任务：\n# 生成定时任务 crontab \u0026lt;-u user\u0026gt; file # 编辑定时任务 crontab \u0026lt;-u user\u0026gt; -e # 删除定时任务 crontab \u0026lt;-u user\u0026gt; -r # 列举定时任务 crontab \u0026lt;-u user\u0026gt; -l 在本文的示例中，博主没有指定 -u 这个参数，这是因为博主使用的是 root 用户。事实上，在指定了编辑器的情况下，你可以使用 -e 参数通过交互式的命令行来编写定时任务：\n# 使用 vim 作为 crontab 的编辑器 export EDITOR=\u0026#34;/usr/bin/vim\u0026#34; crontab -e 此时，我们就可以通过 vim 修改这份 crontab 文件。相信此时此刻，你会有一种感觉，这个 crontab 文件不就是一份由 cron 表达式组成的日程表吗？事实上，Linux 系统会维护一份 crontab 文件，我们自己编写的 crontab 文件会在执行 crontab 命令后合入到这份 crontab 文件里：\n通过命令行交互式编辑定时任务\r需要注意到的是，当我们在 Docker 容器内执行定时任务时，需要确保生成定时任务的这部分脚本，在容器的入口位置被执行，简而言之，我们应该有一个名为 entrypoint.sh 的脚本，并用它来替代容器的默认入口：\nCMD [\u0026#34;sh\u0026#34;, \u0026#34;/usr/docker/entrypoint.sh\u0026#34;] 一旦定时任务被创建出来，我们总是可以使用 crontab -l 命令列举出当前用户下有哪些定时任务：\n显示所有的定时任务\rCrontab 日志问题 在这个过程中，博主发现一个问题，那就是看不到这些定时任务的执行日志。通常，这些日志位于以下路径：/var/log/cron.log，我们可以使用下面的命令：\ntail -f /var/log/cron.log 此时，我们会得到下面的错误，显然，这是日志没有写入的缘故：\n无法查看定时任务日志\r在尝试安装 rsyslog 以及修改配置启用定时任务日志后，该问题依然存在：\nsudo apt-get install -y rsyslog; sudo vim /etc/rsyslog.d/50-default.conf; 有趣的是，这个做法在 Ubuntu 最新版本中是生效的，而两者的区别是：前者是容器，后者是虚拟机。\n查看定时任务日志\r吾上下求索而不得，最终采用下面的方案来解决，即直接重定向脚本输出到容器的标准输出：\necho [$(date)] \u0026#39;Hello\u0026#39; \u0026gt; /proc/1/fd/1 此时，我们就可以在容器中看到对应的日志，虽然这只是一个小问题，可个人感觉还是挺折腾人的：\n重定向输出到容器日志\rCrontab 环境变量 如果你希望在定时任务脚本中引用环境变量，例如：\necho [$(date)] \u0026#39;request a new ticket for HTTP/\u0026#39;$NEXTCLOUD_SERVER_NAME\u0026#39;@\u0026#39;$DOMAIN_SERVER_NAME \u0026gt; /proc/1/fd/1 kinit -kt /etc/apache2/krb-container.keytab HTTP/$NEXTCLOUD_SERVER_NAME@$DOMAIN_SERVER_NAME 此时，你会注意到容器输出的日志中，这些环境变量的值都是缺失的，虽然这些环境变量确实存在：\n定时任务中引用环境变量-A\r如果你求助于搜索引擎，大概虑会得到下面的答案：\n#!/bin/sh . /etc/profile . ~/.bash_profile 事实上，这个方案在主机环境下没有问题。如果是在容器环境下，建议使用下面的方案：\nenv \u0026gt;\u0026gt; /etc/default/locale 此时，就可以达成我们预期的效果：\n定时任务中引用环境变量-B\r参考链接 如何在 Docker 中执行 Crontab How to Setup Rsyslog Server on Debian 11 (Bullseye) Understanding Crontab in Linux With Examples What Is Cron Job ","date":"2022-11-24T12:30:47Z","image":"/posts/integrate-crontab-scheduled-tasks-inside-docker-containers/russian-gd53d0982f_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/integrate-crontab-scheduled-tasks-inside-docker-containers/","slug":"Integrate-Crontab-Scheduled-Tasks-Inside-Docker-Containers","tags":["容器","Linux","Docker","Bash"],"title":"在 Docker 容器内集成 Crontab 定时任务"},{"categories":["编程语言"],"content":"回顾我这些年的工作经历，面向企业(2B)和面向用户(2C)的项目都曾接触过。我个人觉得，面向企业的项目更注重业务，参与决策的人数多、周期长，目的是为企业提供生产经营价值，如缩减成本、提升效率等等，而面向用户的项目更注重体验，参与决策的人数少、周期短，目的是为消费者提供更多的使用价值，本质上是为了圈揽用户和抢夺流量。我在参与这些项目的过程中发现，企业级应用的研发更注重与第三方软件如 SAP、金蝶、用友、ERP 等等的整合，因此，类似单点登录、数据同步这样的需求非常普遍。每当这个时候，我就不由地想起一位前辈。\n时间就像沙漏里的沙一样流逝\r当我还在 Automation 打杂的时候，前辈总是一脸得意地问我：“听说过 AD Domain 吗？”。那时，初出茅庐的我年少轻狂，不好意思说我不会，立马敷衍道：“当然听说过，只是一直没用过”。前辈目光如炬，大抵是看出我心虚，立马不屑一顾地回应道：“那就是不会”。过了几秒钟，前辈不紧不慢地接着说道：“只有学会了 AD Domain，你才算是一只脚踏进了企业级应用开发这个领域，知道吗？”，我点了点头，心道：“这不就和茴香豆的茴字有五种写法一样无聊吗？”。多年后，当 LDAP 这个字眼再次映入眼帘的时候，我内心终于清楚地知道：我错了。\n为什么需要 LDAP 认证 我错在哪里了呢？我想，要回答这个问题，还是需要从企业管理的角度来着手。一个面向用户(2C)的产品，其用户基本上是不受地域因素限制的，而对于一个面向企业(2B)的产品，其用户基本上是在一个层次分明、有着明显边界的范围内。运营一个企业，除了业务系统以外，可能还需要 OA、财务、ERP 等等外围软件的支持，如果是一家互联网公司，可能还需要 DevOps、监控、协作等等方面的支撑。此时，从企业的角度自然是希望可以统一账号体系，这样就衍生出了各种各样的单点登陆和认证方案，单单是博主接触过的就有：OAuth2、CAS、Keycloak、IdentityServer4，这些方案可以说是各有千秋，此中曲折我们按下不表。\n运行在 Windows Server 上的 AD\r这里博主想说的是，一旦企业通过 AD Domain 或者说 Active Directory 来管理用户，就自然而然地牵扯出域登录或者域账号登录的问题。这类围绕 AD Domain 或者说域的问题，我们都可以考虑使用 LDAP 认证或者 Kerberos 认证，特别是后者，主流的软件如 Kafka、Zookeeper、MySQL 等等均支持这一协议，它可以实现在登录本地账户后，免登录打开一个网站的效果。可想而知，这是一个对企业而言极具诱惑力的特性，一个账号打通所有基础设施。当然，我承认 Kerberos 这个协议是非常复杂的，绝非三言两语可以厘清其中的千丝万缕，所以，我们今天只是聊聊 LDAP 认证这个话题。\n通过 LDAP Browser 访问 AD\r可能大家会纠结，LDAP 和 Active Directory 这两者间的关系，事实上， LDAP 是指轻量目录访问协议(Lightweight Directory Access Protocol)，而 Active Directory 则是微软针对该协议的一种实现。当然，微软为了解决域控的问题，利用 LDAP 存储了一部分私有的数据。所以，两者的关系就像是接口和实现类，我们这里只需要 Active Directory 当成一台 LDAP 服务器即可。关于 Active Directory 的基础知识，这里不再做更多的科普。总而言之，通过 LDAP 我们可以对某个网站实现认证，从而达到保护资源的目的。譬如博主目前参与的前端项目，它是没有常规的登录、注册页面的，它采用的就是域账号登录的形式。下面，我们来看看如何集成 LDAP 认证。\n如何集成 LDAP 认证 结合博主在上文中描述的场景，假设我们有一个前端项目通过 Nginx 或者 Apache 进行托管。通常情况下，我们可以直接访问这些前端页面，这意味这些资源是不受任何保护的。对企业来说，它更希望将这些资源保护起来，以确保只有它的员工或者说加入域的用户才可以访问，此时，我们该如何解决这个问题呢？\nNginx 篇 以 Nginx 为例，我们可以通过 nginx-auth-ldap 这个模块来解决这个问题。下面是一个简单的示意图，其基本思路是：在输入用户名和密码后，该模块会连接 LDAP 服务器对用户身份进行校验，如果校验通过，则会以 Basic 认证的的方式生成 Authorization 请求头；反之，将会返回 401 状态码，表示认证失败。\nNginx 集成 LDAP 示意图\r注意到，这是一个第三方的模块，不管你是通过 Docker 或者主机来部署 Nginx，它都不会包含这个模块，此时，我们就需要为 Nginx 安装这个模块。因为 Nginx 采用的是静态编译的策略，所以，安装模块本质上就是同时拉取 Nginx 和模块的源码，然后重新编译生成二进制文件的过程。首先，我们来下载该模块的源码：\ngit clone https://github.com/kvspb/nginx-auth-ldap.git /usr/src/nginx-auth-ldap/; 接下来，我们需要下载 Nginx 源码，无论你是在全新安装的 Nginx 上安装模块，还是在一个安装好的 Nginx 上安装模块，这一步都是必须的，千言万语汇成一句话：Nginx 采用的是静态编译的策略。博主这里是在 nginx:stable-alpine 这个镜像的基础上安装模块：\nwget http://nginx.org/download/nginx-1.22.1.tar.gz tar -zxvf nginx-1.22.1.tar.gz cd nginx-1.22.1 接下来，如果是全新安装 Nginx，那么，你可以像下面这样列出常用的模块，然后用 --add-module 参数指向当前模块的路径：/usr/src/nginx-auth-ldap/ 。请注意，下面的参数经过简化，并不代表实际使用的参数：\n./configure \\ --prefix=/etc/nginx \\ --sbin-path=/usr/sbin/nginx \\ --with-http_addition_module \\ --with-http_auth_request_module \\ --add-module=/usr/src/nginx-auth-ldap/ make \u0026amp;\u0026amp; make install 如果是安装好的 Nginx，那么，这个参数就必须从当前的 Nginx 上继承过来，否则，已经安装好的模块将不会参与编译。原本博主非常喜欢 Nginx ，可这一条博主实在不能容忍，这完全是在用户面前摆烂，难道使用了哪些了模块你心里没点数吗？非要逼着用户去帮你维护这些配置信息？这些又臭又长的参数真的不考虑做持久化吗？吐槽完 Nginx，我们继续来填坑，为了获取 Nginx 当前的配置参数，你可以使用 nginx -V 这个命令(注意：这个 V 必须要大写)，此时，我们就可以得到下面的答案：\n获取 Nginx 配置参数\r这意味着什么呢？这意味着你需要把红框里的这一堆参数放到 ./configure 后面，然后再像上面一样添加 --add-module 参数，这简直是造孽啊！为了简化这个过程，我改进了一下脚本：\n# 匹配 configure arguments ，然后再从第 21 个字符开始截取，即冒号后面的所有内容 NGINX_CONFIG=$(nginx -V 2\u0026gt;\u0026amp;1 | grep \u0026#39;configure arguments\u0026#39; | cut -c21-) ./configure $NGINX_CONFIG --add-module=/usr/src/nginx-auth-ldap/ make \u0026amp;\u0026amp; make install 当然，如果你亲自折腾过这一切，就知道这是痴心妄想，因为提取出来的这组参数会提示各种各样的错误，总之，你需要按照实际的情况来对这组参数就行调整，看着 Dockerfile 里比裹脚布还要长的参数，Nginx 官方难道你们不会心痛吗？当然，当你熬过这一切以后，我们就可以着手 Nginx 的配置啦：\nhttp { # 定义 LDAP 服务器 ldap_server ldap { # LDAP 服务器地址，使用 sAMAccountName 还是 uid 以实际的 AD 配置为准 url ldap://\u0026lt;IP\u0026gt;:389/OU=OPS,DC=company,DC=com?sAMAccountName?one; # 管理员账号, CN/OU/DC 以实际的 AD 配置为准 binddn \u0026#34;CN=\u0026lt;Administrator\u0026gt;,OU=OPS,DC=company,DC=com\u0026#34;; # 管理员密码 binddn_passwd \u0026#34;\u0026lt;Password\u0026gt;\u0026#34;; group_attribute uniquemember; group_attribute_is_dn on; require valid_user; } server { listen 80; server_name localhost; location / { # 启用 LDAP 认证 auth_ldap \u0026#34;Forbidden\u0026#34;; auth_ldap_servers ldap; root /usr/nginx/wwwroot; index index.html; } } } 可以注意到，我们只需要按实际域控来配置 ldap_server 节点，然后为受保护的资源启用 LDAP 认证即可。此时，如果访问前端页面，浏览器将会提示输入用户名和密码：\nNginx 集成 LDAP 效果演示-1\r如果我们输入的用户名或者密码不正确会怎么样呢？浏览器将会孜孜不倦地提示你输入用户名和密码。如果我们点击取消会怎么样呢？此时，如下图所示，它将会返回 401 状态码，表示认证失败：\nNginx 集成 LDAP 效果演示-2\r此外，我们可以注意到，一旦认证通过，我们就可以正常访问前端页面。与此同时，所有的请求都会自动带上 Authorization 请求头，显然，这是一个 Basic 认证：\nNginx 集成 LDAP 效果演示-3\rNginx 集成 LDAP 效果演示-5\r至此，我们就实现了 Nginx 下的 LDAP 认证集成。\nApache 篇 虽然 Nginx 比 Apache 更轻量、社区更活跃，可是在模块管理这方面，Apache 是完全吊打 Nginx 的，类似地，Apache 通过 mod-authnz-ldap 这个模块来实现 LDAP 认证的集成，并且这个模块有二进制包，可以直接通过包管理器来安装，从这方面来看，Nginx 完全不如 Apache ：\n# 安装模块 apt-get update; apt-get install -y libldap2 mod_ldap mod-authnz-ldap; # 启用模块 a2enmod ldap; a2enmod authnz_ldap; 接下来，我们只需要修改一下 Apache 的配置文件即可：\n\u0026lt;VirtualHost *:80\u0026gt; ServerName localhost ServerAdmin webmaster@localhost DocumentRoot /var/www/html LogLevel debug ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined \u0026lt;Location \u0026#34;/\u0026#34;\u0026gt; AuthType Basic AuthName \u0026#34;LDAP SSO\u0026#34; AuthBasicProvider ldap AuthLDAPBindDN \u0026#34;CN=\u0026lt;Administrator\u0026gt;,OU=OPS,DC=company,DC=com\u0026#34; AuthLDAPBindPassword \u0026#34;\u0026lt;Password\u0026gt;\u0026#34; AuthLDAPURL ldap://\u0026lt;IP\u0026gt;:389/OU=OPS,DC=company,DC=com?sAMAccountName?one Require valid-user \u0026lt;/Location\u0026gt; \u0026lt;/VirtualHost\u0026gt; 可以注意到，两者的配置是非常相似，可以实现相同的效果，这里就不再详细展示啦！\nBackend 篇 在前面的示意图里，我们看到还有 Backend 这样一个环节。其实，很多时候，我觉得企业级应用的开发非常别扭，因为它总是在现代中透着些古板，说它现代是因为它在跟进微服务、前后端分离、容器化这些流行趋势，说它古板是因为它总在尝试用新技术做旧时代的东西。譬如，前后端分离以后，JWT 是最常见的认证方式，在这种模式下，服务就应该是无状态的，可实际开发中它还是会搞出来一个 Session 的概念，难道现在还是汤姆猫的时代吗？这个域账号登录的想法确实不错，可它和现在主流的 JWT 是不兼容的，除非在服务器端实现了 Basic 认证。下面是 ASP.NET Core 中实现 Basic 认证的一个简单流程：\nasync Task Invoke(HttpContext context) { try { // 1.提取用户信息 var header = context.Request.Headers[\u0026#34;Authorization\u0026#34;] var authHeader = AuthenticationHeaderValue.Parse(header); var authParams = Convert.FromBase64String(authHeader.Parameter); var credentials = Encoding.UTF8.GetString(authParams).Split(\u0026#39;:\u0026#39;, 2); var username = credentials[0]; var password = credentials[1]; // 2.验证用户信息 var user = await _userService.Authenticate(username, password); if (user != null) { // 3.指定当前用户 var claims = new[] { new Claim(ClaimTypes.NameIdentifier, user.Id.ToString()), new Claim(ClaimTypes.Name, user.UserName), }; var identity = new ClaimsIdentity(claims, \u0026#34;Basic\u0026#34;); var principal = new ClaimsPrincipal(identity); context.User = principal; } } catch {} await _next(context); } 可以看到，Basic 认证的过程其实就是从 Authorization 请求头里提取用户信息并进行验证的过程，博主这里是以中间件的形式来进行说明。按照一般的做法，你可能需要继承 AuthenticationHandler 并重写 HandleAuthenticateAsync() 和 HandleChallengeAsync() 这两个方法，思路其实是完全一样的，这里就不再详细展开说啦！总而言之，如果前端想结合 LDAP 做身份认证，需要后端提供相应的支持。当然，如果你可以在请求传入到后端前自动产生一个 JWT 令牌，那再好不过啦！\n本文小结 本文分享了 Nginx 和 Apache 结合 LDAP 实现身份认证的过程，背景则是企业级应用开发过程中对单点登陆、域账号登录的诉求。在这个过程中，博主对 Nginx 和 Apache 这两款服务器的差异有了更深刻的认识。考虑到 Nginx 采用的静态编译的策略，因此，给 Nginx 安装模块，本质上就是重新编译 Nginx 的过程，这个过程可谓是痛并快乐着，就像你在文章里看到的那样，博主已经不止一次吐槽过 Nginx 这个奇葩的模块管理方式。相比之下，Apache 的舒适感简直爆棚，因为它的模块都可以直接通过包管理器来安装。当我们为网站集成了 LDAP 认证以后，它会在打开站点时提示输入用户名和密码，一旦身份验证通过，Nginx 或者 Apache 会自动地为每个请求生成 Authorization 请求头，考虑到这个认证方式和目前主流的 JWT 认证不兼容，因此，这个方案需要后端实现 Basic 认证。在此基础上，博主提供了一个 ASP.NET Core 实现 Basic 认证的示例。有趣的是，虽然 Kerberos 协议被指出存在安全漏洞，可它还是在用 Basic 认证来传递用户信息，那么，你告诉我，企业应用需要的这种认证方案，到底是好是坏呢？\n","date":"2022-11-15T12:49:47Z","image":"/posts/integrate-ldap-authentication-for-your-server/fantasy-ga56af2520_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/integrate-ldap-authentication-for-your-server/","slug":"Integrate-LDAP-Authentication-For-Your-Server","tags":["Apache","Nginx","LDAP","认证"],"title":"为你的服务器集成 LDAP 认证"},{"categories":["生活感悟"],"content":" 周末独自前往大唐芙蓉园，心中盘桓已久的想法，于此时此地显得不合时宜，因为无论是指渐渐转凉的天气，还是指此消彼长的疫情，都俨然有一点沉默和突兀。更不必说，冬天的雾霾会让整个城市呼吸困难，蓝色的口罩像一张巨网，将相关或者不相关的人们笼络于其中。当年，唐太宗实行科举后看见士子进入考场，不由得感叹一句，“天下英雄尽入吾彀矣”。如今，这个世界被更大的网连接在一起，穿梭在曾经的皇家园林里，零零星星的几棵银杏树，一不小心就成为了此时此地最好的装饰。我不得不承认，此番我是特意来看银杏树叶的。可愈是人多的地方，人就愈是想要成为一切事物的中心。于是，无暇自拍的路人，在一众或专业或业余的摄影师眼里，被迫变成某种陪衬。按照纳什均衡理论，如果每个人都想在最好的位置、最好的角度拍照，那么每个人都无法在最好的位置、最好的角度拍照。可惜理论终究抵不过人心，毕竟谁会甘心落后于人呢？\n不太真实的美\r近来这几天一直在玩 《Stray》 ，对于我这个喜欢猫的人而言，这款猫猫模拟器远比赛博朋克、末日未来这些概念要更吸引人。作为一款独立游戏，其体量可以说是非常地克制，我只用了七个多小时就通关了游戏，如果从艺术性的角度来评价这款游戏，在我心中恐怕只有 《机械迷城》 和 《风之旅人》 可以相媲美，因为它们的故事基本上都没有人类参与，可这些故事里的草灰蛇线始终都有人类的影子。换句话说，当你身处于一个充满霓虹灯和机器人的科幻世界的时候，你会发现这其实是在以猫的视角讲述人类的故事。为什么这样说呢？陪伴着主角的那只无人机 B-12，起初它以为自己是个机器人，随着一路收集的记忆越来越多，渐渐地它意识到自己曾经是人类、是一名科学家，直到在控制室里解锁了全部的记忆，它终于想起了自己的外界者身份，最终在主角的帮助下成功打开穹顶，即使这份成功背后的代价是牺牲自我。\n在控制室打开穹顶的瞬间\r当巨大而沉重的穹顶一层层的打开，猫咪端坐在透明的落地窗前，俯视着这一路走过的贫民窟和中城，菌克在阳光的照耀下被消灭，机器人们终于得以重见光明。我认为这是一种隐喻，B-12 的牺牲更像是一种救赎，一种为了文明的延续和传承而牺牲自我的救赎。当屏幕上打出，“抱歉不能和你一起看到外界了”，我不禁感到怅然若失，因为这不不单单是 B-12 与我的告别，更是这个游戏与我的告别。猫咪最终回到了地面，可这个地面上早就没有人类了，甚至连一开始的那几只猫都没有再出现过。细细回想起来，我在这个游戏里最快乐的时刻，居然是在蚁村这个毫无存在感的章节，那里有两个正在打麻将的机器人，我最喜欢做的事情是故意把麻将打翻在地，然后看着它们一遍遍地从地上捡起麻将，我想说，实在有趣！单独讨论各个章节的话，我感觉制作组还是把主要精力放在了城镇上，蚁村实在太空洞了，而下水道里则会让人感到生理不适，谁能想到最后一次使用完镭射灯，猫咪捡起无人机狂奔这一段居然是我心中的最佳演出。\n蚁村里打麻将的两个机器人\r某天下午午休的时候，我梦见自己变成了一只猫，穿过家里的老房子，我看见父亲和弟弟正在院子里干活，我想和他们说话，可谁会注意到一只小猫咪呢？我疑心这是通关 Stray 以后的后遗症，毕竟我早已习惯了像一只猫一样仰起头、上蹿下跳、左右腾挪。我没有去看周公解梦，我更喜欢某种科学上的解释。思来想去，我觉得我大抵是想家了罢，毕竟，从去年国庆假算起，我已经有一年多时间没有回家了，每当我想回家的时候，疫情就来凑热闹。国家卫健委最近发布了二十条，我不知道过年能不能回去。如鲁迅先生那句名言，“希望是本无所谓有，无所谓无的”，有或者无，不过是唯心而已。某一瞬间，当我突然想起什么的时候，我会期待自己能立马将它们写下来，就像此时此刻的这些文字一样，似花还似非花。这三年来我们用于新冠病毒上的钻研属实有限，毕竟我们就只有隔离、核酸、健康码这三板斧可用，联想到最近有城市推出核酸采集的年卡，我心中/口中已不愿再多说一个字。\n这一幕实在太像刺客信条啦！\r同样是以杨花作喻，苏轼写下这阙《水龙吟》的时候，想到是随风万里的飘摇不定，而百年前的东晋才女谢道韫，想到的则是雪满人间的轻盈皎洁。可是，灞桥的杨柳依依，李白的杨花落尽，又分明是同一种事物。我曾经感慨过新版 《倚天屠龙记》 里四女同舟的桥段，在审美越发趋于同质化、流水线的今天，我彻彻底底地变成一个脸盲症患者。等到如今再看 《天下长河》 的时候，虽然还是康熙、明珠、索额图这些熟悉的名字，可我始终不免旧疾复发。我想，脸盲症大抵是不区分颜值或是性别的，真正让你觉得似花非花的，是这个时代拼命想让所有人变得一样的“排异”心理。自从马斯克收购推特以后，他的话题热度就再没有下降过，只是这一次，让我们记住他的不再是火箭和特斯拉，而是裁员和加班。我有时候会怀疑，这个一直在追寻星辰大海的男人，到底是一个不折不扣的天才，还是一个彻头彻尾的资本家？也许，这两个都是呢？也许，似花非花、雾里看花的世界更显得真实一点……\n","date":"2022-11-11T12:30:47Z","image":"/posts/like-flowers-not-flowers/hd-wallpaper-g4698ef087_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/like-flowers-not-flowers/","slug":"Like-Flowers-Not-Flowers","tags":["生活","随笔","感悟","日常"],"title":"似花还似非花"},{"categories":["编程语言"],"content":"本文是 #视频是不能 P 的系列# 的第三篇。此前，我们已经可以通过 OpenCV 或者 Dlib 实现对人脸的检测，并在此基础上实现了某种相对有趣的应用。譬如，利用人脸特征点提取面部轮廓并生成表情包、将图片中的人脸批量替换为精神污染神烦狗 等等。当然，在真实的应用场景中，如果只是检测到人脸，那显然远远不够的，我们更希望识别出这张人脸是谁。此时，我们的思绪将会被再次拉回到人脸识别这个话题。在探索未知世界的过程中，博主发现 OpenCV 自带的 LBPH 方法，即局部二值模式直方图方法，识别精度完全达不到预期效果。所以，博主最终选择了 Dlib 里的特征值方法，即：对每一张人脸计算一个 128 维的向量，再通过计算两个向量间的欧式距离来判断是不是同一张人脸。在此基础上，博主尝试结合 支持向量机 来实现模型训练。因此，这篇文章其实是对整个探索过程的梳理和记录，希望能给大家带来一点启发。\n原理说明 如下图所示，假设对于每一个人物 X ，我们有 N 个人脸样本，通过 Dlib 提供的 compute_face_descriptor() 方法，我们可以计算出该人脸样本的特征值，这是一个 128 维度的向量。如果我们对这些人脸样本做同样的处理，我们就可以得到人物 X 的特征值列表 feature_list_of_person_x。在此基础上，利用 MumPy 中的 mean() 方法，我们就可以计算出人物 X 的平均特征 features_mean_person_x。最终，我们把人物 X 的平均特征和名称一起写入到一个 CSV 文件里面。至此，我们已经拥有了一个简单的人脸数据库。\nDlib 人脸识别原理说明图\r可以预见的是，一旦我们把人脸特征数值化，那么，人脸识别就从一个图形学问题变成了数学问题。对于图中的待检测人脸，我们只需要按同样地方式计算出特征值，然后从 CSV 文件中找一个距离它最近的特征即可。这里，博主使用的是欧式距离，并且人为规定了一个阈值 0.4， 即：当这个距离小于 0.4 时，我们认为人脸匹配成功；当这个距离大于 0.4 时，我们认为人脸匹配失败。下面的例子展示了使用 Dlib 计算人脸特征值的基本过程：\ndetector = dlib.get_frontal_face_detector() predictor = dlib.shape_predictor(\u0026#39;shape_predictor_68_face_landmarks.dat\u0026#39;) face_reco_model = dlib.face_recognition_model_v1(\u0026#34;dlib_face_recognition_resnet_model_v1.dat\u0026#34;) # 计算特征值 image = Image.open(\u0026#39;/faces/person/sample-0.jpg\u0026#39;) image = cv2.cvtColor(np.asarray(image), cv2.COLOR_RGB2BGR) faces = detector(image, 1) if len(faces) != 0: face = faces[0] shape = predictor(image, face) face_descriptor = face_reco_model.compute_face_descriptor(image, shape) 此时， face_descriptor 就是当前人脸的特征值，这是一个 128 维的向量。我们只需要对每一个人脸样本重复以上步骤，就可以获得人物 X 的特征值列表，进而计算出人物 X 的特征均值。\n实现过程 如下图所示，我们为每一个人物建立了一个文件夹，文件夹的名称即为对应人物的名称：\n人脸样本的目录结构\r显然，对于每一个人物而言，我们只需要遍历该文件夹下的所有图片，即可计算出它的特征均值：\ndef get_mean_features_of_face(path): path = os.path.abspath(path) subDirs = [os.path.join(path, f) for f in os.listdir(path)] subDirs = list(filter(lambda x:os.path.isdir(x), subDirs)) for index in range(0, len(subDirs)): subDir = subDirs[index] person_label = os.path.split(subDir)[-1] image_paths = [os.path.join(subDir, f) for f in os.listdir(subDir)] image_paths = list(filter(lambda x:os.path.isfile(x), image_paths)) feature_list_of_person_x = [] for image_path in image_paths: # 计算每一个图片的特征 feature = get_128d_features_of_face(image_path) if feature == 0: continue feature_list_of_person_x.append(feature) # 计算当前人脸的平均特征 features_mean_person_x = np.zeros(128, dtype=object, order=\u0026#39;C\u0026#39;) if feature_list_of_person_x: features_mean_person_x = np.array(feature_list_of_person_x, dtype=object).mean(axis=0) yield (features_mean_person_x, person_label) 其中，get_128d_features_of_face() 方法用来计算某个图片的特征值：\ndef get_128d_features_of_face(image_path): image = Image.open(image_path) image = cv2.cvtColor(np.asarray(image), cv2.COLOR_RGB2BGR) faces = detector(image, 1) if len(faces) != 0: shape = predictor(image, faces[0]) face_descriptor = face_reco_model.compute_face_descriptor(image, shape) else: face_descriptor = 0 return face_descriptor 好了，当我们计算出每一个人物的特征均值以后，我们需要把当前人物的名称、特征均值一起写入到 CSV 文件里面，这样做的目的是方便我们后面做人脸识别：\ndef extract_features_to_csv(faces_dir): mean_features_list = list(get_mean_features_of_face(faces_dir)) with open(FACES_FEATURES_CSV_FILE, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;) as csvfile: writer = csv.writer(csvfile) for mean_features in mean_features_list: person_features = mean_features[0] person_label = mean_features[1] person_features = np.insert(person_features, 0, person_label, axis=0) writer.writerow(person_features) 此刻，我们就拥有了一个简单的人脸数据库，对于任意一个待检测的人脸，我们只需要计算其特征值，然后再从人脸数据库中找到一个距离最小的特征即可：\ndef compare_face_fatures_with_database(database, image_path): image = Image.open(image_path) image = cv2.cvtColor(np.asarray(image), cv2.COLOR_RGB2BGR) faces = detector(image, 1) campare_results = [] if len(faces) != 0: for i in range(len(faces)): face = faces[i] shape = predictor(image, faces[0]) face_descriptor = face_reco_model.compute_face_descriptor(image, shape) face_feature_distance_list = [] for face_data in database: # 比对人脸特征，当距离小于 0.4 时认为匹配成功 dist = get_euclidean_distance(face_descriptor, face_data[1]) dist = round(dist, 4) if dist \u0026gt;= FACES_FATURES_DISTANCE_THRESHOLD: continue face_feature_distance_list.append((face_data[0], dist)) # 按距离排序，取最小值进行绘制 sorted(face_feature_distance_list, key=lambda x:x[1]) if face_feature_distance_list: person_dist = face_feature_distance_list[0][1] person_label = face_feature_distance_list[0][0] campare_results.append((person_label, person_dist)) return campare_results 可以注意到，我们会遍历人脸数据库中的每一个特征值，然后计算它和当前人脸特征的欧式距离。接下来，我们会对这些距离进行排序，选取距离最小的一组作为匹配结果，下面的代码片段展示了如何去计算一个欧氏距离：\ndef get_euclidean_distance(feature_1, feature_2): feature_1 = np.array(feature_1) feature_2 = np.array(feature_2) return np.sqrt(np.sum(np.square(feature_1 - feature_2))) 如下图所示，我们可以输出每一张人脸的名称，以及当前人脸相对于特征均值的距离，经博主测试，这个方案的正确率可以达到 94.58% 左右，是一种相对比较靠谱的做法：\n人脸识别效果展示\r不过，这个方案的缺点同样非常明显，即：它必须遍历人脸数据库中的每一条数据，这意味着它的时间复杂度是 O(n)。考虑到现实生活中需要录入的人脸数据可能是成百上千的，这个方案的执行效率注定会越来越低，这迫使我们不得不换一种思路来解决这个问题。重新审视人脸识别这个问题，我们会发现，这其实是一个分类问题，就像我们人为地去给一堆照片贴上标签一样。当然，它并不是一个非此即彼的二分类问题，而是一个多分类的问题。如果你接触过 Scikit Learn，那么，你大概会想到通过某种分类器进行模型训练的思路，这里以支持向量机为例：\n# 提取人脸特征和标签 mean_features_list = list(get_mean_features_of_face(faces_dir)) features = list(map(lambda x:x[0], face_encodings_mean_list)) labels = list(map(lambda x:x[1], face_encodings_mean_list)) # 通过支持向量机训练模型 clf = svm.SVC(C=1.0, kernel=\u0026#34;linear\u0026#34;, gamma=\u0026#39;scale\u0026#39;, probability=True) clf.fit(features, labels) 在这种情况下，我们可以利用 joblib.dump() 和 joblib.load() 两个方法对模型进行保存和加载。此时，我们该如何识别一个人脸呢？简单来说，你可以把支持向量机想象成一个线性函数，因此，我们只需要传入待检测图片的特征值，它就可以帮我们计算/预测出人脸的分类。显然，它不需要像前面的方案一样遍历每一条人脸数据，因此，支持向量机这个方案执行效率上要更好一点：\nfeatures = get_128d_features_of_face(image_path) predict_label = clf.predict(features) 当然，世界上没有百分之百完美的方案，支持向量机如果遇到了一张陌生的人脸，它的预测结果就会变得离谱起来，譬如，孙燕姿被识别为堺雅人、堺雅人被识别为胡歌，用鲁迅先生的话讲，屏幕内外充满了快活的空气。虽然，Scikit Learn 里可以用 predict_proba() 方法引入概率来评估结果的准确性，可博主实际使用下来，发现效果并没有好多少。无独有偶，OpenCV 自带的 LBPH 方法，其模型训练和这个思路非常地相似，无非是它接收是一个 Mat 类型的参数：\nrecognizer = cv2.face.LBPHFaceRecognizer_create() detector = cv2.CascadeClassifier(\u0026#34;haarcascade_frontalface_alt2.xml\u0026#34;) # 获取人脸样本及标签 face_samples = get_images_and_labels(DATASETS_DIR) faces = list(map(lambda x:x[0], face_samples)) faceIds = list(map(lambda x:x[1], face_samples)) # 模型训练 recognizer.train(faces, np.array(faceIds)) recognizer.save(\u0026#39;/train/train_data.yml\u0026#39;) # 模型预测 recognizer.read(\u0026#39;/train/train_data.yml\u0026#39;) faceId, confidence = recognizer.predict(image) 它会返回人脸的 Id 以及置信度，我们可以结合这个置信度去做进一步的判断，比如置信度超过 50% 就认为它匹配到了人脸。我一开始提到这个方案的精度达不到要求，就是因为它经常给出错误的预测结果，甚至比支持向量机遇到陌生人脸时还要离谱。所以，对于 OpenCV 官方的这个方案呢，我们知道它是怎么回事就可以了，我个人并不推荐在真实的场景中使用这个方法。当然，OpenCV 后来开始支持 CNN，我们值得更好的人脸识别方案，限于篇幅和精力，这个等以后有机会了再展开说说。\n人脸识别运用到视频中的效果展示：献给我永远的歌晨 CP\r本文小结 本文分享了 OpenCV 和 Dlib 下的人脸识别的实现，由于 OpenCV 自带的 LBPH 方法识别效果不符合预期，博主不得不使用 Dlib 来实现人脸识别。这里采用的思路是，计算出每一个人物的人脸特征均值，它是一个 128 维的向量，通过计算待检测人脸特征值与已知人脸特征值的欧式距离，来判断两张脸是否为同一张脸，经过测试，该方案的识别率可以达到 94.58% ，是一种相对来说比较靠谱的方案。考虑到这个方案的时间复杂度为 O(n) ，我们不得不考虑效率更高的做法。在这个基础上，我们尝试结合支持向量机做进一步的优化，支持向量机最大的问题时，当它面对一张陌生人脸的时候，其预测结果会变得非常离谱，可是在最好的情况下，它的准确率依然接近 97.04%。作为对照组，博主捎带着介绍了一下 OpenCV 自带的 LBPH 方法如何训练模型，权当帮大家扩展思路，只有此中的成败利钝，只有靠大家自己去取舍啦！最后，听说射雕英雄传又要翻拍啦，我宣布我永远都喜欢胡歌这一版的射雕英雄传，哈哈！\n","date":"2022-11-01T22:49:47Z","image":"/posts/dlib-face-recognition-with-machine-learning/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/dlib-face-recognition-with-machine-learning/","slug":"Dlib-Face-Recognition-With-Machine-Learning","tags":["Dlib","Python","图像处理","人脸识别"],"title":"视频是不能 P 的系列：使用 Dlib 实现人脸识别"},{"categories":["数据存储"],"content":"最近拜读了 Artech 大佬的新文章 《几个Caller-特性的妙用》，可以说是受益匪浅。不过，对我而言，最大的收获当属这篇文章里的第二主角，即 ActivitySource 和 Activity，这组 API 可以认为是微软针对 OpenTelemetry 规范的一种实现，即：每一个 Activity 都对应着一个 Span 。在以前的博客 《Envoy 集成 Jaeger 实现分布式链路追踪》 中，我曾经向大家介绍过 OpenTelemetry 规范，并最终结合 Envoy 和 Jeager 实现了非侵入式的、网关层的分布式链路追踪，正所谓“温故而知新”，在这个过程中我意识到其中还有值得去挖掘的东西。譬如，可观测性的三大支柱分别是 Logging、Tracing 和 Metrics 。可当我们接入了 Jeager 、Zipkin 等等的链路追踪系统，我们会发现它和平用到日志系统如 NLog、Serilog、ELK \u0026hellip;等等都相去甚远，好像这两者间存在着一种天然的割裂感，你不得不在了解了服务间的调用关系以后，再一头扎进各种各样的日志文件里。幸运的是，经过数日的探索，我有了一点小小的收获。因此，今天这篇博客我想和大家分享的是，分布式链路追踪系统如何和日志系统进行整合。\n.NET 中的分布式追踪 微软的 官方文档 中，有一个独立的章节来介绍分布式追踪。如果你观察得足够仔细，就会发现官方将其归类为 诊断和检测。我个人认为，这是我们日常开发中经常被忽略的一个东西。早年开发 Windows 桌面程序的时候，每当程序出现异常的时候，经验丰富的前辈总会让你去看一下 Windows 日志。其实，这个 Windows 日志就是 .NET Framework 时代的一种诊断工具。由此我们就可以知道， Diagnostics 就是一种帮助你记录应用程序运行期间的关键性操作及其执行时长的机制，我承认，这听起来和现在流行的 APM 差不多，至少从宏观上来看这个观点是成立的，因为 APM 的核心功能之一就是检测应用程序的关键事件。从 .NET Core 开始，Diagnostics 这个命名空间从 Microsoft 变为了 System 。如下图所示，整个诊断的核心建立在 Activity 这个类，以及 IObservable\u0026lt;T\u0026gt; 和 IObserver\u0026lt;T\u0026gt; 这组观察者模式的 API 上，其基本原理是：通过一系列活动来产生一系列事件，而关心这些事件的订阅者则可以通过这些事件来判断应用程序当前的状态。\nSystem.Diagnostics 命名空间下的核心成员一览\r一个最典型的场景是，当我们调用一个由 ASP.NET Core 托管的 Web 服务的时候，它可以在日志中输出本次请求的时长，这背后的功臣其实 DiagnosticsSource 和 DiagnosticsListener，我们来看下面的例子：\n// 定义观察者：MyDiagnosticObserver public class MyDiagnosticObserver : IObserver\u0026lt;KeyValuePair\u0026lt;string, object\u0026gt;\u0026gt; { //... public void OnCompleted() { } public void OnError(Exception error) { _logger.LogError(error.Message); } public void OnNext(KeyValuePair\u0026lt;string, object\u0026gt; pair) { _logger.LogInformation($\u0026#34;{pair.Key}, {pair.Value}\u0026#34;); } } // 订阅 Microsoft.AspNetCore 主题 var listener = new DiagnosticListener(\u0026#34;Microsoft.AspNetCore\u0026#34;); var observer = new MyDiagnosticObserver(); listener.Subscribe(observer); 此时，我们可以注意到，微软在请求开始、请求结束的地方做了埋点，因此，对于上面我们提出的问题，如果想要计算本次请求的时长，其实只需要用这两个时间戳相减。诚然，这个问题的解决方案还有很多，可最为重要的一件事情是，我们可以换一种角度来审视这一切。截止到目前为止，ASP.NET Core、EntityFramework Core 等等都已经对 Diagnostics 支持，这使得我们可以利用这些诊断信息来排查应用程序的性能问题或者跟踪调用链。\n利用 DiagnosticListener 订阅诊断日志\rActivitySource 与 Activity OK，现在让我们把目光聚焦在 Activity、ActivitySource 以及 AcitivityListener 这条线索上来，我们上面提到诊断信息可以运用于调用链的跟踪，其实就是指这部分内容。目前，分布式追踪普遍使用的术语叫做 Span，由于 .NET 中的 Activity 比 Span 出现地更早，当时 Span 的概念还不为人所知，所以，微软就一直沿用了这个术语。我个人猜测，这极有可能是为了和后来出现的 Span\u0026lt;T\u0026gt; 类型区分开来。总而言之，这里的 Activity 和 Span 是等价的，它本身具有像 SpanId、TraceId、ParentSpanId 等等表示树状节点的信息，可以携带像 Baggage、Tags 等等键值对信息，或者是事件信息。下面是 Activity 的一个基本用法：\n// 1.定义活动源 var _activitySource = new ActivitySource(\u0026#34;MyTrace\u0026#34;) // 2.定义活动监听器 ActivitySource.AddActivityListener(new ActivityListener() { ShouldListenTo = _ =\u0026gt; true, Sample = (ref ActivityCreationOptions\u0026lt;ActivityContext\u0026gt; options) =\u0026gt; ActivitySamplingResult.AllData, ActivityStopped = activity =\u0026gt; { logger.LogInformation($\u0026#34;{activity.DisplayName},ParentSpanId={activity.ParentSpanId},TraceId={activity.TraceId},SpanId={activity.SpanId},Duration={activity.Duration}\u0026#34;); var stringBuilder = new StringBuilder(); foreach (var kv in activity.TagObjects) { stringBuilder.AppendLine($\u0026#34;{kv.Key}={kv.Value}\u0026#34;); } if (stringBuilder.Length \u0026gt; 0) logger.LogInformation($\u0026#34;TagObjects:{stringBuilder.ToString()}\u0026#34;); } };) 如你所见，我们定义了一个活动源 MyTrace ，同时为其指定了一个监听器，这使得我们可以这些活动的信息。有道是：万事俱备，只欠东风，接下来，我们来考虑如何产生活动。假设一件事情需要三个步骤来完成，如果每一个步骤对应一个活动，那么我们可以像下面这样子来编写代码：\npublic async Task\u0026lt;string\u0026gt; InvokeAsync() { using (_activitySource.StartActivity()) { return await RunStep1Async(); } } // Step1 private async Task\u0026lt;string\u0026gt; RunStep1Async() { using (var activity = _activitySource.StartActivity()) { await Task.Delay(100); activity.AddTag(\u0026#34;Description\u0026#34;, \u0026#34;This is Step1\u0026#34;); _logger.LogInformation(\u0026#34;Step1 is running\u0026#34;); return await RunStep2Async(); } } // Step2 private async Task\u0026lt;string\u0026gt; RunStep2Async() { using (var activity = _activitySource.StartActivity()) { await Task.Delay(200); activity.AddTag(\u0026#34;Description\u0026#34;, \u0026#34;This is Step2\u0026#34;); _logger.LogInformation(\u0026#34;Step2 is running\u0026#34;); return await RunStep3Async(); } } // Step3 private async Task\u0026lt;string\u0026gt; RunStep3Async() { using (var activity = _activitySource.StartActivity()) { await Task.Delay(300); activity.AddTag(\u0026#34;Description\u0026#34;, \u0026#34;This is Step3\u0026#34;); _logger.LogInformation(\u0026#34;Step3 is running\u0026#34;); return \u0026#34;Hi, I am always here\u0026#34;; } } 此时，会发生什么呢？可以注意到，每一个步骤对应了一个 Activity 或者说是一个 Span，它本身有一个唯一的 SpanId，并且都拥有相同的 TraceId，这显然符合我们的调用链，即：Step1 -\u0026gt; Step2 -\u0026gt; Step3。当然，因为这些调用存在是层级关系，所以，你会注意到第一个活动的 SpanId 就是第二个活动的 ParentSpanId，以此类推。话说到这种程度，我想，大家都能理解，这可不就是 Span 吗？\n利用 ActivityListener 监听活动\r.NET 与 OpenTelemetry 在此前的文章中，博主曾非常粗浅提到过 OpenTelemetry 规范，它由微软和谷歌共同发起，终极目标是：实现 Metrics、Tracing、Logging 的融合及大一统，作为 APM 的数据采集终极解决方案。不得不说，这是一个非常宏伟的理想，毕竟跟随着 Logstash、Filebeat、Fluentd 这一路走过来，日志收集的方案一直是层出不穷，有时候我会觉得这一切混乱无比。坦白讲，我不愿意接触 K8S 的一大理由，就是不想再去学习一套配置语法。可是话说回来，虽然 OpenTelemetry 还没有发布正式版，可这丝毫不影响我们在 .NET 中提前体验一番，我们继续使用上面的例子来进行说明：\nSdk.CreateTracerProviderBuilder() .SetResourceBuilder(ResourceBuilder.CreateDefault().AddService(\u0026#34;Program\u0026#34;)) .AddSource(\u0026#34;MyTrace\u0026#34;) .AddConsoleExporter() .AddJaegerExporter(options =\u0026gt; { options.AgentHost = \u0026#34;\u0026lt;You Agent Host\u0026gt;\u0026#34;; options.AgentPort = 6831; }) .Build(); 通过上面这段代码，我们就可以收集这些活动到 Jeager，平时我们使用 Jeager 更多的是分析跨服务、跨进程的调用链，其实它一样可以运用到进程内，如果我们能把打通进程内外，那么，整个链路追踪就从逻辑上就实现了闭环。当我们在一个微服务的体系中排查问题的时候，经常遇到的一个困境就是，我必须从头到尾、一个点一个点的排查，无论我是否需要知道上下游的业务细节，那一刻无数冗余的信息都如排山倒海一般拥挤过来。不知道大家是不是和我有类似的感受，即：有时候我们拆分了微服务出来，好像拆分了，又好像没有。\n利用 OpenTelemetry 收集 Acitivity 到 Jeager\r除了上面这种半自动化采集的方式，OpenTelemetry 的 .NET SDK 里同样提供了自动化采集的方式，主要针对 HttpClient 和 ASP.NET Core 的诊断信息：\nservices.AddOpenTelemetryTracing(builder =\u0026gt; { builder .AddConsoleExporter() .AddSource(serviceName) .SetResourceBuilder(ResourceBuilder.CreateDefault().AddService(serviceName)) .AddHttpClientInstrumentation() // HttpClient 诊断信息采集 .AddAspNetCoreInstrumentation(); // ASP.NET Core 诊断信息采集 }); 这意味着，当你访问一个接口或者是通过 HttpClient 调用第三方接口时，OpenTelemetry 可以自动将其采集到指定的目标，如果大家对 OpenTelemetry 感兴趣的话，可以参照官方文档做更进一步的探索！\n分布式追踪和日志整合 好了，到现在为止，关于 Activity、ActivitySource 以及 AcitivityListener 这条故事线的探索已经全部完成，甚至它可以结合 OpenTelemetry 为分布式链路追踪带来一点新思路。可是，大家对这一切真的就没有一点疑问吗？对博主而言，我真正困惑的点是，即使有一天 OpenTelemetry 可以统一 Metrics、Tracing、Logging ，可至少在现在这个阶段，我们使用得最多的还是 NLog、Serilog 这些方案。此时，当务之急是如何让新旧两个体系完美地整合在一起。所以，接下来我们来聊聊分布式追踪和日志的整合，这里以 NLog 为例。\nNLog.DiagnosticSource NLog.DiagnosticSource 是一个 NLog 的扩展库，它可以利用 System.Diagnostics.Activity.Current 将一个活动的信息渲染到日志中，譬如，这里最重要的一个参数是 TraceId，如果我们的日志中有这个参数，我们就可以在 ELK 中精确地查询出上下文相关的一组日志，而不是通过时间或者关键字去做模糊匹配。如下图所示，我们可以在 NLog.config 这个配置文件中使用诊断数据中的字段，这里以 TraceId 和 SpannId 为例：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; ?\u0026gt; \u0026lt;nlog xmlns=\u0026#34;http://www.nlog-project.org/schemas/NLog.xsd\u0026#34; xsi:schemaLocation=\u0026#34;NLog NLog.xsd\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; \u0026lt;extensions\u0026gt; \u0026lt;add assembly=\u0026#34;NLog.DiagnosticSource\u0026#34;/\u0026gt; \u0026lt;/extensions\u0026gt; \u0026lt;targets\u0026gt; \u0026lt;target xsi:type=\u0026#34;File\u0026#34; name=\u0026#34;file\u0026#34; fileName=\u0026#34;${basedir}/logs/${shortdate}.log\u0026#34;\u0026gt; \u0026lt;layout xsi:type=\u0026#34;JsonLayout\u0026#34; includeAllProperties=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;attribute name=\u0026#34;time\u0026#34; layout=\u0026#34;${longdate}\u0026#34; /\u0026gt; \u0026lt;attribute name=\u0026#34;level\u0026#34; layout=\u0026#34;${level:upperCase=true}\u0026#34; /\u0026gt; \u0026lt;attribute name=\u0026#34;message\u0026#34; layout=\u0026#34;${message}\u0026#34; /\u0026gt; \u0026lt;attribute name=\u0026#34;exception\u0026#34; layout=\u0026#34;${exception:format=ToString}\u0026#34; /\u0026gt; \u0026lt;attribute name=\u0026#34;traceId\u0026#34; layout=\u0026#34;${activity:property=TraceId}\u0026#34; /\u0026gt; \u0026lt;attribute name=\u0026#34;spanId\u0026#34; layout=\u0026#34;${activity:property=SpanId}\u0026#34; /\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/target\u0026gt; \u0026lt;/targets\u0026gt; \u0026lt;rules\u0026gt; \u0026lt;logger name=\u0026#34;*\u0026#34; minlevel=\u0026#34;Trace\u0026#34; writeTo=\u0026#34;file\u0026#34; /\u0026gt; \u0026lt;/rules\u0026gt; \u0026lt;/nlog\u0026gt; 在此情形下，上下文相关的日志将会有相同的 TraceId。显然，这对于我们排查问题非常有用，你可以通过 Jeager 获得 TraceId ，然后再通过 TraceId 查询日志，我觉得这样就很好！\n在日志文件中集成 SpanId 和 TraceId\r更多关于 NLog.DiagnosticSource 使用的细节，请大家可以参考其官方文档。\n自定义 LayoutRenderer 当然，微软的东西好像天生就是一种原罪，很多时候，你可能根本不会用这个听都没听说过的 Acitivity ，你可能更愿意使用自己生成的 TraceId，在这种情况下，NLog.DiagnosticSource 这个库就不再适用啦，因此，在最后的这点篇幅里，我想分享一下 Nlog 里自定义的 LayoutRenderer 的实现。提到这个话题的时候，我其实是蛮感慨的。因为我以前特意研究过 NLog 里的 LayoutRenderer 、Serilog 里的 Sink，原因是上家公司的日志不支持全文索引，当时的想法是把日志写到 Elasticsearch 里面，所以，花了一点时间去了解这些东西。无独有偶，在生成 TraceId 这个问题上，我已经见过了好几种做法，有的在客户端生成，有的在服务器端生成，有的在网关层生成，可以说是莫衷一是。由此可见，这个自定义的 TraceId 其实是挺普遍的一个需求。\nEnvoy 集成 Jaeger 实现分布式链路追踪\r以博主以前写的博客 《Envoy 集成 Jaeger 实现分布式链路追踪》 为例，我的这个 TraceId 是在网关层生成的，此时的做法是通过一个中间件来实现：\npublic async Task InvokeAsync(HttpContext context) { if (context.Request.Headers.ContainsKey(\u0026#34;x-b3-traceid\u0026#34;)) { CallContext.SetData(\u0026#34;TraceId\u0026#34;, context.Request.Headers[\u0026#34;x-b3-traceid\u0026#34;]); } else { CallContext.SetData(\u0026#34;TraceId\u0026#34;, Guid.NewGuid().ToString(\u0026#34;N\u0026#34;)); } await _next(context); } 其中，CallContext 是一个基于 AsyncLocal\u0026lt;T\u0026gt; 的辅助类，用于在异步代码间共享数据。如果你经历过 .NET Framework 时代，应该会知道这个 CallContext 的作用：\npublic static class CallContext { private static ConcurrentDictionary\u0026lt;string, AsyncLocal\u0026lt;object\u0026gt;\u0026gt; _states = new ConcurrentDictionary\u0026lt;string, AsyncLocal\u0026lt;object\u0026gt;\u0026gt;(); public static void SetData\u0026lt;T\u0026gt;(string name, T data) =\u0026gt; _states.GetOrAdd(name, _ =\u0026gt; new AsyncLocal\u0026lt;object\u0026gt;()).Value = data; public static T GetData\u0026lt;T\u0026gt;(string name) =\u0026gt; _states.TryGetValue(name, out AsyncLocal\u0026lt;object\u0026gt; data) ? (T)data.Value : default(T); } 现在，这个 TraceId 已经被存放到 CallContext 里面啦，我们只要从某个地方将其读取出来即可，为此，我们需要实现下面的自定义 LayoutRenderer ：\n[LayoutRenderer(\u0026#34;my-trace-id\u0026#34;)] public class MyTraceIdLayoutRenderer : LayoutRenderer { protected override void Append(StringBuilder builder, LogEventInfo logEvent) { if (Activity.Current != null) { builder.Append(Activity.Current.TraceId.ToString()); } else { var traceId = CallContext.GetData\u0026lt;string\u0026gt;(\u0026#34;traceid\u0026#34;); builder.Append(traceId); } } } 此时，我们可以在 NLog.config 中使用这个新的模板，至此我们就实现了分布式链路追踪与日志的整合：\n\u0026lt;layout xsi:type=\u0026#34;JsonLayout\u0026#34; includeAllProperties=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;attribute name=\u0026#34;time\u0026#34; layout=\u0026#34;${longdate}\u0026#34; /\u0026gt; \u0026lt;attribute name=\u0026#34;level\u0026#34; layout=\u0026#34;${level:upperCase=true}\u0026#34;/\u0026gt; \u0026lt;attribute name=\u0026#34;message\u0026#34; layout=\u0026#34;${message}\u0026#34; /\u0026gt; \u0026lt;attribute name=\u0026#34;exception\u0026#34; layout=\u0026#34;${exception:format=ToString}\u0026#34; /\u0026gt; \u0026lt;attribute name=\u0026#34;traceId\u0026#34; layout=\u0026#34;${my-trace-id}\u0026#34; /\u0026gt; \u0026lt;/layout\u0026gt; 当然，在使用这个新模板前，按照 NLog 官方的说法，你最好是全局注册一下先：\nConfigurationItemFactory.Default.LayoutRenderers .RegisterDefinition(\u0026#34;my-trace-id\u0026#34;, typeof(MyTraceIdLayoutRenderer)); 本文小结 本文的灵感来自于 Artech 大佬的新文章 《几个Caller-特性的妙用》 一文，可是对于我来说， Activity、ActivitySource 以及 AcitivityListener 这条故事线好像更重要一点。正所谓：“温故而知新”，在这个过程中我发现了 Activity 和 Span 在功能上的相似性，进而接触到了 OpenTelemetry ，当我们尝试把这两个事物结合在一起的时候，就会发现它完全可以运用到进程内的链路追踪。如果说，年初时候写的 《Envoy 集成 Jaeger 实现分布式链路追踪》 一文中的做法更符合云原生的理念，那么，此时此刻，这种在应用程序里接入 SDK 的做法是否就显得落后呢？我想，或许各有千秋，就像我有时候会觉得，在代码里创建一个个的 Activity 显得多此一举，我更愿意通过 AOP 的方式让这些细节不可感知，可仔细想想知道这个想法太过天真，因为只要你在意这些 Activity 的层级关系，这个过程就一定是会被使用者感知到的。在这个基础上，我们把 TraceId 写入到日志文件里，这样我们排查问题就有了一个清晰的流程：首先，通过 Jeager 或者 Zipkin 搞清楚调用链路，等拿到 TraceId 以后再去查询相关的日志，如果能避免在无关的事情/信息上浪费时间，何乐而不为呢？\n","date":"2022-10-15T12:30:47Z","image":"/posts/integration-of-distributed-tracing-system-and-logging-system/bridge.jpg","permalink":"https://qinyuanpei.github.io/posts/integration-of-distributed-tracing-system-and-logging-system/","slug":"Integration-Of-Distributed-Tracing-System-And-Logging-System","tags":["OpenTelemetry","Diagnostics","Jeager","NLog"],"title":"浅议分布式链路追踪与日志的整合"},{"categories":["数据存储"],"content":"很多年后，当我在命令行中熟练地操作 Git 的时候，我总会不由地想起从前意气风发的自己。毕竟不知不觉间，三十岁的年龄已然被更年轻的人们嫌弃“苍老”，除却生理上不可逆转的自然衰老，更多的或许是一种心态上的衰老。以前，我是非常鄙夷在 Git 仓库里提交 Word 或者 Excel 文件这种行为的，甚至连理由都给得十分正当，即：这种文件不利于差异的对比和合并。后来，参与的项目越来越多，渐渐认识到 Markdown 始终是一种小众的格式，你没有办法要求所有人都去适应 Markdown。所以，当我说我在心态上变成了一个老人的时候，其实是指，我不再对这件事情那么执着。当然，人生本来就是一个解决麻烦再制造麻烦的过程。当你默许了在 Git 仓库里提交非文本文件的行为，当这些非文本文件随着时间推移变得越来越大时，就出现了 Git 大文件上传、存储等等一系列的问题。因此，今天这篇文章，我们来聊聊 Git 里的大文件。\n提交前的未雨绸缪 其实，博主不愿意在 Git 仓库里上传 Word 或者 Excel 文件，一个最为直接的理由是，它会成为我们拉取或者推送代码时的累赘。君不见，腾讯硬生生在手机 QQ 里内置了一个虚幻 4 引擎，想象一下，如果把这么多的文件都放到 Git 仓库里，每次做一点修改该有多痛苦啊！事实上，Github 对文件大小的限制是 100M，Gitlab 对文件大小的限制则是 600M，一旦超过这个限制，就会被判定为大文件。因此，Atlassian、GitHub 等组织一起开发了针对 Git 的 Large File Storage 扩展，即：Git LFS。其原理是延迟地下载大文件的相关版本来减少大文件对仓库的影响，具体来说，就是在 checkout 到工作区的时候才会真正去下载大文件的内容。如果大家想了解更多 Git LFS 的细节，可以阅读下面这份文档：https://www.atlassian.com/git/tutorials/git-lfs，这里不再考虑对其进行二次加工。\nGit LFS 原理示意图\r当你准备向一个 Git 仓库提交大文件的时候，首先，你需要下载和安装 Git LFS 扩展并执行命令：\ngit lfs install 其次，在 Git 仓库中，你需要通过 git lfs track 命令告诉 Git LFS，你希望它帮你管理哪些文件：\ngit lfs track \u0026#34;*.jpg\u0026#34; 这里以 *.jpg 为例，它表示希望 Git LFS 对所有的 .jpg 格式的文件进行管理，这一步的结果是生成一个名为 .gitattributes 的文件。因此，理论上你可以直接编辑这个文件来达到相同的目的：\n*.jpg filter=lfs diff=lfs merge=lfs -text 最后，你需要确保 .gitattributes 文件能被 Git 仓库追踪：\ngit add .gitattributes 一旦完成了这个准备工作，你就可以像平时一样提交和推送变更：\ngit add awesome.jpg git commit -m \u0026#34;Add a awesome image file\u0026#34; git push origin main 下图演示了 Git LFS 从初始化到应用的整个过程，可以注意到，这个过程中会生成各种钩子脚本：\nGit LFS 的初始化与应用\r提交后的亡羊补牢 扁鹊见蔡桓公的故事，告诉我们防微杜渐的道理。可如果你和博主一样，是在提交了一个大文件以后，才开始接触 Git LFS 的相关内容。那么，你大概会遇到下面的错误：\nGit 上传文件超过 100M 的错误提示\r你可能会想，既然是这个文件的大小超过了 Github 的限制，那我把这个文件删除掉是不是就可以了呢？经过博主一番挣扎，发现这个思路完全是徒劳的。因为这个大文件就像持续了三年的新冠病毒一样，只要存在被它感染过的历史记录，都会在推送时提示上面的错误。所以，删除这个大文件并不能真正解决问题，我们必须要确保此前的提交历史中没有这个大文件。下面分享一个神奇的命令 git filter-branch ：\ngit filter-branch -f --index-filter \u0026#39;git rm --cached --ignore-unmatch /src/CSharpWasm/mono-6.12.0.182-x64-0.msi\u0026#39; 通过这个命令，我们就可以重写整个提交历史：\n从提交历史中删除指定的大文件\r此时，我们只需要按提交大文件前的步骤，重新提交该文件即可，并且可以突破 100M 的限制：\n成功提交一个超过 100M 的大文件\r本文小结 我写作的话题，基本上可以分为两类：有计划的和无计划的。显然，这一篇属于后者，它就像育碧旗下的开放世界，当你漫游其中总会遇到那些令人意外的支线任务。我个人挺喜欢这种由点及面的认知模式，即从一个特定的问题逐步过渡到一个更为宽泛的知识或者体系上面。在这篇文章里，无法提交一个超过 100M 的大文件是表，不了解 Git LFS 是里，更有趣的一点是，解决问题的过程通常是由“果”反推出“因”，而撰写博客的过程则是由“因”顺推出“果”。从这个角度来看的话，对读者“揣着明白装糊涂”这才是最难把握的一个分寸，平时写长文章总担心写不清楚，而写短文章则担心像流水账。对于 Git LFS 来说，Git 仓库存储的其实是一个指针文件，真正的内容则是存储在 LFS 服务器里，主流的代码托管平台如 Github、Gitlab 等都支持 Git LFS，我们只需要安装 Git LFS 的客户端扩展即可。当然，我对非文本文件的合并依然持悲观态度，想象一下，两个人同时修改了一个大文件，一个已经推送到远程，一个已经提交到本地，那么，当他们尝试合并代码的时候，又会发生什么事情呢？作为一名程序员，每天被安排排查和解决问题，简直是家常便饭，可解决问题会有尽头吗？我想，大概率是没有的罢！\n参考链接 https://git-lfs.github.com https://github.com/git-lfs/git-lfs/wiki/Tutoria https://www.atlassian.com/git/tutorials/git-lfs ","date":"2022-10-10T12:30:47Z","image":"/posts/a-story-of-git-large-file-storage/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/a-story-of-git-large-file-storage/","slug":"A-Story-Of-Git-Large-File-Storage","tags":["Git","LFS","技巧","记录"],"title":"关于 Git 大文件上传这件小事"},{"categories":["编程语言"],"content":"最近，博主为 FakeRPC 增加了 WebSocket 协议的支持。这意味着，我们可以借助其全双工通信的特性，在一个连接请求内发送多条数据。FakeRPC 目前最大的遗憾是，建立在 HTTP 协议上而不是 TCP/IP 协议上。因此，考虑 WebSocket 协议，更多的是为了验证 JSON-RPC 的可行性，以及为接下来的要支持的 TCP/IP 协议铺路。也许，你从未意识到这些概念间千丝万缕的联系，可如果我们把每一次 RPC 调用都理解为一组消息，你是不是就能更加深刻地理解 RPC 这个稍显古老的事物了呢？在编写 FakeRPC 的过程中，我使用了 .NET 中的全新数据结构 Channel 来实现消息的转发。以服务端为例，每一个 RPC 请求经过 CallInvoker 处理以后，作为 RPC 响应的结果其实并不是立即发回给客户端，而是通过一个后台线程从 Channel 取出消息再发回客户端。 那么，博主为什么要舍近求远呢？我希望，这篇文章可以告诉你答案。\nChannel 入门 Channel 是微软在 .NET Core 3.0 以后推出的新的集合类型，该类型位于 System.Threading.Channels 命名空间下，具有异步 API 、高性能、线程安全等等的特点。目前，Channel 最主要的应用场景是生产者-消费者模型。如下图所示，生产者负责向队列中写入数据，消费者负责从队列中读出数据。在此基础上，通过增加生产者或者消费者的数目，对这个模型做进一步的扩展。我们平时使用到的 RabbitMQ 或者 Kafka，都可以认为是生产者-消费者模型在特定领域内的一种应用，甚至于我们还能从中读出一点广义上的读写分离的味道。\n生产者-消费者模型示意图\r罗曼·罗兰曾说过，世界上只有一种真正的英雄主义，那就是在认清生活的真相后，依然热爱生活。此时此刻，看着眼前的这幅示意图若有所思，你也许会想到下面的做法：\nclass Producer\u0026lt;T\u0026gt; { private readonly Queue\u0026lt;T\u0026gt; _queue; public Producer(Queue\u0026lt;T\u0026gt; queue) { _queue = queue; } } class Consumer\u0026lt;T\u0026gt; { private readonly Queue\u0026lt;T\u0026gt; _queue; public Consumer(Queue\u0026lt;T\u0026gt; queue) { _queue = queue; } } 我承认，这个思路理论上是没有问题的，可惜实际操作起来槽点满满。譬如，生产者应该只负责写，消费者应该只负责读，可当你亲手把一个队列传递给它们的时候，想要保持这种职责上的纯粹属实是件困难的事情，更不必说，在使用队列的过程中，生产者会有队列“满”的忧虑，消费者会有队列“空”的烦恼，如果再考虑多个生产者、多个消费者、多线程/锁等等的因素，显然，这并不是一个简单的问题。为了解决这个问题，微软先后增加了 BlockingCollection 和 BufferBlock 两种数据结构，这里以前者为例，下面是一个典型的生产者-消费者模型：\nvar bc = new BlockingCollection\u0026lt;int\u0026gt;(); // 生产者 var producer = Task.Run(() =\u0026gt; { for (var i = 0; i \u0026lt; Count; i++) { bc.Add(i); Console.WriteLine(\u0026#34;Producer Write Item: {0}\u0026#34;, i); } bc.CompleteAdding(); }); // 消费者 var consumer = Task.Run(() =\u0026gt; { while (!bc.IsCompleted) { if (bc.TryTake(out var item)) { Console.WriteLine(\u0026#34;Consumer Read Item: {0}\u0026#34;, item); } } }); await Task.WhenAll(producer, consumer); 可以注意到，现在我们再去实现一个生产者-消费者模型，其难度基本为零。与此同时，BlockingCollection\u0026lt;T\u0026gt; 和 BufferBlock\u0026lt;T\u0026gt; 都是线程安全的集合，这可以让我们在多线程环境下更加得心应手。回想我过去的种种经历，每当我需要使用那些线程信号量进行线程同步的时候，我都不得不小心翼翼地在 Bug 边缘游走。那么，你也许会问，既然我们已经有 BlockingCollection\u0026lt;T\u0026gt; 和 BufferBlock\u0026lt;T\u0026gt; 这样的数据结构，为什么我们还需要 Channel 呢？作为一名最普通不过的程序员，有无数多个 Bug 随着时间的推移都慢慢消失了，而那些曾经令我们殚精竭虑的问题，同样在被不停地刷新着答案。\nBlockingCollection、BufferBlock、Channel 的性能对比\r如图所示，我们测试了读写 10000 条数据的场景下，三种数据结构各自的性能表现，显而易见 Channel 的性能是最好的，所以，你告诉我，这到底算不算一个理由，难道还有什么东西比性能更令人兴奋的吗？当你的电脑显卡不能带你领略刺客信条的“神话三部曲”，甚至连在本机部署 Stable Diffusion 都变成一种奢望的时候，你不得不承认，这一点点微不足道的性能优化，是这个预言摩尔定律将会失效的时代里，所剩无几的匠心独运。\n// 创建一个有限容量的 Channel var boundedChannel = Channel.CreateBounded\u0026lt;int\u0026gt;(100); // 创建一个无限容量的 Channel var unboundedChannel = Channel.CreateUnbounded\u0026lt;string\u0026gt;(); 好了，对于 Channel 我们该从哪里开始说起呢？ 可以注意到，创建一个 Channel 是非常简单的，除非你打算创建的是一个有限容量的 Channel。还记得我们一开始提出的问题吗？在生产者-消费者模型中，一个容量有限的固定，一定会无可避免地出现队列“满”的情形，此时，我们就需要制定某种策略或者机制来完善整个模型。对于这个问题，Channel 的解决方案是 BoundedChannelFullMode ：\nvar boundedChannel = Channel.CreateBounded\u0026lt;string\u0026gt;( new BoundedChannelOptions(100) { FullMode = BoundedChannelFullMode.Wait }); 注意到，这是一个枚举类型，事实上，它共有 Wait、DropNewest、DropOldest、DropWrite 四个取值，默认为 Wait。其中：\nWait：当队列已满时，写入数据时会返回 false，直到队列内有空间时可以继续写入。 DropNewest：移除最新的数据，即从队列尾部开始移除元素。 DropOldest：移除最旧的数据，即从队列头部开始移除元素。 DropWrite：可以写入数据，但是数据会被立即丢弃。 除了队列“满”或者队列“空”的问题，我们还考虑过多线程环境下的生产者-消费者模型可能会遇到的问题。值得庆幸的是， Channel 天生就支持多线程，我们可以通过 ChannelOptions 的 SingleWriter 和 SingleReader 来指定 Channel 是否是单一的消费者或者生产者，默认情况下，这两个值都是 false :\nvar boundedChannel = Channel.CreateBounded\u0026lt;string\u0026gt;( new BoundedChannelOptions(100) { SingleWriter = true, SingleReader = false, FullMode = BoundedChannelFullMode.Wait }); 例如，通过以上代码片段，我们就可以创建出一个单生产者、多消费者的 Channel ，对于 Channel 而言，其最重要的两个成员分别是 Writer 和 Reader , 前者对应生产者，类型定义为：ChannelWriter\u0026lt;T\u0026gt;；后者对应消费者，类型定义为：ChannelReader\u0026lt;T\u0026gt;，这一次，我们做到了真正意义上的读写分离：\n// 生产者生产数据 channel.Writer.TryWrite(\u0026#34;大漠孤烟直，长河落日圆。\u0026#34;); // 消费者消费数据 // 模式一：一次读一个 while (await channel.Reader.WaitToReadAsync()) { while (channel.Reader.TryRead(out var item)) { // 在这里写具体的处理逻辑 } } // 模式二：一次全部读出来 while (await channel.Reader.WaitToReadAsync()) { await foreach (var item in channel.Reader.ReadAllAsync()) { // 在这里写具体的处理逻辑 } } 也许，你对这个话题意犹未尽，可我不得不非常遗憾的告诉你，这就是 Channel 最为核心的用法了。怎么样，是不是感觉非常简单？这的确符合微软一贯的作风，即：让一个复杂的东西变得简单好用。关于 Channel 更多的细节，这里不再赘述，大家可以去阅读官方文档。我发誓，MSDN 和 MDN 是我见过写得最好的文档。\nChannel 应用 OK，在对 Channel 有了一个基本的印象后，我们来看看它在具体场景中的应用。回到本文一开始作为引子出场的 FakeRPC，当我考虑为其添加 WebSocket 协议的支持时，我首当其冲要面对的问题是：如何把对一个方法的调用映射到 WebSocket 通信上面。对于普通的 HTTP 协议而言，因为它遵循的是请求-响应模型，所以，它可以自然而然地和一个方法的调用产生联系。可当我们的视角切换到一个双工通信的 WebSocket 协议上时，这个问题就突然变得有趣起来。众所周知，对于 WebSocket 来说，第一次连接是常规的 HTTP 协议，而一旦连接建立就不再需要 HTTP 协议。此时，双方的交流将会是有来有回。最终，FakeRPC 采用了下面的方案来提供 WebSocket 协议的支持：\nFakeRPC 如何支持 WebSocket 协议\r在这个方案中，CallInvoker 是真正负责处理请求的核心组件，对于客户端来说，这个工作主要是按照请求的方法和参数组装为 FakeRpcRequest，然后再调用 ClientWebSocket 实例的 SendAsync() 方法发送消息给服务器端。除此之外，它还需要从服务器端接收消息，因为每一条消息都携带着 Id ，因此，我们可以非常容易地分辨出哪一条消息是回复给自己的。在此基础上，博主使用了一个后台线程从 Channel 中读取消息，这样，发送消息和接收消息实际上是工作在两个不同的线程上。对于服务器端来说，在消息的处理上是相似的，不同的是，服务器端从 Channel 中读取消息是为了发送给客户端，而客户端从 Channel 读取消息则是为了传递结果给代理类。下面的代码，展示了上面提到的一部分客户端的实现细节：\n// 客户端发送消息 private async Task SendMessage(FakeRpcRequest request) { var payload = await _messageSerializer.SerializeAsync\u0026lt;FakeRpcRequest\u0026gt;(request); await _webSocket.SendAsync(new ArraySegment\u0026lt;byte\u0026gt;(payload), WebSocketMessageType.Binary, true, CancellationToken.None); OnMessageSent?.Invoke(_webSocket, request); } // 客户端从 Channel 中读取消息 private async Task ReadMessagesFromQueue() { try { while (await _messagesToReadQueue.Reader.WaitToReadAsync()) { while (_messagesToReadQueue.Reader.TryRead(out var message)) { try { var response = await _messageSerializer.DeserializeAsync\u0026lt;FakeRpcResponse\u0026gt;(message.Array); OnMessageReceived?.Invoke(_webSocket, response); } catch (Exception e) { _logger.LogError(e, $\u0026#34;Failed to send message due to {e.Message}\u0026#34;); } } } } catch (TaskCanceledException) { } catch (OperationCanceledException) { } catch (Exception e) { _logger.LogError(e, $\u0026#34;Restart listen message queue due to {e.Message}\u0026#34;); ListenMessageQueue(); } } 当然，ClientWebSocket 实例在收到消息的时候，实际上是将消息写入 Channel，某种意义上你可以理解为，CallInvoker 同时承担着生产者和消费者的角色，并且生产者和消费者运行在两个不同的线程上：\nvar bytes = stream.ToArray(); var response = await _messageSerializer.DeserializeAsync\u0026lt;FakeRpcResponse\u0026gt;(bytes); _logger?.LogInformation(\u0026#34;Send response to {0}/{1}, payload:{3}\u0026#34;, request.ServiceName, request.MethodName, response.Result); _messagesToReadQueue.Writer.TryWrite(new ArraySegment\u0026lt;byte\u0026gt;(bytes)); 此时，借助于动态代理，我们可以非常轻松地调用一个 RPC 接口，并且它是以长连接的形式运行在 WebSocket 协议上：\nvar _clientFactory = serviceProvider.GetService\u0026lt;FakeRpcClientFactory\u0026gt;(); // 调用 GreetService var greetProxy = _clientFactory.Create\u0026lt;IGreetService\u0026gt;(new Uri(\u0026#34;ws://localhost:5000\u0026#34;), FakeRpcTransportProtocols.WebSocket, FakeRpcMediaTypes.Default); var reply = await greetProxy.SayHello(new HelloRequest() { Name = \u0026#34;张三\u0026#34; }); reply = await greetProxy.SayWho(); // 调用 ICalculatorService var calculatorProxy = _clientFactory.Create\u0026lt;ICalculatorService\u0026gt;(new Uri(\u0026#34;ws://localhost:5000\u0026#34;), FakeRpcTransportProtocols.WebSocket, FakeRpcMediaTypes.Default); var result = calculatorProxy.Random(); 可以注意到，我们不仅在传输协议上实现了抽象，而且在消息协议上实现了抽象，你可以像以前一样自由地使用 MessagePack 或者 Protobuf ，为了证明 Channel 真的对性能提升有用，这里我放一张 FakeRPC 的 brenchmark 测试结果：\nFakeRPC 不同通讯协议、消息协议性能对比\r我们可以看到，MessagePack 不论是在 HTTP 协议下还是 WebSocket 协议下，始终都有着不俗的表现，这让我开始期待，它能否在 TCP/IP 协议上继续书写这个传奇，就在几天前，我刚刚完成了 TCP/IP 协议下的二进制消息定义，自从序列化和反序列化被抽象到 IMessageSerializer 接口以后，我们将会有更多的机会去支持更多的消息协议，随着育碧官方官宣了新作，刺客信条：幻景，我对 FakeRPC 的这个名字的理解，已经延续到了刺客兄弟会的理念，即：Nothing is true，或者说，万物皆虚。因为，从某种意义上来讲，RPC 不过是隐藏了那些蜿蜒曲折的中间过程，让你产生了可以像调用本地方法一样调用远程方法的错觉，甚至在设计二进制消息协议的时候，我突然意识到，我不过是再一次发明了 HTTP 协议。那么，这一切还有意义吗？当然有！做人嘛，开心就好了呀！\nvar buffer = new BufferBlock\u0026lt;int\u0026gt;(); // Producer async static Task Producer(IEnumerable\u0026lt;int\u0026gt; values) { foreach (var value in values) { await buffer.SendAsync(value); } buffer.Complete(); } // Consumer async static Task Consumer(Action\u0026lt;int\u0026gt; process) { while (await buffer.OutputAvailableAsync()) { process?.Invoke(await buffer.ReceiveAsync()); } } var range = Enumerable.Range(0, 100); await Task.WhenAll(Producer(range), Consumer(n =\u0026gt; Console.WriteLine(n))); BufferBlock 是微软 TPL DataFlow 中的重要组件之一，其基本思想是：数据流是由一个又一个的数据块组成，一个块处理完毕后将会链接到下一个块上。每一个块以消息的形式接收和缓存来自一个或多个源的数据，当一个块接收到信息时，该块会对输入做出反应，与此同时，该块的输出将传递到下一个块中。总而言之，除了 BufferBlock 以外，还有 ActionBlock、TransformBlock、BroadcastBlock 等等的块，这里我们会提到 BufferBlock，最大的原因是它采用了生产者-消费者模型，并且 BlockingCollection 、 BufferBlock 、Channel 其实代表了 .NET 的不同阶段，而回想起不同阶段时的你，这注定是一个令人唏嘘的故事啦！沿用这种思想，我们就找到了 Channel 的新玩法：\n// GetFiles Task\u0026lt;Channel\u0026lt;string\u0026gt;\u0026gt; GetFiles(string root) { var filePathChannel = Channel.CreateUnbounded\u0026lt;string\u0026gt;(); var directoryInfo = new DirectoryInfo(root); foreach (var file in directoryInfo.EnumerateFileSystemInfos()) { filePathChannel.Writer.TryWrite(file.FullName); } filePathChannel.Writer.Complete(); return Task.FromResult(filePathChannel); } // Analyse async Task\u0026lt;Channel\u0026lt;string\u0026gt;[]\u0026gt; Analyse(Channel\u0026lt;string\u0026gt; rootChannel) { var counterChannel = Channel.CreateUnbounded\u0026lt;string\u0026gt;(); var errorsChannel = Channel.CreateUnbounded\u0026lt;string\u0026gt;(); while (await rootChannel.Reader.WaitToReadAsync()) { await foreach (var filePath in rootChannel.Reader.ReadAllAsync()) { var fileInfo = new FileInfo(filePath); if (fileInfo.Extension == \u0026#34;.md\u0026#34;) { var totalWords = File.ReadAllText(filePath).Length; counterChannel.Writer.TryWrite($\u0026#34;文章 [{fileInfo.Name}] 共 {totalWords} 个字符.\u0026#34;); } else { errorsChannel.Writer.TryWrite($\u0026#34;路径 [{filePath}] 是文件夹或者格式不正确.\u0026#34;); } } } counterChannel.Writer.Complete(); errorsChannel.Writer.Complete(); return new Channel\u0026lt;string\u0026gt;[] { counterChannel, errorsChannel }; } // Merge async Task\u0026lt;Channel\u0026lt;string\u0026gt;\u0026gt; Merge(params Channel\u0026lt;string\u0026gt;[] channels) { var mergeTasks = new List\u0026lt;Task\u0026gt;(); var outputChannel = Channel.CreateUnbounded\u0026lt;string\u0026gt;(); foreach (var channel in channels) { var thisChannel = channel; var mergeTask = Task.Run(async () =\u0026gt; { while (await thisChannel.Reader.WaitToReadAsync()) { await foreach (var item in thisChannel.Reader.ReadAllAsync()) { outputChannel.Writer.TryWrite(item); } } }); mergeTasks.Add(mergeTask); } await Task.WhenAll(mergeTasks); outputChannel.Writer.Complete(); return outputChannel; } // Run var filePathChannel = await GetFiles(@\u0026#34;/hugo-blog/content/posts/\u0026#34;); var analysedChannels = await Analyse(filePathChannel); var mergedChannel = await Merge(analysedChannels); while (await mergedChannel.Reader.WaitToReadAsync()) { await foreach (var item in mergedChannel.Reader.ReadAllAsync()) { Console.WriteLine(item); } } OK，这三个方法做了一件什么样的事情呢？我个人以为，这其实就是我们上面提到的数据流，首先，我们通过 GetFiles() 方法获得指定目录内的文件信息；然后，这些信息交给 Analyse() 方法去做处理，这里做的事情是统计出 markdown 格式文件的字符串，以及筛选出那些非 markdown 格式的文件或者子目录；最后，通过 Merge() 函数，我们将上一步的结果进行汇总输出。如果用一幅图来表示的话，它应该是下面这样的流程：\n利用 Channel 实现数据流模式\r从某种意义上来讲，这是一种“分治”策略，即：把一个大任务分解为若干个小任务，再将这些小任务的结果合并起来。很多年前，我曾在一本讲并行编程的书上见过类似的代码片段，那个时候我已经对 Google 的 MapReduce 略有耳闻，后来又接触到了 Parallel ，我突然意识到，如果 Map() 和 Reduce() 两个函数运行在一台远程服务器上，那么这个过程可以认为是 RPC，而运行在远程服务器上的这些函数，其实是在并行地执行着某种运算，那么这个过程可以认为是并行计算。当这些并行计算，使用的是世界各地的可伸缩计算资源时，那么这个过程其实就是云计算。所以说，写作这个过程还是挺有意思的，对不对？\n本文小结 很多年前，Wesley 老大哥聊起怎么把日志写到数据库的话题，那个时候，我们都还没有听说过 Elasticsearch 这个词汇。所以，我们当时能想到的方案，是打算用 BlockingCollection 来做一个阻塞式的队列，换句话讲，就是从 NLog 或者 Log4Net中拿到日志以后，将这些日志全部放在 BlockingCollection 里面，然后再考虑将其写入到数据库或者某种输出源。后来，我陆陆续续地接触了 NLog 里的 Target，Serilog 里的 Sink，大概知道了这一切是如何运作的，甚至这些日志组件都可以支持把日志输出到不同的地方。从某种意义上看，这种认知上的变化，或许宣告着过去那种不成熟思维的过时，可恰恰是因为这些不成熟的思维，会不断督促你刷新对一件事物的认知，换句话讲，我们的人生其实就是由无数个过去串联起来的，无论曾经的你有多糗多颓多丧，它们都在不遗余力地告诉你，我曾经真的在这个世界上存在过，就像今天有了更好用的 Channel , 并不意味着我们过去的思考没有意义，至少当我们提到生产者-消费者模型的时候，我会想起 Wesley 老大哥、想起 BlockingCollection。说不定啊，这就是时光对我这个记性太好的人的一种回应：你可以怀念过去，但请不要期待一切都能保持如初，在这个世界上，你只能做自己能做到的事情。\n","date":"2022-09-15T12:52:10Z","image":"/posts/getting-started-with-the-.net-in-process-queue-channel/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/getting-started-with-the-.net-in-process-queue-channel/","slug":"Getting-Started-With-The-.NET-In-Process-Queue-Channel","tags":["Channel",".NET","Queue","刺客信条"],"title":".NET 进程内队列 Channel 的入门与应用"},{"categories":["编程语言"],"content":" 在很长的一段时间里，我们的项目中一直使用 OnMethodBoundaryAspect 这个基类来记录每个方法的日志。诚然，FodyWeavers.xml 这个文件的存在，早已在冥冥之中暗示我，Fody 才是这座冰山下真正的墨西哥湾暖流。可惜，因为某种阴差阳错的巧合，譬如两者都使用了 OnMethodBoundaryAspect 这个命名，这导致我过去一直以为我们使用的是 PostSharp。如果你是用过 ReSharper 或者 Rider 这些由 JetBrains 出品的工具，你大概会听说过 PostSharp。不过，有趣的是，JetBrains 和 PostSharp 其实没有半毛钱的关系，两者唯一相似的地方，或许是它们都不姓微软:joy:。当我们谈论 PostSharp 的时候，我其实想说的是静态编织。由此，我们就引出了今天这篇文章的主题，即: .NET 中的静态编织。而对于静态编织，我们这里只需要知道，它是一种在编译时期间将特定的字节码插入到目标类和方法的技术。\n再从 AOP 说起 想不到吧，此去经年，我再一次聊起了 AOP 这个话题。众所周知，AOP 是指面向切面编程 (Aspect Oriented Programming)，而所谓的切面，可以认为是具体拦截的某个业务点。对于面向对象编程的语言来说，一个业务点通常就是一个方法或者函数。因此，我们谈论 AOP 这个话题的时候，更多的是指在某个方法执行前后插入某种处理逻辑。此时，广义的 AOP 就有静态编织和动态代理两种形式，前者发生在编译时期间，后者发生在运行时期间。如下图所示，我们平时使用的 Castle DynamicProxy 、 AspectCore、DispatchProxy 等等都属于动态代理的范畴，这些都是在运行时期间对代码进行“修改”；而我们今天要讨论的 Fody ，则是属于静态编织的范畴，顾名思义，它是在编译时期间对代码进行“修改”。我们知道，按照实现方式上的不同， AOP 又可以分为代理模式和父子类重写两种“修改”方式。至此，我们对于 AOP 的认知范围被进一步扩大，就像我们以前学习数学的时候，我们对于对于“数”的定义，是先从有理数扩充到无理数，后来又从实数扩充到虚数。那么，屏幕前的你，真的搞懂 AOP 了吗？\n广义上的面向切面编程\rFody 的初体验 作为一个类库，Fody 在使用上并没有任何非同寻常的地方。这意味着，你可以像使用任何一个第三方库一样，直接通过 NuGet 来安装：\ndotnet add package Fody --version 6.6.3 可惜，这样或许会令你感到失望。因为对于 Fody 而言，我们通常使用的是它的插件 (Add-In) 而不是 Fody 本身，除非当你需要真正编写一个插件。身处西安这个十三朝古都，你一定听说过鼎鼎大名的三秦套餐，即：凉皮、冰峰、肉夹馍。这里，我们就以 Rougamo.Fody 这个插件为例来快速体验一下 Fody 。首先，我们通过 NuGet 来安装该插件：\ndotnet add package Rougamo.Fody --version 1.1.2 接下来，我们来一起编写下面的代码，一个可以附加到方法上的特性 LoggingAttribute ：\n[AttributeUsage(AttributeTargets.Method)] public class LoggingAttribute: MoAttribute { public override void OnEntry(MethodContext context) { Console.WriteLine(\u0026#34;执行方法 {0}() 开始, 参数：{1}.\u0026#34;, context.Method.Name, JsonConvert.SerializeObject(context.Arguments)); } public override void OnException(MethodContext context) { Console.WriteLine(\u0026#34;执行方法 {0}() 异常, {1}.\u0026#34;, context.Method.Name, context.Exception.Message); } public override void OnExit(MethodContext context) { Console.WriteLine(\u0026#34;执行方法 {0}() 结束.\u0026#34;, context.Method.Name); } public override void OnSuccess(MethodContext context) { Console.WriteLine(\u0026#34;执行方法 {0}() 成功.\u0026#34;, context.Method.Name); } } 为了测试这个特性的效果，相应地，我们再准备几个简单的函数：\n[Logging] static int Add(int a, int b) =\u0026gt; a + b; [Logging] static Task\u0026lt;int\u0026gt; AddAsync(int a, int b) =\u0026gt; Task.FromResult(a + b); [Logging] static decimal Divide(decimal a, decimal b) =\u0026gt; a / b; 此时，如果我们尝试调用这些方法，就可以获得下面的结果：\n使用 Rougamo.Fody 记录日志\r可以注意到，这个插件可以悄无声息地帮我们“织入”指定代码，就像小时候妈妈为我们织毛衣一样。那么，这一切究竟是怎么做到的呢？ 我想，这一切要从编译原理开始讲起 :ghost: 。\n.NET 编译原理示意图\r我们都知道，对于 C# 这门语言来说，它第一步是被编译为 IL，即中间代码，然后再经过 JIT 编译为本地代码。当然，现在 .NET 已经支持 AOT 编译模型。可不管使用哪一种编译模型，Fody 的势力范围始终都是 IL 被送到 JIT 或者 AOT 之前，即编译时期间。因此，Fody 或者说静态编织的本质，其实就是对第一步生成的 IL 进行修改，如下图所示，Fody 通过通过插件或者 ModuleWeaver 来对 IL 进行修改：\n.NET 静态编织原理示意图\r对于我们这个示例而言，你可以认为，是 Fody 帮我们把 LoggingAttribute 里的这些代码，悄无声息地“编织”进了我们的代码里，这一点怎么验证呢？其实，我们只需要通过 Ildasm.exe 这个工具对代码进行反编译即可，通常情况下，这个工具位于以下路径：C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools。此时，结果如下图所示：\n谁动了我的代码 A\r可以看到，我们自己写的 Add() 方法，确实被 Fody 给修改了，它用一个 try-catch 语句块把我们的代码包裹在其中，然后在 catch 和 finally 块里分别调用 OnException() 和 OnSuccess() 两个方法。果然，哪有什么岁月静好，不过是在有人替你负重前行，对不对？\n第一个 Add-In 好了，从接触 Fody 到现在，我们一直都是在使用别人写好的插件。虽然通过反编译代码，我们大概知道了 Fody 对我们的代码做了什么。可是，博主相信大家和我一样，对于 Fody 的原理还是云里雾里。下面，我们通过一个自定义的插件来展示更多的细节。按照程序世界的惯例，我们还是从 HelloWorld 开始，对于每一个 Fody 插件而言，其命名风格类似于 Rougamo.Fody，因此，我们创建一个名为 HelloWorld.Fody 的类库项目：\ndotnet new classlib --name HelloWorld.Fody 接下来，我们定义一个特性 HelloWorldAttribute ，这个特性里什么都不用写，我们这里只用它来打标记：\n[AttributeUsage(AttributeTargets.Method)] public class HelloWorldAttribute : Attribute { } 这里我们希望做一件什么事情呢？我们希望每一个打上 [HelloWorld] 标签的方法，都可以自动输出一句：Hello World. 。参考 Fody 的插件开发规范，我们定义一个名为 ModuleWeaver 的类，它继承自 BaseModuleWeaver 这个类，为了使用这个类型，你还需要通过 NuGet 安装 FodyHelpers 这个包。默认情况下，我们需要重写 Execute() 和 GetAssembliesForScanning() 两个方法：\npublic override void Execute() { foreach (var type in ModuleDefinition.Types) { foreach (var method in type.Methods) { var customerAttribute = method.CustomAttributes.FirstOrDefault(x =\u0026gt; x.AttributeType.Name == nameof(HelloWorldAttribute)); if (customerAttribute != null) { ProcessMethod(method); } } } } 首先，这里的 ModuleDefinition 属性定义来自父类，它包含了本次编织中所有可以使用的类型信息，如果你稍微仔细一点，就会发现 ModuleDefinition 其实是定义在 Mono.Cecil 这个程序集里面的，这恰恰印证了我们一开始的说法，即：Fody 是基于 Mono.Cecil 的扩展库。当然，这些细节暂时无关紧要。因为对我们来说，我们只需要遍历所有类型中的方法，然后判断这个方法上面有没有附加 HelloWorldAttribute 这个特性即可。至于具体怎么实现 ProcessMethod() 这个方法，我们可以先暂时放到到一边。\npublic override IEnumerable\u0026lt;string\u0026gt; GetAssembliesForScanning() { yield return \u0026#34;mscorlib\u0026#34;; yield return \u0026#34;System\u0026#34;; } 其次，考虑到我们需要调用的 Console.WriteLine() 方法是位于 System 命名空间下面，因此，我们需要在 GetAssembliesForScanning() 这个方法里返回 System 和 mscorlib ，这一步的目的是告诉 Fody ，它应该去哪里找这些引用了的类型。\npublic static void Echo() { Console.WriteLine(\u0026#34;Hello World.\u0026#34;); } OK，让我们先暂时将眼睛从 Fody 上移开，既然 Fody 的秘诀是修改 IL ，我们不妨先从 IL 上入手。这里，我们准备一个 Echo() 方法，然后看看它编译为 IL 会变成什么样子：\n从 C# 代码到 IL 代码\r这里简单翻译下这段代码，IL_0000 这一行表示把字符串 Hello World. 放入栈中；IL_0005这一行表示调用 Console.WriteLine() 这个方法；IL_000a 这一行表示返回值。当然，对于 void 类型的方法来说，这一句相当于什么都不做。这样，我们就认识了三个指令：ldstr、call、ret。这里再增加一个指令 nop，它相当于我们平时写代码时的空行，有了这四个指令，我们就可以尝试用 IL 来编程啦！如果你接触过 Emit, 上面这段代码可以改写为下面这样：\n// 创建一个无参、无返回值的方法 var dynamicEcho = new DynamicMethod(\u0026#34;DynamicEcho\u0026#34;, null, Type.EmptyTypes); // 利用 IL 填充方法体 var ilGenerator = dynamicEcho.GetILGenerator(); ilGenerator.Emit(OpCodes.Ldstr, \u0026#34;Hello World.\u0026#34;); ilGenerator.Emit(OpCodes.Call, typeof(Console).GetMethod(\u0026#34;WriteLine\u0026#34;, new Type[] { typeof(string) })); ilGenerator.Emit(OpCodes.Ret); // 调用方法 var dynamicEchoAction = dynamicEcho.CreateDelegate(typeof(Action)) as Action; dynamicEchoAction.Invoke(); 为什么要写这样一段代码呢？首先，是因为我确实不会 Emit，我需要了解它；其次，写 Emit 能加深我们对于 ldstr、call、ret 这三个指令的理解。有了这个基础，我们就可以来实现 ProcessMethod() 这个方法啦：\nprivate MethodInfo _writeLineMethod =\u0026gt; typeof(Console).GetMethod(\u0026#34;WriteLine\u0026#34;, new Type[] { typeof(string) }); private void ProcessMethod(MethodDefinition method) { // 获取当前方法体中的第一个 IL 指令 var processor = method.Body.GetILProcessor(); var current = method.Body.Instructions.First(); // 插入一个 Nop 指令，表示什么都不做 var first = Instruction.Create(OpCodes.Nop); processor.InsertBefore(current, first); current = first; // 构造 Console.WriteLine(\u0026#34;Hello World\u0026#34;) foreach (var instruction in GetInstructions(method)) { processor.InsertAfter(current, instruction); current = instruction; } } private IEnumerable\u0026lt;Instruction\u0026gt; GetInstructions(MethodDefinition method) { yield return Instruction.Create(OpCodes.Nop); yield return Instruction.Create(OpCodes.Ldstr, \u0026#34;Hello World.\u0026#34;); yield return Instruction.Create(OpCodes.Call, ModuleDefinition.ImportReference(_writeLineMethod)); } 这里，每一个 Instruction 对应着 IL 代码中的一条指令，一旦理解了这一点，静态编织本质上不就是修改 IL 代码吗？所以，我们的做法是：在原有方法的第一个指令前插入一个 nop 指令，然后再在这个 nop 指令后面插入我们自己的指令。这部分指令我们已经用 Emit 写过一次，这里直接抄下来就好啦！至此，我们就完成了整个 HelloWorld.Fody 插件的编写，该插件的目录结构如下：\nHelloWorld.Fody |--- HelloWorld.Fody.csproj |--- HelloWorldAttribute.cs |--- ModuleWeaver.cs OK，现在我们还有最后一个问题要解决，即：Fody 是如何找到这些插件的。一开始在体验 Rougamo.Fody 的时候，我们完全没有这方面的困惑，这是因为它可以在程序生成时，自动找到对应的插件并在 FodyWeavers.xml 文件中生成下列内容：\n\u0026lt;Weavers xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:noNamespaceSchemaLocation=\u0026#34;FodyWeavers.xsd\u0026#34; \u0026gt; \u0026lt;Rougamo /\u0026gt; \u0026lt;/Weavers\u0026gt; 是的，Fody 的秘密终于被揭晓了，它通过这个 XML 文件来决定编译时使用哪些插件。不过，博主在写这篇文章的时候发现，我们自己编写的这个插件，不管是通过项目引用还是包引用的方式，Fody 都提示找不到对应的插件，这意味着编译会失败，即使我照猫画虎一般地在 FodyWeavers.xml 文件中加入\u0026lt;HelloWorld /\u0026gt; 节点。最终，我在 官方文档 中找到了答案，Fody 默认会从下面三个路径检索插件：\nNuGet Package 目录 解决方案路径下的 Tools 目录 解决方案中叫做 Weavers 的项目 不过，最简单粗暴的做法，还是在工程文件中指定插件的路径。虽然，我还是不知道，我写的这个插件和别人写的插件，到底差在哪里？\n\u0026lt;Project xmlns=\u0026#34;http://schemas.microsoft.com/developer/msbuild/2003\u0026#34;\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;WeaverFiles Include=\u0026#34;$(MsBuildThisFileDirectory)..\\weavers\\HelloWorld.Fody.dll\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Project\u0026gt; 现在，万事具备，只欠东风，我们修改一下 Ehco() 方法，添加 [HelloWorld] 这个特性：\n[HelloWorld] public static void Echo() { Console.WriteLine(\u0026#34;Hello Fody.\u0026#34;); } 此时，我们会得到下面的结果。其中，Hello World. 是我们通过静态编织插入的代码，Hello Fody. 是 Echo() 方法自身的代码：\nHelloWorld.Fody 插件使用效果展示\r现在，你会作何感想，是不是对 AOP 的印象更深了一点，忽略那些切面、拦截器、代理对象、被代理对象\u0026hellip;等等的概念，你最终会发现，AOP 越来越呈现出一种返璞归真的状态，毕竟，我们只需要在某个方法体的第一条指令前、最后一条指令后，各自插入一组指令即可。虽然从我接触编程的那一刻起，已经完全不需要再去学习晦涩难懂的汇编语言，可是写这篇文章的时候，我好像知道了写汇编是一种什么样的感觉:smirk:\u0026hellip;\n本文小结 广义的面向切面编程，有静态编织和动态代理两种形式，它们都可以在某个方法执行前后插入某种处理逻辑。不同的地方在于，前者发生在编译时期间，后者发生在运行时期间。对于 .NET 而言，最常见的静态编织方案是 PostSharp 和 Mono.Cecil，两者的区别是：一个付费、一个免费。本文介绍的 Fody 是一个基于 Mono.Cecli 的扩展库，通过 Fody 的各种插件，我们可以向已有代码织入特定的功能，譬如 Rougamo.Fody 这个插件可以让我们对方法进行拦截。基于这个原理，我们实现了一个完全不同于动态代理的拦截器。动态编织的本质是修改 IL 代码，对于这一点我们可以通过 ILdasm.exe 这个工具来验证。为了进一步了解 Fody 是如何修改 IL 代码的，我们参照 Fody 的规范实现了一个自定义的插件，在这个过程中，我们了解了几个常见 IL 指令，以及如何通过 Emit 来生成 IL 指令。此时，我们就接触到比表达式树更为底层的东西，而操作 IL 指令更是让我们体会到写汇编语言的酸爽，同时让我们对 .NET 的编译原理有了更为直观的认识。\n","date":"2022-08-23T12:52:10Z","image":"/posts/implement-static-weaving-of-dot-net-via-fody/weaving-loom-g2d21b4b53_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/implement-static-weaving-of-dot-net-via-fody/","slug":"Implement-Static-Weaving-Of-Dot-NET-Via-Fody","tags":["Fody","PostSharp","AOP","静态编织"],"title":"使用 Fody 实现 .NET 的静态编织"},{"categories":["编程语言"],"content":"最近，我收到一位读者朋友的私信，问我 ELK 为什么没有下篇，道德感极强的我不得不坦诚相告，显然这一篇鸽了。这就是说，鸽子不单单会出现在吴宇森的电影里，只要你试图拖延或者逃避，你一样有鸽子可以放飞。话说回来，新冠疫情已然持续了三年，而这篇文章其实是我在新冠元年写下的。某年某月，彼时彼刻，立春过后紧接着是上元节，阳光已透过玻璃宣示着春天的到来，可在这一墙之隔的里里外外，仿佛是两个气候迥异的世界。记忆里那种每天都和消毒水、口罩打交道的日子，后来就变成了一种习以为常、甚至有一点唏嘘的常态化生活。在这过去的三年里，恍惚中已经发生太多的事情，譬如 ELK 早已变成了 EFK，譬如前女友有了新的男朋友，在一切的物是人非背后，在一切的断壁残垣下面，我想，我还是用这个旧题目来讲一个新的故事罢！\n从 Logstash 到 Filebeat 当初准备写这个系列的时候，ELK 还是经典的 Elastaicsearch 、 Logstash 和 Kibana 组合，如下图所示，Logstash 从各种不同的数据源收集数据，通过内置的管道对输入的数据进行加工。最终，这些数据会被存储到 Elastaicsearch 中供 Kibana 完成数据可视化。 即使放到三年后的今天来看，这张图依然是非常经典的一幅图。为什么这么说呢？因为自此以后，可视化日志分析平台的搭建，基本都是围绕这三个方面展开，甚至 Logstash 的继任者 Filebeat、Fluentd、Fluent-Bit 等等无一不沿用了 Logstash 的这套管道设计，足可见其对后来者的影响之深远。不过，作为先驱出现的 Logstash，其本身是采用 Java 语言开发的，其插件则是采用 Ruby 语言开发的，特别是第一点，一直让 Logstash 在性能问题上遭人垢病。在实际使用中，你常常需要在每一台服务器上安装 Logstash ，这意味着它在 CPU 和内存上的占用会比较高。\n经典的 ELK 全家桶组合\r为了解决这个问题，Elastic 官方推出了被称为 Beats 的下一代日志收集方案， 这是一种基于 Go 语言开发、更加轻量级的、资源占用更少的日志收集方案，可以认为是 Logstash 的替代品, 而 Filebeat 正好是其中一种实现。关于这两者的区别，我想，使用下面的比喻或许会更恰当一点， Logstash 就像一个轰鸣声不断的垃圾转运车，虽然可以让你直接把垃圾丢车上拉走，可你不得不忍受一整天的噪音；Filebeat 则像一个拎着扫帚和簸萁的环卫工人，那里有需要就去哪里清扫，不单单效率高而且不会让你感觉扰民，下面是一张来自 Elastic 官方文档 中的示意图：\nFilebeat 日志收集示意图\r从这里我们可以看出， Filebeat 由两个主要的组件 Inputs 和 Harvester 组成。其中， Harvester 是一个负责读取单个文件内容的采集器，它可以打开和关闭一个文件，并将内容发送到指定的输出；Inputs 顾名思义就是输入，对于 Filebeat 而言，其实就是指各种不同类型的日志文件，譬如 Filebeat 可以支持 Kafka、Redis、MQTT、TCP、UDP、Stdin、Syslog 等等的输入。从某种意义上讲，你可以把 Filebeat 理解为一个文件扫描服务。例如，下面的配置表示 Filebeat 将会从一个指定的路径读取日志文件：\nfilebeat.inputs: - type: log enabled: true paths: - /var/logs/*.log 我们都知道，通常情况下，Docker 会将其日志存储在以下位置：/var/lib/docker/containers/*/*.log，并且其日志格式为 JSON。考虑到 Filebeat 本身就支持 JSON 格式，所以，你还可以通过下面类似的方式来收集 Docker 容器的日志：\nfilebeat.inputs: - type: log enabled: true paths: - /var/lib/docker/containers/*/*.log 除此以外，我们提到 Filebeat 内置了大量的模块，使其可以支持 Apache、Kakfa、Envoy、MySQL 等等第三方系统的日志采集，通常位于 module.d这个文件夹中。如下图所示，禁用的模块将会以 .disable 结尾：\nFilebeat 中内置的模块一览\r接下来，为了使用这些模块。首先，我们要在 filebeat.yml 这个配置文件中增加下列内容，这相当于指定了模块所在的目录：\nfilebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: true 此时，我们进入容器并执行下列命令就可以启用 envoyproxy 这个模块：\n./filebeat modules enable envoyproxy 实际上，这个命令的作用就是移除 .disable 这个后缀，同时重新载入配置。同理，如果我们将 enbale换成 disable 则可以禁用一个模块。模块可以理解为官方帮你提前写好了各种 Input 配置，当然，如果光是启用一个模块是没有用的，你还需要对这些模块进行简单的配置，最基础的一点，就是你需要告诉这些模块你的日志文件存储在哪里，以博主这里的 Envoy 为例，我们执行下面的命令：\nvi ./modules.d/envoyproxy.yml 此时，我们可以看到下面的内容，我们只需要覆盖 var.paths这个参数即可：\n# Module: envoyproxy # Docs: https://www.elastic.co/guide/en/beats/filebeat/7.5/filebeat-module-envoyproxy.html - module: envoyproxy # Fileset for native deployment log: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. var.paths: [\u0026#34;/var/logs/request_log.log\u0026#34;] 对于 Filebeat 而言，其最基础的输出是 Logstatsh 和 Elasticsearch，下面是相应的配置，当然，这里给出的都是最最基础的、没有任何套路的配置，如果你还需要密码、证书这些更细节的东西，请以官方文档为准，谢谢。\n# 输出日志到 Logstash output.logstash: hosts: [\u0026#34;logstash:5044\u0026#34;] # 输出日志到 Elasticsearch output.elasticsearch: hosts: [\u0026#34;192.168.50.162:9200\u0026#34;] 这里，我们以输出到 Elasticsearch 为例，默认情况下，Filebeat 会生成类似于 filebeat-6.3.2-2017.04.26 这样的索引文件，如果你不满足于这个索引名称，你可以在 output.elasticsearch 配置项下设置 index 字段，博主这里依然采用默认配置，如图所示，我们在 Kibana 中可以找到下面的索引：\n通过 Filebeat 采集到的日志数据-A\r更进一步地，我们可以看到每个经过 Envoy 代理的请求日志。这样，我们就达到了通过 Filebeat 收集日志的目的。和你们一样，在看到这个结果前，我已经在心里演练了无数遍的当当当当：\n通过 Filebeat 采集到的日志数据-B\r坦白地讲，如果所有的日志格式都能做到统一规范，譬如，全部采用 JSON 格式来存储，此时，通过 Filebeat 来收集日志还是挺惬意的一件事情，因为不需要操心日志解析或者说 Grok 这种东西。其实 Logstash 在性能上的瓶颈我还没有触及，真正劝退我的正是这套基于 Ruby 的 DSL，曾经我选择 Hexo 而不是 Jekyll，某种程度上是因为我抵触 Ruby 这门语言。总而言之，你可以自行决定是否跳过 Logstash 这个环节，因为据官方说后来的 Eleasticsearch 已经具备数据加工和过滤的能力，从这个角度看起来， Logstash 并不是唯一的选择，你觉得呢？更详细的代码和配置请参考我的 Github。\n从 Filebeat 到 Fluentd 当然，Filebeat 的轻量级是以牺牲了一部分功能为代价的，譬如最典型的缺陷是，它不具备像 Logstash 那样强大的日志解析能力，在 codec 和 filter 这两个方面存在短板，虽然官方内置了大量的模块使其可以支持 Nginx、Envoy、Kafka、Redis 等等日志收集，可实际上，Filebeat 更为普遍的使用场景还是从一个文件里读出日志，然后将其丢给 Logstash 或者 Elasticsearch，这意味着缓冲的压力只会出现在输出这个阶段。所以，你会注意到，现在人们开始考虑把日志转发到 Kafka 或者 Redis，以期达到“削峰”的目的。此时，Filebeat 的轻量级仿佛就成为了一种罪过，人们基于 Filebeat 开发了 Fluntd，一种支持多个输入/输出，插件和功能更为丰富的日志收集工具。\nFluentd 日志收集示意图\r不过，身处在一个技术更新换代非常快的时代，“过期”才是这个时代永恒的话题，一如王家卫电影里的那句经典台词：不知道从什么时候开始，在什么东西上都有个日期，秋刀鱼会过期，肉罐头也会过期，连保鲜纸都会过期 ，我开始怀疑，在这个世界上，还有什么东西是不会过期的？我不知道这是不是一种悲哀，因为等到博主打算写这篇文章的时候，Fluentd 已然有了新的挑战者 Fluent-Bit，更神奇的是这两个产品都是来自同一家公司的，难道国外一样喜欢赛马的吗？总而言之，虽然这里的标题是 Fluentd，但我实际上用的是 Fluent-Bit，关于这两者间的关系，官方曾给出过一个对比：\nFluentd 与 Fluent-Bit 的对比\r从这张图中，我们可以看出，两者在性能上的差异并不显著，区别只是在实现的语言、插件数量上面，因为 Fluentd 需要依赖 Ruby 环境，我果断选择了放弃，好吧，我承认是因为现在的公司在用 Fluent-Bit，我客观上受到了这一影响。因此，我接下来分享的内容，始终都是以 Fluent-Bit 为准，我将会分享两种容器下的日志收集策略，即 Tail 模式 和 Forward 模式。\nTail 模式 Tail 模式， 这里指 Fluent-Bit 中的一种输入模式，你可以认为它就是 Filebeat 里最经典的基于日志文件的收集模式，如图所示，假设我们我们有多个应用，它们内部已经通过 NLog 或者 Serilog 输出了 JSON 格式的日志文件，此时，我们只需要通过配置 Fluent-Bit 中的输入/输出就可以实现日志的收集：\nFluent-Bit 日志收集 - Tail 模式\r我们前面提到过， Fluent-Bit 支持多个输入和输出，下面是一个简单的示例，它可以将当前 CPU 的使用情况输出到控制台中，可以注意到，输入和输出是通过标签来进行匹配的：\n[INPUT] Name cpu Tag cpu_logs [OUTPUT] Name stdout Match cpu_logs 更进一步地，你可以考虑将其输出到 Elasticsearch，为此，我们需要新增加一个 OUTPUT 节点：\n[INPUT] Name cpu Tag cpu_logs [OUTPUT] Name stdout Match cpu_logs [OUTPUT] Name es Match cpu_logs Host efk_es Port 9200 Index cpu_logs Logstash_Format On Logstash_Prefix cpu_logs Logstash_DateFormat %Y-%m Include_Tag_Key On 此时，如果不出意外的话，你可以在 Kinana 中看到下面的信息，这就是我们通过 Fluent-Bit 采集到的日志信息，这相当于一个简单的热身，目的是帮助大家快速地掌握 Fluent-Bit 配置文件的结构 ：\nFluent-Bit 日志收集 - Tail 模式 - 采集 CPU 日志\r类似地，对于一般的日志文件，我们可以按下面这种方式来进行配置，它表示从 /var/log/*.log 这个路径采集日志，按 JSON 格式解析并将其输出到 Elasticsearch 中：\n[INPUT] Name tail Path /var/log/*.log Path_Key On Parser json Tag app_logs Mem_Buf_Limit 5MB [OUTPUT] Name es Match app_logs Host efk_es Port 9200 Index app_logs Logstash_Format On Logstash_Prefix app_logs Logstash_DateFormat %Y.%m Include_Tag_Key On 需要说明的是，在 Fluent-Bit 中，每个输入或者过滤器的 Parser 是通过全局的 Parser_File 来指定的，该配置项位于 [SERVICE] 节点下：\n[SERVICE] Flush 1 Daemon Off Log_Level info Parsers_File parser.conf HTTP_Server On HTTP_Listen 0.0.0.0 HTTP_Port 2020 相对应地，我们需要准备一个 parser.conf 文件，这里我们定义了 json 和 docker 两种解析器，你还可以基于正则表达式定义更多的解析器：\n[PARSER] Name json Format json Time_Key time Time_Format %Y-%m-%d %H:%M:%S %z [PARSER] Name docker Format json Time_Key time Time_Format %Y-%m-%d %H:%M:%S.%L Time_Keep On 一切准备就绪后，你就可以在 Kibana 中看到对应的日志啦，眼尖的朋友会发现这是一个典型的 ASP.NET Core 应用。还是那句话，如果能从日志源头上进行规范化，那么采集日志的时候就会相对轻松一点：\nFluent-Bit 日志收集 - Tail 模式 - 采集应用日志\r如果希望了解更多细节，请参考： https://github.com/Regularly-Archive/2022/tree/main/src/ELK/EFK。\nForward 模式 Forward 模式需要借助 Docker 的 logging-driver ，我们前面提到过，Docker 产生的日志文件，默认是存储在 /var/lib/docker/containers/*/*.log 这个路径下，并且这些日志文件是 JSON 格式的，这里面隐含的信息，其实就是 Docker 的 logging-driver，简单来说，正因为 Docker 的 logging-driver 默认值是 json-file，所以，它才会有产生 JSON 格式的日志文件这样一种行为。事实上，除了 json-file 以外，Docker 还支持下面的这些“驱动”：\nDocker 的 Logging Driver 有哪些？\r在这里，我们会发现一个熟悉的身影 Fluentd，所以，如果你问我 Fluentd 相比 Filebeat 还有什么优点，现在我或许会说，它天然地被 Docker 的日志驱动支持着。因为 Fluentd 和 Fluent-Bit 师出同门，所以，后者完全支持前者的协议。此时，我们的日志收集策略就不再是读取日志文件，而是直接通过 Docker 转发到 Fluent-Bit，这种模式就被成为 Forward 模式，它是 Fluent-Bit 中的一种新的输入。\nFluent-Bit 日志收集 - Forward 模式\r如图所示，在编排服务的环节，我们将应用的 logging-driver 设置为 fluentd，同时指定 fluentd 或者 fluent-bit 的地址。此时，我们只需要在 Fluent-Bit 的配置文件中处理输入/输出即可完成日志的收集：\nversion: \u0026#39;3\u0026#39; service: fluent_bit: image: cr.fluentbit.io/fluent/fluent-bit container_name: efk_flb volumes: - ./config/fluent-bit/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:rw - ./config/fluent-bit/parser.conf:/fluent-bit/etc/parser.conf:rw ports: - \u0026#34;2020:2020\u0026#34; - 24224:24224 - 24224:24224/udp - 5140:5140/udp app: build: app/ container_name: efk_app ports: - 2333:80 logging: driver: fluentd options: tag: docker-app fluentd-address: fluent-bit:24224 depends_on: - fluent_bit 其中 24224 这个端口是 Forward 模式需要用到的一个端口，它是在哪里用到了呢？其实是在 Fluent-Bit 的配置文件 fluent-bit.conf 里面：\n[INPUT] Name forward Listen 0.0.0.0 Port 24224 buffer_chunk_size 1M buffer_max_size 5M tag forward_logs [OUTPUT] Name es Match forward_logs Host efk_es Port 9200 Index forward_logs Logstash_Format On Logstash_Prefix forward_logs Logstash_DateFormat %Y.%m Include_Tag_Key On 简单来说，Fluent-Bit 会接收经由 Docker 转发的日志，我们再将其输出到 Elasticsearch 中，这种情况下我们不用关心应用的日志存储在什么地方，只要是输出到控制台的日志，我们转发到 Fluent-Bit 即可，显而易见，这是一种容器级别的日志收集方案。以前用 Logstash 和 Filebeat 的时候，我曾纠结过这个这个问题，而直到此时此刻，我才真正找到答案。所以，这是否说明，时间和地点更重要，如果晚一点遇见某个人，你是不是会做得稍微好一点？\nFluent-Bit 日志收集 - Forward 模式 - 采集应用日志\r如果希望了解更多细节，请参考： https://github.com/Regularly-Archive/2022/tree/main/src/ELK/EFK。\n本文小结 准备这篇文章的这几天，有时会在脑海中重复说着一句话，大意是说，你可以怀念过去，可你不能一直都期待别人永远保持最初的样子。对我而言，这篇兴起于新冠元年的系列文章，一切都早已与当时相去甚远，因此，怀着一种物是人非、沧海桑田的心态来写这篇文章，我大抵是有一点从头再来的觉悟在里面。顺着 Logstash -\u0026gt; Filebeat -\u0026gt; Fluent-Bit 这样的脉络一点点探索，这三年间陆陆续续地经历了从 Wndows 到 Linux 再到 Docker 这种平台上的变化，日志收集的关注点更是从单个日志文件扩展到微服务、集群。整体而言，非侵入式的日志收集始终要比侵入式的日志收集要好，在源头上控制日志格式要比通 Grok 或者正则来加工要好。也许。日志收集的工具不止 ELK，可一番探索下来你会发现它们是殊途同归，最难的工作其实是如何组合或者协调这些工具，这篇文章的内容相对琐碎，对于那些我没有写出来的内容，欢迎大家在评论区积极留言、参与讨论，好了，以上就是这篇文章的全部内容啦！\n","date":"2022-08-07T16:01:13Z","image":"/posts/3687594959/logs-g459bb0417_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/3687594959/","slug":"3687594959","tags":[".NET Core","ELK","日志","监控"],"title":".NET Core + ELK 搭建可视化日志分析平台(下)"},{"categories":["前端开发"],"content":"相信大家已经注意到我博客有了一点变化，因为博主最近利用空闲时间对博客进行了优化。经过博主的不懈努力，首屏渲染时间从原来的 2.0 秒缩短到了 1.7 秒。虽然这个优化相当得感人，不过我还是在这个过程中有所收获。Stack 这个主题中大量使用了图片这种元素，特别是首页中那些作为文章封面而存在的图片。我原本是打算借鉴一下 Wincer 这位网友的博客样式，可是考虑到选择封面、图片尺寸\u0026hellip;等等的因素，我最终还是决定写一个相对“平庸”的布局样式，即你现在看到的这个版本，本次优化的重点主要在于使用 CDN 加速、对图片进行压缩、编译期生成缩略图、使用懒加载这些常见的策略。在今天这篇博客中，我们来重点聊一聊前端图片的懒加载，希望能为大家带来一点新的启发或者思考。\n什么是懒加载 懒加载，即：LazyLoad，其核心全在于“懒”这个字眼上。虽然，这个字在生活中更多的是表示一种贬义，可正如气体有活性和惰性的区别，这里我们将其理解为延迟加载，或许会更合适一点，因为生活早已告诉我们，只要你打算偷懒，就一定会造成拖延。因此，懒加载其实就是一种通过延迟加载对网页性能进行优化的方法。一个典型的例子是，当网页中有滚动条的时候。此时，网页的一部分区域对于浏览器视窗而言是不可见的。如果将一次性将其加载出来，这其实是一种资源的浪费，因为你不确定用户是否有耐心浏览完整个网页。在对网页的浏览量进行评估的时候，通常都会有一个跳出率的概念。可想而知，用户更容易被网页上的超链接吸引，在不同的网页间跳转。退一步讲，如果一个网页上有非常多的图片，等待这些图片全部加载完会浪费大量时间，进而影响到用户体验。博主原本就是为了减少首屏渲染时间，所以，不管从哪一个角度来看，懒加载或者说延迟加载，对于前端的性能优化都有着极其重要的意义，而这正是博主写作这篇文章的原始动机所在。\n骨架屏利用懒加载来提升用户体验\r如何实现懒加载 我们知道，对于图片而言，我们只要设置了其 src 属性，它就可以自动载入图片。因此，图片的懒加载，其实就是让设置 src 属性这个行为延迟执行，譬如，当一张图片出现在用户的视野当中的时候，我们再去设置其 src 属性，这样就可以达到延迟加载的目的。显然，首次需要加载的图片数量越少，首屏渲染时间就会越短，这不正是我们想要达到的目的吗？基于这种朴实无华的思路，这里我们介绍三种实现延迟加载的方案，如果大家还有更好的方案，欢迎大家在评论区补充或者讨论。\n监听滚动事件 首先，我们最容易想到的一种思路是，监听网页的滚动事件，因为我们更希望看到的结果是，当元素滚动到可视视口内的时候再去加载。此时，问题的关键是如何判断当前元素在可视视口内，在解决这个问题之前，我们先来看看下面这幅图片，它展示了网页中的 clientHeight、scrollTop 以及 offsetTop 这三个数值间的关系：\nclientHeight、scrollTop 以及 offsetTop\r可以注意到，当 clientHeight(H) + scrollTop(S) \u0026gt; offsetTop 的时候，即表示当前元素位于可视视口内。基于这个思路，我们可以编写出下面的代码：\nlet lazyLoadByDefault = function(imgs) { var H = document.documentElement.clientHeight; var S = document.documentElement.scrollTop || document.body.scrollTop; for (var i = 0; i \u0026lt; imgs.length; i++) { if (H + S \u0026gt; getTop(imgs[i])) { if (imgs[i].getAttribute(\u0026#39;data-loading\u0026#39;) == \u0026#39;lazy\u0026#39; \u0026amp;\u0026amp; imgs[i].getAttribute(\u0026#39;data-src\u0026#39;)) { let src = decodeURI(imgs[i].getAttribute(\u0026#39;data-src\u0026#39;)) imgs[i].src = src imgs[i].removeAttribute(\u0026#34;data-loading\u0026#34;) } } } } window.onload = window.onscroll = function() { var imgs = document.querySelectorAll(\u0026#39;img\u0026#39;); lazyLoadByDefault(imgs) } 其中，getTop() 方法用于计算 offsetTop，为什么不直接使用这个值呢，因为这个值是相对于父元素而言的，所以，考虑到元素嵌套的问题，我们必须要计算出每一个层级相对于父级的 offsetTop，然后再将它们累加起来。除此之外，我们给每个 Img 元素增加了一个自定义属性 data-src，它里面放置的就是真正的图片地址，我们只要在合适的时机将其赋值给 src 即可。当然，为了效果更好一点，你可以准备一张表示 loading 的图片放在 src 属性上：\nlet getTop = function(e) { var T = e.offsetTop; while(e = e.offsetParent) { console.log(e) T += e.offsetTop; } return T; } 相应地，此时我们需要像下面这样来准备 HTML 结构：\n\u0026lt;img src=\u0026#34;./imgs/loading.gif\u0026#34; data-src=\u0026#34;./imgs/1.jpg\u0026#34; data-loading=\u0026#34;lazy\u0026#34; alt=\u0026#34;1\u0026#34; /\u0026gt; 考虑到滚动事件可以引起图片的重复加载，这里采用的方案是：增加一个一个自定义属性 data-loading，并在加载完后移除该属性。定义这样一个属性的原因，一定程度上是为了避免和 loading='lazy' 撞车，而关于这个特性，我们会在下面的内容中做进一步的讲解。在这里，如果你对于 clientHeight、scrollTop 以及 offsetTop 这三个数值一脸懵逼的话，我们还有一种稍微简单一点的做法，即调用 getBoundingClientRect() 这个方法，它会返回当前元素相对于可视视口的位置信息。此时，只要满足 top \u0026lt; clientHieght 这个条件即可。因此，上面的代码可以进一步简化为：\nlet lazyLoadByDefault = function(imgs) { var H = document.documentElement.clientHeight; for (var i = 0; i \u0026lt; imgs.length; i++) { var bounding = imgs[i].getBoundingClientRect() if (bounding.top \u0026lt;= H) { if (imgs[i].getAttribute(\u0026#39;data-loading\u0026#39;) == \u0026#39;lazy\u0026#39; \u0026amp;\u0026amp; imgs[i].getAttribute(\u0026#39;data-src\u0026#39;)) { let src = decodeURI(imgs[i].getAttribute(\u0026#39;data-src\u0026#39;)) imgs[i].src = src imgs[i].removeAttribute(\u0026#34;data-loading\u0026#34;) } } } } 下面是博主编写的一个简单示例，仅供大家参考：\nSee the Pen Marker-Clusterer by qinyuanpei (@qinyuanpei)\ron CodePen.\rIntersectionObserver 除了使用上面的手工计算的方式，我们还可以使用 IntersectionObserver 这个 API。关于这个 API，官方是这样描述的： IntersectionObserver 接口提供了一种异步观察目标元素与其祖先元素或顶级文档视窗交叉状态的方法。祖先元素与视窗被称为根。我们尝试将其翻译成人话，大意就是说，这个 API 可以判断目标元素是否与顶级文档视窗交叉(重叠)，两者重叠其实就是说目标元素在可视视口内。一旦理解了这一点，我们就可以轻而易举地写出下面的代码：\nlet lazyLoadByObserver = function(lazyImages) { let lazyImageObserver = new IntersectionObserver(function(entries, observer) { entries.forEach(function(entry) { if (entry.isIntersecting) { let lazyImage = entry.target; if (lazyImage.getAttribute(\u0026#39;data-loading\u0026#39;) == \u0026#39;lazy\u0026#39; \u0026amp;\u0026amp; lazyImage.getAttribute(\u0026#39;data-src\u0026#39;)) { let src = decodeURI(lazyImage.getAttribute(\u0026#39;data-src\u0026#39;)) lazyImage.src = src lazyImage.removeAttribute(\u0026#34;data-loading\u0026#34;) lazyImageObserver.unobserve(lazyImage); } } }); }); lazyImages.forEach(function(lazyImage) { lazyImageObserver.observe(lazyImage); }); } var imgs = document.querySelectorAll(\u0026#39;img\u0026#39;); lazyLoadByObserver(imgs); 可以注意到，我们会尝试去观察每一个 Img 元素，当它和可视视口交叉(重叠)时，表明它正位于可视视口内，此时，我们会从 data-src 属性上读取图片的地址，然后将其赋值给 src 属性。这样，我们就实现了图片的懒加载。我个人认为，这个 API 非常好用，考虑到第一种方案，你可能会因为搞不清楚那些数值而导致计算上的错误，这个方案就相对简单一点。当然，你真正应该考虑的是，它的兼容性如何：\nIntersectionObserver 兼容性\r从这张图中可以看出，除了寿终正寝的 IE 浏览器，一腔孤勇的 Safari 浏览器，这个 API 的兼容性还是挺不错的。如果你依然对这一点感到如履薄冰，可以使用下面的代码来进行兼容性判断：\nvar imgs = document.querySelectorAll(\u0026#39;img\u0026#39;); if (\u0026#34;IntersectionObserver\u0026#34; in window) { lazyLoadByObserver(imgs) } else { lazyLoadByDefault(imgs) } 相信你已经猜到，博主的博客其实是混合着使用了上面两种方案，在使用懒加载以后，首页打开的时候将不会再加载所有封面图片，而是等到这些封面图片出现在可视视口内的时候再去加载，正是这一点点微不足道的工作，让博客的首屏渲染时间缩短了 0.3 秒，我想说，这实在有趣！\nSee the Pen Marker-Clusterer by qinyuanpei (@qinyuanpei)\ron CodePen.\r浏览器原生方案 原生懒加载在不同浏览器中的支持情况\r从 Chrome 77、Firefox 75 及其以上版本开始，浏览器开始支持图片和框架的原生懒加载特性。这意味着，从这一刻开始，我们有了浏览器级别的原生懒加载方案，即：在 \u0026lt;img\u0026gt; 或者 \u0026lt;iframe\u0026gt; 标签上添加 loading='lazy' 这组属性即可，下面是一个非常朴素的示例：\n\u0026lt;img src=\u0026#34;./example.jpg\u0026#34; loading=\u0026#34;lazy\u0026#34; alt=\u0026#34;this is a example for image lazy loading.\u0026#34;\u0026gt; 由于是浏览器级别的懒加载方案，所以，它不需要我们再像上面两种方案一样，编写额外的 JavaScript 代码。事实上，loading 除了 lazy 这个取值以外，它还有下面两个取值：\nlazy：图片或者框架使用懒加载，即元素即将可见的时候加载，且浏览器内部会定义一个元素和可视视口的距离的阈值，越接近该值表明元素越可见。 eager：立即加载图像或者框架，无论元素是否在可视视口内可见。 auto: 默认值，当元素没有显式地设置 loading 属性或者 loading 属性不合法的时候采用，此时图片或者框架会按照浏览器自己的策略来加载，需要注意的是，目前该值已被废弃。 这个方案应该是三种方案中最简单的，可实际中还是多多少少有一些问题，譬如没有办法给定一个占位图片、每次加载图片的数量与屏幕高度、网速、窗口尺寸等因素有关，甚至连加载图片的顺序都是不确定的，这就意味着这中间有很多难以控制的因素，如果考虑到兼容性或者 Polyfill 的问题，这个方案就显得没那么有吸引力了，我在测试的过程中发现，有时候它是一次性就把多张图片加载出来了。所以，我个人的观点是，想偷懒的话可以直接用这个特性，可是如果要控制懒加载过程中的细节，这个特性就显得非常鸡肋了。\nSee the Pen Marker-Clusterer by andypotts (@andypotts)\ron CodePen.\r如图所示，随着你滑动鼠标滚轮，你会注意到它在加载指定的图片，这就是原生懒加载的基本用法啦！\n本文小结 本文主要分享了前端图片懒加载的三种实现思路，即监听滚动事件、IntersectionObserver 以及浏览器原生支持的 loading='lazy'。懒加载的基本思路是延迟加载，对图片而言，我们更希望它可以在即将出现在用户视野中的时候去加载，因为这样能减少不必要的资源请求，同时可以缩短首屏渲染时间。因此，图片的懒加载是前端性能优化过程中不可或缺的一种优化策略。判断一个图片是否位于可视视口内，可以采用手工计算的方式，当然这里更推荐使用 IntersectionObserver 这个 API。 loading='lazy' 是一种浏览器级别的懒加载的特性，虽然它的用法非常简单，可是考虑到整个懒加载的过程，对用户而言完全就是个黑箱，因此，如果你想更精确地控制懒加载的细节，譬如给定一个占位图片，这种情况下该方案就显得非常鸡肋啦。更不必说，它里面有很多不确定的或者难以控制的因素，我个人觉得前两种方案结合起来会更好一点。好了，以上就是这篇博客的全部内容啦，谢谢大家！\n","date":"2022-08-02T22:49:47Z","image":"/posts/the-story-behind-the-lazy-loading-of-front-end-pictures/fallow-deer-gb774cbe7f_640.jpg","permalink":"https://qinyuanpei.github.io/posts/the-story-behind-the-lazy-loading-of-front-end-pictures/","slug":"The-Story-Behind-The-Lazy-Loading-Of-Front-End-Pictures","tags":["懒加载","LazyLoad","JavaScript","前端"],"title":"聊一聊前端图片懒加载背后的故事"},{"categories":["生活感悟"],"content":" 桃花潭里没有桃花，正如老婆饼里没有老婆，只要你没有期待，就永远不会失落。不同的是，长安这座城市，难保不会教人想起，那桩发生在天宝年间的人类早期电信诈骗事件。相传，对李白仰慕已久的汪伦，深知李白好饮酒、喜游历，便诚恳地给李白写了一封信，称当地有万家酒楼、十里桃花。李白听闻以后，欣然前往，结果万家酒楼变成了万氏酒楼、十里桃花变成了桃花渡口。可即便如此，李白还是被汪伦的盛情打动，在临别时写下那首脍炙人口的诗篇《赠汪伦》。如果仔细考究一番的话，这个桃花潭也许在安徽，而想到李白就会想到的长安，李白总共不过待了三年，从这个角度看，桃花潭这个名字或许是长安沾了李白的光。乘着三号线地铁从地下转为地上，依次经过桃花潭、广运大桥、世博园，三四年前的我，还是一个喜欢掺和草莓音乐节的文青，而如今再度故地重游的时候，七月的桃花潭已经零零星星地浮起荷花，对我而言，我的七月就是从这里开始的，仿佛某种不一样的风景。\n刺杀首相 熟悉我的朋友，应该都知道我喜欢玩《刺客信条》这款游戏，我曾经和康师傅一起见证波士顿倾茶事件、参与邦克山战役，我曾经和谢伊一起在里斯本大地震中心有余悸，我曾经和爱德华一起目睹黑胡子惨死在面前……作为一名刺客，我经常在游戏中刺杀各种历史人物，譬如约翰·彼凯恩、查尔斯·李、劳伦斯·华盛顿……等等，可当一件真实的刺杀事件发生在自己的有生之年，这种感觉更多的是一种震惊。因为我以往对刺杀的认知，更多的是像荆轲刺秦、安重根刺杀伊藤博文、宗次郎刺杀大久保利通(参见浪客剑心)、裴迪南夫妇遇刺引发一战、美国总统肯尼迪遇刺……毫无例外，这些事情对我来说都是相当久远的事情，其模糊程度丝毫不亚于我出生那一年苏联解体，而这一次它就真实地发生在我三十岁的人生节点上，所以，它带给我的冲击是完全不同的。事情发生以后，人们的心态显得非常微妙，因为有幸灾乐祸的，更有深谋远虑的。面对好事，人们就觉得那难以触及的远方和自己息息相关；面对坏事，人们就觉得“今朝有酒今朝醉”、“各人自扫门前雪”那是正道的光。\n以刺杀为主题的游戏：刺客信条\r在伊坂幸太郎的《金色梦乡》里，曾虚构过一起刺杀日本首相的案件。主人公青柳是一个普通的宅急送司机，在他平淡如水的生活里，唯一的高光时刻是两年前救过某个女明星。在被好友森田约出来钓鱼之后，他就被卷入到了这起刺杀日本首相的案件当中，从公众眼中的正义化身，被迫变成政府的通缉犯。在这场国家机器和个体的对抗中，面对突如其来的构陷，再无后路可言的青柳，不得不靠着人类最大的武器——习惯和信赖，展开一场惊心动魄的生死大逃亡。在现实生活中，人们无一例外地都喜欢看到一个公众人物从神坛上跌落下来。因此，当冰冷的体制对弱势的个体实施迫害的时候，最好的办法就是令对方的形象彻底失真。人们常说，为了更多的人，牺牲掉一小部分人在所难免，可在一个常态的社会里，遇到非常时期人们该如何自处？如果牺牲掉的一小部分人里刚好有你，你又该如何展开自救？就像这将近三年的新冠疫情，频频被顶上热搜的报道，固然是我们希望别人看见的，可在我们看不见的地方，是否还有我们看不见的牺牲？即使他们是像青柳一样，并不是罪犯或者真正对社会有危害的危险人物。\n同名电影《金色梦乡》剧照：众生\r金色梦乡 我以为，人类的情感是建立在某个事件上的共同记忆，譬如青柳一行人大学毕业后在烟花厂一起打工，前女友始终记得男主用拇指按压电梯按键的习惯，以及男主和父母独特而隐晦的报平安的方式……正是这些情感的连接点，构成了这本书里最温情的一面。十一月的仙台，在这场蓄谋已久的阴谋里，或许是冰冷刺骨的，可当漫天烟花凌空绽放时，一定会有某个人在某个地方与我心意相通，那一刻披头士的 Golden Slumbers 在耳边响起，年少时的故梦、逃亡途中留下的眼泪和汗水、终于以一种炽热的状态在夜空中快速熔化，而后又复归于平淡和冰冷。那天，看完《人生大事》，从电影院出来我就发了条朋友圈，大意说，以后死了做成烟花好像是种不错的选择。我不知道，这是不是一种呼应，就像披头士的歌词里写道，“曾经有一条回家的路”，可到故事结尾的时候，青柳已经再找不回曾经的自己，他通过整容手术改头换面，以一个新的身份继续生活下去，而那些美好的回忆，终于还是变成，再也无法回头的金色梦乡，即使是对成为“优等”人的青柳雅春而言。\n同名电影《金色梦乡》剧照：烟火\r所以，大概还是要感谢伊坂幸太郎，亲手了编织这样一场梦。这段时间，书实在没有怎么看，因为自从得知 Kindle 要退出中国市场，忍不住一声喟叹，甚至连带着对微信读书的好感都有所影响，倒不是我厚此薄彼，实在是这段时间接触到了新的媒介——播客，在某种意义上，是比微信读书更适合拿来听的一种介质。这一切的契机来自早见 Hayami，一开始是因为她在上海疫情期间写的一篇文章《我在方舱，看见老人们的孤岛求生》。后来，就听了她的两期播客，听她讲亲密关系、讲“言语犹如微小剂量的砷”。对于这个只有 26 岁的年轻人，我折服于她文笔之细腻、观点之新颖。可有的时候，我觉得她像是变了一个人，动辄喜欢输出群体性失望情绪，比如典型的“男人不行”。作为一个写作者，我当然知道，写别人愿意听到的声音，更容易带来流量，可如果你被读者的声音绑架，你就会失去创作的自主性。有时候，我觉得男人和女人仿佛是来自两个星球的生物，女人抱怨说，这个世界上没有正常的男人；而男人亦抱怨说，这个世界上没有正常的女人。可讽刺的是，这个正常的定义都掌握在对方手中，人人都不想被别人定义，可人人又都想要定义别人。如果我说我支持平权，你们是不是依然会对我口诛笔伐，就像大家都忘了安倍是“骑墙派”这件事情一样。有时候，看到前任经常在网上掺和这些事情，我唯有感慨，这终究是我回不去的金色梦乡啊……\n同名电影《金色梦乡》剧照：涅槃\r双标现场 当然，人类对待事物的看法，不单单会受性别这种非此即彼的的因素的影响，哪怕是同一个因素，依然摆脱不了如宿命一般的双标。朋友 T 君家中亲人去世，此君不无遗憾地说到，“老人家才活了 80 多岁就走了”，对于这种说法，我深以为然，毕竟从我 18 岁开始，每年都会听到，“你都多少多少岁了”，类似这样的话来，我们总是催促着小孩长大，又像哄小孩一样安抚着老人。如此一来，小孩仓促间长大，突然背负起时代和家庭给予的期望、责任；而老人略显笨拙地拨弄着智能手机，拐杖上贴满了各种各样的核酸贴纸，生怕因为落后被这个时代嫌弃。对此，我想说：都挺好。2022 年注定是一个多事之秋，除了隔三差五卷土重来的疫情，俄乌冲突、安倍遇刺、约翰逊辞职、斯里兰卡破产、断供潮……，每一件事情看起来都可以写入历史教科书，只是我越来越难以回答，造成这些事件的直接原因和根本原因分别是什么。也许，这些问题到底是需要后人来回答，我有我的历史局限性和阶级局限性。我甚至不知道，安倍遇刺这个事件会不会成为育碧未来的素材，可从一名刺客信条玩家的角度来看，山上徹也实在算不上一名好的刺客。\n真正的刺客：十步杀一人，千里不留行\r这个月月初，我在豆瓣的下厨房小组发了篇帖子，整理了我平时经常做的快手饭菜，没想到网友们对我的厨艺还是有一点认同的，不像某个人一直蹭我的饭，还整天对我的厨艺颇有微词。这让我意识到，做饭这件事情是具有私密性的，因为每个人对饭菜的口感、喜好各不相同，甚至吃饭这件事情在每个人心中的重要性亦有所差别。我是一个对吃和穿没什么太高要求的人，上班带饭很大原因上是为了省钱，所以，我不能理解有的人嚷嚷着带红烧肉是什么心态。做饭的人都知道，做饭 80% 的工作量都在前期准备阶段，所以，作为一个每天早上起来做饭的人，我必须让自己避开那些复杂的菜式。因为，说到底，我是一个怕麻烦的人，向来不喜欢麻烦别人。对吃饭的人而言，只要带上嘴巴和筷子就行，可对做饭的人来说，不单单要关心买菜做菜，还要关心柴米油盐酱醋茶、关心洗碗刷锅。写到这里的时候，我大概明白了，做家务这件事情到底累在哪里，所谓的眼里有活儿还是没活儿，更多的是角色与身份上的认知差异。一旦想通了这一点，你就没必要再为这件事情纠结。因为，于我而言，所谓吃饭的仪式感，一大半都在做饭的过程中，我最喜欢三下五除二、风卷残云的感觉啦！\n同名电影《金色梦乡》剧照：优等\r总体而言，不论国家与国家之间，还是人与人之间，这个世界总是在不遗余力地寻求表面的平衡，而过于寻求平衡的这个过程，注定是逆人性、反人类的，就像我们都在寻求长期稳定的亲密关系，可两个人之间的关系，并不完全遵循万有引力定律，而是像磁场、像风、像雨一样有强有弱，换言之，这种关系并不稳定，并且将永远处于动态平衡之中。毕竟，没有人喜欢世界上到处都是被操控着的提线木偶，比起表面上的平衡，保持这个世界原本的质感，是不是要显得更为重要一点呢？\n","date":"2022-07-16T22:49:47Z","image":"/posts/miscellaneous-feelings-of-july/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/miscellaneous-feelings-of-july/","slug":"Miscellaneous-Feelings-Of-July","tags":["随笔","生活","感悟","思考"],"title":"杂感·七月寄望"},{"categories":["前端开发"],"content":"众所周知，Vue Router 是 Vue 中重要的插件之一，特别是在当下流行的 单页面应用/SPA 中，这种感觉会越来越明显。此时，路由的作用就是根据 URL 来决定要显示什么内容。诚然，页面这个概念在工程/模块中依然存在，可当你开始关注最终发布的产物时，你会发现本质上它只有一个页面。无论你选择 hash 或者是 history 模式的路由，它都像是在同一张纸上反复写写画画，让你看起来觉得它有很多个不同的页面。回顾早期的前端项目，它往往会有多个不同的页面组成，我们是通过一个个的超链接来实现不同页面间的跳转。如今，这一切都已一去不复返，我们只能在单页面应用的世界里继续披荆斩棘。当然，绝大多数的普通用户无法感知到这种程度的变化，在他们的眼中，那依然不过是普通的一个超链接。那么，当一个项目中充斥着各种各样的超链接的时候，这个问题就值得我们单独拿出来讲一讲。所以，今天这篇博客的主题是路由和外部链接。请注意，这是一组相对通用的概念，不受限于任何一个前端框架，我们只是选择了使用 Vue 来进行说明。\n问题现状 我们的项目存在着大量的超链接以及导航菜单，在 UI 设计阶段，通常不会有人关心，一个链接到底是内部链接还是外部链接。与此同时，由于 HTML 这门标记语言的极大灵活性，实现一个导航链接的方式有 N 多种，可以是一个 a 标签，可以是一个 div 标签，甚至可以是一个 span 标签。虽然 Vue Router 里提供了 router-link 组件，可在实际的项目中，需要综合考虑团队风格和第三方 UI 库的因素，甚至有时候，再没有设计规范的情况下，可能大家连 router-link 组件都不愿意用或者说压根就没机会用。\n这样就造成一个非常尴尬的局面，当你需要为页面编写业务代码的时候，你不得不在各种各样的超链接上浪费时间，只要不是通过 a 标签实现的，你都必须处理它点击的事件，更不必说，你还要区分这个链接是一个内部链接还是一个外部链接，原因是 Vue Router 不支持外部链接，你不得不通过 window.location 或者 window.open() 的这样的方式来实现“曲线救国”，试想，如果每一个都这么折腾一遍，你还会觉得有趣吗？\n而在我们的项目里，实际上它还需要从网页端唤起应用，这样便又涉及到了 URL Schemes 这个话题。除了 Android 和 iOS 这个平台上的差异，单单就 Windows 而言，其基于注册表的方案对协议提供者的约束并不强，如果团队内对此没有任何规范的话，你将面对各种千奇百怪的参数传递方式。听到这里，你是不是感觉头都大了一圈？如果因为某种原因，它还需要你每次都传递一个令牌过去，你告诉我，你准备如何让这一切的混乱与不堪重新归于宁静呢？\n学如逆水行舟，不进则退\r改进思路 OK，现在假设，我们制止这场混乱的方式，是强迫大家都去使用 router-link 这个组件，虽然它最终渲染出来就是一个 a 标签。相信参加工作以后，大家都会有这样一种感觉，那就是工作中 99.9% 的事情，都是在最好和最坏中间选一个过渡状态，然后不断地为之投入精力或者叫做填坑，甚至有很多东西，从来都不是为了让一件事情变得更好而存在。作为这个地球上脆弱而渺小的个体，时间、生命、爱，每一样东西都像缓缓从指尖滑落的沙子，我们实在是太喜欢这种可以掌控点什么的感觉了。所以，如果一件事情没法从道理或者科学上讲通的话，那就用制度或者规范来作为武器，在一个连国家都可以宣布破产的年代，大概，话语权比是非对错更重要。因此，在博主的博客里，在这小小的一方天地里，不妨假设我有这种话语权，可以强迫大家都使用 router-link 这个组件。我们讲，Vue Router 不支持外部链接，一个非常直观的理由是，当我们写出下面的代码时，它会完全辜负我们的期望：\n\u0026lt;router-link to=\u0026#34;https://blog.yuanpei.me\u0026#34;\u0026gt;Go\u0026lt;/router-link\u0026gt; 显然，我们期望它可以跳转到 https://blog.yuanpei.me 这个地址，可你只要亲自试一下，就会知道这是你的一厢情愿。因为，此时浏览器地址栏中的地址会显示为：\nhttp://localhost:8080/#/https://https://blog.yuanpei.me 当然，我们的用户不会操心这种事情，正如他们从来不会去刻意地分辨，这是一个内部链接还是一个外部链接。这里讲一下博主的思路，博主打算在 router-link 的基础上再做一层封装，内部链接通常是以/ 来开头的，基于这个特点，我们可以区分出这是一个内部链接还是一个外部链接。针对内部链接，我们继续使用 router-link 组件；针对外部链接，我们直接使用 a 标签即可。此时，对应的 Vue 模板定义如下：\n\u0026lt;template\u0026gt; \u0026lt;a v-if=\u0026#34;isExternal\u0026#34; :href=\u0026#34;formatedUrl\u0026#34; :target=\u0026#34;target\u0026#34;\u0026gt; \u0026lt;slot\u0026gt;\u0026lt;/slot\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;router-link v-else v-bind=\u0026#34;originProps\u0026#34;\u0026gt; \u0026lt;slot\u0026gt;\u0026lt;/slot\u0026gt; \u0026lt;/router-link\u0026gt; \u0026lt;/template\u0026gt; 在这里，我们对外暴露了 to 和 target 两个属性，前者允许我们传入一个字符串或者对象，后者可以控制这个链接的打开方式，是在当前窗口还是一个新窗口中打开：\nexport default { name: \u0026#34;MyRouterLink\u0026#34;, props: { to: { type: [Object, String], default: () =\u0026gt; { path: \u0026#39;/\u0026#39; }, required: true, }, target: { type: String, default: () =\u0026gt; \u0026#39;\u0026#39;, }, }, // ... } 还记得我们是怎么区分内部链接和外部链接的吗？只需要判断传入的 URL 是否以 / 开头。在这里，我们需要对 to 的类型进行判断：\ncomputed: { isExternal() { if (typeof(this.to) === \u0026#39;object\u0026#39;) { return this.to.path \u0026amp;\u0026amp; this.to.path[0] !== \u0026#39;/\u0026#39; } if (typeof(this.to) === \u0026#39;string\u0026#39;) { return this.to \u0026amp;\u0026amp; this.to[0] !== \u0026#39;/\u0026#39; } return false }, } 当然，在某些情况下，这个 URL 允许使用者传入查询参数(QueryString)。这里，我们用 formatedUrl 这个计算属性来统一进行处理：\ncomputed: { formatedUrl() { let url = \u0026#34;\u0026#34;; if (typeof(this.to) === \u0026#39;object\u0026#39;) { url = this.to.path } else if (typeof(this.to) === \u0026#39;string\u0026#39;) { url = this.to } let queryArray = []; if (this.to.query) { for (let key in this.to.query) { const value = encodeURIComponent(this.to.query[key]); queryArray.push(`${key}=${value}`); } } if (queryArray.length == 0) { return url; } if (url.indexOf(\u0026#34;?\u0026#34;) != -1) { url = `${url}${queryArray.join(\u0026#34;\u0026amp;\u0026#34;)}`; } else { url = `${url}?${queryArray.join(\u0026#34;\u0026amp;\u0026#34;)}`; } return url; }, } 最后，需要特别说明的是 originProps 这个计算属性，虽然我们封装了 router-link 这个组件，但我们希望这个新组件是兼容 router-link 本身自带的属性的。此时，我们可以采用下面的方式来处理，具体可以参考官方文档：vm.$attrs：\ncomputed: { originProps() { return { ...this.$props, ...this.$attrs }; }, } 现在，万事具备，我们来试用一下这个新的组件，看看效果如何：\n\u0026lt;header\u0026gt; \u0026lt;my-router-link to=\u0026#34;/home\u0026#34;\u0026gt;首页\u0026lt;/my-router-link\u0026gt; \u0026lt;my-router-link to=\u0026#34;/message\u0026#34;\u0026gt;消息\u0026lt;/my-router-link\u0026gt; \u0026lt;my-router-link to=\u0026#34;https://blog.yuanpei.me\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;博客\u0026lt;/my-router-link\u0026gt; \u0026lt;my-router-link :to=\u0026#34;{ path: \u0026#39;https://www.baidu.com/s\u0026#39;, query: { wd: \u0026#39;天气\u0026#39; } }\u0026#34; target=\u0026#34;_blank\u0026#34; \u0026gt; 百度 \u0026lt;/my-router-link\u0026gt; \u0026lt;my-router-link :to=\u0026#34;{ path: \u0026#39;tencent://\u0026#39;, query: { uin: \u0026#39;875974254\u0026#39;, site: \u0026#39;Vue\u0026#39;, menu: \u0026#39;yes\u0026#39; }, }\u0026#34; \u0026gt; QQ \u0026lt;/my-router-link\u0026gt; \u0026lt;/header\u0026gt; 我们可以注意到，现在它可以同时支持内部链接和外部链接，并且我们可以传递一个对象来更好地控制 URL 的细节，当然，它还可以从桌面唤起 QQ 应用，只要协议提供方采用类似的传参方式，那么，这个方案其实可以做到一劳永逸的。完整的代码我已上传到 Github，方便大家可以做进一步的探索。\n从网页端唤起应用\r话题延伸 坦白讲，在我写这篇文章的时候，我一直在思考一个问题，即：如何给所有出站的超链接携带令牌信息？这个想法其实是在解决别人产生的问题，譬如，从子系统 A 跳转到子系统 B 的过程中，为了实现所谓的“免登录”，大佬们提议直接把令牌信息附加到 URL上传递过去，先不说令牌信息刷新和过期的问题，就单单是令牌信息附加到 URL上这一项，看起来都是非常愚蠢的做法，众所周知，浏览器对针对 GET 请求时的 URL 长度存在限制，你这不是直愣愣地往人家枪口上撞吗？放着 CAS、Keycloak 这种成熟的方案不用，非要用这种掩耳盗铃式的半桶水方案？也许，人类还真就喜欢做这样的事情，毕竟这样可以制造出问题和麻烦，让别人有事可做。听我说，谢谢你，因为有你\u0026hellip;吐槽归吐槽，一开始我是写了一个自定义指令来做这个事情：\nVue.directive(\u0026#39;attach-query-string\u0026#39;,function (el, binding) { if (el.tagName === \u0026#39;A\u0026#39;) { const token = resolveToken() const userId = resolveUserId() const posting = resolvePostings()[0] || \u0026#39;\u0026#39; if (el.href.indexOf(\u0026#39;?\u0026#39;) != -1){ el.href = `${el.href}\u0026amp;token=${token}\u0026amp;userId=${userId}\u0026amp;deviceType=${DeviceType.RCT}\u0026amp;posting=${posting}` } else { el.href = `${el.href}?token=${token}\u0026amp;userId=${userId}\u0026amp;deviceType=${DeviceType.RCT}\u0026amp;posting=${posting}` } } }) 注意到，这个指令只对 a 标签有效，所以，那些花里胡哨、奇形怪状的超链接依然是个令人头疼的问题，我们先忽略它们就好：\n\u0026lt;a :href=\u0026#34;item.link\u0026#34; :target=\u0026#34;item.openNewTab ? \u0026#39;_blank\u0026#39; : \u0026#39;_self\u0026#39;\u0026#34; v-attach-query-string \u0026gt; {{ item.name }} \u0026lt;/a\u0026gt; 这个指令表示，它将会在 mounted 和 updated 的时候触发相应的逻辑，对于大多数的超链接而言，其 src 只会初始化一次，所以，这个方案基本上可行的，唯一的难点在于，并不是所有人都会如你期望的那样使用 a 标签。当然，我内心深处永远相信 jQuery 一把梭，所以，通常尝试过 querySelectorAll() ，但我始终觉得这样子显得有点丑陋，说好的不再操作 DOM 了呢？如果按照我们现在的思路，其实可以在组件内部统一处理，下面是一个简单的实现：\ncomputed: { formatedUrl() { let url = \u0026#34;\u0026#34;; if (typeof(this.to) === \u0026#39;object\u0026#39;) { url = this.to.path } else if (typeof(this.to) === \u0026#39;string\u0026#39;) { url = this.to } let queryArray = []; // 统一追加参数 const token = resolveToken() const userId = resolveUserId() const posting = resolvePostings()[0] || \u0026#39;\u0026#39; queryArray.push(`token=${token}`) queryArray.push(`userId=${userId}`) queryArray.push(`posting=${posting}`) // 处理组件传入的参数 if (this.to.query) { for (let key in this.to.query) { const value = encodeURIComponent(this.to.query[key]); queryArray.push(`${key}=${value}`); } } if (queryArray.length == 0) { return url; } if (url.indexOf(\u0026#34;?\u0026#34;) != -1) { url = `${url}${queryArray.join(\u0026#34;\u0026amp;\u0026#34;)}`; } else { url = `${url}?${queryArray.join(\u0026#34;\u0026amp;\u0026#34;)}`; } return url; }, }, 从本质上讲，这两种方案做得事情是完全相同的，无非是拥有了新知识或者技能以后，再去重新审视过去的种种选择，人虽然始终没有办法打破自身的历史局限性，可是能从新知识或者技能中不断丰富自我的认知，这又属实是种颇具幸福感的事情，因为，从这一刻起，你已经告别了昨天的自己，真正做到了“且将新火试新茶”。回过头来再次审视这个问题的时候，你会觉得哪一种更好呢？欢迎大家在评论区留下你的答案。\n本文小结 本文介绍了一种针对 Vue Router 进行扩展的思路，主要是为了解决 router-link 不支持外部链接跳转的问题。关注这个问题的契机，则是来源于项目中大量存在着的超链接和导航菜单。其中，除了指向站内的内部链接，还有指向站外的外部链接，而这些外部链接中，又牵扯到从网页端唤醒应用的问题，所以，我们需要一种相对统一的机制来处理这些内部细节，因此，就有了今天的这篇博客。除此以外，因为一部分人的愚蠢决定，我们必须要在所有出站的 URL 上附加令牌信息，针对这个问题，博主先是尝试了自定义指令的做法，然后又在现在的方案上做了一点处理，这使得我们能把精力放在真正重要的地方。从整体上而言，如果在设计 UI 前，就定好这样一种规范，所有人都使用这个统一的组件，这个问题处理起来会稍微简单一点，可惜，从人类让一群人一起编程的那一刻起，这种人与人间的磨合和牵制就会一直存在，正所谓“有人的地方就有江湖”，身处江湖的人，多少会有点身不由己的磕磕绊绊，本文完！\n","date":"2022-07-12T22:49:47Z","image":"/posts/implementation-of-vue-router-extension-that-supports-external-link/cover.jpeg","permalink":"https://qinyuanpei.github.io/posts/implementation-of-vue-router-extension-that-supports-external-link/","slug":"Implementation-Of-Vue-Router-Extension-That-Supports-External-Link","tags":["前端","Vue","路由","思考"],"title":"支持外部链接跳转的 Vue Router 扩展实现"},{"categories":["编程语言"],"content":"2020 年年底的时候，博主曾心血来潮地开启过一个系列：视频是不能 P 的，其灵感则是来源于互联网上的一个梗，即：视频不能 P 所以是真的。不过，在一个美颜盛行的时代，辨别真伪实在是一件奢侈的事情，在各种深度学习框架光环的加持下，在视频中实现“改头换面”已然不再是新鲜事儿，AI 换脸风靡一时的背后，带来是关乎隐私和伦理的一系列问题，你越来越难以确认，屏幕对面的那个到底是不是真实的人类。古典小说《红楼梦》里的太虚幻境，其牌坊上有幅对联写道，“假作真时真亦假，无为有处有还无”。果然，在这个亦真亦幻的世界里，哪里还有什么东西是不能 PS 的呢？在“鸽”了很久很久之后，博主决定要来更新这个系列啦，让我们继续以 OpenCV 作为起点，来探索那些好玩、有趣的视频/图像处理思路，这一次呢，我们来聊聊 OpenCV、Dlib 和 表情包，希望寓教于乐的方式能让大家感受到编程的快乐！\n环境准备 python -m pip install opencv-python python -m pip install opencv-contrib-python python -m pip install Pillow python -m pip install numpy python -m pip install imutils python -m pip install dlib 请注意，如果通过 pip 安装 dlib 不大顺利，你可以到 https://github.com/sachadee/Dlib 这个仓库中下载对应的 .whl 文件。例如，博主使用的是 64 位的 Windows 系统，而我的 Python 版本是 3.7，因此，我下载的是 dlib-19.22.99-cp37-cp37m-win_amd64.whl 这个文件。此时，我们可以用下面的方式来安装 dlib：\npython -m pip install dlib-19.22.99-cp37-cp37m-win_amd64.whl 除此以外，我们还需要下载 dlib 所需的模型文件，下载地址为：http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2。下载该文件并解压后，可以得到一个 .dat 格式的文件，这就是我们用来做人脸识别的模型文件，即人脸的 68 个特征点检测，这个我们会在下面做更进一步的说明。\n初探 Dlib 其实，如果是简单的人脸检测，OpenCV 完全足矣。如果你读过我这个系列的第一篇文章，就会知道 OpenCV 人脸检测的实际效果如何。事实上，它会使用一个矩形来表示检测到的人脸范围，这里，我还是用堺雅人主演的电视剧《半泽直树》来作为说明，可以注意到每一个矩形对应着一张人脸：\nOpenCV人脸检测效果展示\r可是，如果我们希望对这些“人脸”进行比对以期望达到人脸识别的目的，这个精度对我们来说就显得捉襟见肘啦！为了解决这个问题， dlib 中使用的是一种被称之为 68 face landmarks 的方法，简单来说，就是用 68 个特性点来定位一个人的面目五官，为了方便大家理解，我们一起来看下面的例子：\n# 初始化 detector 和 predictor detector = dlib.get_frontal_face_detector() predictor = dlib.shape_predictor(\u0026#39;shape_predictor_68_face_landmarks.dat\u0026#39;) # 载入人脸图片 image = cv2.imread(\u0026#34;./faces/face1.jpg\u0026#34;) # 检测人脸 \u0026amp; 绘制特征点 face_rects = detector(image, 0) for index, face in enumerate(face_rects): shape = predictor(image, face_rects[index]) shape = face_utils.shape_to_np(shape) for idx, (x,y) in enumerate(): cv2.circle(img, (x, y), 1, (0, 0, 255), -1) cv2.putText(img, str(idx + 1), (x,y), cv2.FONT_HERSHEY_SIMPLEX, 0.2, (255, 255, 255), 1, cv2.LINE_AA) cv2.imshow(\u0026#34;image\u0026#34;,image) 此时，我们可以得到下面的结果：\n使用 dlib 进行人脸检测\r可以注意到， dlib 可以进一步识别出人脸中的特征点，譬如眼睛、眉毛、鼻子、嘴巴等等，这让我们有了更多可以探索的乐趣。举个例子，司机在道路上疲劳驾驶，有可能会引发交通事故，如果我们能实时分析司机的面目表情，就可以为这个世界做出一点小小的努力。一个常见的思路是计算“眼睛”部分的宽高比，因为当人眼睛闭合的时候，相当于“眼睛”部分纵向的特征点间距变小。再比如，我们可以通过嘴角和眉毛的弧度来“揣测”一个人的喜怒哀乐，让计算机不再是冷冰冰地一台机器。也许，你听说过“三庭五眼”这套理论，所以，从某种意义上来讲，这个思路还可以扩展到“看相”这个方向。果然，你可以永远相信神仙姐姐的颜值呢\u0026hellip;\n移花接木 OK，在对 dlib 有一个初步印象以后，我们来说说，在今天这篇博客里，博主到底想做一件什么样的事情。如下图所示，我们希望借助 dlib 逐步地“抠取”出人物表情，最终再和这个经典的“熊猫人”融合在一起，从而达到从某任意图片生成表情包的目的。当然啦，这个想法并不容易实现，因为你从这个图片中就可以看到它的最终效果。图中使用的素材出自半泽直树第二季，当半泽直树遇上对手黑崎骏一，瞬间碰撞出一种惺惺相惜的 CP 感，作为雅人叔的忠实粉丝，还有什么比做成表情包更直抒胸臆的表达方式呢？\n制作表情包的过程说明\r68 个特征点的绘制，在初探 dlib 的环节已经讲过，这里就不再赘述啦！这里，我们先来说说人脸的矩形范围如何获得，可能有读者朋友会问，这个矩形的作用是什么？其实，不管我们用怎样不规则的一个多边形来裁切图片，我们最终得到的一定是具备长和宽的矩形。因此，这个矩形就是帮我们定位整个脸的范围。下面是对应的代码片段：\ndef get_facemark_rect(shape): shape2np = face_utils.shape_to_np(shape) (x0, y0) = shape2np[0] minX = maxX = x0 minY = maxY = y0 for (x,y) in shape2np: if x \u0026lt; minX: minX = x if x \u0026gt;= maxX: maxX = x if y \u0026lt; minY: minY = y if y \u0026gt;= maxY: maxY = y return (minX, minY, maxX - minX, maxY - minY) 可以注意到，代码非常地朴实无华，只需要分别找到 x 和 y 的最小值/最大值，就可以确定人脸的矩形范围。如下图所示，红色线条呈现出的即为人脸的矩形范围。作为对比，我们同时绘制了 dlib 本身自带的一个矩形范围，这个矩形范围大致等同于 OpenCV 人脸检测的效果：\n获得人脸的矩形范围\r事实上，如果你回头去看我绘制特征点这一部分的代码，会发现这里有一个变量 face 一直没有用到，它实际上是 dlib里定义的一种类型 rectangle，从命名上我们就可以知道，这是一个表示矩形的数据结构。在 OpenCV 中，我们可以使用下面的代码片段来绘制一个矩形：\n# 使用自己计算出的矩形范围 (x, y, w, h) = get_facemark_rect(shape) cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 1) # 使用 dlib 自带的矩形范围 x, y, w, h = face.left(), face.top(), face.right() - face.left(), face.bottom() - face.top() cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 1) 考虑到，博主这里需要的是一个相对精确的面部的范围，大家可以结合自己的需要，选择其中一种即可。接下来，我们来考虑如何根据这 68 个特征点进行“抠脸”，为此，我们只需要找到那些表示面部轮廓的点即可，通过上面的图形，我们可以看出，脸的下半部分轮廓点为 1 到 17，上半部分轮廓点为 18 到 27。因此，我们只需要用这些点构造一个多边形即可：\ndef create_face_mask(image, facemark): shape, face = facemark shape2np = face_utils.shape_to_np(shape) mask = np.zeros(image.shape, dtype=np.uint8) points = np.concatenate([shape2np[0:16], shape2np[26:17:-1]]) cv2.fillPoly(img=mask, pts=[points], color=(255,) * image.shape[2]) return mask 这里，我们首先创建了一个和原图片同样大小的图片 mask , 然后调用 fillPoly() 对多边形进行填充。这样，我们就得到了一个和人脸轮廓完全一致的掩膜：\n利用人脸轮廓创建掩膜\r在 OpenCV 中，可以通过 bitwise_and() 函数来对两张图片进行“叠加”, 而在这个示例中，原图片为 image，掩膜图片为 mask，因此，对应的代码片段如下：\nimage = cv2.bitwise_and(image, mask) 此时，我们可以得到下面的结果，可以注意到，雅人叔的脸已经被我“抠取”出来了，哈哈！\n成功抠取出雅人叔的脸\r当然啦，这个图片对我们来说太大了！毕竟，我们需要的是雅人叔的这张脸，而不是这个黑乎乎的背景。此时此刻，前面我们计算出来的矩形范围就派上用场啦，因此，我们对图片做一次裁切，OpenCV中裁切图片是非常容易的，我们只需要像对待数组一样指定一个范围：\n(x, y, w, h) = get_facemark_rect(shape) image = image[y:y + h, x:x + w] 当然，这里你会发现一个问题，裁切出来的图片带着黑色背景，这显然不利于我们和“熊猫人”进行融合。这里，博主提供的解决方案是：创建一张同样大小的图片，再把除了黑色以外的颜色全部复制过来，因为黑色再在 NumPy 中使用一个元素全部为 0 的数组来表示。那么，这样就简单多啦：\ndef create_white_image(image): white = np.zeros(image.shape, dtype=np.uint8) for i in range(0, image.shape[0]): for j in range(0, image.shape[1]): white[ i, j ] = np.uint8(255) return white # 创建一张背景色为白色的图片，复制除黑色以外的每一个像素 white = create_white_image(image) for i in range(0, image.shape[0]): for j in range(0, image.shape[1]): if ((image[i, j] != 0).all()): white[i, j] = image[i, j] 接下来，为了让雅人叔的脸更贴近“熊猫人”的气质，我们对图片运用一次 threshold() 函数。事实上，到这一步为止，博主一直没有找到特别好的方法，因为人脸上可能会有阴影造成的深浅变化，这样就没有办法 100% 的转化为黑白图片。除了这种固定阈值的方案，博主同样尝试自适应阈值的方案，即 adaptiveThreshold() 函数。不过，从最终效果来看，自适应阈值的方案表现并不好，下面是这两种方案的效果对比：\n固定阈值和自适应阈值效果对比\r代码层面没什么悬念，转灰度图再调用相应的函数即可：\n# 转灰度图 image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # 固定阈值 + THRESH_BINARY _, image1 = cv2.threshold(image, 45, 255, cv2.THRESH_BINARY) # 自适应阈值 + ADAPTIVE_THRESH_GAUSSIAN_C image2 = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,9,7) # 自适应阈值 + ADAPTIVE_THRESH_MEAN_C image3 = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,9,7) 如果抛开最终呈现出来的效果的好坏不谈，到这里我们勉强算是达到了预期目的，我们将这个结果以图片的形式保存下来。接下来，我们只需要把雅人叔的脸和“熊猫人”组合在一起即可：\n# 读取熊猫人和人脸图片 panda = Image.open(\u0026#34;PandaMan.jpg\u0026#34;) face = Image.open(\u0026#34;face.jpg\u0026#34;) w_g, h_g = bg.size w_f, h_f = face.size ratio = h_f / w_f # 根据熊猫人的大小对人脸进行缩放 w_new = int(w_g * 0.4) h_new = int(ratio * w_new) resized = face.resize((w_new, h_new),Image.ANTIALIAS) # 计算左上角坐标 left = int((w_g - w_new) / 2) top = int((h_g / 2 - h_new) / 2) # 把人脸贴到指定位置 panda.paste(resized, (left, top, left + w_new, top + h_new)) panda.save(\u0026#39;output.jpg\u0026#39;) 那么，对不起了，雅人叔，我实在是太喜欢你在不同的剧里呈现出来的形象，半泽直树、古美门、德川家定、山南敬助、真田信繁\u0026hellip;，每一个角色都能让人惊呼这居然是同一个人，更不用说，有那种根植于文学和话剧的清澈感，能在演绎角色的同时加入个人的理解。雅人叔，请变成我的表情包吧，哈哈！\n最终合成的表情包效果\r本文小结 其实，一开始在规划这篇文章的时候，我原本是打算写一写“换脸”这个话题的。可是后来发现，换脸涉及的那些知识，对现在的我而言，实在是一座难以逾越的高山，所以，最后不得不退而求其次，写一个相对简单的话题，可即使这样，写这篇文章还是花费了我挺长时间，因为一直没有找到更好的方法来处理人脸。所以，这篇文章写到这种程度，其实是因为我不想一直拖延下去。简单总结一下，这篇文章介绍了 dlib 的用法，相对于 OpenCV 而言，它可以拿到人脸的 68 个特征点，而基于特征点我们可以对人脸检测/识别做进一步的探索，比如驾驶过程中司机的疲劳检测、表情分析、活体检测等等，而这篇文章主要利用了其中的轮廓点来生成多边形掩膜，来达到抠取人脸的目的。在这个基础上，我们实现了通过照片制作表情包的想法，虽然目前实用性并不强，可我觉得处理图像的这个过程还是特别有意思。以前，有人讲过程序员的三大浪漫，即操作系统、编译原理 和 图像学，这种浪漫大概只有程序员能理解。好了，以上就是这篇博客的全部内容啦，如果大家对人脸的处理还有更好的方案，欢迎大家在评论区积极留言。\n","date":"2022-07-01T22:49:47Z","image":"/posts/make-memes-with-opencv-and-dlib/rectangle-of-facemark.png","permalink":"https://qinyuanpei.github.io/posts/make-memes-with-opencv-and-dlib/","slug":"Make-Memes-With-OpenCV-And-Dlib","tags":["OpenCV","Python","Dlib","图像处理"],"title":"视频是不能 P 的系列：OpenCV 和 Dlib 实现表情包"},{"categories":["编程语言"],"content":"一直打算写一篇关于 ASP.NET Core 集成测试 的文章，因为一旦说起单元测试这个话题，多多少少会牵动我内心深处的理想主义色彩，虽然如今已然是程序员职业生涯的第七年，可在我看来依然有太多东西在原地打转。这一路跌跌撞撞地走过来，在不同的公司里，见识到了形态各异的研发流程，接触到了貌合神离的敏捷思想，阅读过了风格迥异的框架/架构。当时间节点来到 2022 年，惊觉 .NET 诞生业已 20 周年，虽然技术一直在不断向前发展，可我个人感觉，我们并没有在工程化上取得多少感人的进步，譬如单元测试、需求管理，这些听起来丝毫不影响写代码的方方面面。回首往昔，有坚持写单元测试的公司，有从来不写单元测试的公司，有因为业务或者人力扩张而放弃写单元测试的公司，俨然是软件研发领域的众生相。作为程序员，每天除了和各种 Bug 斗智斗勇以外，接触最多的当属测试或者叫做 QA，所以，今天这篇博客，我们一起来聊聊 ASP.NET Core 里的集成测试。\nMoq：万物皆可模拟吗 我们说，单元测试这个话题，多少带点理想主义色彩，究其本质，是因为我们相信，只要软件中的最小可测试单元的输出符合预期，那么，整个软件的输出就是符合预期的。对于程序员而言，软件中的最小可测试单元，通常是一个方法或者函数，因此，通常意义上的单元测试，是指对一个模块、一个方法/函数或者一个类进行正确性检验的测试工作，并且这个工作讲究隔离性，换句话说，是指软件中的最小可测试单元在不依赖外部因素的情况下进行的独立测试。最近这几年，大家会发现，随着微服务、云原生、Serverless 等等理念的流行，我们的软件正在变得越来越复杂，复杂到让你打断点、单步调试都成为一种奢望。在这种情况下，单元测试的理想主义色彩就开始凸显出来，现实世界中的软件常常存在着大量的依赖或者说耦合，而为了消除这些外部因素，人们会在单元测试中使用 Mock 这一技术来进行模拟。不过，博主想说的是，万物皆可模拟吗？\n什么是单元测试？\rMoq 是 .NET 平台下最常用的模拟库，它可以利用动态代理出模拟一个接口的行为。前面提到，单元测试针对的是最小的测试单元，而当这个最小的测试单元依赖某个外部因素的时候，就需要对其进行模拟，从而保证整个测试环节满足隔离性的要求。举个例子，没有人会为了喝一口水而专门去挖一口井。此时，喝水这个动作即是最小的测试单元，而这个动作本身依赖着一口井，所以，我们需要对井这个外部因素进行模拟。我相信，这足以道出 Mock 和 单元测试 这两者间千丝万缕的的联系。以喝水这件事情为例，我们该如何模拟出一口井呢？假设我们可以通过下面的接口 IWaterProvider 来获得一定体积的水：\ninterface IWaterProvider { Water GetWater(); } 此时，按照 Moq 的套路，我们可以快速地挖一口“井”出来：\nvar mock = new Mock\u0026lt;IWaterProvider\u0026gt;(); mock.Setup(x =\u0026gt; x.GetWater()).Returns( new Water() { Name = \u0026#34;农夫山泉\u0026#34;, Volume = 1.5M } ); // 现在，你已经有了一口井 :) var well = mock.Object; var water = well.GetWater(); Assert.Equal(\u0026#34;农夫山泉\u0026#34;, water.Name); Assert.Equal(1.5M, water.Volume); 可我们同样了解到，真实的软件世界其实是现实世界的一种投影，这意味着现实世界的复杂性绝不会凭空消失，它只会换一种形式再重新进入软件世界。你看，虽然这个世界的熵始终是在不断增加的，可它归根到底是遵循某种守恒定律的。每当这时，我都会想起一位前辈语重心长的话：“没有银弹”。真实的软件世界里，依赖项常常会有多个，这就需要我们模拟多个依赖项，所以，写单元测试这件事情，本身是有沉没成本在里面，首先是写单元测试有一定的门槛，其次是维护单元测试需要时间和精力。例如，下面是对一个控制器下的方法进行测试的代码片段：\nvar mock = new Mock\u0026lt;IEmployeeRepository\u0026gt;(); mock.Setup(repo =\u0026gt; repo.GetAll()) .Returns(new List\u0026lt;Employee\u0026gt;() { new Employee() { Id = 100, Name = \u0026#34;张三\u0026#34; }, new Employee() { Id = 200, Name = \u0026#34;李四\u0026#34;} }); var controller = new EmployeesController(mock.Object); var result = controller.Index(); var viewResult = Assert.IsType\u0026lt;ViewResult\u0026gt;(result); var employees = Assert.IsType\u0026lt;List\u0026lt;Employee\u0026gt;\u0026gt;(viewResult.Model); Assert.Equal(2, employees.Count); Assert.Equal(100, employees[0].Id); Assert.Equal(\u0026#34;张三\u0026#34;, employees[0].Name); 可以注意到，测试控制器的方法，与测试普通方法基本一致，难点是控制器中有非常多的“特性”，譬如 Form 表单、 ModelState、Cookie、HttpContext、Redirect等等，这些在实际操作中并不能做到 100% 的模拟。上家公司时常有发邮件、发短信这样的业务场景，如果不去测试的话，作为程序员的我会非常没有安全感；可如果去测试的话，你就要了解配置、额度等等的细节。在现在这家公司，那些靠消息/事件驱动的业务都需要用到 Kafka，每次大家都通过打断点来联调的时候我就觉得痛苦，可扪心自问，确实没有更好的办法进行模拟。再后来，我就干脆直接构造消息然后再传到回调函数里面。从这个过程，我们就可以看出，万物并非皆可模拟，虽然作为一名《刺客信条》玩家，我心中早已笃定：万物为虚，万事皆允。\n万物为虚，万事皆允\r从微服务的角度出发，当你需要十来个服务相互协同方能正常工作的时候，最小测试单元或者单个服务通过验证，其实并不能保证整个系统运行正常。这样无疑会引出一个问题，那就是，我们每一个人的精力始终是有限的，按照“关注点分离”的原则，我们不应该关注我们不需要的信息，可在这种情况下，你的测试显然无法做到独善其身，如果你依赖了别人的服务或者组件，那么你不得不花时间去了解这些细节。所以，每次我的 Leader 强调要做集成测试的时候，我内心都是拒绝的，因为集成测试会花费更多的时间和精力，当你必须要依靠别人才能去做一件事情的时候，这意味着你会失去主动性，可惜现实生活中这样的事情俯拾皆是。程序员做久了，你会喜欢上那种掌控全局的“上帝视角”，可现实生活中超出你控制范围的事情属实是不能更多。不知不觉间，我们已经来到了集成测试的路口。\nTestServer：快来测试你写的 API 好了，下面我们来聊聊集成测试。集成测试，顾名思义就是在单元测试的基础上，将所有的模块组装成子系统或者系统，然后再进行联合测试。因为相关的实践表明，单个模块可以单独、正常地工作，并不能保证连接起来就可以正常工作。大家平时所说的“联调”，其实就是一种集成测试，因为它是把各个部件组装好以后再进行测试，更不必说，大家平时写好了 API 接口，会使用 Postman 或者 Apifox 这样的工具进行测试。为了方便 ASP.NET Core 中 API 的测试，微软提供了 TestServer，它可以让我们在没有 IIS 或者任何外部事物的情况下对 Web 应用进行测试。在使用 TestServer 前，请确保你已经安装了 Microsoft.AspNetCore.Mvc.Testing 这个包：\n// 方式一：通过 WebHostBuilder 构建 TestServer var webHostBuilder = new WebHostBuilder() .UseStartup\u0026lt;YourStartup\u0026gt;(); using (var server = new TestServer(webHostBuilder)) using (var client = server.CreateClient()) { // 通过 HttpClient 调用 API var result = await client.GetStringAsync(\u0026#34;/path/to/your/api/endpoint/\u0026#34;); } // 方式二：通过 HostBuilder 构建 TestServer var host = await new HostBuilder() .ConfigureWebHost(webBuilder =\u0026gt; { webBuilder .UseTestServer() .UseStartup\u0026lt;YourStartup\u0026gt;(); }) .StartAsync(); using (var server = host.GetTestServer()) using (var client = server.CreateClient()) { // 通过 HttpClient 调用 API var result = await client.GetStringAsync(\u0026#34;/path/to/your/api/endpoint/\u0026#34;); } 这里，博主提供了两种方式来构建一个 TestServer，本质上两种方案都差不多，除了直接使用已有的 Startup 类，你同样可以显式地调用 ConfigureServices() 和 Configure() 这两个方法，以最大限度地使用依赖注入，这样，我们写测试的时候会更加得心应手一点。有时候，我们需要对整个 ASP.NET Core 管道里的中间件进行测试，在 TestServer 出现以前，我们只能通过打断点、单步调试的方式来进行验证，而此时此刻，我们有了更好的做法，假设我们有下面的中间件 RequestCultureMiddleware，它可以以查询字符串的形式修改当前的 CultureInfo ：\npublic class RequestCultureMiddleware { private readonly RequestDelegate _next; public RequestCultureMiddleware(RequestDelegate next) { _next = next; } public Task Invoke(HttpContext context) { var cultureQuery = context.Request.Query[\u0026#34;culture\u0026#34;]; if (!string.IsNullOrWhiteSpace(cultureQuery)) { var culture = new CultureInfo(cultureQuery); CultureInfo.CurrentCulture = culture; CultureInfo.CurrentUICulture = culture; } return _next(context); } } 接下来，按照一开始的思路，我们来构建对应的 TestServer，还记得怎么使用 ASP.NET Core 里的中间件吗？我们依样画葫芦即可，个人感觉还是挺简单的：\nvar host = await new HostBuilder() .ConfigureWebHost(webBuilder =\u0026gt; { webBuilder .UseTestServer() .ConfigureServices(services =\u0026gt; { // 注入中间件 services.AddScoped\u0026lt;RequestCultureMiddleware\u0026gt;(); services.AddRouting(); }) .Configure(app =\u0026gt; { // 使用中间件 app.UseMiddleware\u0026lt;RequestCultureMiddleware\u0026gt;(); app.UseRouting(); // 构造一个 API 端点 app.UseEndpoints(endpoints =\u0026gt; { endpoints.MapGet(\u0026#34;/echo\u0026#34;, async context =\u0026gt; { var text = context.Request.Query[\u0026#34;text\u0026#34;]; await context.Response.WriteAsync(text); }); }); }); }) .StartAsync(); using (var server = host.GetTestServer()) using (var client = server.CreateClient()) { var text = \u0026#34;Hello\u0026#34;; var culture = \u0026#34;zh-CN\u0026#34;; var endpoint = $\u0026#34;/echo?text={text}\u0026amp;culture={culture}\u0026#34; var result = await client.GetStringAsync(endpoint); Assert.Equal(text, result); var cultureInfo = new CultureInfo(culture); Assert.Equal(cultureInfo.Name, CultureInfo.CurrentCulture.Name); Assert.Equal(cultureInfo.Name, CultureInfo.CurrentUICulture.Name); } 除了中间件，我们还可以对 HttpContext 进行测试，这里主要利用了 TestServer的 SendAsync() 方法：\nusing (var server = host.GetTestServer()) { // 返回经过后端处理过的 HttpContext var context = await server.SendAsync(c =\u0026gt; { c.Request.Method = HttpMethods.Get; c.Request.Path = \u0026#34;/echo\u0026#34;; c.Request.QueryString = new QueryString(\u0026#34;?text=Hello\u0026#34;); }); Assert.Equal(200, context.Response.StatusCode); } 类似地，因为 TestServer 上暴露出了 IServiceProvider，所以，理论上我们可以使用任何注入到 IoC 容器中的组件，这其中自然包扩 gRPC，下面是一个对 gRPC进行测试的示例：\nusing (var server = host.GetTestServer()) { var messageSrvClient = server.Services .GetRequiredService\u0026lt;MessageSrv.MessageSrvClient\u0026gt;(); var messages = (await messageSrvClient.GetAllMessagesAsync()).Notes; Assert.True(messages.Count \u0026gt; 0); } 截至到目前，在最新的 .NET 6.0 这一版本中，我们还可以使用 WebApplicationFactory 来进行测试，它主要的改进点是消除了对 Startup 类的需求，允许你直接使用 Program 这个入口类：\nvar application = new WebApplicationFactory\u0026lt;Program\u0026gt;() .WithWebHostBuilder(builder =\u0026gt; { // ... // Configure Your TestServer }); var client = application.CreateClient(); 不过，实际使用中发现，这里还是可以传 Startup类进去，并且，接下来要分享的内容和它息息相关。\nxUnit：更优雅的集成测试 在写测试的过程中，如果每次都创建一个 TestServer，不单单麻烦，而且效率非常低，所以，微软官方的建议是让测试类实现 IClassFixture\u0026lt;TFixture\u0026gt; 接口，这是 xUnit 中的一个特性，其作用是让 TFixture 这个具体的类型，在运行第一个测试用例前被初始化。而如果 TFixture 这个类型实现了 IDisposable 接口，则 xUnit 会在运行最后一个测试用例后调用其 Dispose() 方法。直白一点的说法就是，它解决的是测试类中共享的数据如何初始化、如何销毁的问题，对我们而言，我们当然希望 TestServer 只初始化一次，下面是一个基本的示例：\npublic class WebAppTest : IClassFixture\u0026lt;WebApplicationFactory\u0026lt;Startup\u0026gt;\u0026gt; { private readonly WebApplicationFactory\u0026lt;Startup\u0026gt; _factory; public WebAppTest(WebApplicationFactory\u0026lt;Startup\u0026gt; factory) { _factory = factory; } [Fact] public async Task InMemeryDBTest() { // 使用 WithWebHostBuilder() 方法对 Startup 里的行为进行自定义或者覆盖 var factroy = _factory.WithWebHostBuilder(builder =\u0026gt; { builder.ConfigureServices(services =\u0026gt; { services.AddDbContext\u0026lt;ChinookContext\u0026gt;(options =\u0026gt; { options.UseInMemoryDatabase(\u0026#34;InMemoryDB\u0026#34;); }); }); }); var serviceProvider = factroy.Services; using (var scope = serviceProvider.CreateScope()) { var respository = scope.ServiceProvider .GetRequiredService\u0026lt;IBaseRepository\u0026lt;VehicleRecord\u0026gt;\u0026gt;(); var dbContext = scope.ServiceProvider .GetRequiredService\u0026lt;ChinookContext\u0026gt;(); respository.Add(new VehicleRecord() { FleetNum = \u0026#34;12138\u0026#34;, StatusCode = \u0026#34;AVB\u0026#34; }); await dbContext.SaveChangesAsync(); var instance = await respository.GetFirstOrDefaultAsync( x =\u0026gt; x.FleetNum == \u0026#34;12138\u0026#34; ); Assert.NotNull(instance); Assert.True(instance.StatusCode == \u0026#34;AVB\u0026#34;); } } } 可以注意到，这里我们对默认的 _factory 进行了一点加工，因为我们不希望这些测试代码对当前的数据库产生影响，所以，我们通过 WithWebHostBuilder() 方法对默认的 ChinookContext 进行了覆盖，使其可以使用一个基于内存的数据库，这在写测试的时候，其实是一个非常不错的特性，因为这样确保了一个用例可以重复多次运行，或者是我们希望能够隔离开发环境和测试环境，此时此刻，你都可以考虑对默认的注入行为进行覆盖，甚至你还可以考虑实现自定义的 WebApplicationFactory，并对其中的核心方法进行重写：\npublic class CustomWebApplicationFactory\u0026lt;TStartup\u0026gt; : WebApplicationFactory\u0026lt;TStartup\u0026gt; where TStartup : class { protected override void ConfigureWebHost(IWebHostBuilder builder) { // ... } protected override IHost CreateHost(IHostBuilder builder) { // ... return base.CreateHost(builder); } protected override TestServer CreateServer(IWebHostBuilder builder) { // ... return base.CreateServer(builder); } protected override void ConfigureClient(System.Net.Http.HttpClient client) { // ... } } 相信看到这里，大家就明白了，真正做事情的还是 TestServer，无非是从前台转换到后台。写单元测试的好处大家都知道，可是当业务频繁发生变动的时候，维护这些单元测试就变成了一种负担，正如鲁迅先生所言，“我大抵是倦了”。\n本文小结 写作计划中的话题，终于又减少了一项，我内心还是有一点开心，因为有些事情一直拖延下去，便不见得有什么太好的结果。这篇博客主要讲的是 ASP.NET Core 里的集成测试，而一开始的着眼点则是单元测试 和 Mock。考虑到真实软件世界里的复杂性，“万物皆可模拟”，大概只能是一种美好的想象，并且实践告诉我们，单个模块可以单独、正常地工作，并不能保证连接起来就可以正常工作，显然，这是集成测试产生或者说存在的一个契机。我们从最简单的 API 的测试，引出了 TestServer，然后在这个基础上分享了如何利用 TestServer 对 Controller/API、中间件、HttpContext 以及 gRPC进行测试。最后，博主为大家介绍了微软官方推荐的最佳实践，即通过 WebApplicationFactory 来实现更优雅的集成测试，这里捎带着介绍了一下 xUnit 里的 IClassFixture，它解决的是测试类中共享的数据如何初始化、如何销毁的问题。好了，以上就是这篇博客的全部内容啦，如果大家对文章中的内容有任何建议或者意见，欢迎大家在评论区积极留言，谢谢大家！\n","date":"2022-06-07T15:49:47Z","image":"/posts/不得不说的-ASP.NET-Core-集成测试/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/i-have-to-say-asp.net-core-integration-testing/","slug":"I-Have-To-Say-ASP.NET-Core-Integration-Testing","tags":["Moq","单元测试","集成测试","TestServer"],"title":"不得不说的 ASP.NET Core 集成测试"},{"categories":["编程语言"],"content":"在上家公司工作的时候，我们有部分业务是采用事件/消息驱动的形式。虽然，当时博主还没能用上诸如 Kafka、RabbitMQ 这样的消息中间件，可数据库 + Quartz 这样一个堪称“简陋”的组合，完全不影响博主对事件/消息驱动这种思想的启蒙。后来，在实现数据库审计、数据同步 等问题的时候，更是从实践层面上加深了这一印象。再后来，博主陆陆续续地接触了 DDD，其中 领域事件 的概念，第一次让博主意识到，原来事件可以和聚合根产生某种联系。退一步讲，即使你没有接触过 DDD，你只要听说过 MediatR 或者 CQRS，相信你立马就能明白我在说什么。最近的一次 Code Review，这个问题再次浮出水面，一个人在面对过去的时候，会非常容易生出物是人非的感慨，代码和人类最大的区别就在于，代码可以永远以某种永恒的形式存在，就像很多年后我打开高中时候用 Visual Basic 编写的程序，它依然可以像我第一次看见它一样运行。所以，一直在变化的大抵是我，无非是人类更擅长自我说服，它让你相信你一直“不忘初心”。因此，今天我想再聊聊 DDD 视角下的 EFCore 与 领域事件。\n似曾相识燕归来 其实，人生中有特别多的似曾相识，就像 Wesley 老大哥和我说起 Kubernetes 的时候，我脑海中一直浮现着的画面，是第一次见到他的时候，他意气风发地给我讲 MSBuild 和 单元测试。为什么会记得他意气风发的样子呢？大概是有一天我到他这个年龄的时候，我终于羡慕彼时彼刻的他，还拥有着这样一副意气风发的面孔罢。对于大部分事件/消息驱动的业务，相信大家都见到过类似下面这样的代码片段：\n// 保存订单 var orderInfo = new OrderInfo( address: \u0026#34;陕西省西安市雁塔区大雁塔北广场\u0026#34;, telephone: \u0026#34;13456789091\u0026#34;, quantity: 10, remarak: \u0026#34;盛夏白瓷梅子汤，碎冰碰壁铛啷响\u0026#34; ); _repository.Insert(orderInfo); _chinookContext.SaveChnages(); // 发布消息 var orderInfoCreateEvent = orderInfo.Adapt\u0026lt;OrderInfoCreateEvent\u0026gt;(); eventBus.Publish(orderInfoCratedEvent) 这段代码非常容易理解，当我们创建完一个订单以后，需要发布一条订单创建的消息。当时组内做 Code Review 的时候，大家都普遍认为，Publish() 需要放在 SaveChanges() 后面，理由是：如果 Publish() 放在 SaveChanges() 前面，可能会出现消息发出去了，而数据没有保存成功的情况。这个想法当然没有问题，唯一的问题在于，实际业务中构造消息的过程绝不可能如此简单，如果它依赖中间过程的变量或者参数，你不可能总是有机会把这个过程放到 SaveChanges() 后面，更不必说，实际业务中可能会要求你在订单里处理客户相关的事件。显然，这种方案对代码的侵入非常严重。那么，有没有更好一点的方案呢？\n// Entity 定义，适用于无单主键或使用联合主键 public abstract class Entity : IEntity { private List\u0026lt;IDomainEvent\u0026gt; _domainEvents = null; public IReadOnlyCollection\u0026lt;IDomainEvent\u0026gt; DomainEvents =\u0026gt; _domainEvents?.AsReadOnly(); // 添加事件 public void AddDomainEvent(IDomainEvent eventItem) { _domainEvents = _domainEvents ?? new List\u0026lt;IDomainEvent\u0026gt;(); _domainEvents.Add(eventItem); } // 移除事件 public void RemoveDomainEvent(IDomainEvent eventItem) { _domainEvents?.Remove(eventItem); } // 清除事件 public void ClearDomainEvents() { _domainEvents?.Clear(); } public abstract object[] GetKeys(); public virtual DateTime CreatedAt { get; set; } public virtual string CreatedBy { get; set; } } // Entity 定义，适用于单主键 public abstract class Entity\u0026lt;TKey\u0026gt; : Entity, IEntity\u0026lt;TKey\u0026gt; { public TKey Id { get; set; } public override object[] GetKeys() =\u0026gt; new object[] { Id }; } 我们不妨来换一种思路，既然我们期待这些 Publish() 相关的代码片段总是在 SaveChanges() 后面执行，那么，我们是不是可以将这些事件/消息存储下来，然后在某个合适的时机进行触发呢？当然，利用 .NET 里的委托就能达到这种延迟执行的目的，我们这里采用的方案是为每个 Entity 增加一个 DomainEvents 的属性，并通过重写 DbContext 的 SaveChanges() 方法来实现消息的分发，这里我们暂时不考虑事务，因为对于像 Kafka、RabbitMQ 这样的消息队列，基本上都不支持消息的撤回，换言之，其实这里是保证不了 SavaChanges() 和 Publish() 的一致性的：\n// OrderInfoCreatedEvent 继承自 DomainEvent public class OrderInfoCreatedEvent : DomainEvent { public string Remark { get; set; } public string Address { get; set; } public string Telephone { get; set; } public decimal Quantity { get; set; } } // DomainEvent 实现了 IDomainEvent 接口 public class DomainEvent : IDomainEvent { public Guid EventId { get; set; } = Guid.NewGuid(); } 此时，注意到，所有的消息都实现了 IDomainEvent 接口，所以，对于一开始的示例，我们可以像下面这样来改造。这里，我们采用 DDD 的思想来改造这段代码，即按照“充血模型”，为 OrderInfo 类添加更多的行为，我们不妨假设，当创建订单的时候，需要产生一条 OrderInfoCreatedEvent 消息；当修改订单地址的时候，需要产生一条 OrderInfoUpdatedEvent 消息，我们来看看改造以后的代码会变成什么样子：\nvar orderInfo = new OrderInfo( address: \u0026#34;陕西省西安市雁塔区大雁塔北广场\u0026#34;, telephone: \u0026#34;13456789091\u0026#34;, quantity: 10, remarak: \u0026#34;盛夏白瓷梅子汤，碎冰碰壁铛啷响\u0026#34; ); // 确认订单 orderInfo.Confirm(); _repository.Insert(orderInfo); // 修改地址 orderInfo.ModifyAddress(\u0026#34;陕西省西安市雁塔区卜蜂莲花超市\u0026#34;); await _chinookContext.SaveChangesAsync(); 其中，OrderInfo 内部定义了 Confirm() 和 ModifyAddress 两个方法用来处理对应的领域事件：\npublic class OrderInfo : Entity\u0026lt;Guid\u0026gt; { // .... public void Confirm() { CreatedBy = \u0026#34;System\u0026#34;; CreatedAt = DateTime.Now; AddDomainEvent(this.Adapt\u0026lt;OrderInfoCreatedEvent\u0026gt;()); } public void ModifyAddress(string address) { Address = address; AddDomainEvent(this.Adapt\u0026lt;OrderInfoUpdatedEvent\u0026gt;()); } } 可以注意到，鉴于 Entity 这个基类中可以操作领域事件，所以，我们只需要在合适的位置调用 AddDomainEvent() 即可。当然，按照 DDD 的思想，业务通常是针对某个聚合根来开展的，所以，你会看到人们更倾向于让 DomainEvent 成为聚合根的一部分。这里，博主的主要目的是想证明这种方案的可行性，个人以为，即使是放在实体上，一样是无伤大雅。大家可能会疑惑，博主你这样改造完以后，Publish()相关的代码片段哪里去了呢？还记得博主说过要对 DbContext 动一点小手术吗？我们一起来看看：\npublic class ChinookContext : DbContext { private readonly IDomainEventDispatcher _domainEventDispatcher; public ChinookContext(IDomainEventDispatcher domainEventDispatcher) { _domainEventDispatcher = domainEventDispatcher; } // .... public override async Task\u0026lt;int\u0026gt; SaveChangesAsync( CancellationToken cancellationToken = default) { var entities = ChangeTracker.Entries() .Where(x =\u0026gt; x.Entity is Entity \u0026amp;\u0026amp; ((Entity)x.Entity).DomainEvents.Any()) .Select(x =\u0026gt; (Entity)x.Entity) .ToList(); foreach (var entity in entities) { await _domainEventDispatcher.DispatchDomainEvent(entity.DomainEvents, cancellationToken); entity.ClearDomainEvents(); } return await base.SaveChangesAsync(cancellationToken); } } 可以注意到，我们对 DbContext 的 SaveChangesAsync() 方法进行了重写，并利用 EntityFramework 的 ChangeTracker 特性对附加在每个实体的 DomainEvents进行收集，此时，我们只需要把每一个领域事件发布出去即可。请注意，DbContext 中 SaveChanges() 方法拥有多个重载形式，保险起见，你应该重写所有方法，这里仅仅以 SaveChangesAsync() 方法作为演示。对于 IDomainEventDispatcher 这个接口而言，它的定义其实非常简单，就单纯是一个事件分发器，你可以提供任何消息中间件，如 Kafka、RabbitMQ 等等的实现：\npublic interface IDomainEventDispatcher { public Task DispatchDomainEvent\u0026lt;TDomainEvent\u0026gt;( IEnumerable\u0026lt;TDomainEvent\u0026gt; domainEvents, CancellationToken cancellationToken = default ) where TDomainEvent : IDomainEvent; } 在大多数关于 DDD 的文章中，当提到“事件”这个概念的时候，下面的这个事件处理器的定义一定不会缺席。这其实可以牵扯出领域事件和集成事件的区别，领域事件，可以认为是一个领域内，不同聚合根之间互相传递事件，因此，领域事件通常都是进程内的事件，像 MediatR 这样的库就非常合适；而集成事件，则是不同微服务间互相传递事件，因此，集成事件一定是跨服务、跨进程的事件，像 Kafka、RabbitMQ 这样的消息队列就非常合适。\npublic interface IEventHandler\u0026lt;TEventData\u0026gt; : IEventHandler where TEventData : IEventData { void HandleEvent(TEventData eventData); } 虽然，博主目前工作中接触的主要是集成事件，甚至我们发布到 Kafka 中的是二进制形式的 Protobuf，可博主还是在尝试不断思考，是不是当下这个处境就是做好的方案。坦白讲，这篇博客里的内容并不算新颖，因为类似的 EventBus，我在两年前左右就曾亲手实现过，至少在上家公司的时候，面对同样的侵入式的“消息服务”，我个人当时是非常推崇这种事件/消息驱动业务的理念的。现在回过头再看，其实是因为整个物流的生命周期是确定的，业务上的分歧更多的是在中间环节产生，所以，至少在当时看来，事件/消息驱动业务的理念是完全正确的，唯一的问题在于“面条式”的业务代码被这些“消息服务”侵入地面目全非。直到来到现在的公司，发觉业务被 Kafka 肢解地支离破碎，平时工作中大家问的最多的问题居然是，Topic 是啥？Protobuf 是啥？Command 还是 Event？这无疑又让我对这种方案的合理性产生怀疑，作为一个人类，果然都拥有着始终都打不破的历史局限性啊！\n且将新火试新茶 OK，到目前为止，我们基本上讲清楚了整个方案的运作机制，其实，早在两年前，博主就曾使用过类似的技术来实现 数据库审计，彼时彼刻，我对于 DDD 和 领域事件，更多的是一种浅尝辄止的态度，甚至在上家公司工作时的核心冲突，是源于它有大量的数据同步的需求，我们需要一种更优雅的方式来“通知”数据的变更，而不是在代码里到处“埋点”，所以，从某种意义上来讲，今时今日与那年那月是如此的似曾相识，我还是想找到一种方法来规避这些“埋点”。诚然，重写 DbContext 的 SaveChnages() 方法是一种方案，不过，自从 EntityFramework 支持 SaveChanges Events 特性以后，我们又有了一种新的选择。\npublic event EventHandler\u0026lt;SavingChangesEventArgs\u0026gt; SavingChanges; public event EventHandler\u0026lt;SavedChangesEventArgs\u0026gt; SavedChanges; public event EventHandler\u0026lt;SaveChangesFailedEventArgs\u0026gt; SaveChangesFailed; 简单来说，这个特性为 DbContext 增加了三个事件，分别表示保存中、保存后、保存失败，所以，像这种保存到数据库以后再发消息的“一心流”，我们可以使用用下面的方法：\n_chinookContext.SavedChanges += async (s, e) =\u0026gt; { var context = s as ChinookContext; var entities = context.ChangeTracker.Entries() .Where(x =\u0026gt; x.Entity is Entity \u0026amp;\u0026amp; ((Entity)x.Entity).DomainEvents.Any()) .Select(x =\u0026gt; (Entity)x.Entity) .ToList(); foreach (var entity in entities) { await _domainEventDispatcher.DispatchDomainEvent(entity.DomainEvents); entity.ClearDomainEvents(); } }; 这个方案看起来还不错，特别是你没有机会去修改 DbContext 的时候，这会是个非常完美的思路。当然，如果觉得这个方案还不过瘾，你还可以考虑繼承 SaveChangesInterceptor 实现一个拦截器，下面以其中一个虚方法 SavedChanges() 为例：\npublic class DbContextInterceptor : SaveChangesInterceptor { public override int SavedChanges( SaveChangesCompletedEventData eventData, int result) { // 获取 DbContext var context = eventData.Context; // .... return base.SavedChanges(eventData, result); } } 显然，只要在这里拿到 DbContext，剩下的事情就变得非常简单啦！什么叫做“且将新火试新茶”呢？大概就是当你看到这段代码的时候，突然意识到两年前写的数据库审计，应该可以使用这个方法重构一下。这个世界上的东西，每一分每一秒都在发生着变化，连同在这里写下这些只言片语的我，连同在屏幕前看到这些文过饰非的你，也许，追求永恒不变的人都是贪心的人，只是我们自己不愿意承认罢了。可恰恰是因为这个世界纷繁多变，所以，矢志不渝、海枯石烂这种只在童话故事里出现的字眼，始能衬托出这人世间的弥足珍贵，不是吗？就像我以前以为代码不会变，可当你看到过去的代码，突然有了新的体会的时候，这一切终究还是变了。大抵，一切事物间的关系，像磁场一样有强弱的区别，只需要一个人轻轻地走开，留给对方熟悉世界里的仓促与陌生。\n谁道人生无再少 从写下这篇文章的那一刻，我已然三十岁啦，至少从年龄上我已不能再被叫做少年。现在写博客更像是一种心态的描摹，甚至有非常多的内容或者想法，都是在写作过程中一点点追加上去的，所以，你看到我选用了几句诗做了这篇文章的二级标题，根本原因是一开始计划写的时候，只有一个非常模糊的大纲。写这篇文章的动机，一来是先后在两家公司遇到伴随着“埋点”而生的代码侵入问题，二来则是对消息/事件驱动这种业务模式的反思。譬如过去实现 EventBus 都是用 EventHandler 来处理消息，强类型/泛型带来是依赖注入、程序集扫描方面的便利。而新公司则是采用委托来实现消息的订阅处理，两种模式都可以工作得非常好，我属实说不上来哪一种会更好一点。在这个过程中，渐渐地理解了过去理解不了的原理，所以，这应该可以算作一种收获，DDD 对当下业务而言是否合适，我还没有找到答案，不过，单纯地去使用“充血模型”应该可以算做一种进步，就像以前实现 ValueObject 要考虑很多东西，而现在可以直接使用 Record，这种意识有时候是潜移默化的、甚至是可遇不可求的，也许，这就是一个人开始变老的征兆，谁知道呢？谢谢大家，本文完！\n","date":"2022-05-28T16:37:47Z","image":"/posts/review-efcore-and-domain-events-from-ddd-perspective/Domain-Model-Ordering-MicroService.png","permalink":"https://qinyuanpei.github.io/posts/review-efcore-and-domain-events-from-ddd-perspective/","slug":"Review-EFCore-And-Domain-Events-From-DDD-Perspective","tags":["DDD","EFCore","领域事件","Kafka"],"title":"再议 DDD 视角下的 EFCore 与 领域事件"},{"categories":["前端开发"],"content":"大概一周前，在某个「微雨燕双飞」的下午，我正穿梭于熙熙攘攘的车流人海当中，而被雨水濯洗过的天空略显灰白，傍晚亮起的路灯恍惚中有种朝阳初升的错觉，内心更是涌现出一种「一蓑烟雨任平生」的豁达，我还没来得及给这场内心戏添油加醋，兴哥的电话突然打断了我的思绪。一番攀谈交心，我了解到，他想问的是前端容器化部署的相关问题。虽然，靠着兴哥的睿智、果敢，他第二天就想明白了整个事情的来龙去脉；但是，这完全不影响我水一篇博客出来。所以，今天这篇文章，我们来聊聊前端项目的容器化部署，并提供一个极简的实践教程，这里以 Vue.js 为例，希望对大家有所启发。\n你说，这像太阳吗？\r首先，我们来编写 Dockerfile，这里采用的是多阶段构建的做法，第一个阶段，即 build，主要是利用 node.js 基础镜像来实现前端项目的发布，所以，你可以看到 package.json、npm install 以及考虑到国情的 cnpm install 这些前端项目中喜闻乐见的东西，安装完依赖以后我们通过 npm run build 来完成打包，这取决于你项目中实际使用的脚本或者命令，如果你不喜欢 npm，你同样可以用 yarn 来编写这些指令，只要你喜欢就好。做人嘛，最重要的是开心！\n# build FROM node:lts-alpine as build WORKDIR /app COPY package*.json ./ RUN npm install -g cnpm --registry=https://registry.npm.taobao.org RUN cnpm install COPY . . RUN npm run build # deploy FROM nginx:stable-alpine as deploy COPY --from=build /app/dist/ /usr/nginx/wwwroot COPY /nginx/nginx.conf /etc/nginx/nginx.conf EXPOSE 80 CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] OK，第二个阶段，即 deploy，前端发布出来的产物是无法直接在浏览器里打开的，这一点你平时用 Vue.js 的脚手架的话应该会注意到。所以，此时我们需要一个静态文件服务器来托管这些产物，这些产物通常会被放到 dist 目录，因此，在这一阶段主要就是把这个目录里的内容拷贝到 Nginx 下面，这里我们用的是 wwwroot。当然，如果你还怀念曾经的 LAMP 組合，同样可以替换为 Apache。我们在这个世界的一切努力，无非是为了比别人多一种选择，甚至有时候你完全没有选择。可是在计算机的世界里，你可以尽情地去创造，而这则是我的选择，从我高中在班级电脑上写出第一个 Visual Basic 程序开始，我庆幸能一直坚持这份热爱到现在。\nworker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; root /usr/nginx/wwwroot; index index.html; } } 这里提到了 Nginx，那么，自然而然地，对于 Nginx 的配置问题就无可避免。可惜，事实是：每一个声称配置文件更灵活的人，从来都不会去摆弄配置文件，只有程序员天天和这些配置文件打交道。幸运的是，对于简单的静态文件服务器而言，它的配置并不算特别复杂，还记得前面的 wwwroot 吗？其实，它是在这里定义的，对应了 server 节点中的 root 属性，从这里我们可以看到，容器内部默认监听 80 端口，默认页面是 index.html。如果你现在还不太了解 Nginx 的配置文件，相信我，这几个配置已足够你快速上手啦！我不大愿意再写教程的原因是，我不喜欢每个步骤都要截图，并且还要在图上做好标记。显然，对于程序员而言，懒惰是一种美德。\nversion: \u0026#34;3.8\u0026#34; services: font_mock: build: context: ./ dockerfile: Dockerfile image: font_mock ports: - \u0026#34;50001:80\u0026#34; volumes: - \u0026#39;/etc/localtime:/etc/localtime:ro\u0026#39; networks: envoymesh: networks: envoymesh: {} 好了，现在万事具备，我们再来写一个 docker-compose.yaml 来对服务进行编排，可以注意到，我们把 50001 端口绑定到了 80 端口，这就和前面呼应上了，对不对？剩下的就没什么好说的啦，不再一一赘述，如果你看不懂，可以先去了解一下 docker-compose。此时，我们运行 docker-compose up 命令，就可以看到下面的结果：\n通过 Docker 部署前端项目\r没错，我为了节省写作时间，直接使用了 在 Vue.js 中使用 Mock.js 实现接口模拟 这篇文章里的项目，果然，我再次完美发扬了 “懒惰” 这种优秀的美德，总而言之，到这里我们已经成功地通过容器技术部署了一个前端项目，在这个基础上，你还可以接入 Envoy 这个代理层 或者 是为 Nginx 添加 SSL 证书等等\u0026hellip;当然，这些都是后话啦，各位比我聪明千倍、万倍的读者朋友们可以进一步去完善它，这篇短浅易懂的文章就当作诸位的入门教程好啦，愿「他山之石，可以攻玉」，谢谢大家，本文完！\n# 创建 CA 密钥 openssl genrsa -out ca.key 1024 # 利用 CA 密钥生成自签名 CA 证书 openssl req -new -x509 -days 3650 -key ca.key -out ca.crt # 创建服务器端证书密钥 openssl genrsa -des3 -out server.key 1024 # 生成服务器端证书 openssl req -new -key server.key -out server.csr # 利用 CA 证书对服务器端证书进行签名 openssl ca -in server.csr -out server.crt -cert ca.crt -keyfile ca.key 此时，我们会得到服务器端证书文件 server.crt 以及密钥文件 server.key，我们将其配置到 Nginx 中即可，下面是修改后的 Nginx 配置文件：\nworker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; listen 443 ssl; server_name localhost; # 证书文件 ssl_certificate /usr/nginx/ssl/server.crt; # 密钥文件 ssl_certificate_key /usr/nginx/ssl/server.key; # SSL 会话缓存大小为：1M ssl_session_cache shared:SSL:1m; # SSL 会话超时时间为：5min ssl_session_timeout 5m; # 支持 TLS1.0/1.1/1.2 三个版本 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; root /usr/nginx/wwwroot; index index.html; } } 需要注意的是，在创建证书时，对应的 Common Name 应该和网站的域名保持一致，这里的示例域名为：https://www.snowfly.com，在本地调试的时候，我们可以通过修改 hosts 文件来进行测试：\n通过 OpenSSL 创建证书\r通常，你需要把自己创建的证书导入到 受信任的根证书颁发机构 这个分类下面，这样，就可以通过 HTTPS 协议访问你的站点啦！\n在浏览器中查看 HTTPS 证书\r番外预告！！！接下来会写一篇关于 ASP.NET Core 单元/集成测试的文章，敬请期待！\n","date":"2022-05-17T13:30:47Z","image":"/posts/a-simplified-tutorial-on-containerized-deployment-of-front-end-projects-for-vue/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/a-simplified-tutorial-on-containerized-deployment-of-front-end-projects-for-vue/","slug":"A-Simplified-Tutorial-On-Containerized-Deployment-Of-Front-End-Projects-For-Vue","tags":["容器","Vue","Nginx","Envoy"],"title":"Vue.js 前端项目容器化部署实践极简教程"},{"categories":["生活感悟"],"content":" 昨天从外面回来的时候，夕阳的余晖已被街市上的灯火掩没，直到渐渐地远离了闹市，夜晚的氛围终于在微风中扑面而来。抬头看时，深蓝色的天空中零星点缀着三两颗星星，我来不及驻足，已被人流裹挟着向前走去。像往常一样，我转身走进菜场，人依然是那些人，不过陌生和熟悉实在是两种风景，譬如眉清目秀的“豆腐西施”，陌生时你只觉得清冷，而熟悉时你会觉得杀伐果断。这一个月下来，卖菜的阿姨甚至记住了我，看我买了圆白菜、胡萝卜和洋葱，便问我是打算做炒面麽，我还是像过去一样点点头，她立马催促我抓紧时间去买面条，因为再晚一点就要收摊了。所幸的是，家里还有点面条，让我不必如此仓促、窘迫。\n这个五一小长假，我还是没能回去，疫情让我回家的期盼从春节推到五一，再推下去便只有国庆节了，不知命运是否会让我如愿以偿。我问阿姨，难道你们放假都不休息吗？阿姨只是笑着说，我们放假了你去哪里买菜啊！我倒不是担心这个，只是在上班族眼中的 996、大小周，在这个世界上一部分人的眼中是如此的稀疏平常。我立马联想到了我的母亲，我每次就算回到家里，她还是像往常一样忙碌着，仿佛全然没有假期这种概念，可知道我喜欢吃饺子，每次都忙不迭地给我包饺子，大概是因为一切似曾相识，一码通升级的那几天，阿姨拜托我教她使用新版小程序。一刹那间，我突然意识到，在家里的时候，我就是这样教父母使用智能手机。\n人与人的缘分，常常就是从这些小事情开始，虽然它像风筝线一样脆弱，脆弱到只要我们搬一次家、换一所学校、见一次面……，也许，结果就会变得千差万别。前几天，我在地铁上看见一个穿着汉服的小女孩在哭闹，我以为她是玩累了就准备让座给她，结果她是意犹未尽、不想回家，瞬间有种小丑竟然是我自己的感觉；昨天，我在地铁上看见一个推着婴儿车的同龄人，可听到对方给一两岁的孩子讲惯性，我果然还是忍不住在心里笑出了声，大概小孩能听见我内心的声音，他忽然抬起头盯着我，眼珠滴溜溜地转了半天。坦白地说，我有没有遇见这些人，其实对这些人的生命轨迹毫无影响，可在那一刻，我有收到一点点微弱的讯息，一种生活中不经意间流露出来的美。诚然，这一瞬间的相遇，不会影响我回家的方向，不会影响我内心的想法，可它就像夜空里不甘寂寥的星星一样，自作主张地装饰了一番我的生活。\n朋友曾经问我，是不是特别喜欢小孩子？我想，没人能拒绝一个在夕阳下大口吃着米花的孩童，可喜欢是不是就一定要生一个呢？与其说我喜欢小孩子，不如说我更喜欢它们懵懂、天真、无忧无惧的状态，而一切的岁月静好、世事安稳，背后负重前行的那个人，我看不到、更感受不到，所以，我会觉得浪漫、觉得可爱，世界上到底有没有互相感同身受的两个人呢？我想，我永远无法回答，我唯一能做的就是不停地怀念，那些已经永远逝去的日子，或者是夏天坐在凉席上吃西瓜，帮爷爷拨弄头上的白头发，或者是牵着一个人的手在汹涌的人潮里穿梭，两边满是红色的灯笼高高垂下。有人说，世上有种温柔叫做《夏目友人帐》，可四月终究是在熙熙攘攘中滑走啦，就像疫情过后再去青龙寺，终究是错过了樱花呢？\n太阳照在我的背上是如此的温暖惬意，很多东西就像陈年的老风湿一样，如果能经常拿出来晒一晒，那么下雨天就不会感到那么疼痛。以前，我不理解，为什么老年人晒太阳，一坐就是半天。等到长大了才明白，目之所及，皆是回忆；心之所想，皆是过往；眼之所看，皆是遗憾。前几天，我在家里翻来覆去地找了好几遍，找一副放了很久的画，也许是搬家的时候弄丢了吧，虽有遗憾，不逃避、不沉沦，这大概就是我当下的心境。以前出去压马路，我说我是为了学习别人的穿搭，朋友说我这是东施效颦。现在出去，会说这样穿蛮好看的，假如我有 1 米 8 的大长腿，穿出来一定比他还要好看。就这样，一路从真维斯、森马逛到海澜之家，再到优衣库，虽然我没有社牛症，可以当众裸露上身试衣服，可至少看见喜欢的衣服，会对着镜子亲自穿来试试看，期待更好的自己，接纳当下的自己，这就是我现在的生存哲学。\n某个时刻，报话大楼的钟声缓缓敲响，这是对四月的告别，这是对五月的期盼，我看了看手机，相差两分钟，可那又有什么关系呢？早上做完核酸，这回发的是王昭君的贴纸，我说这是打算出塞吗？不知道目的地是哪里？当然，最坏的结果无非是被封在小区里，不要说出塞，连家都出不去。朋友又开始讲扩大桃花运云云，这人怎么这么迷信啊！古人说，「人间四月芳菲尽，山寺桃花始盛开」，四月芳菲已尽，为了扩大桃花运，我应该去山上寺庙里出家当和尚。呵，果然啊，只有魔法才能打败魔法啊。\n","date":"2022-05-03T12:30:47Z","image":"/posts/say-good-bye-to-april/20220503221743.jpg","permalink":"https://qinyuanpei.github.io/posts/say-good-bye-to-april/","slug":"Say-Good-Bye-To-April","tags":["随笔","生活","四月","感悟"],"title":"再见，人间四月天"},{"categories":["编程语言"],"content":"很多年前，星爷在《食神》这部电影里大彻大悟，「只要用心，人人都是食神」。从那个时候起，这句话就隐隐约约带着返璞归真、回归本心的意思。如同电影里描绘的餐饮行业一样，在资本市场的裹挟下，造神这项运动显得轻而易举，这个食神可以是史蒂·周，可以是唐牛，可以是任何人。因此，当穷困潦倒的史蒂芬·周，因为一碗叉烧饭而落泪的时候，我想，这或许是一种直面自我的顿悟。毕竟，电影里的星爷原本就不会做饭。《舌尖上的中国》带火了一句话，“高端的食材，往往只需要最简单的烹饪”，在我看来，这同样是一种“人人都是食神”的自我暗示。多年以后，互联网行业炙手可热的彼时彼刻，一句“人人都是产品经理”让无数人发现，提需求的门槛居然如此的低。其实，早在 1967 年，德国艺术家约瑟夫·博伊斯就曾语出惊人，“人人都是艺术家”，联想到“鸡娃”教育下的各种艺术特长培训班，这句话大概是真的。你内心深处是否同样保留着某种艺术家的梦呢？那么，此时此刻，博主想和大家分享的话题是图像的风格化迁移。\n走近风格化迁移 提到风格化迁移这个概念的时候，大家可能会感到陌生，所以，我们不妨用相近的概念来进行类比。纵观人类的历史长河，初唐四杰、唐宋八大家的诗文各有千秋，李杜诗篇、苏辛长短句各领风骚，更不必说书法上的颜筋柳骨、苏黄米蔡。我曾经在碑林博物馆密密麻麻的石碑中，近距离看到人们如何将石碑上的文字拓下，我开始在脑海里徜徉，是否人类一切伟大的创造都是起源于模仿？这种思绪最终在艾伦·图灵的传记电影 《模仿游戏》 中找到了某种回应，就像人工智能领域里的神经网络，其实就是在模仿人类的大脑进行思考，甚至退一万步讲，当我们还是一个婴儿的时候，襁褓中的牙牙学语、蹒跚学步，这其实还是一种模仿。那么，如果要给风格化迁移下一个定义的话，其实就是让人工智能来对某种风格或者特点进行“模仿”，以图像的风格化迁移为例，它可以将梵高、莫奈或者毕加索的绘画风格“移植”到一张目标图片上，如下图所示：\nNeural-Style-Transformer 示意图\r它可以借由梵高《星空》这副作品中的色彩，来「绘制」一副不一样的向日葵，虽然，梵高一生中创作了无数幅向日葵，在他人生的不同阶段，或表达对生命的渴望，或刻画出死亡的压抑。由此可见，风格化迁移其实可以理解为，不同流派绘画风格的一种“模仿”。当然，这一切都是由计算机通过特定的算法来实现，你可以想象一下，当你通过描摹字帖的方式来练字时，本质上就是在模仿那些书法家们的笔划，而如果将一切的行为都转化为数学公式，这其实就是一种风格化迁移啦！\n卷积神经网络(CNN)在图像风格化迁移上的应用\r目前，图像的风格化迁移，主要的算法支撑来自下面这两篇文章：\nA Neural Algorithm of Artistic Style Instance Normalization: The Missing Ingredient for Fast Stylization 其中，前者提出“用神经网络来解决图像风格化迁移”的思路，而后者则是在此基础上引入了“可感知的损失”这一概念，如果大家有兴趣的话，不妨读一读下面这篇文章，它更像是一篇综述性质的文章，可以帮助你快速了解图像风格化迁移的前世今生，个人感觉，读这类文章会让你快速地认识到自己的无知，这或许是一件好事。\nNeural Style Transfer: A Review 坦白讲，博主是第一次接触神经网络。所以，要学习陶渊明，「好读书，不求甚解」。如果大家确实对这块内容感兴趣的话，还是建议亲自去读一下这些文章，我就不在这里班门弄斧啦！(逃\n体验风格化迁移 好了，当我们对图像风格化迁移有了一定的了解以后，下面我们来快速体验下图像风格化迁移。OpenCV 在 3.3 版本后，正式引入了 DNN ，这使得我们可以在 OpenCV 中使用 Caffe、TensorFlow、Torch/PyTorch 等主流框架中训练好的模型。这里，我们主要参考了 OpenCV 官方的 示例代码:\ndef style_transfer(pathIn=\u0026#39;\u0026#39;, pathOut=\u0026#39;\u0026#39;, model=\u0026#39;\u0026#39;, width=None, jpg_quality=80): \u0026#39;\u0026#39;\u0026#39; pathIn: 原始图片的路径 pathOut: 风格化图片的路径 model: 预训练模型的路径 width: 设置风格化图片的宽度，默认为None, 即原始图片尺寸 jpg_quality: 0-100，设置输出图片的质量，默认80，越大图片质量越好 \u0026#39;\u0026#39;\u0026#39; ## 读入原始图片，同时调整图片至所需尺寸 img = cv2.imread(pathIn) (h, w) = img.shape[:2] if width is not None: img = cv2.resize(img, (width, round(width*h/w)), interpolation=cv2.INTER_CUBIC) (h, w) = img.shape[:2] ## 载入预训练模型 net = cv2.dnn.readNetFromTorch(model) net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV) ## 将图片构建成一个 blob：设置图片尺寸，将各通道像素值减去平均值 ## 比如 ImageNet 所有训练样本各通道统计平均值，然后执行一次前馈网络计算 avg = (103.939, 116.779, 123.680) blob = cv2.dnn.blobFromImage(img, 1.0, (w, h), avg, swapRB=False, crop=False) net.setInput(blob) output = net.forward() ## reshape 输出结果, 将减去的平均值加回来，并交换各颜色通道 output = output.reshape((3, output.shape[2], output.shape[3])) output[0] += avg[0] output[1] += avg[1] output[2] += avg[2] output = output.transpose(1, 2, 0) ## 输出风格化后的图片 cv2.imwrite(pathOut, output, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality]) 接下来，为了让这段代码可以正常工作，你需要通过 pip 安装 opencv-python 这个包：\n# OpenCV 核心包，必须安装 python -m pip install opencv-python==4.5.5.64 # OpenCV 扩展包，建议安装 python -m pip install opencv-contrib-python==4.5.5.64 OpenCV 官方的示例代码中是不带预训练模型的，它使用的是由热心网友 jcjohnson 开发的项目：fast-neural-style。当然，请不要高兴得太早，因为这个项目中同样不带预训练模型，博主一开始就是犯了这个错误，以为直接把这个项目克隆下来就可以了，果然，这里吃了没有文化的亏啊！\nFast-Neural-Style 项目结构\r总而言之，如果这里你下载下来的模型文件大小只有几个 KB 的话，建议你还是手动下载模型文件比较好一点，两种模型的下载链接，我都放在下面啦，你只需要替换模型名称即可！\nhttp://cs.stanford.edu/people/jcjohns/fast-neural-style/models/instance_norm/candy.t7 http://cs.stanford.edu/people/jcjohns/fast-neural-style/models/eccv16/starry_night.t7 事实上，fast-neural-style 这个项目共提供了 10 种不同的风格，它们分别是：基于 Instance Normalization: The Missing Ingredient for Fast Stylization 这篇文章的 6 种风格，以及基于 A Neural Algorithm of Artistic Style 这篇文章的 4 种风格：\neccv16 中提供的 4 种风格\rinstance_norm 中提供的 6 种风格\r这里，我们准备了一张向日葵的图片，并选择 instance_norm/candy.t7 这个模型来进行风格迁移，我们来一起看看能从中得到什么：\nstyle_transfer(\u0026#39;sunflower.jpg\u0026#39;, \u0026#39;sunflower_candy.jpg\u0026#39;,\u0026#39;models\\instance_norm\\candy.t7\u0026#39;) 此时，我们可以得到下面的结果，我个人认为，这有一种接近油画或者水粉画的感觉。因为某种特殊的原因，我对这些美术知识有一点粗浅的了解，所以，当我决定写这篇博客的时候，我总是不免会感慨丛生，画家用颜料来表达自我，诗人用文字来书写人生，一如今天我们用照片和视频来记录生活，可不管我们承载内容的媒介有多丰富，我们对于理解和认同的需求感从未减弱，正所谓，“人生得一知己足矣 ，斯世当以同怀视之”。\n通过风格化迁移生成的图像-01\r类似地，下面是利用剩下的模型生成的图片，我个人更喜欢第 5 张图片，看起来俨然一副素雅的手绘风。通过计算机来生成图像，其中最大的好处是不需要和颜料打交道。我从前有位学美术的朋友，每次见到对方，你总能从那个人换洗过衣服上找到油彩的痕迹：\n通过风格化迁移生成的图像-03\r通过风格化迁移生成的图像-05\r通过风格化迁移生成的图像-06\r风格化迁移对普通的照片尚且有如此魔力，如果把它运用到名家作品上又会怎么样呢？这里，我选择是 约翰内·维米尔 的 《戴头巾的少女》，这幅画出名到了什么程度呢？漫威“寡姐”斯嘉丽·约翰逊、周冬雨、张靓颖等一众女明星都曾模仿过这一形象：\n戴头巾的少女使用风格化迁移后的效果\r怎么样，换一种方式来欣赏名家作品，是否会感到别有风味？而这种感觉，就像让达芬奇放下手里的鸡蛋去画漫天星空，就像让梵高用冷色调的笔触去描摹被刺死在浴缸里的马拉，为原本就扑朔迷离的历史添上一点天马行空的想象，虽然这一切可能都没什么意义，可人生嘛，好玩不就行啦！\n自定义模型训练 到现在为止，我们已体验到通过 fast-neural-style 实现“画画”的乐趣，虽然屏幕前的你，可能并不会认同这种乐趣。大概几年前，我有过一位来自印度的同事，他原本是来中国留学，毕业后找工作就一直留在西安。他主要从事沟通方面的工作，特别是那些需要和外国人打交道的场合。因此，在完全不懂技术的他的眼中，我在电脑面前不停敲击键盘、编程这件事情，在他看来就像是在“画画”。当有一天我真的在用代码“画画”的时候，我突然就想到了他，而人生正是由无数个这样的小插曲组成。\n王希孟-千里江山图卷(局部)\r接下来，我想尝试下自定义模型训练，譬如，用传统的国画来训练模型，然后将其运用到某一张图片上。为什么我会产生这样的想法呢？因为，曾经会有人因为选择国画还是油画，特意来征求我个人的意见，即使对方心中早已有答案，即使我在美术专业上全然是个外行，可那种被人信赖和依赖的感觉，我自始至终都特别怀念，如果有一种东西可以画出一个人内心所想，我们是不是就可以在沟通和理解上少一点遗憾。某种意义上，写博客就是在不断地输出自我的认知，尽管这个过程漫长而且煎熬。\n网友创作的核酸上河图(局部)\r起初，我是参照 fast-neural-style 这个项目来编写 Dockerfile，因为我只有一个非 GPU 的环境来训练模型。其实，在 #40 和 #146 这两个 Issues 下，已然有热心的朋友实现了容器化。可惜，我一直无法解决在Dockerfile內克隆代码报错的问题，再加上搭建 Torch 环境并不是那么顺理。因此，我最终找到了它的替代品：fast_neural_style_train，它是基于 PyTorch 实现的，使用我更为熟悉的 Python 果然要舒服一点，如果你喜欢 TensorFlow，同样可以选择对应实现的版本。这里我简单说下训练过程：\ngit clone https://github.com/cleexiang/fast_neural_style_train cd fast_neural_style_train python -m pip install -r requirements.txt 如你所见，我们需要克隆项目、安装依赖，这个步骤基本没有太多难度。训练模型需要一个数据集，这里我们选用的 COCO 2014 的数据集，大概有 13G 左右的样子，下面是对应的下载链接，建议使用镜像地址来下载：\nhttp://images.cocodataset.org/zips/train2014.zip (官方地址) https://pjreddie.com/media/files/train2014.zip (镜像地址) 解压后你会得到大概 8 万张左右的图片，按照 PyTorch 的要求，你需要将其放置在一个文件夹中，然后将其作为一个整体放在你的数据集根目录。这句话是什么意思呢？譬如，你的数据集目录为 /dataset/, 这些图片你放置在一个叫做 /train2014/ 的目录中，那么，整个的目录结构应该是 /dataset/train2014/。下面我们开始训练，请注意，下面的所有目录都相对于 /fast_neural_style_train/neural_style/ 而言：\ncd neural_style python neural_style.py train \\ --dataset ./dataset/ \\ --style-image ./myStyle.jpg \\ --save-model-dir ./trained \\ --epochs 2 --cuda 0 其中，myStyle.jpg 是我用来训练的传统水墨画，./trained 用来指定模型输出的目录，--cuda 0 表示通过 CPU 来训练模型，如果你有 GPU 环境，可以考虑把这个参数改成 1。执行命令后，首次会下载 vgg16 模型，这是一个用于卷积神经网络的模型，如果网络环境不稳定，可以多尝试几次，剩下的就是静静地等待啦，以博主的渣电脑举例，一个小时大概能跑 2000 张图片，所以，这 8 万多张图片，大概需要跑 40 个小时，太上老君炼丹的乐趣你体会到了吗？\n在博主电脑的 CPU 上训练模型截图-1\r在博主电脑的 CPU 上训练模型截图-2\r通常来讲，只要跑完这些 Epoch，你就可以训练出一个特定的模型，Torch 训练出来的模型是 .t7 格式，PyTorch 训练出来的模型是 .pt、.pth 以及 .pkl 格式，理论上，我们只需要通过下面的命令，就可以验证训练出来的模型的效果。不过，博主这里出了一点点意外，在离成功只有一步之遥的时候，或许是因为 Windows 的自动更新，或许是因为某种难以预料的原因\u0026hellip;，总而言之，电脑重启了一次，这是我第二次失败了，而第一次是跑到一半卡住了，我终于知道，为什么这个世界需要有专业的机器学习平台存在！\npython neural_style.py eval --content-image ./input.jpg / --model ./myStyle.pt / --output-image ./output.jpg / --cuda 0 遗憾自然是有的，不过程序可以跑无数次，最多是花费一点点时间。可人生的很多事情，都是没有办法再重新来过的，当你遇见不同的人和事，你不知道未来会迎来什么样的结局，就像我花费了这 40 多个小时训练模型这件事情一样，也许下一次还会失败呢？可那又有什么关系，毕竟，生活里唯一不变的，只有变化本身啦！无论好坏，稍后我都会更新这件事情的结果，允许我先潦草地为这篇文章写下结尾。\n本文中用来训练模型的传统水墨画\r本文中用来验证模型的桂林山水\r如上图所示，这里我们使用一张传统水墨画来训练模型，并尝试将这种风格迁移到一张桂林山水的图片上，此时，东方世界的传统水墨画、西方世界的照相机，会碰撞出怎么样的火花呢？也许，下面的结果会让你感到失望，因为机器学习有时候就像炼丹，可不是人人都能炼成火眼金睛。关于这三种模型格式，官方的说法是，模型文件只有是否保留模型结构的区别，除此以外，无非是扩展名不一样罢了！\n自定义模型风格化迁移效果\r自定义模型风格化迁移效果-输入2\r自定义模型风格化迁移效果-输出2\r可以注意到，实际的效果并不显著，不知道是不是因为传统国画中使用的颜色有关。以前看《国家宝藏》 的时候，《千里江山图》中使用的颜色基本都是从矿物中提取出来的，也许，正是因为颜料的来之不易，人们在作画时便多了一份虔诚。前段时间，有网友创作出了《清明上河图》风格的“核酸上河图”，千年以后，后人观察我们今天的所作所为，是不是就和我们看宋朝时候的生活风貌一样呢？\n色彩让世界变得丰富，但不要戴有色眼镜\r本文小结 数年前，我曾拜读过 阮一峰 老师翻译的 《黑客与画家》，在这本书中，作者表达的观点是，黑客与画家在本质上是接近的，他们都在试图创作出优秀的作品，都需要想象力和创造力，都需要持续的关注细节、追求卓越。其实，任何一种艺术，不管是否重要，如果你想要在该领域出类拔萃，就必须全身心投入。毫无疑问，黑客是数字时代的手工艺人。所以，在写今天写这篇文章的时候，我觉得这种“混搭”或者说“跨界”，仿佛是两种身份的一次重合，在电脑上花费 40 多个小时来训练一个模型，是不是有种“都云作者痴，谁解其中味”的自嘲呢？图像的风格化迁移，个人觉得是特别有趣的一个领域，因为它可以把艺术和审美这种抽象的东西，转化为一种数学上的表达，就像黄金分割比、莫乌比斯环……也许，在某个像素的背后，就隐藏着蒙娜丽莎微笑的秘密呢？作为一个双子座，我生命中 80% 的时间都在理性和感性中纠缠，这大概是一种宿命。也许，我的使命就是去找出这样一个公式，好描摹我这像风一样来去不定的心的轨迹，本文完，谢谢大家！\n","date":"2022-05-01T13:32:47Z","image":"/posts/a-introduction-to-stylized-migration-of-python/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/a-introduction-to-stylized-migration-of-python/","slug":"A-Introduction-To-Stylized-Migration-Of-Python","tags":["Python","OpenCV","美术","画家"],"title":"Python 图像风格化迁移助力画家梦想"},{"categories":["前端开发"],"content":"最近这段时间，我一直在参与一个前端项目。每当我从庸碌的生活中赢得片刻喘息的时候，我不由得感慨，在程序员朴实无华且枯燥的职业生涯里，写自己喜欢的代码的机会少之又少，写别人喜欢的代码的机会俯拾皆是，更多的时候像是“为他人作嫁衣裳”。古人云，“遍身罗绮者，不是养蚕人”，当每天面对着被改得面目全非的代码的时候，内心固然早已波澜不惊、宠辱偕忘，可还是会期待美好的事情发生，因为从工程化的角度而言，每天都在思考的事情，其实就是怎么样做会更好一点。过去这些年里，微服务、前后端分离的呐喊声不绝于耳，实际应用过程中则是会遇到各种各样的问题。在今天这篇文章里，我想和大家聊聊 Vue.js 结合 Mock.js 实现接口模拟这个话题，为什么选择这个话题呢？我个人认为，它实际上触及了前后端分离的“灵魂”，并且由此可以引出像文档管理、流程控制等等一系列研发协同的问题。你或许会忍不住问道，前后端分离的“灵魂”是什么呢？各位看官们稍坐，且听我一一道来！\n问题现状 在谈到前后端分离这个话题的时候，在公司层面上对应地往往是组织架构的分离，典型的做法就是让前端和后端成为两个不同的团队，其中，前端团队负责表示层的实现，不限于页面布局、样式风格、交互逻辑等等；后端团队负责数据接口的实现，不限于数据库设计、接口设计、编写 API 等等。对应到 Vue.js 里，前端团队负责写各种各样的页面/组件、数据绑定，后端团队负责提供各种各样的数据接口，这听起来非常地合理，对不对？的确，主流的前后端分离实践都是这样讲的，所以，我们只要套用这个模型，就可以达到预期的效果，对不对？可惜，人类习惯于为这个世界寻找某种颠扑不破的真理，可恰恰人类本身才是这个世界里最不稳定的存在？疫情常态化的当下，每次都被病毒一通嘲讽，抄作业都不会抄啊！\n前后端分离模式下的协同开发\r首先，第一个问题，前、后端团队没有形成“契约”，前端团队拿到原型以后就开始设计页面，ViewModel 中的字段命名、定义完全是由前端团队凭“感觉”写出来的，人类离谱就离谱在，可以靠“感觉”这种玄之又玄的东西决定很多事情。这样做的后果就是，后面真正对接后端接口的时候，发现大量的字段没法对应上，不得不再折腾一遍数据绑定，如果是中途由别人来接手，那么面对的可能就是不同的数据结构间的映射转换。试想，后端程序员尚有 AutoMapper 和 Mapster 可以用，前端程序员可就没有那么幸运啦！更不必说，前端天生比后端面临更频繁的改动，只要涉及到页面布局、交互逻辑的变化，ViewModel 的修改基本无可避免，这样就导致同一个页面多次返工，我相信这个结果大家都不想看到。\n其次，当前、后端团队约定好接口文档以后，双方都按照这份接口文档去完成各自的开发工作，这样听起来简直不能更合理对不对？实际上，在后端团队完成接口开发以前，前端团队会有一段时间的“真空期”或者“黑写期”，因为前端并不知道这段代码能否在真实的环境下工作。此时，前端团队可能会造一点假数据来进行接口模拟，得益于 JavaScript 这门语言的高度灵活、自由，前端团队可能会直接调用一个本地函数来返回假数据，这意味着它并不会触发真实地 HTTP 请求。那么，当有一天后端团队完成了接口开发，你将会把这些本地函数替换为 Axios 的方法，甚至在更极端的情况下，前端团队不能访问后端团队的接口，此时，双方会就本地函数还是 Axios 方法产生一场拉锯战，你告诉我，还有什么比这更折磨一个人的吗？\n所以，综合下来，其实是两个非常普遍的问题：\n第一，前、后端团队如何制定一份对协同有利的接口文档，这份文档是通过工具生成还是人工编写。我个人是特别讨厌用 IM 或者邮件来发送接口文档的，因为没办法做到版本控制或者说让所有手中都有一份最新的接口的文档。\n第二，如何管理项目中用到的各种假数据，以及如何让项目在假数据和真实接口中“无痛”切换。前端项目的特点是所见即所得，这让它比看不见、摸不着的后端项目更受用户青睐，毕竟还有什么比能让用户亲眼看到更亲切的东西呢？\n在“小步快跑、快速迭代”的敏捷思想的驱使下，我们经常需要给用户演示各种功能。也许，在某个时刻，页面上的数据亦真亦假，你还会觉得，管理这些假数据没什么意义吗？而这正是驱使我了解 Mock.js 的动力所在，世上的很多事情，你未必能如愿以偿、做到最好，可你依然要了解什么是最好，“山不厌高，海不厌深”，向不那么完美的世界妥协是现实，永远值得去追寻更完美的世界是理想，这两者在我心目中并不冲突，你觉得呢？\n改进思路 OK，既然找到了问题的症结所在，我们逐一对症下药即可，就像“三过家门而不入”的大禹，选择用疏导的方式治水，让洪水通过疏通的河道流到大海中去，而不是靠一味地“堵”，程序中 90% 的代码都是在给用户“打补丁”，防止对方做出什么骚操作来，那么，是不是可以用某种方式去引导对方呢？我最讨厌听到的话就是，用户想要怎么怎么样，这是没有办法的事情，如果只需要一个传话筒，我们为什么不直接用传呼机呢？作为一个老古董，恐怕现在的 00 后都不知道什么是传呼机。你生命中当下流行或者推崇的东西，总有一天会过期。可即便如此，你还是要全力以赴。显然，这是个哀伤的故事。\nSwagger 对于接口文档的管理问题，我自始至终都推荐 Swagger 这个神器，因为我和这个世界上的绝大多数的程序员一样，都认同一种相对朴素的价值观，即 “懒惰是一种美德”。因为我不喜欢靠人工来维护接口文档，所以，只要有机会用上 Swagger，我一定会用 Swagger 来管理接口文档。不管是过去写 API 和 MVC，还是现在写 gRPC。对我来说，选择 Swagger 是一件自然而然的事情，因为我懒，因为我不理解为什么有人需要导出 Word 或者 Pdf 格式的接口文档。也许，Swagger 那千篇一律的页面风格会让人感到无所适从，喜欢的人非常喜欢，讨厌的人非常讨厌。在前、后端分离的项目中，有一份白纸黑字的接口文档，显然要比“口口相传”靠谱得多。当然，如果你有足以媲美 Swagger 的接口文档管理工具/平台，欢迎大家在评论区留言分享。下面是我曾经写过的关于 Swagger 的文章：\n通过 ApiExplorer 为 Swagger 提供 MVC 扩展 gRPC 搭配 Swagger 实现微服务文档化 .NET Core POCOController 在动态 Web API 中的应用 ASP.NET Core gRPC 打通前端世界的尝试 Mock.js 好了，下面我们来介绍今天这篇博客的主角：Mock.js。如果你有养成写单元测试的好习惯，那么，你一定对 Mock 的概念了如指掌。在 .NET 的生态中，有一个大名鼎鼎的模拟库：Moq，它可以让我们更加方便地模拟各种对象的行为。什么情况下需要模拟呢？我想，是目标对象不可用的时候。如果把这个认知迁移到前端开发中，我们就会发现前端依赖最多的其实是后端的接口。那么，有没有一种方案，可以让前端在后端接口不可用的情况下，模拟出调用后端接口的效果呢？而这，就是 Mock.js 存在的意义。事实上，我一开始就讲到，在后端团队完成接口开发以前，前端团队会有一段时间的“真空期”或者“黑写期”，而这段时间显然是最需要进行模拟的一个环节。综上所述，不管是技术层面还是流程层面，这个模拟的阶段都是真实存在着的，并非是人为捏造或者臆想出来的东西。\n在考虑引入 Mock.js 以前，项目中充斥着类似于下面这样的代码。你不得不承认，JavaScript 是一门灵活而且强大的编程语言，考虑到它返回的是一个 Promise，所以，它和调用后端接口相比没有任何区别，你可以直接在组件中调用这个方法：\nexport async function getMessages() { return new Promise(resolve =\u0026gt; { const data = [{ \u0026#39;id\u0026#39;: \u0026#39;065CB06C-E082-0E55-24A5-54917C4BD182\u0026#39;, \u0026#39;eventTime\u0026#39;: \u0026#39;2017-07-07 08:50:16\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;人生若只如初见，何事秋风悲画扇\u0026#39; },{ \u0026#39;id\u0026#39;: \u0026#39;C0C3298D-6A91-DE5B-EBD2-30F60E59E4EE\u0026#39;, \u0026#39;eventTime\u0026#39;: \u0026#39;2017-08-07 18:50:16\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;醉后不知天在水 满船清梦压星河\u0026#39; } //... ] return resolve({ code: 200, data: data }) }) } 这个方案最大的问题是，当后端接口准备就绪以后，你还需要用实际的请求过程比如 Axios 替换掉这个方法，虽然这个工作量并不算特别大，可我总觉得这不是一个正确的思路，虽然代码频繁地改动对程序员来说是家常便饭，我是个不大愿意在同一件事情上来回折腾的人，我更喜欢古龙武侠小说里那种一招制敌的感觉，我们来一起看看 Mock.js 是如何解决这个问题的：\n// 通过 npm 安装 npm install mockjs // 通过 yarn 安装 yarn add mockjs 首先，我们通过 npm 或者 yarn 安装 Mock.js，对于上面的这个例子，我们可以像下面这样改造。一开始我们有提到，如果去管理这些假数据，我这里的建议是按照模块建立相应的文件夹，并最终通过一个统一的入口 /mock/index.js 来导入这些假数据。\n// ./mock/message/index.js const Mock = require(\u0026#39;mockjs\u0026#39;); const Random = Mock.Random; // 生成 10 条消息并赋值给 data 字段 const messageList = Mock.mock({ \u0026#39;data|10\u0026#39;: [{ eventTime: () =\u0026gt; Random.datetime(), content: () =\u0026gt; Random.csentence(5, 10), id: () =\u0026gt; Random.guid() }] }) export function getMessageList() { return { code: 200, data: messageList.data } } 接下来，我们在 /mock/index.js 这个文件中注册相应的路由即可：\nconst Mock = require(\u0026#39;mockjs\u0026#39;); import { getMessageList, addMessage } from \u0026#39;./message\u0026#39; Mock.setup({ timeout: 500 }) Mock.mock(/\\/api\\/messages/, \u0026#39;get\u0026#39;, getMessageList) 需要注意的是，这里一定要使用 /\\/api\\/messages/ 这种带转义符的方式来定义路由，它表示这个请求会被拦截，实际调用的是 getMessageList 这个方法，如果我们直接写 /api/messages/ 会返回 404，这一点非常重要。现在，我们就可以直接在组件或者页面内访问这个路由：\nthis.$http.get(\u0026#34;/api/messages\u0026#34;).then((res) =\u0026gt; { let resData = res.data; if (resData.code == 200) { this.messageList = resData.data; } }); 其中，$http 是挂载到 Vue 原型上的 Axios 对象，显而易见，只要前、后端都严格按照文档来定义这个路由，那么，这个代码是不需要再调整的。等后端接口开发完成以后，我们只需要切换到正式地址联调即可。当然，前端和后端可能会部署在不同的服务器上，而这个对我们的影响，无外乎是修改一下 Axios 的 baseURL。那么，Vue 怎么知道什么时候调用 Mock，什么时候调用真实接口呢？答案就藏在入口文件 main.js 中，只要我们导入了 /mock/index.js 这个文件，Mock 就会生效：\nimport \u0026#39;./mock/index\u0026#39; 如下图所示，这里的的消息列表完全由 Mock.js 驱动，其中的消息内容、时间等都是随机生成的：\nVue.js 搭配 Mock.js 使用效果\r在平时的工作中，我常常听到一个词叫做“造数据”，本质上这依然属于 Mock 的范畴。现实生活中使用到的数据，毫无疑问会比这篇文章中的例子更加复杂和多样化，典型的有地址、邮箱、身份证号、IP 等等。对于这些，Mock.js 同样可以模拟，下面列举了常见的 API，更详细的可以参考官方文档中的 示例：\n// 随机生成省份、直辖市或者自治区 Random.province() Mock.mock(\u0026#39;@province\u0026#39;) Mock.mock(\u0026#39;@province()\u0026#39;) // 随机生成 URL Random.url() Mock.mock(\u0026#39;@url\u0026#39;) Mock.mock(\u0026#39;@url()\u0026#39;) // 随机生成中文段落 Random.cparagraph() Mock.mock(\u0026#39;@cparagraph\u0026#39;) Mock.mock(\u0026#39;@cparagraph()\u0026#39;) // 随机生成日期 Random.datetime() Mock.mock(\u0026#39;@datetime\u0026#39;) Mock.mock(\u0026#39;@datetime()\u0026#39; 如果用使用过 .NET 里的 Bogus 这个库，相信我，你会对这一切感到相当亲切。\n登高望远 OK，写到这里一切，从点题的角度来看，这篇文章已经可以画上完美的句号，因为 Mock.js 在 Vue 中的基本用法，其实已经完全讲完啦！当然，我们在这个基础上尝试更多的东西。比如，YAPI，这同样是一个功能强大的 API 管理平台，它可以导入 Swagger 格式的接口文档，并为每一个 API 接口创建对应的 Mock 接口，在这种情况下，前端可以使用对应的 Mock 接口完成前期开发。当然，我认为，一个逻辑上自洽的流程应该是，定义接口、生成文档、生成 Mock 接口、使用 Mock 接口、使用真实接口，大家觉得呢？\n通过 YAPI 管理 API 接口\r个人心目中，只有国产软件 Apifox 能勉强达到这一点，如图所示，每当我们填入一个 API 接口定义的时候，它会为我们创建对应的用例，每个用例对应一个 Mock 的地址，某种程度上讲，它的确做到它对外宣称的集 API 文档、API 调试、API Mock、API 自动化测试 等多种特性于一身，同样地，它可以导出各种格式的 API 文档，我个人觉得这个软件挺好用的，YAPI 每次都需要打开网页去增加新接口，这个从流程上讲更潜移默化一点。\n通过 Apifox 管理 API 接口\r虽然但是，有那么多的工具帮助你管理文档、Mock 接口，可你还是做不好一个项目，你说，这是为什么呢？我猛然间想起去年写过的一篇博客，《使用 HttpMessageHandler 实现 HttpClient 请求管道自定义》，里面就在写，如何借助 HttpMessageHandler 做 API 接口的 Mock，人啊，兜兜转转，大抵又回到了原点罢！\n","date":"2022-04-15T22:49:47Z","image":"/posts/interface-mock-implemention-using-mock.js-in-vue.js/cover.png","permalink":"https://qinyuanpei.github.io/posts/interface-mock-implemention-using-mock.js-in-vue.js/","slug":"Interface-Mock-Implemention-Using-Mock.js-In-Vue.js","tags":["Vue","Mock","前端","研发"],"title":"在 Vue.js 中使用 Mock.js 实现接口模拟"},{"categories":["编程语言"],"content":"在此之前，我曾写过一篇博客，《Envoy 集成 Jaeger 实现分布式链路追踪》，主要分享了 ASP.NET Core 应用如何结合 Envoy 和 Jeager 来实现分布式链路追踪，其核心思想是：生成一个全局唯一的 x-request-id ，并在不同的微服务或者子系统中传播该信息。进而，可以使得相关的信息像一条线上的珠子一样串联起来。在此基础上，社区主导并产生了 OpenTracing 规范，在这个 规范 中，一个 Trace，即调用链，是由多个 Span 组成的有向无环图，而每个 Span 则可以含有多个键值对组成的 Tag。不过，当时我们有一个非常尴尬的问题，那就是每个微服务必须显式地传递相关的 HTTP 请求头。那么，是否有一种更优雅的方案呢？而这就是我们今天要分享的内容。首先，我们来回头看看当初的方案，这是一个非常朴实无华的实现：\n[HttpPost] public async Task\u0026lt;IActionResult\u0026gt; Post([FromBody] OrderInfo orderInfo) { var paymentInfo = new PaymentInfo() { OrderId = orderInfo.OrderId, PaymentId = Guid.NewGuid().ToString(\u0026#34;N\u0026#34;), Remark = orderInfo.Remark, }; // 设置请求头 _httpClient.DefaultRequestHeaders.Add( \u0026#34;x-request-id\u0026#34;, Request.Headers[\u0026#34;x-request-id\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add( \u0026#34;x-b3-traceid\u0026#34;, Request.Headers[\u0026#34;x-b3-traceid\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add( \u0026#34;x-b3-spanid\u0026#34;, Request.Headers[\u0026#34;x-b3-spanid\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add( \u0026#34;x-b3-parentspanid\u0026#34;, Request.Headers[\u0026#34;x-b3-parentspanid\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add( \u0026#34;x-b3-sampled\u0026#34;, Request.Headers[\u0026#34;x-b3-sampled\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add( \u0026#34;x-b3-flags\u0026#34;, Request.Headers[\u0026#34;x-b3-flags\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add( \u0026#34;x-ot-span-context\u0026#34;, Request.Headers[\u0026#34;x-ot-span-context\u0026#34;].ToString()); // 调用/Payment接口 var payload = JsonConvert.SerializeObject(paymentInfo) var content = new StringContent(payload, Encoding.UTF8, \u0026#34;application/json\u0026#34;); var response = await _httpClient.PostAsync(\u0026#34;/Payment\u0026#34;, content); var result = response.IsSuccessStatusCode ? \u0026#34;成功\u0026#34; : \u0026#34;失败\u0026#34;; return new JsonResult(new { Msg = $\u0026#34;订单创建{result}\u0026#34; }); } 这里，最大的问题是，传递 HTTP 请求头的代码片段对正常的业务代码存在入侵，当别人调用某个微服务或者子系统的接口时，必须要加上这些代码片段，这实在是一件难受的事情。子曰，“己所不欲，勿施于人”，如果一段代码，你自己都感觉看不下去，那就说明这代码该重构啦！下面，我们考虑对其进行重构。首先，通过 NuGet 安装一个微软提供的包：\ndotnet add package Microsoft.AspNetCore.HeaderPropagation 接下来，我们在 Startup 中对 HeaderPropagation 进行简单配置：\nservices.AddHeaderPropagation(options =\u0026gt; { // 如果请求头中含 X-BetaFeatures 字段，则传播该字段对应的值 options.Headers.Add(\u0026#34;X-BetaFeatures\u0026#34;); // 如果请求头中不含 X-BetaFeatures 字段，则生成一个新的值并进行传播 // 注意，这里以 GUID 为例 options.Headers.Add(\u0026#34;X-BetaFeatures\u0026#34;, context =\u0026gt; { return Guid.NewGuid().ToString(\u0026#34;N\u0026#34;); }); }); 那么，这些请求头会传播到哪里呢？答案是 HttpClient，所以，你可以想到，不管是 RESTful 风格的 API 还是 gRPC 都可以享受到这一便利：\n// 传播所有注册过的请求头，如 X-BetaFeatures services .AddHttpClient(\u0026#34;Ezio\u0026#34;) .AddHeaderPropagation(); // 仅传播指定的请求头，如 X-BetaFeatures、X-Experiments services .AddHttpClient(\u0026#34;Altaïr\u0026#34;) .AddHeaderPropagation(options =\u0026gt; { options.Headers.Add(\u0026#34;X-BetaFeatures\u0026#34;, \u0026#34;X-Experiments\u0026#34;); }); 最后，我们还需要在请求管道中启用相应的中间件：\npublic void Configure(IApplicationBuilder app, IWebHostEnvironment env) { // ... // 注意：请放置在 app.UseRouting(); 前面 app.UseHeaderPropagation(); // ... } 对于我们而言，我们需要让 Jeager 相关的请求头传播下去，因此，我们只需要像下面这样改造即可。当然，这些请求头会由 Envoy 自动生成，所以，我们同样不需要考虑它不存在的情况：\nservices.AddHeaderPropagation(opt =\u0026gt; { opt.Headers.Add(\u0026#34;x-request-id\u0026#34;); opt.Headers.Add(\u0026#34;x-b3-traceid\u0026#34;); opt.Headers.Add(\u0026#34;x-b3-spanid\u0026#34;); opt.Headers.Add(\u0026#34;x-b3-parentspanid\u0026#34;); opt.Headers.Add(\u0026#34;x-b3-sampled\u0026#34;); opt.Headers.Add(\u0026#34;x-b3-flags\u0026#34;); opt.Headers.Add(\u0026#34;x-ot-span-context\u0026#34;); }); services.AddHttpClient(\u0026#34;PaymentService\u0026#34;, client =\u0026gt; { client.BaseAddress = new Uri(\u0026#34;http://127.0.0.1:9090\u0026#34;); }) .AddHeaderPropagation(); 改造后的效果如何呢？博主表示，一切非常完美！\nIt\u0026amp;rsquo;s Amazing！\r关于这个中间件内部是如何运作的，大家可以阅读它的 源代码，博主这里画了一个简单的示意图来辅助说明：\nHeaderPropagation 中间件示意图\r可以注意到，这个中间件内部会维护一个叫做 HeaderPropagationValues 的对象实例，其生命周期为 Singleton，当有入站请求产生时，它会尝试从 HttpContext 中读取指定的请求头，并保存到 HeaderPropagationValues 实例的 Headers 属性中中。当我们注入 HttpClient 的时候，中间件内部会创建一个 HeaderPropagationMessageHandler 实例，它继承自 DelegatingHandler。如果你看过我以前的文章，《使用 HttpMessageHandler 实现 HttpClient 请求管道自定义》，相信你会在电光火石间明白我在说什么。总而言之，通过这个 Handler，你就可以把保存下来的请求头添加到 HttpClient 的实例上，相当于我们一开始手动设置请求头的这个环节，这样，这些请求头就可以“自动”传播下去啦！\n通过 HeaderPropagation 中间件传递请求头字段\r其实，除了这个分布式链路追踪的场景，更一般的场景，或许是认证的场景。譬如，客户端通过认证服务拿到了一个令牌，它在向后端发起请求的时候，会把这个令牌添加到请求头中。此时，我们只需要确保所有后端服务都配置了这个中间件，令牌会随着调用链路一路传播下去，这样，是不是比每个服务间都相互协商如何传递身份信息要好的多呢？我想，这是毫无疑问的，做正确的事情永远比单纯的做事情要重要得多。好啦，以上就是这篇博客的全部内容啦，如果大家对博客内容有任何意见或者建议，欢迎大家在评论区留言，谢谢大家！\n","date":"2022-04-07T09:34:36Z","image":"/posts/asp-net-core-using-headerpropagation-for-distributed-tracking/HeaderPropagation.drawio.png","permalink":"https://qinyuanpei.github.io/posts/asp-net-core-using-headerpropagation-for-distributed-tracking/","slug":"ASP-NET-Core-Using-HeaderPropagation-For-Distributed-Tracking","tags":["Envoy","Tracing","Jeager",null],"title":"利用 ASP.NET Core 中的标头传播实现分布式链路追踪"},{"categories":["生活感悟"],"content":"\r最近读了一本书，来自瑞典作家弗雷德里克·巴克曼的处女作，《一个叫欧维的男人决定去死》，该书于 2015 年被改编成同名电影，主要讲述了一个孤独老者生命中最后三周的故事，它的情节是如此的简单和质朴：一个一心赴死的、固执老人，不断尝试使用各种方法“杀”死自己，结果因为一对新邻居的到来而频频被打断。随着寻求欧维帮助的人越来越多，欧维对于这个世界的牵挂越来越深，妻子死后变成黑白两色的单调世界，开始被这些贸然闯入的人们刷上新的色彩，而欧维同样成为了别人生命里的色彩。所以，这是一个温情而治愈的故事，欧维从决定自杀到放弃自杀，从排斥邻居到接纳邻居，这一系列的转变，让我不由得感慨：原来生活本身，就是人与人之间的羁绊，终其一生，我们追求的幸福感，不过是被别人需要和认同。\n莫名地想起项脊轩志\r一开始看到这个设定，会很容易地联想到东野圭吾的 《嫌疑人 X 的献身》，大抵都是一个对生活绝望的人一心求死，结果因为某一个人或者某一件事而做出改变。我想，大家对类似的设定感兴趣，或许是我们都渴望着被某一个人救赎，可即使是像石神哲哉这样智力超群的人，依然不会得到爱，我个人更愿意将这种情感，理解为一种介于人性和神性之间的美。从某种意义上来讲，沉浸在数学世界里的石神哲哉，妻子过世后只有黑白两色的欧维，其实是非常相似的两个人，都是一个自认为不被社会认可和需要的人，因为一次次被需要，然后重新体会到生命的价值。可惜，欧维比石神哲哉更幸运一点，因为他有一群笨拙而又温柔的邻居，正是这些“不速之客”们的“袭扰”，一点点地让这个外表固执、内心温柔的老人敞开心扉，最终同这个满目疮痍的世界完成和解。\n曾经被称为“来自地狱的恶邻”的男人\r回顾欧维的一生，命运带给了他无数的苦楚，幼年丧母、少年丧父，失去至爱双亲的欧维，从父母那里继承的“遗产”，不过是一栋摇摇欲坠的房子、一辆即将散架的萨博 和 一块早已变形的腕表。当然，更不幸的是，十八岁那年，一场大火让他连这座木屋都要失去。直到后来，他遇见了一个人，那个人注定要在他生命中出现，然后和他组成家庭。那是一个开朗大方、热爱生活的漂亮女孩，她的一抹绛唇、红色高跟鞋，刹那间变成了欧维生命中唯一的色彩，“人们总说，欧维眼里的世界非黑即白，而她是色彩，他的全部色彩。”，在遇见索雅以前，欧维的生活没有方向，他只是机械地完成着每天的工作，像机器一样精确而古板地生活着，“从来没人问过欧维，遇见她之前他是怎么生活的。但要是有人问起，他一定会回答说，自己没有生活。”，同样地，对索雅来说，在遇见欧维以前，她只爱书、她的爸爸和猫，可命运还是决定，让两个性格迥异的人产生联系。\n在火车上邂逅生命里唯一的色彩\r童话故事告诉我们，王子最终一定会和公主走到一起，命运让索雅邂逅了不善言辞的欧维，让她看到了欧维木讷外表下的温柔、固执性情里的可靠，她就像夏日里温暖的一束阳光，突然照进了欧维黑白分明的二维世界里。在索雅的鼓励下，欧维考取了建筑工程师，有了属于自己的家庭。索雅的书实在太多了，甚至连厨房和架子上到处都是书。每当这个时候，他总是不声不响地为她做出一个个书架，直到那些书架填满了整整一面墙。一个真正爱你的人，会默默地把你说过的每一句话都记在心里。很多时候，欧维就是在这种平静而又温柔的日子里，静静地等待着即将出生的孩子。可惜，命运并不能体谅这种一眼万年的情感，它只是面无表情地向欧维施加着生命不能承受的苦楚，在和妻子去西班牙旅行的途中，一场意外让索雅失去了孩子、失去了行走能力，后半辈子只能在轮椅上度过。\n在轮椅上追逐生命里不变的色彩\r对于欧维而言，他更喜欢那些有确定规则和结果的东西，就好像只要遵循发动机的原理，就可以让那辆萨博启动一样。父亲曾教会他最重要的品质——诚实，此后的很多年里，他固执地坚持着那些别人不以为然的规则，譬如每天早上都在社区里巡逻、检查每户人家的仓库门锁是否完好、明令禁止别人在社区里开车、查看垃圾是否合理摆放……等等，所以，他完全不能理解这个世界，为什么没人愿意为妻子修建轮椅的步道，为什么身穿“白衬衫”的政府工作人员像冷血动物一样不通人情……每当这个时候，索雅总是温柔地对他说，“人要么死去，要么就想办法活着。”，她从未失去过对于生活的那份勇气和乐观，她选择踩踏着丈夫修建好的那段斜坡一次次走进教室，并最终成为了孩子们严眼中最好的老师，她失去了自己的孩子，她拥有着无数的孩子。在索雅被癌症夺走生命以前，这份炽热与光明始终温暖着欧维的心。\n生存还是毁灭？这是一个问题\r对生命而言，其实就是一个不断失去着的过程，当时间的皱纹一点点地爬上额头，这个命运多舛的男人还是失去了所有。在妻子去世半年以后，工厂里告诉他以后不用再去上班，虽然他曾像大多数人一样，在这里耗费四十多年的时光，“他做了一切社会需要他做的事，工作，从不生病，结婚，贷款，缴税，自食其力，开正经的车，社会是怎么报答他的呢？它冲进办公室让他卷铺盖回家，这就是报答。某个星期一，突然他就没用了”。也许，正是从那一刻起，欧维决定用死亡来对抗这个冷漠的世界，这个社会已经没法再生产出结实到可以吊死人的绳子，“这是一个还没过期就已经过时的世界。整个国家都在为没人能正经做事起立鼓掌，毫无保留地为平庸欢呼喝彩”，欧维同样不习惯这个没人会换轮胎、装开关、换瓷砖、粉刷墙壁、倒拖斗车的社会，如果说索雅是他生命里唯一的色彩，那么，当这唯一的色彩消失不见的时候，这个叫做欧维的男人决定去死。\n一个叫欧维的男人决定去死\r毫无疑问，我们都是孤独的，而唯有爱才是生命里终极且永恒的救赎，真正让欧维不再寻死的救赎是爱，是那种发自内心的，对别人的关爱，正是一次又一次的“麻烦”，让别人意识到，这个有一点古怪、固执的老人，虽然有一套严谨、古板的处世哲学，其实内心深处是一个特别温柔的男人。曾经，他有过一个最要好的朋友鲁尼，他们一起为社区制定了公约、一起参与社区委员会主席的竞选，可最终他们因为汽车品牌的分歧而分道扬镳。也许，真正让欧维厌恶的，并不是好友买了一辆与自己完全不同的汽车，而是这个世界，对这个一无所有的老人不大温柔。欧维无法适应平板电脑这种新型电子设备、无法适应信用卡代替现金，他可以为了一张优惠券和店员争执上半天……这样一个被称为“来自地狱的恶邻”的男人，在妻子离开这个世界以后，显得与整个世界格格不入，他最“正常”的时候，是像年轻时那样穿好西装、戴上帽子、手里捧着一束鲜花去墓地看望妻子，那一刻他的世界突然又有了色彩。\n在彼此的生命里互为装饰\r真正的转机来自一对叫做帕瓦尼的新邻居的出现，他们找欧维帮忙倒拖斗车、借梯子修房子，让欧维开车带他们去医院、顺便帮助照看孩子。后来，他们甚至让欧维教他们开车……这一次又一次的“麻烦”，突然间让欧维卸下了冰冷的外壳，他以为他是生命里只有黑白两色，可在孩子们的眼中，他是这个世界上唯一的色彩，甚至成为邻居眼中的“老父亲”，孩子们眼中的“外公”，他突然意识到，原来他对于这个世界而言还有存在的意义，原来他还可以被别人需要、被这个世界需要……从那以后，这个失去了太多东西的男人，决定与这个冰冷的世界达成和解，他不再对抗那些命运带给他的痛苦和折磨，而是选择平静、温和的面对它们，他救下了一只在雪中瑟瑟发抖的猫咪、替曾经的朋友鲁尼修好暖气、把曾经给自己孩子准备的摇篮留给邻居刚刚降生的孩子……英雄迟暮，他曾失去过母亲、父亲、妻子和孩子，而这一刻，他有了一个“女儿”，成为了三个孩子的“外公”，在不同人的生命里互为装饰……\n最美好的永远都成为回忆\r没有人是一座孤岛，我们一生追求的幸福感，很大程度上都来自这种认同感或者说被别人需要的感觉，我们总说，人是一切社会关系的集合，正是在被别人不断地“麻烦”着、“被需要”着，我们才能正视生命个体在这个广袤世界里的价值，虽然，这种关系的琐碎，曾令我们产生种种喜怒哀乐的的情绪，可生命的过程原本就属于体验派，你追求的一切事物都会消亡然后又被新的事物替代，就像生老病死之于生命、春华秋实之于四季。爱会不会消失，我始终不得而知，我只知道，欧维在面对心脏病、面对死亡时，多了一份平静和坦然。他曾经尝试过无数种寻死的方法，可就是这样一个“心大”的老头儿，此刻，终于能卸下那张冰冷的面具，以他本来的面目示人。多年以前，当他在火车上第一次遇见那个女人时，她正踩着深红色的高跟鞋，涂抹着深红色口红的嘴唇边正挂着微笑，他们两个人是那样的年轻，欧维抬起手的一瞬间，他的世界充满了色彩，欧维依然穿着那件浅蓝色的西装、戴着那顶圆形帽子，他没有说话，我想，应该是在对着某个人微笑……\n","date":"2022-03-29T09:34:36Z","image":"/posts/a-man-called-ove/P2369981748.jpg","permalink":"https://qinyuanpei.github.io/posts/a-man-called-ove/","slug":"A-Man-Called-Ove","tags":["读书","感悟","随笔","情感"],"title":"读《一个叫欧维的男人决定去死》"},{"categories":["编程语言"],"content":"几天前，某人同我抱怨，说是某接口无法正常工作，坦白地讲，这只是程序员生命里再枯燥不过的日常，因为无论“好”或者“不好”，他们都要努力回应来自灵魂深处的那声“为什么”。所以，善待程序员的方式之一，就是不要总问他“为什么”，因为他已经听了太多的“为什么”。经过一番攀谈交心，我了解到是模型绑定出了问题。原来，他需要实现一个导出/下载功能，因为他不确定能否通过 Envoy 代理来自 gRPC 的文件流，故而，他选择了传统的 Web API，结果不曾想在模型绑定上栽了跟头。听完了他的话，我不禁陷入了沉思，难道 gRPC 真的不能做文件的上传和下载吗？常言道，“实践出真知”，所以，今天这篇博客，我们来聊聊利用 gRPC 实现文件的上传和下载。\n定义 Protobuf 首先，我们来看 Protobuf 的定义，此前介绍 gRPC 流式传输相关内容的时候，我一直找不到一个更为贴切的场景，而此时此刻，我只想说，冥冥中自有天意，难道还有比上传和下载更好的例子吗？\nservice FileService { rpc UploadFile(stream UploadFileRequest) returns (UploadFileResponse); rpc DownloadFile(DownloadFileRequest) returns (stream DownloadFileResponse); } //Upload message UploadFileRequest { string FileName = 1; bytes Content = 2; } message UploadFileResponse { string FilePath = 1; } //Download message DownloadFileRequest { string FilePath = 1; } message DownloadFileResponse { bytes Content = 1; } 其中，UploadFile是一个针对客户端的流式接口，DownloadFile是一个针对服务器端的流式接口，可以注意到，这其实非常符合我们平时对于上传/下载的认知，即，对上传而言，客户端以二进制流的形式作为输入；对下载而言，服务器端以二进制流的形式作为输出。在 Protobuf 的定义中，二进制流可以使用 bytes类型来表示，因此，我们在 UploadFileRequest 和 DownloadFileResponse 这两个类型中，统一使用 Content 这个字段来表示上传或者下载过程中的二进制流。\n实现上传 首先，我们来看上传的实现。此时，客户端将以流的方式写入 UploadFileRequest 这个参数。假设我们这里不考虑多个文件的上传，那么，我们有两种策略来处理 UploadFileRequest 这个参数。方案一是直接把整个文件取出来。然后一次性发出去；方案二则是把整个文件分成不同的块，然后按照顺序逐个发出去。考虑到，我们这里关注的是流式传输，我们采用第二种方案来实现：\nvar uploadResult = fileServiceClient.UploadFile(); var uploadPath = Path.Combine(Directory.GetCurrentDirectory(), \u0026#34;ACRouge.png\u0026#34;); using (var fileStream = File.OpenRead(uploadPath)) { var sended = 0L; var totalLength = fileStream.Length; var buffer = new byte[1024 * 1024]; // 每次最多发送 1M 的文件内容 while (sended \u0026lt; totalLength) { var length = await fileStream.ReadAsync(buffer); sended += length; var request = new UploadFileRequest() { Content = ByteString.CopyFrom(buffer), FileName = uploadPath }; await uploadResult.RequestStream.WriteAsync(request); } } await uploadResult.RequestStream.CompleteAsync(); var reply = await uploadResult.ResponseAsync; 毫无疑问，这里的关键是：设置一个固定大小的缓冲区，每次从文件中读取出一部分，然后将其发送出去，直到整个文件都读取完为止。这样做的好处是，服务器端在接收到这些二进制“块”以后，只需要按照接收顺序写入文件即可。下面，给出的是对应的服务器端的实现：\npublic override async Task\u0026lt;UploadFileResponse\u0026gt; UploadFile( IAsyncStreamReader\u0026lt;UploadFileRequest\u0026gt; requestStream, ServerCallContext context) { var requests = new Queue\u0026lt;UploadFileRequest\u0026gt;(); while (await requestStream.MoveNext()) { var request = requestStream.Current; requests.Enqueue(request); } var first = requests.Peek(); var fileExt = Path.GetExtension(first.FileName); var fileName = $\u0026#34;{Guid.NewGuid().ToString()}{fileExt}\u0026#34;; var filePath = Path.Combine(_webHostEnvironment.ContentRootPath, \u0026#34;Upload\u0026#34;, fileName); var fileFolder = Directory.GetParent(filePath); if (fileFolder != null \u0026amp;\u0026amp; !fileFolder.Exists) fileFolder.Create(); using (var fileStream = File.Open(filePath, FileMode.Append, FileAccess.Write)) { var received = 0L; while (requests.Count() \u0026gt; 0) { var current = requests.Dequeue(); var buffer = current.Content.ToByteArray(); fileStream.Seek(received, SeekOrigin.Begin); await fileStream.WriteAsync(buffer); received += buffer.Length; } return new UploadFileResponse() { FilePath = $\u0026#34;/Upload/{fileName}\u0026#34; }; } } 这里，我们用一个先入先出的队列来存储由客户端发送的二进制“块”，并根据队列中的首个元素来生成服务器端文件的远程路径，因为这里是单个文件的上传，对于同一批次的二进制流而言，它们对应的是同一个文件。当服务器端接收完该文件后，会返回一个远程路径，客户端可以通过这个远程路径来下载文件。如下图所示，一张 6M 左右的图片，被分成 6 个“流”发送到了服务器端：\n利用 gRPC 实现文件上传\r实现下载 下面，我们再来看看下载的实现。其实，服务器端下载的实现，与客户端上传完全一致。为什么这样说呢？因为它们都是从本地读取文件，然后将其以流的形式发送给对方，相当于两者身份的一次“互换”。所以，这次我们先从服务器端开始。在此之前，我们上传完文件以后，会返回一个远程路径。现在，客户端通过这个远程路径来下载对应的文件：\npublic override async Task DownloadFile(DownloadFileRequest request, IServerStreamWriter\u0026lt;DownloadFileResponse\u0026gt; responseStream, ServerCallContext context) { var filePath = Path.Combine(Directory.GetCurrentDirectory(), request.FilePath); if (File.Exists(filePath)) { using (var fileStream = File.OpenRead(filePath)) { var received = 0L; var totalLength = fileStream.Length; var buffer = new byte[1024 * 1024]; // 每次最多发送 1M 的文件内容 while (received \u0026lt; totalLength) { var length = await fileStream.ReadAsync(buffer); received += length; var response = new DownloadFileResponse() { Content = ByteString.CopyFrom(buffer), TotalSize = totalLength }; await responseStream.WriteAsync(response); } } } } 同样地，客户端在收到服务器端发送的二进制“块”以后，“照葫芦画瓢”，按照顺序写入即可：\nvar downloadRequest = new DownloadFileRequest() { FilePath = \u0026#34;/Upload/424017fc-0b58-4cad-9264-75efe3701444.png\u0026#34; }; var downloadResult = fileServiceClient.DownloadFile(downloadRequest); var downloadPath = Path.Combine(Directory.GetCurrentDirectory(), downloadRequest.FilePath); if (File.Exists(downloadPath)) File.Delete(downloadPath); using(var fileStream = File.Open(downloadPath, FileMode.Append, FileAccess.Write)) { var received = 0L; while (await downloadResult.ResponseStream.MoveNext(CancellationToken.None)) { var current = downloadResult.ResponseStream.Current; var buffer = current.Content.ToByteArray(); fileStream.Seek(received, SeekOrigin.Begin); await fileStream.WriteAsync(buffer); received += buffer.Length; received = Math.Min(received, current.TotalSize); } } 可以注意到，在缓冲区大小相同的情况下，同一张图片会再次被分成 6 个“块”下载下来：\n利用 gRPC 实现文件下载\r现实挺骨感 有时候，我感觉自己写东西越来越“肤浅”，因为那种特别有深度的“奇技淫巧”，不单单意味着“可遇不可得”，更意味着你需要投入更多的时间和精力。回到一开始的话题，我们利用 gRPC 实现了文件的上传和下载，确实没什么难度，但我们可曾收到过特别令人兴奋的反馈？仔细思索一番，好像确实是这样，因为我并不觉得这个过程中有什么成就感。问题出在哪里了呢？我想，大概是我们并没有解决一开始的问题，无论上传还是下载，它都有一个十分具体的使用场景，那就是 Web 浏览器。遗憾的是，虽然我们写流式传输写得非常嗨，可这一切并不能直接作用于 Web 浏览器环境，你说，还有什么比这个更尴尬的吗？\ngRPC-Web \u0026#43; Envoy Porxy 的理想很丰满\r通常，在面对这个问题的时候，你可以考虑 Envoy 和 gRPC-Web 这样两个方向。其中，对于 Envoy 而言，它提供一部分过滤器或者说插件来提供相关的支持：\ngRPC-JSON transcoder filter：它可以让我们通过 JSON API 来消费 gRPC 接口，并且适用于大多数的 Unary gRPC API，那么，它是否适用于 Streaming gRPC API 呢？官方说它支持客户端和服务器端的 Streaming，不支持双向的 Streaming，目测是 Envoy 对请求和响应进行了缓存，所以，实时性非常差，并且由于客户端采用了 HTTP/1.1，并不是真正的 Streaming。\ngRPC HTTP/1.1 bridge：一种通过 HTTP/1.1 来调用 gRPC (HTTP/2) 的机制，采用和 gRPC 一样的数据编码方式，即 Protobuf，缺点是只支持 Unary gRPC API，Streaming 就更不用说啦！\ngRPC-Web filter：可以认为是 Envoy 针对 gRPC-Web 协议的一种实现，可是 gRPC-Web 本身牺牲掉了一些东西，比如客户端的 Streaming，以及双向的 Streaming，至于 服务器端的 Streaming， 则需要开启 application/grpc-web-text 模式，存在一定限制。\n综合以上信息，我们可以得出一个结论，即，至少在现阶段，基于 gRPC 实现的上传和下载，是无法直接在浏览器环境下使用的。当然，我们还有变通的方案，那就是用传统的 Web API 再包装一层，虽然这样没什么意思，可你会发现，这样比直接用 HTTP 实现分片上传/下载要简单许多，某种意义上，可以算作一种心理安慰，下面是一个简单的实现：\n// POST api/files/upload [HttpPost(\u0026#34;Upload\u0026#34;)] public async Task\u0026lt;ActionResult\u0026gt; Upload() { var form = Request.Form; if (form.Files.Count == 0) return BadRequest(); var uploadResult = _fileServiceClient.UploadFile(); var fileToUpload = form.Files[0]; using (var fileStream = fileToUpload.OpenReadStream()) { var sended = 0L; var totalLength = fileStream.Length; var buffer = new byte[1024 * 1024]; while (sended \u0026lt; totalLength) { var length = await fileStream.ReadAsync(buffer); sended += length; var request = new UploadFileRequest() { Content = ByteString.CopyFrom(buffer), FileName = fileToUpload.FileName }; await uploadResult.RequestStream.WriteAsync(request); } } await uploadResult.RequestStream.CompleteAsync(); var reply = await uploadResult.ResponseAsync; return Ok(reply.FilePath); } // GET api/files/download [HttpGet(\u0026#34;Download\u0026#34;)] public async Task\u0026lt;ActionResult\u0026gt; Download(string filePath) { var downloadRequest = new DownloadFileRequest() { FilePath = filePath }; var downloadResult = _fileServiceClient.DownloadFile(downloadRequest); using (var fileStream = new MemoryStream()) { var received = 0L; while (await downloadResult.ResponseStream.MoveNext(CancellationToken.None)) { var current = downloadResult.ResponseStream.Current; var buffer = current.Content.ToByteArray(); fileStream.Seek(received, SeekOrigin.Begin); await fileStream.WriteAsync(buffer); received += buffer.Length; received = Math.Min(received, current.TotalSize); } if (received \u0026gt; 0) return File(fileStream, \u0026#34;application/octet-stream\u0026#34;, Path.GetFileName(filePath)); else throw new FileNotFoundException(filePath); } } } 本文小结 当我把这个结果告诉某人的时候，某人一脸嫌弃说，那就是无解。的确，对于无解的事情，我们只能学着去接受，学着去和自己和解，这是我在接受一个普通人的命运以后，时不时会在心里默念的一句话。这篇文章尝试了用 gRPC 来实现文件的上传与下载，而最终令我们感到无力的一件事情，则是 gRPC Streaming API 在浏览器环境下的支持不完整这件事情，HTTP/1.1 与 HTTP/2 的爱恨情仇，也许会在将来的某一天彻底终结，但对屏幕前的你我而言，你唯一能把握的永远只有当下，我是一个在游走在理性和感性之间的人，感性过多，让我的技术博客不再那么纯粹；理性过多，让我对事物间隐藏的美视而不见。我准备用一生去调和这两种人格，因为，作为一个双子座，我不得不去对抗时间、失去和错过的无解。\n","date":"2022-03-20T09:34:36Z","image":"https://grpc.io/img/landing-2.svg","permalink":"https://qinyuanpei.github.io/posts/use-grpc-to-realize-file-upload-and-download/","slug":"Use-gRPC-to-realize-file-upload-and-download","tags":["gRPC","Streaming","上传","下载"],"title":"利用 gRPC 实现文件的上传与下载"},{"categories":["编程语言"],"content":"“这是最好的时代，这是最坏的时代”，英国作家查尔斯·狄更斯在两百多年前写下的这句话，如果从辩证的角度来看，它或许可以适用于任何一个时代。我们生活在一个怎样的时代呢？我想，或许是一个矛盾的时代。因为，有时它让你对未来有无限的期待，有时它又会让你陷入无尽的绝望，特别是当集体和个人的命运形成强烈反差的时候，当实用主义、精致利己主义开始盛行的时候，我们偶尔会感慨罗曼蒂克的消亡、怀念从前慢、追忆芳华，可下一秒就被卷入到同时间赛跑的庸庸碌碌当中。生活节奏越来越快，人们越来越追求实时、速度、效率，选择当下的同时，意味着选择实时满足，譬如，我想吃一块美味的蛋糕，我现在就要吃。与之相对的，则被称之延迟满足，譬如，制定一个长期的写作计划以实现个人知识网络的构建。由此可见，人生本来就有快有慢、有张有弛，此时，便引入了这篇文章的主题——延迟队列。\n什么是延迟队列 延迟队列，即 DelayQueue，所以，顾名思义，首先，它是一个队列，对于队列这种数据结构，相信大家都不陌生啦！这是一种先入先出(FIFO)的数据结构，就像现实生活中排队讲究先来后到一样，普通队列中的元素都是有序的。相比普通队列，延迟队列主要多了一个延迟的属性，此时，元素何时出队不再取决于入队顺序，而是入队时指定的延迟时间，它表示该元素希望在经过该指定时间后被处理。从某种意义上来讲，延迟队列更像是一种以时间作为权重的集合。我想，单纯地介绍概念，不一定能真正深入人心，所以，请允许我举几个生活中的例子：当你在网上购物的时候，如果下单后一段时间内没有完成付款，那这个订单就会被自动取消；当你通过 Outlook 预约了会议以后，Outlook 会在会议开始前 15 分钟提醒所有与会人员；当你在网上叫外卖以后，平台会在订单即将超时前 10 分钟通知外卖小哥\u0026hellip;这样看起来，是不是顿时觉得延迟队列的使用场景还是挺广泛的呢？因为工作上的关系，博主接触类似场景的机会还是蛮多的，所以，想系统地研究下相关的技术，最终，就有了今天这篇博客，下面我们来看看具体的实现方式有哪些。\n延迟队列的实现方式 延迟队列思维导图\r我知道，在一个短视频横行的时代，人们的注意力注定要被那些实时满足的事物消耗掉，在我有预感到，不会有多少人愿意在我这篇自以为是的文字前驻留的时候，我唯有识趣地放出这个思维导图，TLDR的这种心理，其实我完全可以感同身受，因为看一部电影永远比看一本书容易，当媒介从文字变成图片再到视频，本质上是我们获取信息的能力下降了，我们变得只能接受低密度的信息。当然，这是一个时代的症结，你可以拥有你的选择，是独善其身还是随波逐流？\n数据结构 JDK 中提供了一个延迟队列的实现 DelayQueue，位于 Java.util.concurrent 这个包下面，它是一个 BlockingQueue，本质上封装了一个 PriorityQueue，队列中的元素只有到达了Delay时间，才允许从队列中取出。如下图所示，队列中放入三个订单，分别设置订单在当前时间的第 5、10、15 秒后取消：\n延迟队列示意图\r对于 Java 中的 DelayQueue 而言，其对应的代码实现如下面所示：\nOrder Order1 = new Order(\u0026#34;Order1\u0026#34;, 5, TimeUnit.SECONDS); Order Order2 = new Order(\u0026#34;Order2\u0026#34;, 10, TimeUnit.SECONDS); Order Order3 = new Order(\u0026#34;Order3\u0026#34;, 15, TimeUnit.SECONDS); DelayQueue\u0026lt;Order\u0026gt; delayQueue = new DelayQueue\u0026lt;\u0026gt;(); delayQueue.put(Order1); delayQueue.put(Order2); delayQueue.put(Order3); System.out.println(\u0026#34;订单延迟队列开始时间:\u0026#34; + LocalDateTime.now().format(DateTimeFormatter.ofPattern(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;))); while (delayQueue.size() != 0) { Order task = delayQueue.poll(); if (task != null) { System.out.format(\u0026#34;订单:{%s}被取消, 取消时间:{%s}\\n\u0026#34;, task.name, LocalDateTime.now().format(DateTimeFormatter.ofPattern(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;))); } Thread.sleep(1000); } 其中，Order 类要求实现 Delayed 接口，可以注意到这个 compareTo() 方法和 .NET 里的 IComparable 完全一样 :)\npublic class Order implements Delayed { @JsonFormat(locale = \u0026#34;zh\u0026#34;, timezone = \u0026#34;GMT+8\u0026#34;, pattern = \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;) private long time; String name; public Order(String name, long time, TimeUnit unit) { this.name = name; this.time = System.currentTimeMillis() + (time \u0026gt; 0 ? unit.toMillis(time) : 0); } @Override public long getDelay(TimeUnit unit) { return time - System.currentTimeMillis(); } @Override public int compareTo(Delayed o) { Order Order = (Order) o; long diff = this.time - Order.time; if (diff \u0026lt;= 0) { return -1; } else { return 1; } } } 此时，我们可以得到下面的结果，三个订单分别在第 5、10、15 秒后被执行，这样就实现了一个最简单的延时队列。我不会告诉你，为了得到这个演示结果，我特意搭建了一个 Java 环境：\nJava 中的 DelayQueue 效果演示\r.NET 中一直没有提供类似的实现，直到 .NET 6.0 中新增了 PriorityQueue 这个数据结构，它允许我们为队列中的元素定义一个优先级，此时，我们可以用下面的方法实现上面的功能：\nvar utcNow = DateTime.UtcNow; var queue = new PriorityQueue\u0026lt;FooBar, long\u0026gt;(); queue.Enqueue(new FooBar() { Foo = \u0026#34;001\u0026#34;, Bar = \u0026#34;100\u0026#34; }, new DateTimeOffset(utcNow.AddSeconds(5)).ToUnixTimeSeconds()); queue.Enqueue(new FooBar() { Foo = \u0026#34;002\u0026#34;, Bar = \u0026#34;200\u0026#34; }, new DateTimeOffset(utcNow.AddSeconds(10)).ToUnixTimeSeconds()); queue.Enqueue(new FooBar() { Foo = \u0026#34;003\u0026#34;, Bar = \u0026#34;300\u0026#34; }, new DateTimeOffset(utcNow.AddSeconds(15)).ToUnixTimeSeconds()); while (queue.Count \u0026gt; 0) { var current = new DateTimeOffset(DateTime.UtcNow).ToUnixTimeSeconds(); var flag = queue.TryPeek(out var item, out var timestamp); if (!flag || current \u0026lt; timestamp){ continue; } else { item = queue.Dequeue(); _logger.LogInformation($\u0026#34;{DateTimeOffset.UtcNow}:Hello DelayQueue, {item.Foo}, {item.Bar}.\u0026#34;); } } 基本思路是，每次生成一个时间戳作为队列元素的“权重”，然后用当前时间和这个时间戳进行比较，如果时间到了，则从队列中出队，否则继续轮询：\n.NET 中的 PriorityQueue 效果演示\r可以注意到，它可以按照我们预期的时间和顺序，从队列中取出相应的元素，考虑到这个方法里使用了轮询，做法着实算不上优秀，不过对于我们理解 DelayQueue 非常有帮助，属于一种最基础的的实现。\n定时任务 接下来，我们来说第二种实现方式，定时任务，这种方式就非常的朴实无华啦，因为对于一个延迟执行的任务而言，其本质就是一个定点执行、执行一次的定时任务啦，所以，理论上普通的 Timer 一样可以做这件事情。不过，考虑到任务的持久化、分布式等等的问题，我们还是建议使用相对成熟的定时任务框架，例如 Quartz.NET、Hangfire 等等来实现。这里博主以 Quartz.NET 为例：\npublic async Task PutJob\u0026lt;T\u0026gt;(TimeSpan delay, T jobData, Action\u0026lt;T\u0026gt; callback) { var jobDetail = JobBuilder.Create\u0026lt;DelayJob\u0026lt;T\u0026gt;\u0026gt;() .WithIdentity(Guid.NewGuid().ToString(\u0026#34;N\u0026#34;), JobGroup) .UsingJobData(JobParameters, JsonConvert.SerializeObject(jobData)) .Build(); jobDetail.JobDataMap[JobDelegate] = callback; var trigger = TriggerBuilder.Create() .WithIdentity($\u0026#34;{jobDetail.Key.Name}Trigger\u0026#34;, JobGroup) .ForJob(jobDetail.Key) .StartAt(DateTimeOffset.UtcNow.Add(delay)) .WithSimpleSchedule(x =\u0026gt; x .WithRepeatCount(0) .WithIntervalInSeconds(0) ) .Build(); await _scheduler.ScheduleJob(jobDetail, trigger); } 对于 Quartz 而言，核心的对象只有三个：Job、Trigger 和 Schedulerb，通过这三个对象，我们就可以创建一个定时任务，其中， DelayJob\u0026lt;T\u0026gt; 是表示一个带参数的任务，它实现了 IJob 接口，可以在任务执行时触发对应的委托：\ninternal class DelayJob\u0026lt;T\u0026gt; : IJob { public Task Execute(IJobExecutionContext context) { var jobDetail = context.JobDetail; var callback = jobDetail.JobDataMap[QuartzDelayQueue.JobDelegate] as Action\u0026lt;T\u0026gt;; var jobData = context.MergedJobDataMap[QuartzDelayQueue.JobParameters]?.ToString(); var jobParam = JsonConvert.DeserializeObject\u0026lt;T\u0026gt;(jobData); callback?.Invoke(jobParam); return Task.CompletedTask; } } 使用时非常简单，只要给一个延迟时间和回调函数即可：\nawait _delayQueue.PutJob( TimeSpan.FromSeconds(10), new FooBar() { Foo = \u0026#34;Foo\u0026#34;, Bar = \u0026#34;Bar\u0026#34; }, x =\u0026gt; _logger.LogInformation($\u0026#34;{DateTimeOffset.UtcNow}:Hello DelayQueue, {x.Foo}, {x.Bar}.\u0026#34;) ); 基于 Quartz 实现延时任务\r可以注意到，Quartz 在指定时间成功触发了回调函数，这样就达到了延时执行的目的。\nRedis 接下来，分享两种基于 Redis 实现延迟队列的做法，分别基于 Redis 的 Key 过期机制 和 Redis 的 ZSet 结构，前者依赖 Redis 提供的发布-订阅机制，后者则是利用 ZSet 里每个成员的 score 属性实现排序。\n基于 Redis 的 Key 过期机制 这个做法主要是利用 Redis 中的 Key 过期机制，简单来讲，就是利用 Redis 中的发布/订阅功能，如果我们开启了 Redis 的 Key 过期事件监听，那么，当某个 Key 过期的时候，Redis 就会把这条消息发布出来，通过订阅这个事件，从而达到延迟队列的效果。首先，确保 Redis 开启了 Key 过期事件监听，修改 Redis 的配置文件 redis.conf 如下：\nnotify-keyspace-events Ex 在这种情况下，如果我们为某一个 Key 指定了过期时间，那么，当到达这个过期时间以后，Redis 会向名为 __keyevent@0__:expired 的频道中推送一条消息，消息的内容为过期的这个 Key，其中 @0 表示默认的 Redis 库，这里以 CSRedis 这个库为例来进行演示：\npublic Task PutJob\u0026lt;T\u0026gt;(TimeSpan delay, T jobData, Action\u0026lt;T\u0026gt; callback) { var guid = Guid.NewGuid().ToString(\u0026#34;N\u0026#34;); // Default Database // EXPIRED_KEYS_CHANNEL = \u0026#34;__keyevent@{0}__:expired\u0026#34;; var channel = string.Format(EXPIRED_KEYS_CHANNEL, 0); _redisClient.Set(guid, jobData, delay); _redisClient.Subscribe((channel, new Action\u0026lt;CSRedisClient.SubscribeMessageEventArgs\u0026gt;(msg =\u0026gt; { if (msg.Body != guid) return; callback?.Invoke(jobData); }))); _logger.LogInformation($\u0026#34;{DateTimeOffset.UtcNow}:Put a new delay job.\u0026#34;); return Task.CompletedTask; } 代码非常好理解，写入 Key 的时候设置一个过期时间，然后订阅 Key 过期的事件，因为 Key 过期事件的内容就是对应的 Key，所以，需要做一次判断避免重复触发。此时，我们可以得到下面的结果：\n基于 Redis 的 Key 过期机制实现延迟队列\r可以注意到，该任务在第 29 秒时创建，经过 5 秒后，因为 Key 过期而触发回调函数。需要说明的是，Redis 里的发布/订阅是不保证可靠性的，针对所有试图通过 Redis 实现消息队列的想法，我只想说，如果数据量不大，并且不需要可靠性保证的话，可以凑活着用一用，否则，还是建议使用专业的消息队列。\n基于 Redis 的 ZSet 结构 接下来，我想介绍的是 Redis 中的 ZSet，即有序集合。其实，从一开始的 DelayQueue 大家就能注意到一件事情，那就是这个延迟队列最重要的是，要给一个“权重”来实现排序。所以，在 .NET 6.0 没有发布以前，人们为了实现类似 DelayQueue 的数据结构，通常只能通过 SortedList 这个类型来实现，感兴趣的朋友不妨参考这个项目：DelayQueue，这里面最大的难点是什么呢？SortedList是一个线程不安全的集合，需要考虑锁的问题，这说明什么呢？这说明模拟 DelayQueue 的关键是找到这样一个有序集合，显然 ZSet 刚好就是这样一个类型，它里面有一个 score 属性，我们只需要把延迟时间放到这个属性上即可。\npublic class ZSetDelayQueue\u0026lt;T\u0026gt; where T : class { private readonly CSRedisClient _redisClient; private const string QueueName = \u0026#34;DelayQueue\u0026#34;; public ZSetDelayQueue(CSRedisClient redisClient) { _redisClient = redisClient; } public Task Enqueue(T item, TimeSpan delay) { var score = new DateTimeOffset(DateTime.UtcNow.Add(delay)).ToUnixTimeSeconds(); _redisClient.ZAdd(QueueName, (score, JsonConvert.SerializeObject(item))); return Task.CompletedTask; } public async Task\u0026lt;T\u0026gt; Dequeue() { var score = new DateTimeOffset(DateTime.UtcNow).ToUnixTimeSeconds(); ; var records = _redisClient.ZRangeByScore(QueueName, 0, score, 1); if (records.Count() \u0026gt; 0) { var item = JsonConvert.DeserializeObject\u0026lt;T\u0026gt;(records[0]); await _redisClient.ZRemAsync(QueueName, item); return item; } return null; } public bool IsEmpty() { var count = _redisClient.ZCount(QueueName, 0, decimal.MaxValue); return count == 0; } } 好了，现在一切都顺利成章了，元素入队的时候计算出对应的时间戳，这个时间戳就是 ZSet 里的 score 属性，调用ZAdd() 即可；同理，元素出队，则是利用 ZRangeByScore() 返回从 0 到 当前时间戳内的一个元素，显然，如果当前时间戳大于或者等于该元素的时间戳，表示这个元素设定的延迟时间已经到了，此时，我们需要调用ZRem() 命令将其从集合中移除，和 Java 里面的 DelayQueue 类似，Redis 会按照 score 属性由小到大排序，这样时间早的会被先取出来，时间晚的会被后取出来，不得不说，这一切堪称完美，接下来就非常简单啦！\nvar redisZSetDelayQueue = _serviceProvider.GetService\u0026lt;ZSetDelayQueue\u0026lt;FooBar\u0026gt;\u0026gt;(); await redisZSetDelayQueue.Enqueue(new FooBar() { Foo = \u0026#34;001\u0026#34;, Bar = \u0026#34;100\u0026#34; }, TimeSpan.FromMinutes(1)); await redisZSetDelayQueue.Enqueue(new FooBar() { Foo = \u0026#34;002\u0026#34;, Bar = \u0026#34;200\u0026#34; }, TimeSpan.FromMinutes(2)); await redisZSetDelayQueue.Enqueue(new FooBar() { Foo = \u0026#34;003\u0026#34;, Bar = \u0026#34;300\u0026#34; }, TimeSpan.FromMinutes(3)); while (!redisZSetDelayQueue.IsEmpty()) { var item = await redisZSetDelayQueue.Dequeue(); if (item == null) { continue; } else { _logger.LogInformation($\u0026#34;{DateTimeOffset.UtcNow}:Hello DelayQueue, {item.Foo}, {item.Bar}.\u0026#34;); } } 这个就和一开始的例子非常接近了，对吧？ 效果如何呢，我们一起来看看：\n基于 Redis 的 ZSet 类型实现延迟队列\r可以看到，三个任务分别在 1 分钟、2 分钟 和 3 分钟后执行，这个延迟队列，个人表示还行，哈哈！事实上，基于 Redis 的延迟队列，业界的方案还是蛮多的，个人比较推荐 有赞 技术团队的方案，感兴趣的朋友可以在本文的基础上做进一步的探究，我个人关注这个话题，是因为我不太喜欢定时任务轮询的做法，虽然这是一种万金油式的做法，我个人更喜欢下面的做法。\n消息队列 OK，提到消息队列的话，参照面试八股文，我们会说，消息队列最主要的作用是削峰平谷，换句话说，消息队列可以将短时间内堆积的大量的请求任务“削峰”，然后“平摊”到平时请求任务较少的时段，所以，好像平时一提起 RabbitMQ 或者 Kafka 这样的东西，大家脑海中浮现出来的就是高并发、高吞吐、高性能这种类似糖尿病“三多一少”的存在，回顾我们一开始从生活中得到的启示，有没有一种可能，我们使用消息队列，并不单单是为了让这条消息被快速地消费，而是可以“让子弹飞一会儿”呢？我想，一切皆有可能。下面，我们以 RabbitMQ 为例，来展示如何实现一个延迟队列：\nRabbitMQ 死信队列工作流程示意图\r如图所示，假设消息发送方把消息投递到延迟交换机 default.delay.exchange，该交换机绑定了延迟队列 default.delay.queue，显然，正常情况下，消息会出现在这个延迟队列中。接下来，为了让死信机制生效，我们必须对这个延迟队列做一点设置，这里主要有三个参数，x-message-ttl 表示队列中消息的存活时间，x-dead-letter-exchange 表示消息过期以后再次投递时的死信交换器，x-dead-letter-routing-key 表示消息过期以后再次投递时的路由键名。通常情况下，在 RabbitMQ 中消息进入死信队列的前提有三种，即消息过期、队列已满和消息被拒绝。其中，x-max-length 和 x-max-length-bytes 这两个属性，可以分别用来指队列中的最大消息数、最大字节数、而消息被拒绝，则是指主动调用BasicReject() 方法，针对这两种情况触发的死信，我们这里可以不用太关心，因为我们显然考虑的是因为消息过期而触发的死信。OK，讲完了理论，我们来看看代码层面具体是如何实现的吧！\nusing (var channel = _connection.CreateModel()) { // 普通/延迟交换机 default.delay.exchnage var exchangeNormal = \u0026#34;default.delay.exchnage\u0026#34;; channel.ExchangeDeclare(exchangeNormal, \u0026#34;direct\u0026#34;, true, false, null); // 普通/延迟队列 var queueNormal = \u0026#34;default.delay.queue\u0026#34;; var arguments = new Dictionary\u0026lt;string, object\u0026gt; { [\u0026#34;x-message-ttl\u0026#34;] = 5000, [\u0026#34;x-dead-letter-exchange\u0026#34;] = \u0026#34;default.deadletter.exchange\u0026#34;, [\u0026#34;x-dead-letter-routing-key\u0026#34;] = \u0026#34;dead.routingKey\u0026#34; }; channel.QueueDeclare(queue: queueNormal, true, false, false, arguments: arguments); // 绑定交换器 channel.QueueBind(queueNormal, exchangeNormal, \u0026#34;normal.routingKey\u0026#34;); // 发送消息 var body = Encoding.UTF8.GetBytes(JsonConvert.SerializeObject(jobData)); var properties = channel.CreateBasicProperties(); properties.DeliveryMode = 2; channel.BasicPublish(exchange: exchangeNormal, routingKey: \u0026#34;normal.routingKey\u0026#34;, mandatory: true, basicProperties: properties, body: body); } 简单来说，某一个队列如果需要死信队列，那么你就需要为其设置x-message-ttl、x-dead-letter-exchange、x-dead-letter-routing-key 这三个属性即可，你完全不用关心消息是如何投递到这个死信队列中，而对于消息的消费者来说，它只需要从这个死信队列中接收消息即可，因为能被投递到死信队列里的消息，一定是因为消息时间到了或者说过期了，这样就等于间接实现了延迟队列：\n// 死信交换机 default.deadletter.exchange var exchangeDead = \u0026#34;default.deadletter.exchange\u0026#34;; _consumerChannel.ExchangeDeclare(exchangeDead, \u0026#34;direct\u0026#34;, true, false, null); // 死信队列 default.deadletter.queue var queueDead = \u0026#34;default.deadletter.queue\u0026#34;; _consumerChannel.QueueDeclare(queue: queueDead, true, false, false, null); // 绑定交换器 _consumerChannel.QueueBind(queueDead, exchangeDead, \u0026#34;dead.routingKey\u0026#34;); // 消费消息 _basicConsumer = new EventingBasicConsumer(_consumerChannel); _consumerChannel.BasicConsume(queue: queueDead, autoAck: false, consumer: _basicConsumer); _basicConsumer.Received += (s, e) =\u0026gt; { var body = Encoding.UTF8.GetString(e.Body.ToArray()); // TODO: _consumerChannel.BasicAck(e.DeliveryTag, false); }; 如下图所示，消息首先会被发送到延迟队列 default.delay.queue 中，此时，消息还没有过期，不会触发死信机制，注意到，这时候队列中会有 4 条消息：\nRabbitMQ 死信队列工作流程-01\r一段时间后，消息过期，触发死信机制。此时，消息会被在再次转发到死信交换机 default.deadletter.exchange 中，并最终达到死信队列 default.deadletter.queue：\nRabbitMQ 死信队列工作流程-02\r至此，我们就利用 RabbitMQ 里的 TTL + DLX 特性实现了一个延迟队列，达到了延迟执行的目的。不过，只要你使用消息队列，就一定会遇到消息堆积的问题，而一旦发生消息堆积，延迟执行的这个时间可能就会不准，如果你特别看重这个时间准确与否，那么，实际运作中还有一部分工作完要做。我们目前用定时任务轮训的做法，最大的问题是它产生大量重复且无用的请求，每天单单是相关日志就上百兆，这就算是我下班以后的一点探索，我现在依然觉得，那个定时任务的 API 设计得莫名其妙。\nRabbitMQ 死信队列工作流程-03\r最后，我们来说说 Kafka，虽然 Kafka 单机的 QPS 要远远超过 RabbitMQ 1 到 2 个数量级，但这种快是以牺牲一部分功能作为代价的，像典型的重试和死信，这两样儿都需要使用者自己去实现，比如死信，我们现在是为每个 topic 创建一个对应的死信的 topic 来实现的，比如，我们有一个 topic 叫做 orderInfo，与之相对应地，我们会同时创建一个叫做 orderInfo_DLQ 的 topic，作为它的死信队列。当然，你还需要一个机制去收集和转发过期消息，基本上你还是需要一个 Timer 去做某种轮询，也许，是因为它选择了 Kafka，所以，需要一个定时任务系统来作为补充，毕竟，技术选型这种问题，注定是要政治正确的啦！\n本文小结 《七种武器》是著名武侠小说家古龙先生的代表作之一，原本指长生剑、孔雀翎、碧玉刀、多情环、霸王枪、离别钩等七种精妙绝伦的武器，这里则是用来指实现延迟队列的各种方法，延迟队列适用于那些需要延迟执行的场合，在如今这样一个追求实时性、快节奏生活的时代，人们对快乐和满足的要求有实时和延时的区别，用罗翔老师的话来讲，即时快乐是一种低级的快乐，是一种短暂的、易得的快乐，从这个角度来看，延时满足则是一种需要培养和付出的高级快乐。此中优劣，我们不必去分个泾渭分明，就像这些不同的实现方式，更多的只是场景上的差异，而非功能上的差异，延迟队列可以认为是一种以时间作为权重的、有序的集合；Java 里的 DelayQueue，.NET 里的 PriorityQueue，可以实现进程内的、单机版的延迟队列；而像 Quartz、Hangfire 这类任务调度系统，则可以更精确地控制时间；通过 Redis 里的发布-订阅、ZSet，我们让 DelayQueue 离分布式稍微接近了一点；而 RabbitMQ 里的 TTL + DLX 特性，则让博主比两年前更加理解死信队列……这难道不是一种延时满足吗？你以前不明白的概念，有一天突然有了新的认识，我想，这就是整个过程的意义所在。当然，时间轮算法对我来说还有一点点难，我能留到未来的某一天争取搞懂它吗？好了，以上就是这篇博客的全部内容啦，祝各位晚安，谢谢大家。\n","date":"2022-03-07T09:34:36Z","image":"/posts/summary-of-the-principle-and-implementation-of-delay-queue/DelayQueue-XMind.png","permalink":"https://qinyuanpei.github.io/posts/summary-of-the-principle-and-implementation-of-delay-queue/","slug":"Summary-Of-The-Principle-And-Implementation-Of-Delay-Queue","tags":["DelayQueue","Quartz","Redis","RabbitMQ"],"title":"七种武器：延迟队列的原理和实现总结"},{"categories":["编程语言"],"content":"最近一直在研究 gRPC 的 ServerReflection，顾名思义，这是 gRPC 里提供的反射接口，当你需要获取某个接口的描述信息，或者是希望动态调用 gRPC 的时候，这一切就会变得非常有用，如果你经常使用 gRPC UI 这款工具来调试 gRPC 接口，那么，你一定会注意到一件事情，即它要求服务端必须支持 ServerReflection API，而这一点在 ASP.NET Core 中已经得到支持，对此感兴趣的朋友可以参考官方文档。当然，这并不是我想表达的重点(我就知道)。重点是什么呢？在使用 ServerReflection API 的过程中，我发现它采用了 gRPC 双向流的方式来进行交互，在过去的日子里，我研究过诸如 WebSocket、Server-Sent Events 等等服务器推送的技术，我意识到这是一个非常接近的技术，所以，今天这篇文章，我们来一起聊聊 gRPC 中的流式传输。\n从 HTTP/2 说起 首先，我想说，流式传输并不是一个新的概念，这一切就好像，即使你从来没有听过流媒体的概念，可这并不妨碍你追剧、刷短视频，隐隐然有种“不识庐山真面目，只缘身在此山中”的感觉。随着网络带宽和硬件水平的不断提升，越来越多的云服务变得像水、电、天然气一样寻常，以此作喻，流式传输，就像你打开水龙头，此时，水就会源源不断地流出来，并且可以做到随用随取。因此，流式传输实际上就是指通过网络传输媒体，例如音频、视频等的技术统称，服务器可以连续地、实时地向客户端发送数据，而客户端不必等所有数据发送完就可以访问这些数据。按照实现方式的不同，流式传输可以分为 实时流式传输 和 顺序流式传输 两种，前者通常指RTP/RTCP，典型的场景是直播；后者通常是指由 Nginx、Apache 等提供支持的顺序下载。\nHTTP/1.1 vs HTTP/2\r如果你对 HTTP/2 有一定了解的话，就会知道它最为人所知的特性是多路复用。在 HTTP/1.1 的时代，同一个时刻只能对一个请求进行处理或者响应，换句话说，下一个请求必须要等当前请求处理完才能继续进行，与此同时，浏览器为了更快地加载页面资源，对同一个域名下的请求并发数进行了限制，所以，你会注意到一个有趣的现象，部分网站会使用多个 CDN 加速的域名，而这正是为了规避浏览器的这一限制，HTTP/1.1 时代，可以称为“半双工模式”。到了 HTTP/2 的时代，多路复用的特性让一次同时处理多个请求成为了现实，并且同一个 TCP 通道中的请求不分先后、不会阻塞，是真正的“全双工通信”。一个和本文更贴近的概念是流，HTTP/2 中引入了流(Stream) 和 帧(Frame) 的概念，当 TCP 通道建立以后，后续的所有操作都是以流的方式发送的，而二进制帧则是组成流的最小单位，属于协议层上的流式传输。\ngRPC 中的流式传输 OK，现在我们正式开始 gRPC 流式传输的话题。首先，对于一个 gRPC 接口而言，它的起源是 Protobuf 定义。所以，一个最为直观的认识是从 Protobuf 定义入手：\n// 普通 RPC rpc SimplePing(PingRequest) returns (PingReply); // 客户端流式 RPC rpc ClientStreamPing(stream PingRequest) returns (PingReply); // 服务器端流式 RPC rpc ServerStreamPing(PingRequest) returns (stream PingReply); // 双向流式 RPC rpc BothStreamPing(stream PingRequest) returns (stream PingReply); 可以注意到，相比普通的 RPC 方法(UnaryCall)，采用流式传输的 gRPC 接口，主要是多了一个stream关键字。当该关键字修饰参数时，表示这是一个客户端流式的 gRPC 接口；当该参数修饰返回值时，表示这是一个服务器端流式的 gRPC 接口；当该关键字同时修饰参数和返回值时，表示这是一个双向流式的 gRPC 接口。作为类比，双向流式的 gRPC 接口，约等于 WebSocket，即客户端、服务器端都可以以流的形式收/发数据；服务器端流式的 gRPC 接口，约等于 Server-Sent Events，即服务器端以流的形式发数据。同理，客户端流式的 gRPC 接口，即客户端以流的的形式发数据。\n我为什么会突然对这个话题产生兴趣呢？个人以为，主要有两个原因：其一，是工作中使用流式传输的机会不多，即使遇到数据量特别大的场合，大家想到的一定是修改 gRPC 数据传输的大小，而不是采用流式传输的做法；其二，是我注意到像 Istio、Envoy、Nacos 等项目，内部都是用 gRPC 作为通信协议，当你需要实现一个控制平面的时候，你会发现那里有大量的流式 gRPC 接口等着你去实现。此前，我有一点关乎 gRPC 的想法，譬如动态地为 gRPC-JSON Transcoder 生成配置信息、利用 ServerReflection 扩展 Swagger 等等，毫无疑问，这一切都需要你去了解 gRPC 的流式传输，一组重要的 API 是 IAsyncStreamReader\u0026lt;T\u0026gt; 和 IAsyncStreamReader\u0026lt;T\u0026gt;:\n// 1、调用 ServerReflection 获取 gRPC 服务 var callResult = _serverReflectionClient.ServerReflectionInfo( deadline: deadline, cancellationToken: cancellationToken ); // 2、定义一个 Task 来解析服务端响应的流 var resolveServiceListTask = Task.Run(async () =\u0026gt; { while (await callResult.ResponseStream.MoveNext(cancellationToken)) { foreach (var service in callResult.ResponseStream.Current.ListServicesResponse.Service) { Console.WriteLine(service.Name); } } }); // 3、客户端以流的方式写入参数 var request = new ServerReflectionRequest() { ListServices = \u0026#34;\u0026#34; }; await callResult.RequestStream.WriteAsync(request); await callResult.RequestStream.CompleteAsync(); // 4、客户端以流的方式读出结果 await resolveServiceListTask; 如图所示，你大概可以理解 gRPC 流式传输的运作过程，RequestStream 实现了 IAsyncStreamWriter\u0026lt;T\u0026gt; 接口，负责流的写入，调用 CompleteAsync() 方法时表示数据已经写完；ResponseStream 实现了 IAsyncStreamReader\u0026lt;T\u0026gt; 接口，负责流的读取，典型的迭代器模式，如果你听说过 C# 8.0 里的异步流(AsyncStream)，就应该会知道，它属于可枚举类(Enumerable)异步变体，我相信，MoveNext() 和 Current 就不需要再做多余的解释啦，哈哈！\n客户端流 对于客户端流式 gRPC 接口而言，客户端负责写入流，服务器端负责读取流，所以，ClientStreamPing() 会生成下面的方法签名，这里是一个简单的实现：\npublic override async Task\u0026lt;PingReply\u0026gt; ClientStreamPing( IAsyncStreamReader\u0026lt;PingRequest\u0026gt; requestStream, ServerCallContext context) { // 从 IAsyncStreamReader\u0026lt;T\u0026gt; 中读取流并放入队列 var requestQueue = new Queue\u0026lt;string\u0026gt;(); while (await requestStream.MoveNext()) { requestQueue.Enqueue(requestStream.Current.RequestId); } // 从队列中取出数据并返回 if (requestQueue.TryDequeue(out var requestId)) { return new PingReply() { RequestId = requestId, Message = \u0026#34;OK\u0026#34; }; } return new PingReply() { RequestId = string.Empty, Message = \u0026#34;\u0026#34; }; } 此时，客户端对应实现如下，继续沿用 IAsyncStreamWriter\u0026lt;T\u0026gt; 的套路：\nvar callResult = heartBeatClient.ClientStreamPing(); await callResult.RequestStream.WriteAsync(new PingRequest() { RequestId = GetCurrentTimeStamp().ToString() }); await callResult.RequestStream.CompleteAsync(); var reply = await callResult.ResponseAsync; 服务器端流 服务器端的流式 gRPC，意味着服务端会通过 IAsyncStreamWriter\u0026lt;T\u0026gt; 以流的方式写入数据：\npublic override Task ServerStreamPing( PingRequest request, IServerStreamWriter\u0026lt;PingReply\u0026gt; responseStream, ServerCallContext context) { responseStream.WriteAsync(new PingReply() { RequestId = request.RequestId, Message = \u0026#34;OK\u0026#34; }); return Task.CompletedTask; } 此时，客户端还是像使用普通方法一样调用即可：\nvar reply = heartBeatClient.ServerStreamPing(new PingRequest() { RequestId = GetCurrentTimeStamp().ToString() }); 双向流 一旦我们熟悉了这个套路，理解双向流就再没有什么难度，客户端和服务器端都用 IAsyncStreamWriter\u0026lt;T\u0026gt; 和 IAsyncStreamWriter\u0026lt;T\u0026gt; 进行读写即可：\n// 读数据 var requestQueue = new Queue\u0026lt;string\u0026gt;(); while (await requestStream.MoveNext()) { requestQueue.Enqueue(requestStream.Current.RequestId); } // 写数据 while (requestQueue.TryDequeue(out var requestId)) { await responseStream.WriteAsync(new PingReply() { RequestId = requestId, Message = \u0026#34;OK\u0026#34; }); } 同理，客户端采用类似的做法，这里我们发 10 次心跳看看：\n// 写数据 var callResult = heartBeatClient.BothStreamPing(); for (var i = 0; i \u0026lt; 10; i++) { await callResult.RequestStream.WriteAsync(new PingRequest() { RequestId = GetCurrentTimeStamp().ToString() }); Thread.Sleep(500); } await callResult.RequestStream.CompleteAsync(); // 读数据 while (await callResult.ResponseStream.MoveNext(CancellationToken.None)) { var reply4 = callResult.ResponseStream.Current; } 此时，我们就可以得到下面的结果：\ngRPC 双向流效果演示\r个人感觉，这个可以用在那些需要做双向通信的场合，譬如心跳检测、数据看板、日志监控等等，坦白来讲，相对于 WebSocket、Server-Sent Events 等等服务器推送技术，gRPC 的双向流优势并不显著，唯一的优势可能是 HTTP/2 多路复用带来的性能上的提升。\n本文小结 最近的状态一直不太好，因为工作中的琐事消耗了大量精力。因而，这篇平淡如白开水般的线性叙事，委实不能被称之为一篇博客，而这大概就是我这段时间的真实写照。作为一名双子座，我的好奇心常常引导着我去关注那些意外的收获，就像 gRPC 里的流式传输，本质上因为我想通过 ServerReflection 为 gRPC 构建一份 Swagger 风格的 API 文档，这个工作目前还有些悬而未决的问题。当然，在这个过程中，大致搞懂了如何去动态调用一个 gRPC 接口，因为 FluentGrpc.Gateway 目前依赖 gRPC 生成客户端代码，或者说这种建立在动态链接库上的方案有一定的缺陷。类似地，工作中发现 Envoy 的 gRPC-JSON Transcoder 插件，需要手动配置每一个 gRPC 服务，开始琢磨怎么能让这个过程更智能一点，发现需要 Envoy 的 xDS API、了解控制平面，这种感觉就像在挖一口井，能不能看到水我不知道，每一铲子下去都有意外收获，这种探险的感觉非常有趣，唯一的平衡点在于，主动的发现永远都会比被动的接受花更多的时间。这篇流水账主要介绍了 gRPC 里流式传输，即客户端流、服务器端流和双向流，如果你足够有心，不妨回过头看看这篇文章 ASP.NET Core gRPC 拦截器的使用技巧分享，我相信你会有不一样的感悟，好了，这就是全部的内容啦，谢谢大家！\n","date":"2022-02-18T09:34:36Z","image":"/posts/grpc-streaming-transmission-minimalist-guide/HTTP_1.1_vs_HTTP_2.0.png","permalink":"https://qinyuanpei.github.io/posts/grpc-streaming-transmission-minimalist-guide/","slug":"GRPC-Streaming-Transmission-Minimalist-Guide","tags":["gRPC",".NET","Streaming","教程"],"title":"gRPC 流式传输极简入门指南"},{"categories":["生活感悟"],"content":"\r最近看了一部叫做《天国王朝》的电影，主要讲述了第三次十字军东征时期的一段故事：法兰克铁匠巴利安，因为受到失散多年的父亲的召唤，亦是为了替自杀而死的妻子寻求救赎，来到了三教圣城——耶路撒冷。其间，父亲亡故，巴利安承袭了爵位和封地，甚至得到了西贝拉公主的青睐，可这依然无法阻止他深陷十字军的政治漩涡。当时，身染麻风病的耶路撒冷王鲍德温四世与阿拉伯传奇英雄萨拉丁，维持着基督教与伊斯兰教之间脆弱的和平；而以居伊和雷纳德为首的好战势力，则通过袭击穆斯林的方式不断挑起争端。战争终于不可避免地爆发了，这正是历史上著名的哈丁之战。此后，狮心王理查一世独自面对东方世界的滚滚黄沙，东西方的军事、宗教和文化碰撞出火花，更是堪比双子星一般的存在。\n天国王朝中的巴利安男爵形象\r圣城耶路撒冷 熟悉刺客信条系列的朋友，此刻应该会想到，这个系列的第一部作品，正是取材于第三次十字军东征时期，主角阿泰尔则是活跃在该时期的一名叙利亚刺客。因为电影中出现了刺客的敌对势力——圣殿骑士(团)。所以，我觉得透过历史去打通电影和游戏会是一件非常有趣的事情。也许，中世纪时期发生的事情，到今天已然无法做到存伪去真，可正如我曾经迷恋过亚瑟王从石头中拔出(伊甸)圣剑的故事一样，圆桌骑士或者说骑士本身，在一个(中二)男人眼中是接近武侠小说里侠客的存在。所以，在一个武侠没落的时代，你就不难理解，我为什么会喜欢上刺客信条这样一款游戏。虽然，这些骑士相当迷信，动辄要通过决斗让上帝来裁决，可就像塞万提斯笔下的堂·吉诃德一样，当他准备大战风车的那一刻，你又会觉得他是一个英雄。\n刺客信条中阿泰尔施展信仰之跃\r故事要从哪里说起呢？我想，应该从鲍德温四世那场著名的战役——蒙吉萨之战说起。那一年，年仅 16 岁的鲍德温四世，率领三千人击败了两万人的萨拉丁军队。这段历史带给我的震撼，丝毫不亚于中国历史上的官渡之战和淝水之战。如果你对此毫无概念，不妨对比一下孙权的合肥之战，当时的孙权号称有十万人的兵力，而张辽只有八百人，史称“孙十万与张八百”。这一场战役令鲍德温四世获得了极好的威望，而雷纳德则因为参与了这场战役而成为主战派的首领。可惜，这场堪称为传奇的胜利仅仅为耶路撒冷换来了两年的和平。1779 年鲍德温四世兵败泉水谷，病情的恶化令他的身体每况愈下，到 1785 年他因麻风病去世的时候，他只有 24 岁，电影里始终以面具示人的就是鲍德温四世。后期的耶路撒冷王国，基本是由他的姐夫居伊以及雷蒙德三世把持，朝局的不稳定无疑加速了耶路撒冷王国的覆灭。\n天国王朝中的鲍德温四世形象\r相传，鲍德温四世出生时，是由其伯父鲍德温三世主持洗礼的。当时，国王鲍德温三世决定把自己的名字作为礼物赠送给这个新生儿，一旁的大臣开玩笑地说道，“作为一国之王，如果只是赠送一个名字，是否显得太过吝啬了呢？”。国王听完以后，大笑一声，指着圣(真)十字架说道，“那我就再送一份礼物给他——耶路撒冷之王”。后来，鲍德温三世突然得了重症，而他又没有子女。于是，他的侄子，即鲍德温四世继承了他的王位，成为了新一任的国王。对于历史而言，我们永远无法假设，我们无法想象这个 16 岁就击萨拉丁的少年，如果没有染上麻风病，又会在历史上留下怎么样的故事？本片的男主角巴利安，其原型在历史上被称为“伊贝林的巴利安男爵”，曾经参与过蒙吉萨之战，支持雷蒙德三世摄政，基本可以认为是主和派。电影中主张袭击穆斯林的，主要是居伊、雷纳德以及圣殿骑士团。\n十字军与圣殿骑士 好了，现在总算和刺客信条产生某种联系了。在刺客信条第一部中，圣殿骑士主要是以十字军的形象出现。事实上，十字军东征是由天主教教会发起的解放圣地耶路撒冷的一项运动，因为按照基督教、伊斯兰教、犹太教各自的说法，耶路撒冷都是它们心目中的圣地。可是，从十一世纪末开始，耶路撒冷及其周边的拜占庭地区，一直都被穆斯林占领。因此，罗马教廷以解放圣地的名义发动了多次东征，这些东征的军队服饰均以红十字作为标志，故而称为“十字军”。十字军占领耶路撒冷以后，很多欧洲人前往圣地朝圣，为了保护这些朝圣者的安全、攻击异教徒，1 名法国贵族和 8 名骑士建立了军事性质的修会，因为其地点位于所罗门神殿的遗址上，故又称为圣殿骑士团。类似的组织，还有医院骑士团、条顿骑士团。其实，在初代刺客信条中，阿泰尔的刺杀对象里就有医院骑士团的成员。\n天国王朝中的十字军形象\r从今天的角度来看，所谓的“讨伐异教徒”的圣战，其本身并不见得有多么神圣，神殿骑士团侵略和掠夺的成分更多一点，反倒是育碧用自由和秩序的命题，让圣殿骑士多了一点人性的光辉，从三代开始，圣殿骑士和刺客都不再是那种非黑即白的设定，甚至到法国大革命前夕，双方都意识到合作的可能性，可惜，这一切终究毁在各自阵营里的狂热分子手上，就像耶路撒冷王国是毁在一个毫无军事素养的居伊一样。法国刺客亚诺·多利安不无遗憾地说到，“现在我懂了，诸行并非都得到允许，而是教条本身即为一种警告”。我们继续说回电影，截止到 1187 年，通过著名的哈丁之战，萨拉丁终于夺回圣地耶路撒冷，甚至缴获了钉死过耶稣的真十字架，他凭借自己出色的军事才能，一举收复了包括阿卡在内的众多港口城市，彻底切断了十字军在海上的补给线。\n天国王朝中的萨拉丁形象\r当然，不甘心的十字军，决定以英格兰国王理查一世(狮心王)和 法兰西国王腓力二世为主力，开始积极筹备又一次的十字军东征，大战一触即发。这一年，阿泰尔22岁，此时的他尚未成为刺客大师，但已然成为组织中的骨干力量。直到1191年，26岁的阿泰尔，被导师阿尔莫林派去所罗门圣殿遗址取得金苹果时，年轻而高傲的阿泰尔准备高调刺杀圣殿骑士罗伯特，这一举动把兄弟会成员马利克等彻底暴露在危险之中。而后面的故事我们都知道了，阿泰尔被剥夺了等级和武器，去完成刺杀9个圣殿骑士的任务。历史上的罗伯特，正是在前一年跟随狮心王舰队参与第三次十字军东征，甚至在1192年受封圣殿骑士团大师，可惜当他遇到阿泰尔的时候，这条仕途注定要永远地停留在1193年9月23日那一天。而这，大概就是，袖剑之下，众生平等，万物为虚，万事皆允。\n世界的十字路口 转眼间，时间来到12世纪，狮心王和萨拉丁，西方和东方，基督教和伊斯兰教，都在这一刻走向了对立面。如果说，土耳其的伊斯坦布尔是亚洲和欧洲的十字路口。那么，毫无疑问，这将会两个当世雄主间的终极对决。1191年9月7日，狮心王理查在阿苏夫会战中击败萨拉丁，在十字军进军耶路撒冷的途中，萨拉丁不断派出穆斯林骑兵进行袭扰，直到1192年1月，十字军抵达贝特努巴城堡，此时，距离圣地耶路撒冷只剩下12英里，耶路撒冷唾手可得。历史更像是一种巧合，那一年萨拉丁的埃及援军从南部抵达战场，狮心王理查一世在英国的统治地位随时都有可能被颠覆，战争顿时陷入了焦灼状态。1192年8月，互相怀有敬意的双方，在雅法城签署停战协定，史称“雅法合约”，正是从那一刻起，声势浩大的第三次十字军东征悄然落下帷幕。\n狮心王理查一世影视形象\r萨拉丁是否归还了真十字架，后世的我们已无从得知。我只知道，从那一刻开始，这对惺惺相惜、势均力敌的对手，再没有等来交手的机会，一年后萨拉丁去世，而理查一世则在回国途中被奥地利公爵扣押。直到1194年，经过英、奥双方多次谈判，理查一世终于获释。可这个拥有狮子般雄心壮志的传奇将领，永远都选择冲锋陷阵、选择身先士卒，获释后不久，他就再次陷入了英法战争的泥潭，于1199年战死沙场。以我狭隘的历史观，后世能匹敌理查一世这次东征运动的将领，也许，只有先后在莫斯科损兵折将的拿破仑和希特勒。在育碧的世界观中，马西亚夫最终没能挡住成吉思汗西征的步伐，原因是成吉思汗手中或许掌握着某种伊甸碎片。1222年，成吉思汗攻下花拉子模汗国首都撒马尔罕，丘处机远赴西域“止杀”，那一年阿泰尔57岁，他已经离开马西亚夫，开始长达20多年的“自我流放”。\n结束隐居生活的阿泰尔\r谁能想到，丘处机路过牛家村的那一年，他的父亲奥马尔正准备潜入萨拉丁的营地，结果在回来的时候不慎触发警报，不得不杀死一名贵族。那一年，萨拉丁大军围攻马西亚夫，在萨拉丁的逼迫之下，奥马尔以生命为代价，从萨拉丁手中换回俘虏艾哈迈德·索菲安以及萨拉丁的撤军。那一年，阿泰尔11岁，几乎在同一天，阿泰尔和阿巴斯同时失去了父亲。阿巴斯不愿意接受父亲叛变的事实，并把一切都归咎于阿泰尔的父亲，两个人内心的芥蒂自此种下。这份怨恨直到阿巴斯死去都没能放下，那一年，阿泰尔80岁，利用从金苹果中学到的知识，阿泰尔研制出了袖枪，而这段故事，我们曾在启示录中，以艾吉欧·奥迪托雷的身份亲眼见证过。对于整个第三次十字军东征而言，阿泰尔的故事更像是惊鸿一瞥，可正是这段难辨真伪的传奇故事，让刺客信条系列成为此后育碧旗下最广为人知的游戏IP，万物为虚，万事皆允，耕耘于黑暗，服侍光明，这就是刺客。\n尾声 育碧说，History Is Our Playground，电影里的故事真真假假，游戏里的故事更不必说，甚至是我这些未曾考据过的文过饰非，可这恰恰是历史最为迷人的地方，一个人的形象，其实是由无数个人的记忆拼凑出来的，你可以去肆意地想象你心目中的鲍德温四世、萨拉丁，亦或者是狮心王理查一世、传奇刺客阿泰尔，只要这一切都能自圆其说。所以，育碧选择用阴谋论、用自由与秩序来填补那些虚构的情节，听起来像一款游戏对不对？可玫瑰岛这个社会实验告诉我们，这一切还真的就是一款游戏，毕竟，意大利唯一一场打赢的战争，对手是一座面积只有400平米的小岛，试想，如果西泽尔·波奇亚泉下有知，怕是连胡子都要气得歪掉？当然，请记住一句话，我说的都是错的！\n","date":"2022-02-03T18:30:45Z","image":"/posts/861688878/P2355584352.jpg","permalink":"https://qinyuanpei.github.io/posts/861688878/","slug":"861688878","tags":["历史","电影","游戏","随笔"],"title":"烟波梦影，从天国王朝到刺客信条"},{"categories":null,"content":"\r在这世界的角落-剧照\r👨‍💻 关于我 谢谢你，在这世界的角落，找到我：\n昵称：飞鸿踏雪 职业：(伪)全栈攻城狮、Yaml/Markdown工程师、刺客信条历史/物理学家、提示词魔法师 方向：数据分析、微服务、Web、AI 技术：.NET、Python、JavaScript 日常：读书(人文/历史/诗词/)、写作(技术/生活)、电影(日剧/科普/纪录片)、烹饪、洞箫、刺客信条 符号：双子座、INTP 🎨 近期工作： 对博客进行重构 微服务架构(Envoy/gRPC) 设计数据交换平台 领域驱动设计(DDD) 消息队列(Kafka) FakeRPC LLM Agent 落地 📖 近期阅读\r7.3\r在这样的雨天\r6.4\r费曼学习法（用输出倒逼输入）\r7.8\r逐利而生\r8.7\r鳗鱼的旅行\r8.6\r我们如何走到今天\r📽 近期观影\r7.3\r一个头两个大\r8.7\r叫我第一名\r6.9\r城市猎人\r6.9\r决胜21点\r8.9\r阿尔法围棋\r🎧 近期播放\rShadows Main Theme\rSteel and Shadows\rEzio\u0026#39;s Family (Shadows Version)\rRise of Yasuke\rThe Fujibayashi Legacy\r🚩 更多的页面： 读书 / 听歌 / 观影 / 友链 / 统计 🏳️‍🌈 更大的世界： 微博 / 知乎 / 豆瓣 🛠️ 更多的工具： 🏠 关于博客 元视角：我手写我心\r如同元编程之于软件工程、元学习之于机器学习。元视角，是一种高层次的、抽象的思维方式，用于观察和思考事物本身的模式和规律。因此，我希望写作成为我的一种思维方式，即：经常反思和分析问题解决过程、寻找不同问题之间的共同模式、思考问题的本质和更高层次的规律。写作固然是将个人的想法或认知传播给读者的过程，可我自始至终都是我的第一个读者。故而，博客以“我手写我心”为表，以元视角反哺认知为里，这便是其名称的由来。\n写作是对抗遗忘的一种方式\r年岁渐长，我逐渐意识到，人生中有太多东西注定会失去。当记忆越发模糊时，你唯一能做的，就是努力让自己不要忘记。因此，在时光如流水般奔涌不息的日子里，我依然希望留下一些痕迹，让未来的自己可以循迹回望。人生是由无数个过去“叠加”而成的，即使那些人或事早已面目全非，我却始终真实地存在于此刻。正如苏轼所言：“人生到处知何似，应似飞鸿踏雪泥”，为了不让那些雪泥鸿爪为时光掩埋，我选择在这里写下这些文字。无论是理性思维的技术文章，还是感性思维的生活感悟，这些或深或浅的思考，至少证明我曾真实地存在过，而这便是我坚持写博客的初心。如果这些自言自语能让你有所收获，那将是我莫大的欣慰与自豪。谢谢你，在这世界的角落，找到我。\n本博客使用 Hugo 驱动，在 Github Pages 与 Vercel 双线部署，由 Travis CI 和 Github Actions 提供持续集成服务，由 不蒜子 和 LeanCloud 提供访客统计服务，由 Waline 提供评论服务，由 jsDelivr 提供 CDN 加速服务。如需订阅本博客，可以通过 RSS 或者在 Github 上 Watch 本项目，如需联系本人，请在博客中留言(注意留下你的邮箱) 或者 发送邮件至：qinyuanpei#163.com(请自行将#替换为@)。\n","date":"2022-02-03T00:00:00Z","image":null,"permalink":"https://qinyuanpei.github.io/about/","slug":"about","tags":null,"title":"关于"},{"categories":["编程语言"],"content":"当我们的应用架构，从单体系统演变为微服务时，一个永远不可能回避的现实是，业务逻辑会被拆分到不同的服务中。因此，微服务实际就是不同服务间的互相请求和调用。更重要的是，随着容器/虚拟化技术的发展，传统的物理服务器开始淡出我们的视野，软件被大量地部署在云服务器或者虚拟资源上。在这种情况下，分布式环境中的运维和诊断变得越来越复杂。如果按照功能来划分，目前主要有 Logging、Metrics 和 Tracing 三个方向，如下图所示，可以注意到，这三个方向上彼此都有交叉、重叠的部分。在我过去的博客里，我分享过关于 ELK 和 Prometheus 的内容，可以粗略地认为，这是对 Logging 和 Metrics 这两个方向的涉猎。所以，这篇文章我想和大家分享是 Tracing，即分布式追踪，本文会结合 Envoy、Jaeger 以及 .NET Core 来实现一个分布式链路追踪的案例，希望能带给大家一点 Amazing 的东西。\n可观测性：Metrics、Tracing \u0026amp;amp; Logging\r分布式追踪 如果要追溯分布式追踪的起源，我想，Google 的这篇名为 《Dapper, a Large-Scale Distributed Systems Tracing Infrastructure》 的论文功不可没，因为后来主流的分布式追踪系统，譬如 Zipkin、Jeager、Skywalking、LightStep……等等，均以这篇论文作为理论基础，它们在功能上或许存在差异，原理上则是一脉相承，一个典型的分布式追踪系统，大体上可以分为代码埋点、数据存储和查询展示三个步骤，如下图所示，Tracing 系统可以展示出服务在时序上的调用层级，这对于我们分析微服务系统中的调用关系会非常有用。\n分布式追踪系统基本原理\r一个非常容易想到的思路是，我们在前端发出的请求的时候，动态生成一个唯一的 x-request-id，并保证它可以传递到与之交互的所有服务中去，那么，此时系统产生的日志中就会携带这一信息，只要以此作为关键字，就可以检索到当前请求的所有日志。这的确是个不错的方案，但它无法告诉你每个调用完成的先后顺序，以及每个调用花费了多少时间。基于这样的想法，人们在这上面传递了更多的信息(Tag)，使得它可以表达层级关系、调用时长等等的特征。如图所示，这是一个由 Jaeger 产生的追踪信息，我们从中甚至可以知道请求由哪台服务器处理，以及上/下游集群信息等等：\n通过 Jaeger 收集 gRPC 请求信息\r目前，为了统一不同 Tracing 系统在 API、数据格式等方面上的差异，社区主导并产生了 OpenTracing 规范，在这个 规范 中，一个 Trace，即调用链，是由多个 Span 组成的有向无环图，而每个 Span 则可以含有多个键值对组成的 Tag。如图所示，下面是 OpenTracing 规范的一个简单示意图，此时，图中一共有 8 个 Span，其中 Span A 是根节点，Span C 是 Span A 的子节点， Span G 和 Span F 之间没有通过任何一个子节点连接，称为 FollowsFrom。\n[Span A] ←←←(the root span)\r|\r+------+------+\r| |\r[Span B] [Span C] ←←←(Span C is a `ChildOf` Span A)\r| |\r[Span D] +---+-------+\r| |\r[Span E] [Span F] \u0026gt;\u0026gt;\u0026gt; [Span G] \u0026gt;\u0026gt;\u0026gt; [Span H]\r↑\r↑\r↑\r(Span G `FollowsFrom` Span F) 事实上，我们上面提到的 Zipkin 和 Jeager 都兼容这一规范，这使得我们可以更加灵活和自由地更换 Tracing 系统。除了 OpenTracing 规范，目前，OpenTelemetry 在考虑统一 Logging、Metrics 和 Tracing，即我们通常所说的 APM，如果大家对这个感兴趣，可以做更进一步的了解。\nEnvoy \u0026amp; Jaeger 目前，主流的服务网格平台如 Istio，选择 Envoy 作为其数据平面的核心组件。通俗地来讲，Envoy 主要是作为代理层来调节服务网格中所有服务的进/出站流量，它可以实现诸如负载均衡、服务发现、流量转移、速率限制、可观测性等等的功能。考虑到不同的服务都可以通过 Gateway 或者 Sidecar 来互相访问，我们更希望通过 Envoy 这个代理层来实现分布式追踪，而不是在每个应用内都去集成 SDK，这正是服务网格区别于传统微服务的地方，即微服务治理需要的各种能力，逐步下沉到基础设施层。如果你接触过微软的 Dapr，大概就能体会到我这里描述的这种变化。\nEnvoy 在 Istio 中扮演着重要角色\r事实上，Envoy 提供了入口来接入不同的 Tracing 系统，以 Zipkin 或者 Jeager 为例，除了前面提到的 x-request-id，它可以帮我们生成类似 x-b3-traceid、x-b3-spanid 等等的请求头。参照 官方文档，它大体上提供了下面 3 种策略来支撑系统范围内的追踪：\n生成 UUID ：Envoy 会在需要的时候生成 UUID，并操作名为 x-request-id 的 HTTP 头部，应用可以转发这个 HTTP 头部用于统一的记录和追踪。 集成外部追踪服务：Envoy 支持可插拔的外部追踪可视化服务，例如 LightStep、Zipkin 或者 Zipkin 兼容的后端（比如说 Jaeger）等等。 客户端追踪 ID 连接：x-client-trace-id 这个 HTTP 头部可以用来把不信任的请求 ID 连接到受信的 x-request-id HTTP 头部上。 这意味着，我们可以从客户端或者由 Envoy 来产生一个 x-request-id，只要应用转发这个 x-request-id 或者 外部追踪系统需要的 HTTP 头部，Envoy 就可以帮我们完成把这些追踪信息告诉这些外部追踪系统，甚至在 Sidecar 模式下这一切都是自动完成的。我在写这篇博客时发现，官方还是比较推崇 Sidecar 模式，即一个服务就是一个 Pod，每个 Pod 里自带一个 Envoy 作为代理，对于 Sidecar 模式而言，它的分布式追踪呈现出下面这样的结构，如果你认真阅读过官方的文档和示例，就会发现其 示例 基本都是这种结构：\nSidecar 模式下的分布式追踪示意图\r考虑到，2022 年还有没有用上 K8S 的人，以及 Catcher Wong 大佬反映 Sidecar 模式比较浪费资源，这里我们还是用 Gateway 模式来实现，譬如我们有两个服务，订单服务(OrderSevice) 和 支付服务(PaymentService)，它们都由同一个 Envoy 来代理，当我们在订单服务中调用支付服务时，就会产生一条调用链。对于大多数的微服务而言，从它被拆分地那一刻起，就不可避免地走向了像蜘蛛网一般错综复杂的结局，此时，它的分布式追踪呈现出下面的结构：\nGateway 模式下的分布式追踪示意图\r如果从代码侵入角度来审视这个问题，Sidecar 模式，每个服务都由 Envoy 去生成或者是设置一系列相关的请求头；而如果采取 Gateway 模式，当你在订单服务里调用支付服务时，无论你使用 HttpClient 还是 gRPC，你都需要确保这一系列的请求头能传递下去，这意味着我们要写一点无关紧要的代码，这样看起来前者更好一点，不是吗？可惜，合适和正确，就像鱼和熊掌一样，永远不可兼得。\nSpan 模型示意图\r关于 Jeager，这是一个由 Uber 开发的、受 Dapper 和 Zipkin 启发的分布式追踪系统，它主要适用于：分布式追踪信息传递、分布式事务监控、问题分析、服务依赖性分析、性能优化这些场景，因为它兼容 OpenTracing 标准，所以 Span 这个术语对它来说依然使用，什么是 Span 呢？它是一个追踪的最小逻辑单位，可以记录操作名，操作开始时间 和 操作耗时，下面是 Jaeger 的架构示意图，大家可以混个眼熟：\nJaeger 的架构示意图\r第一个实例 OK，现在来分享本文的第一个示例，如前文所述，我们要实现的是一个 Gateway 模式下的请求追踪。为此，我们准备了两个 ASP.NET Core 项目，分别来模拟订单服务(OrderService) 和 支付服务(PaymentService)，当我们通过 Envoy 访问 OrderService 的时候，会在其内部访问 PaymentService，以此来验证 Envoy 能否帮我们找到这条调用链。首先，我们来编写 OrderService，代码非常简单，从 HTTP 请求头中拿到 Jeager 需要的字段，并在调用 OrderService 的时候传递这些字段：\n[HttpPost] public async Task\u0026lt;IActionResult\u0026gt; Post([FromBody] OrderInfo orderInfo) { var paymentInfo = new PaymentInfo() { OrderId = orderInfo.OrderId, PaymentId = Guid.NewGuid().ToString(\u0026#34;N\u0026#34;), Remark = orderInfo.Remark, }; // 设置请求头 _httpClient.DefaultRequestHeaders.Add(\u0026#34;x-request-id\u0026#34;, Request.Headers[\u0026#34;x-request-id\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add(\u0026#34;x-b3-traceid\u0026#34;, Request.Headers[\u0026#34;x-b3-traceid\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add(\u0026#34;x-b3-spanid\u0026#34;, Request.Headers[\u0026#34;x-b3-spanid\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add(\u0026#34;x-b3-parentspanid\u0026#34;, Request.Headers[\u0026#34;x-b3-parentspanid\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add(\u0026#34;x-b3-sampled\u0026#34;, Request.Headers[\u0026#34;x-b3-sampled\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add(\u0026#34;x-b3-flags\u0026#34;, Request.Headers[\u0026#34;x-b3-flags\u0026#34;].ToString()); _httpClient.DefaultRequestHeaders.Add(\u0026#34;x-ot-span-context\u0026#34;, Request.Headers[\u0026#34;x-ot-span-context\u0026#34;].ToString()); // 调用/Payment接口 var content = new StringContent(JsonConvert.SerializeObject(paymentInfo), Encoding.UTF8, \u0026#34;application/json\u0026#34;); var response = await _httpClient.PostAsync(\u0026#34;/Payment\u0026#34;, content); var result = response.IsSuccessStatusCode ? \u0026#34;成功\u0026#34; : \u0026#34;失败\u0026#34;; return new JsonResult(new { Msg = $\u0026#34;订单创建{result}\u0026#34; }); } 接下来，PaymentService 就会变得非常简单，因为我们不会真的去对接一个支付系统，所以，就简单意思一下好啦！\n[HttpPost] public IActionResult Post([FromBody] PaymentInfo paymentInfo) { var requestId = Request.Headers[\u0026#34;x-request-id\u0026#34;].ToString(); return new JsonResult(new { Msg = $\u0026#34;支付成功, 流水号：{requestId}\u0026#34; }); } 服务编写好以后，按照惯例，我们使用 docker-compose.yaml 文件来进行编排，除了 OrderService 和 PaymentService，我们还需要 Envoy 和 Jeager，即至少需要四个服务：\nversion: \u0026#39;3\u0026#39; services: envoy_gateway: build: Envoy/ ports: - \u0026#34;9090:9090\u0026#34; - \u0026#34;9091:9091\u0026#34; volumes: - \u0026#34;./Envoy/envoy.yaml:/etc/envoy/envoy.yaml\u0026#34; - \u0026#34;./Envoy/logs/:/etc/envoy/logs/\u0026#34; order_service: build: OrderService/ ports: - \u0026#34;8081:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; payment_service: build: PaymentService/ ports: - \u0026#34;8082:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; jaeger: image: jaegertracing/all-in-one environment: - COLLECTOR_ZIPKIN_HOST_PORT=9411 ports: - \u0026#34;9411:9411\u0026#34; - \u0026#34;16686:16686\u0026#34; 此时，重头戏终于来了，Envoy 是如何连接外部追踪系统的呢？我们可以设置 HttpConnectionManager 这个过滤器下的 tracing 字段，这里我们选择 ZipkinConfig 这个类型，因为 Jaeger 完全兼容 Zipkin，所以，我们可以直接使用这个 Provider。\nfilter_chains: - filters: - name: envoy.filters.network.http_connection_manager typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager generate_request_id: true tracing: provider: name: envoy.tracers.zipkin typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.trace.v3.ZipkinConfig collector_cluster: jaeger collector_endpoint: \u0026#34;/api/v2/spans\u0026#34; collector_endpoint_version: HTTP_JSO 基本上，只要是官方支持的 Provider，我们都可以照猫画虎接入进来，当然每一种 Provider 的配置项可能会不一样，这里我们唯一要注意的是 collector_cluster, 它表示的是指向 Jeager 服务器的一个 Cluster，这意味着我们要为它单独定义一个 Cluster :\nclusters: - name: jaeger type: STRICT_DNS connect_timeout: 0.25s lb_policy: ROUND_ROBIN load_assignment: cluster_name: jaeger endpoints: - lb_endpoints: - endpoint: address: socket_address: address: jaeger port_value: 9411 还记得 Envoy 支撑系统内的分布式追踪的三个支撑策略是什么吗？显然，我们可以通过 generate_request_id 字段来控制 Envoy 生成作用于 x-request-id 的 UUID，我们希望用户从 前端 或者 cURL 中发送的请求，都能自动地带上 x-request-id 请求头，所以，我们这里将其设为 true，这意味着，从现在开始，我们的请求有了这样一个 x-request-id， 其实，如果不考虑 Jeager 的话，我们请求已经可以实现追踪了，只要后续的请求都像我这里一样传递 x-request-id 即可。原因我们已经在前面说过，此时，这些请求没有一个上下文的概念，更不要说要理清楚其中的调用层级，所以，接下来，我们还要做一点微不足道的工作：\nvirtual_hosts: - name: backend domains: - \u0026#34;*\u0026#34; routes: - match: prefix: \u0026#34;/Payment\u0026#34; route: auto_host_rewrite: true prefix_rewrite: /api/Payment cluster: payment_service decorator: operation: PaymentService - match: prefix: \u0026#34;/Order\u0026#34; route: auto_host_rewrite: true prefix_rewrite: /api/Order cluster: order_service decorator: operation: OrderService 如上所示，如果我们希望 Envoy 能记录我们的请求，那么，我们的请求必须要从它这里经过。这听起来像一句废话，可是在我调用 PaymentService已经确保我的请求是从 /Payment 这个路由上发起。默认情况下，在生成 Span 的时候，Envoy 会使用 --service-cluster 这个参数来作为 Span 的名称，这个参数通常写在 Envoy 的启动命令里，在这个示例中，它的取值是 reverse-proxy。仔细一想，会觉得哪里不太对，这样一来，所以的 Span 不就是同一个名字了吗？事实上，一开始我做实验的时候，确实是这个结果。解决方是设置一个 operation。此时，如果我们通过 Postman 访问订单接口 /Order，不出意外的话，我们会收到订单创建成功的结果，在浏览器里输入http://localhost:16686，我们来看看 Jeager 都收集到了哪些信息：\nJeagerUI 数据查询\r从图中我们可以非常容易地识别出 Service 和 Operation 在 Envoy 中分别对应着什么，我们注意到这里检索到了三个 Span，因为博主后来又加了一个 EchoService，从这里我们能看到它整个过程从何时开始，经过多长时间以后结束。如果我们点击它，会看到更加详细的说明，如下图所示：\nJeagerUI 数据展示\r显然，这个调用关系是符合我们预期的，即客户端调用了OrderService，OrderService调用了PaymentService，对于每一次调用，我们均可以从 Span 的 Tag 中获得更多信息，文章中的第三张图，实际上就是出自这里，有了这些信息以后，我们排查或者分析微服务中的问题，是不是感觉容易了很多呢？结合 ELK，你可以知道要去找哪里的日志，而这些正是分布式追踪的意义所在！\n通过 Jaeger 收集 gRPC 请求信息\r好了，到这里为止，关于 Envoy 在分布式追踪上的探索，终于可以告一段落，完整的项目文件我已经放在 Github 上供大家参考，谢谢大家！\n本文小结 可观测性(Logging、Metrics \u0026amp; Tracing) 是当下微服务中重要的一个组成部分，从 ELk 收集日志，到 Prometheus 监控指标， 再到 Jeager 追踪调用链，我们看到了一种完全不同于单体系统中打断点、单步调试的诊断思路，这是否说明，微服务的治理永远是一个绕不过去的话题。在这篇文章里，我们简单介绍了分布式追踪系统，比如最常见的 Zipkin、Jeager、Skywalking、LightStep\u0026hellip;等等，其基本思想是生成一个 x-request-id，并在不同的服务或者应用中传递这个信息。在此基础上，我们介绍了 OpenTracing 规范，即 一个调用链(Trace)，是由多个 Span 组成的有向无环图，而每个 Span 则可以含有多个键值对组成的 Tag。目前，Envoy 官方主推的是 Sidecar 模式，即每个服务分配一个 Envoy 作为代理，考虑到博主目前使用 Gateway 模式更多一点，故结合 ASP.NET Core 和 Jeager 实现了一个简单的示例，这个示例唯一的不足在于，服务或者应用必须显式地传递这些请求头，如果直接集成 SDK，效果应该会比现在好很多，可这样的话，就显得不那么云原生了，如果大家有更好的做法，欢迎在评论区留言和交流。大家可以稍微注意一下 OpenTelemetry 这个项目，如果你需要更完备的可观测性信息收集。好了，以上就是这篇博客的全部内容，晚安，世界。\n","date":"2022-01-14T16:46:23Z","image":"/posts/768684858/Jaeger-Span-Model.png","permalink":"https://qinyuanpei.github.io/posts/768684858/","slug":"768684858","tags":["Envoy","Jaeger","Tracing"],"title":"Envoy 集成 Jaeger 实现分布式链路追踪"},{"categories":["编程语言"],"content":"据我所知，软件行业，向来是充满着鄙视链的，人们时常会因为语言、框架、范式、架构等等问题而争执不休。不必说 PHP 到底是不是世界上最好的语言，不必说原生与 Web 到底哪一个真正代表着未来，更不必说前端与后端到底哪一个更有技术含量，单单一个 C++ 的版本，1998 与 2011 之间仿佛隔了一个世纪。我真傻，我单知道人们会因为 GCC 和 VC++ 而分庭抗礼多年，却不知道人们还会因为大括号换行、Tab 还是空格、CRLF 还是 CR……诸如此类的问题而永不休战。也许，正如 王垠 前辈所说，编程这个领域总是充满着某种 宗教原旨 的意味。回想起刚毕业那会儿，因为没有 Web 开发的经验而被人轻视，当年流行的 SSH 全家桶，对我鼓捣 Windows 桌面开发这件事情，投来无限鄙夷的目光，仿佛 Windows 是一种原罪。可时间久了以后，我渐渐意识到，对工程派而言，一切都是工具；而对于学术派而言，一切都是包容。这个世界并不是只有 Web，对吧？所以，这篇博客我想聊聊非典型 Web 应用场景下的身份认证。\n楔子 在讨论非典型 Web 应用场景前，我们不妨来回想一下，一个典型的 Web 应用是什么样子？打开浏览器、输入一个 URL、按下回车、输入用户名和密码、点击登录……，在这个过程中，Cookie/Session用来维持整个会话的状态。直到后来，前后端分离的大潮流下，无状态的服务开始流行，人们开始使用一个令牌(Token)来标识身份信息，无论是催生了 Web 2.0 的 OAuth 2.0 协议，还是在微服务里更为流行的 JWT(JSON Web Token)，其实，都在隐隐约约说明一件事情，那就是在后 Web 时代，特别是微信兴起以后，人们在线与离线的边界越来越模糊，疫情期间居家办公的这段时间，我最怕听到 Teams 会议邀请的声音，因为无论你是否在线，它都会不停地催促你，彻底模糊生活与工作的边界。那么，屏幕前聪明的你，你告诉我，什么是典型的 Web 应用？也许，我同样无法回答这个问题，可或许，下面这几种方式，即 gRPC、SignalR 和 Kafka，可以称之为：非典型的 Web 应用。\ngRPC 相信经常阅读我博客的朋友，都知道这样一件事情，那就是，过去这半年多的时间，我一直在探索，如何去构建一个以 gRPC 为核心的微服务架构。想了解这方面内容的朋友，不妨抽空看看我前面写过的博客。从整体上来说，我们对于 gRPC 的使用上，基本可以分为对内和对外两个方面。对内，不同的服务间通过 gRPC 客户端互相通信，我们称之为：直连；对外，不同的服务通过 Envoy 代理为 JSON API 供前端/客户端消费，我们称之为：代理。一个简单的微服务示意图，如下图所示：\ngRPC 微服务中的内与外\r目前，这个方案最大的问题，不同的服务间通过 gRPC 客户端直连的时候，无法提供身份认证信息，因为如果是单纯的读，即从某一个服务查询数据，其实是可以接受这种“裸奔”的状态，可一旦涉及到了写，这种方案就显得不大严谨。譬如现在的做法，如果从 HttpContext 里提取不到用户信息，就默认当前用户是 Sys，表示这是一个系统级别的操作。那么，如何解决这个问题呢？我们一起来看一下：\nvar channel = GrpcChannel.ForAddress(\u0026#34;https://localhost:5001\u0026#34;); var client = new Greeter.GreeterClient(channel); var metadata = new Metadata(); metadata.Add(\u0026#34;Authorization\u0026#34;, $\u0026#34;Bearer {token}\u0026#34;); var reply = client.SayHello(new HelloRequest(), metadata); 可以注意到，这里的关键是构造一个Metadata，并在其中传入Authorization头部。当然，这一切的前提是你遵循并且沿用了 ASP.NET Core 身份验证，这里以最常见的 JWT 认证为例：\nservices.AddAuthentication(x =\u0026gt; { x.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme; x.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme; } ).AddJwtBearer(x =\u0026gt; { x.RequireHttpsMetadata = false; x.SaveToken = true; x.TokenValidationParameters = new TokenValidationParameters { ValidateIssuerSigningKey = true, IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(jwtOptions.Secret)), ValidIssuer = jwtOptions.Issuer, ValidAudience = jwtOptions.Audience, ValidateIssuer = true, ValidateAudience = true, }; }); 通常情况下，这里的IssuerSigningKey由一个证书文件来提供，例如最常用的是X509SecurityKey类。为了方便演示，这里采用一组字符串进行签名。不管采用哪一种方式，我们都应该保证它与生成令牌时的参数一致。例如，下面是一个典型的生成 JWT 令牌的代码片段：\nvar claims = new[] { new Claim(ClaimTypes.Name, userInfo.UserName), new Claim(ClaimTypes.Role, userInfo.UserRole) }; var signKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(_jwtOptions.Value.Secret)); var credentials = new SigningCredentials(signKey, SecurityAlgorithms.HmacSha256); var jwtToken = new JwtSecurityToken( _jwtOptions.Value.Issuer, _jwtOptions.Value.Audience, claims, expires: DateTime.Now.AddMinutes(_jwtOptions.Value.AccessExpiration), signingCredentials: credentials ); token = new JwtSecurityTokenHandler().WriteToken(jwtToken); 除了以上两点，请确保你的正确地配置了认证和授权两个中间件，注意它们的顺序和位置：\npublic void Configure(IApplicationBuilder app, IWebHostEnvironment env) { // ... app.UseRouting(); // 注意顺序 app.UseAuthentication(); app.UseAuthorization(); app.UseEndpoints(endpoints =\u0026gt; { endpoints.MapGrpcService\u0026lt;GreeterService\u0026gt;(); }); } 此时，我们就可以在 gRPC 中通过IHttpContextAccessor获取当前用户信息：\npublic override Task\u0026lt;HelloReply\u0026gt; SayHello(HelloRequest request, ServerCallContext context) { var userName = _httpContextAccessor.HttpContext.User?.Identity.Name; return Task.FromResult(new HelloReply { Message = $\u0026#34;Hello {userName}\u0026#34;}); } 当然，考虑到我们使用 gRPC 客户端工厂的场景更多一点，我们更希望这个令牌可以在一开始就准备好，而不是每调用一个方法都需要传一次令牌。此时，我们可以使用下面的做法：\nservices.AddGrpcClient\u0026lt;Greeter.GreeterClient\u0026gt;(o =\u0026gt; { o.Address = new Uri(\u0026#34;https://localhost:5001\u0026#34;); }).ConfigureChannel(o =\u0026gt; { var credentials = CallCredentials.FromInterceptor((context, metadata) =\u0026gt; { if (!string.IsNullOrEmpty(_token) metadata.Add(\u0026#34;Authorization\u0026#34;, $\u0026#34;Bearer {_token}\u0026#34;); return Task.CompletedTask; }); o.Credentials = ChannelCredentials.Create(new SslCredentials(), credentials); }); 至此，关于 gRPC 的身份认证问题终于得到了解决，无论是对内的直连还是对外的代理，我们都可以获得用户的身份信息。需要说明的是，默认情况下，gRPC 允许调用方在不携带令牌的情况下调用接口，所以，我们这里的认证方案更像是一种君子协定。如果希望做更严格的限制，可以考虑在具体的服务上添加 [Authorize]特性，就像我们在控制器上使用该特性一样。\nSignalR SignalR 是由微软提供的一面向实时 Web 应用的、开源的库，你可以认为，它是集 WebSockets、Server-Sent 事件 和 长轮询于一身的综合性的库。当你需要从服务端实时地推送消息到特定的客户端(组)的时候，SignalR 将会是一个不错的选择。譬如可视化的仪表盘或者监视系统，需要接收数据的变更通知以实时地刷新视图；即时通讯(IM)、社交网络、邮件、游戏、协作等方面地应用都需要及时地发出通知。ASP.NET Core 版本的 SIgnalR 在自动处理连接管理方面做出来不小的改善，可很多时候，SIgnalR 里面的 ConnectionId 对我们而言是没有意义的，我们更想知道连接到 Hub 的用户是谁，这样就催生出来 SignalR 身份认证的需求场景，下面，我们就来看看对应的 解决方案。\n// 方式一、AccessTokenProvider var hubConnection = new HubConnectionBuilder() .WithUrl(\u0026#34;http://localhost:5000/echohub\u0026#34;,options =\u0026gt; { options.AccessTokenProvider = () =\u0026gt; Task.FromResult(\u0026#34;\u0026lt;Your Token\u0026gt;\u0026#34;); }) .WithAutomaticReconnect() .Build(); // 方式二、直接追加查询参数 var hubConnection = new HubConnectionBuilder() .WithUrl(\u0026#34;http://localhost:5000/echohub?access_token=\u0026lt;Your Token\u0026gt;\u0026#34;) .WithAutomaticReconnect() .Build(); 首先，SignalR 的身份认证，整体上依然遵循 ASP.NET Core 里的这套认证/授权流程，所以，我们可以继续沿用 gRPC 这部分的代码。考虑到 SignalR 首次发起的是一个 GET 请求，通常的做法是在查询参数中追加令牌参数。当然，现在官方提供了AccessTokenProvider这个属性，允许你构造一个委托来提供令牌。接下来，为了让这个令牌更符合一般的场景，譬如，按照约定它应该出现在 HTTP 请求头的 Authorization 字段上，我们有下面两种方式来对它进行处理：\n// 方式一、设置 JwtBearer 的 Events 属性 services.AddAuthentication(x =\u0026gt; { //... }) .AddJwtBearer(x =\u0026gt; { //... x.Events = new JwtBearerEvents() { OnMessageReceived = context =\u0026gt; { var accessToken = context.Request.Query[\u0026#34;access_token\u0026#34;]; var path = context.HttpContext.Request.Path; if (!string.IsNullOrEmpty(accessToken) \u0026amp;\u0026amp; (path.StartsWithSegments(\u0026#34;/echohub\u0026#34;))) context.Token = accessToken; return Task.CompletedTask; } }; }); // 方式二，编写中间件，注意顺序 app.Use((context, next) =\u0026gt; { var accessToken = context.Request.Query[\u0026#34;access_token\u0026#34;].ToString(); var path = context.Request.Path; if (!string.IsNullOrEmpty(accessToken) \u0026amp;\u0026amp; (path.StartsWithSegments(\u0026#34;/echohub\u0026#34;))) context.Request.Headers.Add(\u0026#34;Authorization\u0026#34;, new StringValues($\u0026#34;Bearer {accessToken}\u0026#34;)); return next.Invoke(); }); app.UseAuthentication(); app.UseAuthorization(); 可以注意到，不管是哪一种方式，核心目的都是为了让令牌能在 ASP.NET Core 的请求管道中出现在它期望出现的地方，这是什么地方呢？我想，应该是为执行认证/授权中间件以前，所以，为什么我说这两个中间件的顺序非常重要，原因正在于此，一旦我们做了这一点，剩下的事情就交给微软，我们只需要通过 HttpContext 的 User 属性获取用户信息即可。\npublic Task Echo(string message) { var userName = Context.User?.Identity?.Name; Clients.Client(Context.ConnectionId).SendAsync(\u0026#34;OnEcho\u0026#34;, $\u0026#34;{userName}:{message}\u0026#34;); return Task.CompletedTask; } 对于 SignalR 而言，同一个用户会对应多个 ConnectionId，当然，我并不需要过度地去关注这个东西，除非我们真的要分清每一个 ConnectionId 具体代表什么。类似地，SignalR 一样可以用 [Authorized] 特性来限制 Hub 是否可以在未认证的情况下使用，甚至它可以配合不同的 Policy 来做更细致的划分：\npublic class UserRoleRequirement : AuthorizationHandler\u0026lt;UserRoleRequirement, HubInvocationContext\u0026gt;, IAuthorizationRequirement { protected override Task HandleRequirementAsync( AuthorizationHandlerContext context, UserRoleRequirement requirement, HubInvocationContext resource) { var userRole = context.User.Claims.FirstOrDefault(x =\u0026gt; x.Type == ClaimTypes.Role)?.Value; if (userRole == \u0026#34;Admin\u0026#34; \u0026amp;\u0026amp; resource.HubMethodName == \u0026#34;Echo\u0026#34;) context.Succeed(requirement); return Task.CompletedTask; } } 这里，我们定义了一个UserRoleRequirement类，其作用是仅仅允许Admin角色访问Echo()方法。此时，为了让这个策略生效，我们还需要将其注册到容器中，如下面的代码片段所示：\nservices .AddAuthorization(options =\u0026gt; { options.AddPolicy(\u0026#34;UserRoleRestricted\u0026#34;, policy =\u0026gt; { policy.Requirements.Add(new UserRoleRequirement()); }); }); 现在，我们可以在集线器(Hub)上控制 Echo() 方法访问权限，考虑到 gRPC 和 SignalR 都可以使用这套身份认证方案，所以，这个做法同样适用于 gRPC，如果你希望实现方法级的权限控制：\n[Authorize] public class EchoHub : Hub { [Authorize(\u0026#34;UserRoleRestricted\u0026#34;)] public Task Echo(string message) { var userName = Context.User?.Identity?.Name; Clients.Client(Context.ConnectionId).SendAsync(\u0026#34;OnEcho\u0026#34;, $\u0026#34;{userName}:{message}\u0026#34;); return Task.CompletedTask; } } Kafka 无独有偶，除了 SignalR ，我们还用到了消息中间件 Kafka，事实上，SignalR 会从 Kafka 拉取消息，并将其发布到订阅了该 Topic 的客户端上，所以，Kafka 在整个系统中，其实扮演着非常重要的角色。如果你只是需要让 Kafka 充当一个消息中介者，那么，你完全不需要考虑 Kafka 的身份认证问题。可一旦你考虑用 Kafka 来做具体的业务，这个问题就会立刻凸显出来。譬如，当我们用 Kafka 来实现一个分布式事务的时候，我们采用了 SAGA 模式，即让主事务负责事务的协调，每个子事务在收到主事务的消息后，执行相应的操作并回复主事务一条消息，再由主事务来决定整个事务应该提交还是回滚。如图所示，下面是一个针对 SAGA 模式的简单示意图：\n分布式事务 SAGA 模式示意图\r关于这个模式的细节，感兴趣的朋友可以从 这里 获取。这里我想说的是，当我们尝试用 Kafka 来做具体的业务的时候，我们其实是无法获得对应的用户信息的，因为此时此刻，基于 ASP.NET Core 的管道式的洋葱模型，对我们而言是暂时失效的，所以，我一直在说的非典型 Web 应用，其实可以指脱离了洋葱模型、脱离了授权/认证流程的这类场景。和 gRPC 类似，当我们需要用户信息，而又无法获得用户信息的时候，该怎么办呢？答案是在 Kafka 的消息中传递一个令牌(Token)，下面是一个简单的实现思路：\nvar producerConfig = new ProducerConfig { BootstrapServers = \u0026#34;192.168.50.162:9092\u0026#34; }; using (var p = new ProducerBuilder\u0026lt;Null, string\u0026gt;(producerConfig).Build()) { var token = \u0026#34;\u0026lt;Your Token\u0026gt;\u0026#34;; var topic = “\u0026lt;Your Topic\u0026gt;\u0026#34;; var document = new { Id = \u0026#34;001\u0026#34;, Name = \u0026#34;张三\u0026#34;, Address = \u0026#34;北京市朝阳区\u0026#34;, Event = \u0026#34;喝水未遂\u0026#34; }; var message = new Message\u0026lt;Null, string\u0026gt; { Value = JsonConvert.SerializeObject(document) }; // 在 Kafka 消息头里增加 Authorization 字段 message.Headers = new Headers(); message.Headers.Add(\u0026#34;Authorization\u0026#34;, Encoding.UTF8.GetBytes($\u0026#34;Bearer {token}\u0026#34;)); var result = await p.ProduceAsync(topic, message); } 显然，这是非常朴素的一个想法，发送 Kafka 消息的时候，在 Kafka 的消息头里增加 Authorization字段，完美借鉴HTTP协议里的做法。那么，相对应地，消费 Kafka 消息的时候需要做一点调整，因为 Kafka 完全独立于 ASP.NET Core 的请求管道，所以，校验令牌的工作此时需要我们来独立完成。不过，请放心，这一切不会特别难，因为JwtSecurityTokenHandler这个类我们已经在前面用过一次：\nvar consumerConfig = new ConsumerConfig { BootstrapServers = \u0026#34;127.0.0.1:9092\u0026#34; }; using (var c = new ConsumerBuilder\u0026lt;Null, string\u0026gt;(consumerConfig).Build()) { c.Subscribe(\u0026#34;\u0026lt;Your Topic\u0026#34;); var cts = new CancellationTokenSource(); var jwtHandler = new JwtSecurityTokenHandler(); var tokenParameters = new TokenValidationParameters() { ValidateIssuerSigningKey = true, IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(\u0026#34;\u0026lt;Your Secret\u0026gt;\u0026#34;)), ValidIssuer = \u0026#34;\u0026lt;Your Issuer\u0026gt;\u0026#34;, ValidAudience = \u0026#34;\u0026lt;Your Audience\u0026gt;\u0026#34;, ValidateIssuer = true, ValidateAudience = true, }; while (true) { var userName = string.Empty; var consumeResult = c.Consume(cts.Token); var headers = consumeResult.Message.Headers; if (headers != null \u0026amp;\u0026amp; headers.TryGetLastBytes(\u0026#34;Authorization\u0026#34;, out byte[] values)) { // 校验令牌 var jwtToken = Encoding.UTF8.GetString(values).Replace(\u0026#34;Bearer\u0026#34;, \u0026#34;\u0026#34;).Trim(); var claimsPrincipal = jwtHandler.ValidateToken(jwtToken, tokenParameters, out SecurityToken securityToken); userName = claimsPrincipal.Identity.Name; // 处理消息 // ...... } } } 不过，我个人感觉，这样会增加消息消费方的工作，更好的做法是采用 CallContext 或者 AsyncLocal 来做统一的处理，这样，消息订阅方只需要关心消息怎么处理即可，考虑到 Kafka 属于 Pull 模式的消息队列，这种思路不见得在性能上有多少提升，更多的是一种简化啦，算是少写一点重复代码：\npublic static void Subscribe\u0026lt;TKey, TValue\u0026gt;( this IConsumer\u0026lt;TKey, TValue\u0026gt; consumer, string topic, CancellationToken cancellationToken, Action\u0026lt;TValue\u0026gt; callback) { consumer.Subscribe(topic); while (true) { try { var consumeResult = consumer.Consume(cancellationToken); if (consumeResult != null) { var headers = consumeResult.Message.Headers; if (headers != null \u0026amp;\u0026amp; headers.TryGetLastBytes(\u0026#34;Authorization\u0026#34;, out byte[] values)) { var jwtToken = Encoding.UTF8.GetString(values).Replace(\u0026#34;Bearer\u0026#34;, \u0026#34;\u0026#34;).Trim(); var userInfo = new JwtTokenResloverService().ValidateToken(jwtToken); UserContext.SetUserInfo(userInfo); } if (callback != null) callback(consumeResult.Message.Value); } } catch (ConsumeException e) { // ... } } } 其中，UserContext内部利用AsyncLocal在不同线程间共享用户信息，这可以让我们在任意位置访问用户信息，因为该扩展方法中会调用一次SetUserInfo()方法，所以，只要在 Kafka 的消息头上维护了Authorization字段，就可以从中解析出常用的用户信息，譬如用户名、用户角色等等。当然，按照 JWT 的 规范，我们最好还是不要在载荷(Payload)中存放敏感信息，如果需要更详细的信息，比如部门、权限等等，建议通过调接口或者查数据的库方式来实现。下面是UserContext的实现细节：\nstatic class UserContext { private static AsyncLocal\u0026lt;UserInfo\u0026gt; _localUserInfo = new AsyncLocal\u0026lt;UserInfo\u0026gt;(); public static void SetUserInfo(UserInfo userInfo) =\u0026gt; _localUserInfo.Value = userInfo; public static UserInfo GetUserInfo() =\u0026gt; _localUserInfo.Value; } 非常的简单，对不对? 作为 CallContext 的继任者，AsyncLocal就是这样的优秀！也许，大家会好奇JwtTokenResloverService是什么？其实，它还是JwtSecurityTokenHandler那一堆东西，下面的代码片段展示了如何从ClaimsPrincipal中提取用户名和角色名称，再次说明，不要在这里放敏感信息：\nvar claimsPrincipal = _jwtHandler.ValidateToken(token, tokenParameters, out SecurityToken securityToken); if (claimsPrincipal != null) { var userInfo = new UserInfo(); userInfo.UserName = claimsPrincipal.Identity.Name; userInfo.UserRole = claimsPrincipal.Claims.FirstOrDefault(x =\u0026gt; x.Type == ClaimTypes.Role)?.Value; return userInfo; } 现在，我们一开始的例子，可以简化成下面这样：\nvar consumerConfig = new ConsumerConfig { BootstrapServers = \u0026#34;127.0.0.1:9092\u0026#34; }; using (var c = new ConsumerBuilder\u0026lt;Null, string\u0026gt;(consumerConfig).Build()) { var cts = new CancellationTokenSource(); c.Subscribe(\u0026#34;\u0026lt;Your Topic\u0026gt;\u0026#34;, cts.Token, message =\u0026gt; { // 获取当前用户信息 var userInfo = UserContext.GetUserInfo(); // 处理消息 }); } Kafka 的故事，讲述到这里本该结尾，可世界上哪里会有完美的方案呢？如果考虑到 Kafka 发生消息堆积的可能性，一旦消息没有被及时处理，那么，这个放在消息头上的令牌可能会出现过期的情况。这种事情就和你使用缓存一样，如果不考虑缓存的击穿、穿透、雪崩三大灾难，那你对缓存的认知简直是肤浅。同样的道理，你要考虑令牌的过期、刷新、撤销，整个认知网络才算是真正建立起来，这些就交给我聪明的读者啦，或者以后有机会单独写一写这些话题。\n本文小结 2021年的最后一天，西安疫情可谓是涛声依旧，居家隔离、远程办公、买不到菜，注定要为这段时光写下注脚。也许，我们都习惯了饭来张口的外卖生活，可在这一刻，当一切线下互动被迫中止的时候，我们终于发现这些“典型”的生活场景变得不再“典型”；当人们开始在微信群里用接龙的方式来寻求生活物资的时候，微信这个社交工具的缺点就被不断地放大；当人们逐渐回归到一种“以物易物”的状态时，我不得不感慨这种从骨子里与生俱来的生存本能；当人们习以为常的“典型”被打破，其本身是否就是一种舒适圈？无论技术还是生活，你是否有做好随时面对“非典型”场景的准备？我想，在与新冠病毒长期对峙的后疫情时代，这是每一个人都应该去思考的问题，这篇博客断断续续地写了好几天，大概 2021 终究还是要这般潦草的过去罢。因为，我是一个长期悲观主义者，我确信这个世界依旧遵循熵增定律。如果你打算反驳，我会说：你说得对。\n","date":"2021-12-28T11:53:29Z","image":"/posts/2478147871/Kafka-Distributed-Transaction.drawio.png","permalink":"https://qinyuanpei.github.io/posts/2478147871/","slug":"2478147871","tags":["gRPC","SignalR","Kafka"],"title":"浅议非典型 Web 应用场景下的身份认证"},{"categories":["编程语言"],"content":"我发现，人们非常喜欢在一件事情上反复横跳。譬如，以编程语言为例，人们喜欢静态的、强类型语言的严谨和安全，可难免会羡慕动态的、弱类型语言的自由和灵活。于是，在过去的这些年里，我们注意到，.NET 的世界里出现了 dynamic 类型，JavaScript 的世界里出现了 TypeScript，甚至连 Python 都开始支持类型标注。这种动与静、强与弱的角逐，隐隐然有种太极圆转、轮回不绝的感觉。果然，“城外的人想冲进去，城里的人想逃出来”，钱钟书先生说的固然是婚姻，可世上的事情，也许都差不多罢！人们反复横跳的样子，像极了「九品芝麻官」里的方唐镜。曾经有段时间，好多人吹捧 Vue3 + TypeScript 的技术栈，有位前辈一针见血地戳破了这种叶公好龙式的喜欢，“你那么喜欢 TypeScript，不还是关掉了 ESLint 的规则，项目里全部都用 Any”。对于这个吐槽，我表示非常真实，因为我们对于动与静、强与弱的心理变化是非常微妙的。常言道，“动态类型一时爽，代码重构火葬场”，你是如何看待编程语言里的动与静静、强与弱的呢？在 gRPC 中我们通过 Protobuf 来描述接口的参数和返回值，由此对服务提供/消费方进行约束。此时，参数和返回值都是静态的、强类型的。如果我们希望提供某种“泛型”的接口，又该如何去做呢？所以，这篇文章我们来聊聊 gPRC 里的 Any 类型。\nProtobuf 里的 Any 类型 在讲 Any 类型前，我想，我们应该想明白，为什么需要这样一个类型？现在，假设我们有下面的 Protobuf 定义：\n// Vehicle message Vehicle { int32 VehicleId = 1; string FleetNo = 2; } // Officer message Officer { int32 OfficerId = 1; string Department = 2; } 此时，按照Protobuf的规范，我们必须像下面这样定义对应的集合：\n// VehicleList message VehicleList { repeated Vehicle List = 1; } // OfficerList message OfficerList { repeated Officer List = 1; } 考虑到，在C# 中我们只需要使用 List\u0026lt;Vehicle\u0026gt; 和 List\u0026lt;Officer\u0026gt; 即可，这样难免就会形成一种割裂感，因为你几乎要为每一种类型建立对应的表示集合的类型，从语义化的角度考虑，我们更希望使用下面的 Protobuf 定义：\nmessage Collection { repeated Any List = 1; } 此时，VehicleList 和 OfficerList 就可以统一到 Collection 这个类型中，这样，不但减少了花在类型定义的时间，更能帮助我们打开一点思路。在过去，我们编写 API 的时候，通常会定义下面的类来返回结果：\npublic class ApiResult\u0026lt;TData\u0026gt; { public int Code { get; set; } public string Msg { get; set; } public TData Data { get; set; } } 类似地，当我们用 gPRC 来做微服务的时候，我们希望在 Protobuf 中沿用这个设计：\nmessage ApiResult { int32 Code = 1; string Msg = 2; Any Data = 3; } 至此，它可以和我们在 C# 中的认知联系起来，不会让你有太多心智上的负担。基于上述两种诉求，我们发现， Protobuf 中存在着需要泛化的场景，你可以理解为，我们需要用 Protobuf 来表示泛型或者模板类这样的东西。幸运的是，Google 为我们定义了 Any 类型，它到底是何方神圣呢？我们一起来看看：\nmessage Any { string type_url = 1; bytes value = 2; } 没错，它就是这样的朴实无华，甚至比古天乐还要平平无奇，简单来说，type_url字段告诉你这是一个什么类型，value字段里则存放对应的二进制数据，而这就是 Any 类型的全部秘密！\n在 .NET 中使用 Any 类型 好了，下面我们来演示，如何在 .NET 中使用 Any 类型。通过前面我们已经知道， Any 类型和我们自定义的消息没有区别，所以，它同样实现了 IMessage 和 IMessage\u0026lt;Any\u0026gt;两个接口，唯一不同的地方在于，它拥有Pack()、Unpack\u0026lt;T\u0026gt;()、TryUnpack\u0026lt;T\u0026gt;()这样几个静态方法，这是实现任意 IMessage 到 Any 相互转换的关键。现在，假设我们现在有如下的 Protobuf 定义：\nmessage AnyRequest { google.protobuf.Any Data = 1; } message AnyResponse { google.protobuf.Any Data = 1; } message Foo { string Name = 1; } message Bar { string Name = 1; } 此时，如果我们希望在 AnyRequest 或者 AnyResponse 里传递 Any 类型，我们可以这样做：\nvar anyRequest = new AnyRequest() // Foo -\u0026gt; Any，默认类型地址前缀 var foo = new Foo(); foo.Name = \u0026#34;Foo\u0026#34;; anyRequest.Data = Any.Pack(foo); // Bar -\u0026gt; Any, 自定义类型地址前缀 var bar = new Bar(); bar.Name = \u0026#34;Bar\u0026#34;; anyRequest.Data = Any.Pack(bar, \u0026#34;type.company.com/bar\u0026#34;); 反过来，我们可以从 Any 中解析出 IMessage ：\nif (request.Data.Is(Foo.Descriptor)) { // Any -\u0026gt; Foo var foo = request.Data.Unapck\u0026lt;Foo\u0026gt;(); } else if (request.Data.Is(Bar.Descriptor)) { // Any -\u0026gt; Bar var bar = request.Data.Unapck\u0026lt;Bar\u0026gt;(); } 默认的 Any 类型，只能对 Protobuf 生成的类型(即实现了 IMessage 接口)进行 Pack ，如果我们想做得更绝一点(最好还是不要)，那么，可以使用自定义的 MyAny 类型：\nmessage MyAny { string TypeUrl = 1; bytes Value = 2; } 相应地，我们为 MyAny 类型编写一点扩展方法：\npublic static class MyAnyExtension { public static MyAny Pack(this object obj, string typeUrlPrefix = \u0026#34;\u0026#34;) { var any = new MyAny(); any.TypeUrl = $\u0026#34;{typeUrlPrefix}/{obj.GetType().FullName}\u0026#34;; var bytes = Encoding.UTF8.GetBytes(JsonConvert.SerializeObject(obj)); any.Value = Google.Protobuf.ByteString.CopyFrom(bytes); return any; } public static T Unpack\u0026lt;T\u0026gt;(this MyAny any, string typeUrlPrefix = \u0026#34;\u0026#34;) { var typeUrl = $\u0026#34;{typeUrlPrefix}/{typeof(T).FullName}\u0026#34;; if (typeUrl == any.TypeUrl) { var json = Encoding.UTF8.GetString(any.Value.ToByteArray()); return JsonConvert.DeserializeObject\u0026lt;T\u0026gt;(json); } return default(T); } public static bool Is\u0026lt;T\u0026gt;(this MyAny any, string typeUrlPrefix = \u0026#34;\u0026#34;) { var typeUrl = $\u0026#34;{typeUrlPrefix}/{typeof(T).FullName}\u0026#34;; return typeUrl == any.TypeUrl; } } 接下来，我们就可以对任意类型进行处理，虽然，此时此刻，从严格意义上来讲，它已不再属于 Protobuf 的范畴，因为序列化/反序列化都交给了 JSON ：\nvar client = serviceProvider.GetService\u0026lt;ProtobufAny.Greeter.GreeterClient\u0026gt;(); client.Ping(new Foo() { Name = \u0026#34;Foo\u0026#34; }.Pack()); client.Ping(new Bar() { Name = \u0026#34;Foo\u0026#34; }.Pack()); client.Ping(new { X = 0, Y = 1, Z = 0 }.Pack()); 这样看起来是不是非常酷？我始终认为，这件事情是有意义的，一个系统中最多的接口显然是查询接口，此时，我们可以构建一个通用的 查询 来处理，使用者只需要传递一个实体、一个Proto，一组过滤条件，它就可以返回对应的数据，这样是不是比写一个又一个差不多的接口要好一点呢？过去我们开发 API，主张用数据传输对象(DTO)来隔离持久化层和业务层，从这个角度来看，Protobuf 本身就是 一种 DTO ，对于大多数相似的、模板化的、套路化的接口，我们完全可以考虑用这种方案来实现，只要双方约定好类型即可。\n// 在业务层构建通用的查询 public QueryReply Query\u0026lt;TInput, TOutput\u0026gt;(SearchParameters searchParameters) where TInput : class { var result = _chinookContext.Set\u0026lt;TInput\u0026gt;().AsQueryable().Search(searchParameters).ToList(); var output = result.Adapt\u0026lt;List\u0026lt;TOutput\u0026gt;\u0026gt;(); var reply = new QueryReply(); reply.List.AddRange(output.Select(x =\u0026gt; x.Pack())); return reply; } // x =\u0026gt; { 1, 2, 3 }.Contains(x.AlbumId) var searchParameters = SearchParameters(); searchParameters.QueryModel = new QueryModel(); searchParameters.QueryModel.Add(new Condition() { Field = \u0026#34;AlbumId\u0026#34;, Op = Operation.StdIn, Value = new int[] { 1, 2, 3} }); // 在服务层解析参数，完全可以由调用方提供 SearchParameters var inputType = Type.GetType(request.InputType); var outputType = Type.GetType(request.OutputType); if (inputType != null \u0026amp;\u0026amp; outputType != null) { var queryMethod = _queryService.GetType().GetMethod(\u0026#34;Query\u0026#34;).MakeGenericMethod(inputType, outputType); QueryReply queryResult = (QueryReply)queryMethod.Invoke(_queryService, new object[] { new DynamicSearch.Core.SearchParameters() }); return Task.FromResult(queryResult); } 本文小结 对于编程语言中的动与静、强与弱，我个人觉得还是要看场景，只要双方定义好契约，我相信，它都可以运作起来，当然，更多的时候，我们是在灵活与严谨间反复横跳。作为一门 DSL，Protobuf 虽然可以对服务提供/消费方产生一定约束，可当我们面对需要泛型或者模板类的场景的时候，这种做法就变成了一种负担，更不必说它缺乏对继承的支持。想象一下，你要写二十多个大同小异的接口，譬如为每一张数据表写一个 GetXXXById() 的接口。此时，我们可以借助 Any 类型来实现类似泛型、模板类的东西，它本质上还是 IMessage 接口的实现类，唯一的不同是增加了 Pack/Unpack 这组静态方法，可以帮助我们实现 Any 和 IMessage 的相互转换，关于本文中使用的的实例，可以参考：ProtobufAny，好了，以上就是这篇博客的全部内容，如果有朋友对文章中的内容和观点存在疑问，欢迎在评论区积极留言，谢谢大家！\n","date":"2021-12-10T11:53:29Z","image":"https://grpc.io/img/landing-2.svg","permalink":"https://qinyuanpei.github.io/posts/2617947988/","slug":"2617947988","tags":["gRPC","Protobuf","Any"],"title":"gRPC 借助 Any 类型实现接口的泛化调用"},{"categories":["编程语言"],"content":"时间终于来到了十二月，据说，《黑客帝国 4：矩阵重生》 将于本月在北美上映，正如同它的片名一样，黑客帝国系列在沉寂了十八年后，终于等来了一次矩阵重生的机会，不可不谓“有生之年”、“爷青回”。提及黑客帝国系列，这是一部公认的、具有划时代意义的科幻电影，除了精彩绝伦的打斗特效，最为影迷所津津乐道的，当属对于人和机器的关系这种颇具哲学意味的问题的探讨。在第二部中，The One 的部分代码被融合到了 Smith 身上，而这使得 Smith 发生变异，成为了可以自我复制的病毒。于是，我们在这里看到了 Neo 和 100 个 Smith 打斗的桥段，类似的桥段还有第三部里的雨中决斗。这些桥段或多或少地影响到了后来的电影，譬如，星爷的 《功夫》 里，阿星与斧头帮、火云邪神打斗的片段；吴京的第一部电影 《狼牙》 里，阿布雨夜大战黑衣人的片段等等。虽然，病毒的自我复制和分布式系统中的复制，是两个完全不同的概念，可当我们试图将电影和现实联系起来的时候，我们还是会不免会心一笑，因为 100 个 Smith ，大概就相当于一个 Smith 的集群；而吞噬了先知能力的 Smith ，大概就相当于这个集群中的 Leader。我们注意到，强如超人般的 Neo，一样架不住越来越多的 Smith ，最后不得不飞走，所谓：“双拳难敌四手”，这足以说明集群的重要性。好了，既然这里聊到了集群，那么我们这次来聊聊 Redis 中的集群模式。\nRedis 集群概述 通过上一篇文章，我们了解到，主从复制的作用主要体现在数据冗余、故障恢复、负载均衡等方面。可很多时候，我们讲分布式，并不是说简单的复制就好啦！相信大家都听说过，水平扩展和垂直扩展这两个概念，特别是数据库的水平扩展，它天然地和分片(Sharding)联系在一起，这意味是我们希望在不同地数据库/表里存储不同地数据。此前，博主曾在 《浅议 EF Core 分库分表及多租户架构的实现》 一文里介绍过数据库的分库/表，作为类比，我们可以归纳出 Redis 集群模式的第一个特点，即：它本质上是一种服务器 Sharding 技术。因为纯粹的主从复制意味着，每台 Redis 服务器都存储相同的数据，显然这造成了资源的浪费，而让每台 Redis 服务器存储不同的数据，这就是 Redis 的集群模式。如下图所示，Redis 集群模式呈现出的一种网状结构，完全不同于主从复制间的单向流动：\nRedis 集群模式示意图\r从图中可以看出，6 台服务器组成了一个网状结构，任意两台服务器间都可以相互通信。也许，大家会好奇一个问题，为什么这里博主就画了 6 台服务器？其实，这一切都是有迹可循的，因为 Redis 官方规定：一个集群中至少需要有 3 个主服务器(Master)。所以，一个 Redis 集群至少需要 6 台服务器。如果从这个角度来审视集群的定义的话，你可以认为 Redis 集群就是由多个主从复制一起对外提供服务。此时，集群中的节点都通过 TCP 连接和一个被称为 Cluster Bus 的二进制协议来建立通信，这里的 Cluster Bus 你可以将其理解为 Kafka 或者 RabbitMQ 这样的支持“发布-订阅”(Pub-Sub)机制的东西，换句话说，集群中的每个节点都可以通过 Cluster Bus 与集群中的其它节点连接起来。节点们使用一种叫做 Gossip 的消息协议，据说，这是一种从瘟疫和社交网站上获得灵感消息传播方式。“六度分割”理论告诉我们，最多通过 6 个人你就能认识任何一个陌生人，同样地，最多通过 6 个节点你就可以把消息传递给任何一个节点。\nRedis 集群模式与主从模式的联系\r目前，使用 Gossip 这一协议的项目有 Redis Cluster、Consul、Apache Cassandra 等等, 关于这个协议以及“六度分割”理论更进一步的细节，大家可以通过搜索引擎来获取，对我们而言，我们只需要知道，Redis 集群需要借助这个协议来实现诸如发现新节点、发送 PING 包、发送集群消息这些功能，因为每个节点除了存储自身的数据以外，还需要记录集群的状态，需要知道哪些节点不可用，需要合适的时机推选出主节点。我们在主从复制这一篇文章中提到过故障恢复，这个概念在这里同样适用，如图所示，S1 是 M1 的从节点，它本身并不是为了扩展请求的并发量而存在的，它需要在主节点宕机的情况下能被提拔为主节点，所以，它主要起一个数据备份的作用。因此，Redis 集群中的读和写，实际上都是在M1、M2 和 M3 上进行的。那么，我们不妨来想这样一个问题，如果 M1 和 S1 都宕机了，此时 Redis 集群还能不能正常工作呢？答案是否定的，因为此时显然不满足最少 3 个主节点的要求。\n一致性哈希算法 OK，我们知道，在面对数据库的水平拆分问题时，一个最为关键的问题是路由，换句话说，我怎么样可以找到到对应的库或者表。常见的思路有范围、哈希和配置等等，而在 Redis 的集群模式下，我们同样会面临这个问题，更一般地，任何需要负载均衡的场景都需要考虑这个问题，譬如，我们对多台服务器的 IP 地址求余，然后按照余数来进行路由，抑或者是按照不同的权重进行随机等等，这看起来像回到了熟悉的负载均衡算法，对吗？事实上，一般的负载均衡算法都会选择哈希算法，而哈希算法本质上就是一种散列函数，它可以把任意长度的输入转化为确定的、固定长度的输出，以最常见的求余为例，它具体是怎么工作的呢？我们来一起看下面的图：\n经典哈希算法\r可以注意到，首先，我们需要对 Key 计算出一个 hash 值，你可以理解为编程语言里的GetHashCode()方法，主要目的是将字符串类型的 Key 转化为整数型方便计算。接下来，按照主节点的数量来进行求余运算(统称为散列运算)，例如，这里我们有 3 台主节点服务器，故而在上面的图示中 n 应该等于 3，而这就是经典的哈希算法啦！那么，Redis 的集群有没有采用这个算法呢？答案还是否定的，因为这个方案里最大的变数就是 n。具体来讲，如果你想要扩容，那么 n 会变大，此时 Key 与服务器间的映射关系会被打破，即使不扩容，一旦某台服务器宕机，Key 与服务器间的映射关系还是会被打破，如果出现散列“碰撞”，无疑会让问题变得更复杂。\n经典哈希算法“碰撞”问题\r可以注意到，当从对 3 取模变成对 2 取模以后，必然会出现两个 1，这就是所谓的散列“碰撞”。所以，对于经典的哈希算法而言，它无法抵消因为 n 的变化而带来的重新 hash 的问题。为了解决这个问题，业界普遍使用的是一致性哈希算法，相比经典的哈希算法，最大的变化在于它将 n 的取值固定下来，即按 2^32 取模，此时，结果会落在 0 到 2^32 - 1 这个区间内，一旦我们将这个区间抽象为一个圆环，利用 CRC16 算法计算出来的值就会落在圆环上的某个地方：\n改良后的哈希一致性算法\r如图所示，假设我们有 A、B、C 三个 Redis 节点，它们均按照顺时针方向排列在整个圆环上，当我们对每一个 Redis 节点进行 hash() 运算以后按 2^32 取模，此时，这个值依然会落在 0 到 2^32 - 1 这个区间内，以此类推，理论上对于任意一台服务器 X，我们总能找到对应的一个值，当该值介于区间(m, n)中，表示它分配在节点 n 上，你可以认为，每个服务器节点，它负责的并不是哈希环上的一个点，而是一个范围。基于这种特性，当某一台 Redis 服务器宕机时，对应的这个区间会变大，相当于流量从这个节点转移到了它的下一个节点，这意味着我们只需要移动一部分数据。同理，如果要增加节点，只需要把移动该节点到它上一个节点间的数据。由此可见，它对原来数据的影响非常小，而这就是一致性哈希算法。\nRedis 哈希槽 截止到现在，我们知道了，一致性哈希算法相对经典哈希算法的优势，可这个算法真的没有问题吗？我们应该会想到，节点在哈希环上的分布是不均匀的，这意味着，每个节点上对应的 Key 的数量各不相同，更进一步，我们可以说，每台 Redis 上存储的数据量不同，这样，原本期待着负载均衡的我们，此刻被光速地打脸。因此，在实际应用中，为了让哈希环分布更均匀一点，会设置所谓的“虚拟节点”。不过，就算你考虑到了这种地步，Redis 还是没有采用这种哈希一致性的方案，你说这是不是有点气人？事实上，它采用的是一种被称为哈希槽的方案：\nRedis 哈希槽示意图\r如图所示，Redis 集群中总共有 16384 个槽位，它是怎么算出来的呢？因为 CRC16 算法产生的 hash 值有 16 个 bit，因此，它可以产生 2^16 即 65536 个值，理论上我们完全可以分配 65536 个槽位，不过考虑到网络带宽、节点数目，作者觉得 16384 个槽位完全够用了，前提是你集群内的节点不超过 1000 个！是不是听起来觉得非常离谱？可有些时候，工程上的事情，还真不能按科学一板一眼的来，不然，你又怎么会见到各种各样的“敏捷开发”、”项目管理”呢？\nRedis 哈希槽槽位分配\rOK，对于 Redis 中的每一个 Key，它经过计算以后会落到某个具体的槽位内，至于槽位会路由到哪个机器上，这完全取决于每台 Redis 服务器的配置，硬盘大就多分一点，硬盘小就少分一点，“能者多劳”，听起来就很科学，对吧！所以，从这个角度来看，相比哈希环，哈希槽可以更加灵活地控制槽位的分布。可世间万物，无一不是双刃剑，你选择了自由，那么需要操心的事情就变多了，至少 Redis 集群不会帮你转移和分配槽位；你选择了秩序，一切井井有条，那么你内心又会期盼自由。“小而美”，想要面面俱到；“大而全”，想要细致入微，难怪刺客组织要和圣殿骑士间的斗争永无止息啊，哈哈……因此，Redis 集群的高可用，实际上非常依赖主从节点的主从复制和故障切换，我承认，我点题了，你发现了吗？\nRedis 集群实战 好了，和上一篇一样，我们先礼后兵，等所有的理论知识都讲完了，再来着眼实际的过程，因为在博主看来，这是一个互相印证的过程，写流水账、记下每一步的操作步骤，这固然可以让你快速上手，但你真的只愿意到此为止吗？博主以前学过 CAD，近来又开始学习 Blender，一个深刻的感受就是，这些软件都需要大量的重复练习，甚至于形成肌肉记忆。可是我们学习知识的过程，是一个反复提炼、内化的过程，我们并不是为了成为某种“熟练工”，我发现有很多朋友，特别喜欢问某软件/工具怎么安装/配置类似的问题，虽然说这是技术人员日常生活里的一部分，但我觉得这不应该成为你特别关注的问题，所以，如果你发现我越来越不像是在写教程，恭喜你，发现了我的改变，谢谢你啊！回到正题，我们还是准备一个最小的集群，即“三主三从”的一个集群，依然使用 docker-compose 进行服务编排。首先，准备下面的目录结构：\nClusters |-- docker-compose.yaml |-- redis-1 | |-- redis.conf |-- redis-2 | |-- redis.conf |-- redis-3 | |-- redis.conf |-- redis-4 | |-- redis.conf |-- redis-5 | |-- redis.conf |-- redis-6 | |-- redis.conf 此时，针对集群模式的 Redis 配置文件，相比主从复制模式要精简许多，以其中一台服务器7001为例：\n# 端口号\rport 7001\r# 启用集群\rcluster-enabled yes\rcluster-config-file nodes_7001.conf\rcluster-node-timeout 5000\rappendonly yes 照葫芦画瓢，我们准备好剩下的7002、7003、7004、7005、7006 这 5台服务器即可。接下来，我们准备服务编排文件docker-compose.yaml:\nversion: \u0026#39;3.1\u0026#39; services: redis1: image: redis:latest container_name: redis-1 restart: always network_mode: \u0026#34;host\u0026#34; volumes: - ./redis-1/redis.conf:/usr/local/etc/redis/redis.conf command: [\u0026#34;redis-server\u0026#34;, \u0026#34;/usr/local/etc/redis/redis.conf\u0026#34;] redis2: image: redis:latest container_name: redis-2 restart: always network_mode: \u0026#34;host\u0026#34; volumes: - ./redis-2/redis.conf:/usr/local/etc/redis/redis.conf command: [\u0026#34;redis-server\u0026#34;, \u0026#34;/usr/local/etc/redis/redis.conf\u0026#34;] redis3: image: redis:latest container_name: redis-3 restart: always network_mode: \u0026#34;host\u0026#34; volumes: - ./redis-3/redis.conf:/usr/local/etc/redis/redis.conf command: [\u0026#34;redis-server\u0026#34;, \u0026#34;/usr/local/etc/redis/redis.conf\u0026#34;] redis4: image: redis:latest container_name: redis-4 restart: always network_mode: \u0026#34;host\u0026#34; volumes: - ./redis-4/redis.conf:/usr/local/etc/redis/redis.conf command: [\u0026#34;redis-server\u0026#34;, \u0026#34;/usr/local/etc/redis/redis.conf\u0026#34;] redis5: image: redis:latest container_name: redis-5 restart: always network_mode: \u0026#34;host\u0026#34; volumes: - ./redis-5/redis.conf:/usr/local/etc/redis/redis.conf command: [\u0026#34;redis-server\u0026#34;, \u0026#34;/usr/local/etc/redis/redis.conf\u0026#34;] redis6: image: redis:latest container_name: redis-6 restart: always network_mode: \u0026#34;host\u0026#34; volumes: - ./redis-6/redis.conf:/usr/local/etc/redis/redis.conf command: [\u0026#34;redis-server\u0026#34;, \u0026#34;/usr/local/etc/redis/redis.conf\u0026#34;] 接下来，我们运行 docker-compose up命令，就可以看到 6 台 Redis 服务器都运行起来，此时，我们需要选择任意一台 Redis 服务器对应的容器内，执行下列命令：\nredis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 \\ 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 \\ --cluster-replicas 1 该命令表示创建一个由 6 台服务器组成的集群，其中，--cluster-replicas 参数表示每个主节点分配一个从节点。此时此刻，当我们按下回车后，就可以看到下面的画面：\n通过 redis-cli 创建 Redis 集群\r可以注意到，当前集群有 3 台主服务器节点：7001、7002 和 7003，有 3 台从服务器节点：7004、7005 和 7006，这符合我们一开始设想的“三主三从”，Redis 内部会为每个节点分配一个唯一的 Id，这个 Id 会和每个节点的 IP 地址、端口号一起保存下来。当我们没有显式地分配每台服务器对应多少个槽位时，它会按照主节点的数量平均地分割。在这里我们只有 3 台主节点服务器，无法被 16384 整除，所以，7002 这个服务器会多分配一个槽位。显然，Redis 需要我们来确定这个集群划分，此时，我们输入yes即可，这样我们就完成了 Redis 集群的搭建。接下来，我们通过 redis-cli 写入信息来看看集群实际的运行效果：\n# 进入 Redis 容器 docker exec -it df5cae3c6004 sh # 连接 7001 服务器 redis-cli -p 7001 -c # 写入一个 key 127.0.0.1:7001\u0026gt; set name yuanpei 我们会发现一个现象，就是 Redis 会告诉你这里有一次重定向，如图所示，它从 7001 重定向到了 7002，这是因为按照 CRC16 算法计算出的结果，name 这个键应该被分配到 7002 上存储：\n通过 redis-cli 向 Redis 集群写入值\r同理，在我们已经知道 name 这个键存储在 7002上的情况下，我们依然可以从集群中的其它节点上读取到这个值：\n通过 redis-cli 向 Redis 集群读出值\r可以注意到，集群依然可以自动地从 7004 重定向到 7002，这表明，我们可以从集群中的任何一个节点上查询数据，并且每个节点上都存储着不同的数据，这可不就是数据分片吗？这恰恰印证了我一开始的观点，即：Redis 的集群模式，本质上是一种服务器 Sharding 技术。果然，博主诚不欺人也，哈哈！\nRedis 集群投票示意图\r到目前为止，我们已经搭建出了一个基本的 Redis 集群，你可能会问，如果集群中某个节点发生宕机该怎么办呢？此时，我们要注意区分，这是主观宕机还是客观宕机。什么是主观宕机呢？就是某个节点认为你挂了；什么是客观宕机呢？就是集群内超过半数的节点认为你挂了。事实上，只有客观宕机会触发故障转移，而所谓的故障转移，其实就是从故障节点的从节点中挑选一个出来继续提供服务，此时，需要有超过半数以上的主节点给这个从节点投票，这样，Redis 集群就会让它当选为新的主节点，谁能想到，在以绝对理性著称的技术世界里，居然会有如此民主而科学的做法呢？可少数服从多数，真的就是对的吗？\n本文小结 坦白讲，这篇文章写起来非常费劲，因为它里面关联的知识点非常密集，从经典哈希算法到一致性哈希算法，再到 Redis 集群里的哈希槽，这是一个循序渐进的认知过程，我们由此认识到，Redis 的集群模式，本质上是由多个主从复制模式组成的服务器分片技术，它通过哈希槽来管理集群内的节点和数据，而节点间则是通过 TCP 协议互相通信，这使得我们可以从任意节点上查询数据，因为 Redis 集群会帮助我们实现连接的重定向。与此同时，Redis 集群内通过投票来实现故障转移，其触发机制是超过半数的节点认为该节点宕机，即客观宕机，此时，就需要从该节点的从节点中筛选一个继续提供服务，当然，这个筛选还是投票决定的，谁能说这不是一种现实世界的投影呢？好了，以上就是这篇博客的全部内容啦，关于 Gossip 协议等更细节的东西，留到以后有机会再写吧\u0026hellip;..！\n","date":"2021-12-01T09:58:59Z","image":"/posts/1213387651/Redis-Clusters-Hash-Advance.drawio.png","permalink":"https://qinyuanpei.github.io/posts/1213387651/","slug":"1213387651","tags":["分布式","集群","Redis"],"title":"分布式丛林探险系列之 Redis 集群模式"},{"categories":["生活感悟"],"content":"\r某个冬天的清晨\r突然间想要写点什么，或许是我终于发觉，一个人内心的疲惫感，是会像水里泛起的涟漪一样，一圈圈地向远方散开去，直至在某个时刻产生了共振，这种感觉就仿佛是，冬天的清晨，于雾气中升起的太阳，虽然无法令你感受到炽热，可依然会令你感到刺目甚至眩晕，圣人有云，“劳心者治人，劳力者治于人”，而这种劳心又劳力的状态，属实是一个普通打工人，经常要去面对的一件事情，因此，我想要说的，本质上是一种薛定谔的状态，它代表着温暖，代表着光明，可以给你生存所需要的一切，可于此同时，它又代表着炽热，代表着煎熬，可以毫无顾忌地灼伤你裸露的心。\n就像一下子击中内心似的，我渐渐意识到，这个世界上的工作，本质上只有两种，一种是问题本身特别难，需要更多的心智上的投入，譬如数学、计算机图形学、物理碰撞等等，一个有意思的事情是，国产单机游戏《古剑奇谭3》和《仙剑奇侠传7》，男主的剑都是直接“浮空”地背在人物身上的，甚至都没有剑鞘，而《巫师3》据说光是一个拔剑/收剑就借助了相当多的技术手段，这其中最大的问题在于穿模，尤其是国产游戏都喜欢给武器加各种发光特效，很多时候为了避免穿模的尴尬，不得不在刀/剑入鞘以后隐藏刀/剑刃这部分的模型，所以，这一类问题有时候属于“看起来简单，实现起来复杂”，联想到曾经有产品经理和程序员因为需求而大打出手的新闻，你就不难理解，人们对于难易程度的认知差异，其实是非常大的，原因就在于，人们无法确定这到底属于那一类问题，有人关注整体，就有人关注局部，就像人的眼睛，视锥体永远有一个范围，这无疑让人类永远只能看到一部分，这似乎会成为某种宿命，本身就很难的问题，大概就是无论从整体还是局部来看，难度都没有变化的问题。此前，有同事要计算客户的分层活跃度，我虽然知道这是一个关乎数学期望的问题，可因为我早就忘记了怎么去算概率密度函数，所以，这个问题我只知道方向而无从下手，换句话说，本身就很难的问题，并不会因为场景的转换而发生变化。\n而相对地，第二种是问题本身不难，但因为流程、方案、组织等因素而变得复杂，不幸的是，我们在生活和工作中遇到的都是这种情况。我们总安慰自己说，买菜做饭用不到高等数学，其实，从某个角度来看，是因为作为普通人的你我，完全没有机会接触到这种“难题”。当然，我并不否认，这种因为复杂度提升而带来的挑战性。联想到最近网上报道的甘肃大爷花132万改造房子的新闻，我终于意识到，工程上的问题是无所谓科学与否的，因为无论装修工艺如何演进，技术与艺术的鸿沟始终存在。软件行业更是类似，不论我们使用多么先进的技术或是框架，多年来羁绊不前的其实是需求分析。我始终不明白，人可以靠着感觉去做一件事情，就好像设计师陶磊可以用傲慢和偏见去替老人做出选择一样，而我们这个行业可以在需求没落实的情况下仓促开发。如果说这两者间有什么本质上的不同，大概摸不着的软件和代码可以永无止境地修改吧！那么，当大爷的红砖毛坯房墙体开始反碱的时候，这是肉眼看得见的崩塌，可代码世界里的崩塌，又有谁会在乎呢？我们又太多太多的难度，完全是自己构建出来的，也许，这样能让大家看起来都在忙碌吧……\n熟悉我的人，都知道我是一个怕麻烦的人，这种惰性，一旦体现在工作上，就表现为对繁琐、重复的厌倦，厌倦含糊不清的规则，厌倦似是而非的表达。每次工作上别人找我问问题，我更希望，对方可以用一句话说清楚他的疑虑，而不是每次都挥手示意我“过来”，我以为这是某种表达能力的欠缺，可很多时候，尤其是研发工作中的来来回回，这种沟通更像是一种“剧本杀”，每个人都利用手中的信息来拼凑出一个故事，然后互相从对方的故事中寻找破绽。这种感觉有时候会让我觉得麻烦，是不是隐隐约约有什么东西再让事情变得复杂起来，虽然我确信，熵增定律一定会让事物的复杂度变高，可假如这一切可以避免呢？我们都听过一个词——内耗，譬如，最典型的想太多、自我否定等等，一个人精神层面的自我内耗会让整个人都变得焦灼起来，而这种问题，如果反映到一个团队或者组织中，大概就是将错就错，“磨刀不误砍柴工”，可如果所有人都宁愿拿着一把生锈的刀去砍柴呢？有的人能忍受这样一把生锈的刀，有的人一定要用瑞士军刀才有生产力，而这就是分歧。\n我一直觉得，我是那个站在技术与人文十字路口的那个人，甚至我能将这种东西完美融合在一起，毕竟，对于一个19岁前写诗的程序员来说，本质上都属于输入/输出的一个过程。可多年以后，我渐渐发现，我是一个重视实用性超过艺术性的人，也许，是技术给了我这种实用主义思维，当我发现我通过 Blender 建模无法调出甜甜圈的颜色的时候，我隐隐约约地从材质编辑器中找到了一点当年写 Shader 时的感觉，我终于理解了当初关于 CSS 不正交的说法，那就是，当一件事情有多种方式可以做到的时候，对于普通人而言，困难已不再是实现这件事情，而是做出选择，装修这项工程是这样，日常的研发工作还是这样，甚至人生都是这样，我们从来到这个世界的那一天起，无数种世界观、人生观、价值观开始交织和缠绕，我们清楚地知道人生的结局是什么，以至于这个真相都不在重要，毕竟，我们每天经历的事情，一直都在教你做出选择，以及不断合理化这个选择，人们常说，选择比努力更重要，那么，聪明的你，你说，哪一件事情更难？\n","date":"2021-11-26T08:48:41Z","image":"/posts/2145169599/冬阳.jpg","permalink":"https://qinyuanpei.github.io/posts/2145169599/","slug":"2145169599","tags":["随笔","感悟","思考"],"title":"写在冬阳升起以前"},{"categories":["编程语言"],"content":"如果说，单体架构系统是坐在家里悠闲地喝着下午茶，那么，毫无疑问，分布式系统将会是一场永远充满惊喜的丛林冒险。从踏上这条旅程的那一刻起，此间种种都被打上分布式的烙印，譬如分布式锁、分布式事务、分布式存储、分布式配置等等，这些词汇拆开来看，“似曾相识燕归来”，每一个我都认识，而一旦放到分布式的场景中，一切就突然变得陌生起来，从过去的经典三层架构、到时下流行的微服务、再到更为前沿的服务网格，一路跌跌撞撞地走过来，大概只有眼花缭乱和目不暇接了。前段时间在做 FakeRpc，这是一个基于 ASP.NET Core 的轻量级 RPC 框架，其间接触了 ZooKeeper、Nacos，后来工作中又接触到了 Kafka、Saga，虽然这些都是不同领域里的分布式解决方案，但是我隐隐觉得它们之间有某种内在的联系，就像所有的分布式系统都存在选举 Leader 的协调算法一样。于是，“喜新厌旧”的双子座，决定新开一个专栏，既然分布式系统是一场永远充满惊喜的丛林冒险，那么，这个专栏就叫做 「分布式丛林冒险系列」好了。一切该从哪里开始呢？我想，还是从 Redis 开始，今天这篇文章，我们来聊一聊 Redis 里的主从复制。\n主从复制概述 从某种意义上来讲，主从复制并不是一个新的概念，因为此前博主介绍过数据库里的主从复制，在 利用 MySQL 的 Binlog 实现数据同步与订阅(上)：基础篇 这篇文章中，博主和大家分享过利用数据库 Binlog 实现数据同步的方案，而 Binlog 正是实现数据库主从复制的重要机制之一，甚至在更多的时候，我们更喜欢换一种说法，即 读写分离。和数据库类似，Redis 中的主从复制，其实，就是指将一台 Redis 服务器中的数据，复制到其它 Redis 服务器。其中，前者被称为主节点(Master)，后者被称为从节点(Slave)，通常情况下，每一台 Redis 服务器都是主节点，一个主节点可以有多个从节点，而一个从节点只能有一个主节点，并且数据只能从主节点单向流向从节点，如下图所示：\nRedis 主从复制示意图\r虽然 Redis 在缓存上的应用做到了家喻户晓的地步，可这并不代表我们能真正得用好 Redis，譬如，博主的上一家公司，基本上没有用到 Redis 的高可用，最多就是一主一从这样的搭配。所以，当时公司里很多人都知道哨兵、集群这些概念，而真正搭过环境的人则是寥寥无几，这正是博主要写这个系列的原因之一。那么，从实用性的角度来看，Redis 的主从复制有哪些实际的作用呢？个人认为，主要有以下几点：\n数据冗余：主从复制相当于实现了数据的热备份，是除了数据持久化以外的一种数据冗余方案。 故障恢复：主从复制相当于一种灾备措施，当主节点主线故障的时候，可以暂时由从节点来提供服务。 负载均衡：主从复制搭配读写分离，可以分担主节点的负载压力，在“读多于写”的场景中，可以显著提高并发量。 高可用：主从复制是高可用的基础，无论是集群模式还是哨兵模式，都建立在主从复制的基础上。 相信大家都听过 CAP 定理，这是分布式系统中的重要理论之一，其基本思想是，一致性(Consistence)、可用性(Availability) 和 分区容忍性(Partition Tolerance)，最多只能同时实现两点，而无法做到三者兼顾，如下图所示：\nCAP 理论\r事实上，对分布式系统的设计而言，本质上就是“鱼和熊掌不可兼得”，关键看你想要做出一个怎么样的选择。例如，同样是注册中心，ZooKeeper、etcd 以及 Consul 都选择了 CP，而 Euraka 则选择了 AP。对于 Redis 而言，单机版的 Redis 可以看作是 CP，因为它牺牲了 A，即可用性。而集群化的 Redis，则可以看作是 AP，通过自动分片和数据冗余，来换取可用性。这其实印证了我们一开始的观点，为什么我们需要 Redis 的主从复制、集群、哨兵这些东西呢？本质上还是为了提高 Redis 的可用性。可能有朋友会问，难道一致性在 Redis 里就不重要了吗？我想，这要从 Redis 主从复制的原理说起。\n主从复制原理 首先，我们要明确一点，Redis 里的主从复制是异步的，这样就回到了一个老生常谈的话题，即：实时一致性 和 最终一致性，显然，在 AP 的场景下，最终一致性这种“弱一致性”实现起来要更容易一点，因为实时一致性这种“强一致性”的方案，意味着所有人都要在这个时间点停下来，等实现一致性以后再继续进行下面的工作。我们甚至都不用钻研那些高深莫测的分布式理论，单单从日常生活的角度来切入，你一定会觉得这样子做事情效率低到爆炸，所以，追求实时一致性并不是说不可取，而是感觉这样有一点得不偿失，难道你要为了一致性而牺牲可用性吗？博主曾经接触过一个基于 Ignite 构建的缓存组件，对方声称数据的一致性比可用性更重要。所以，作为一个被很多项目依赖的基础设施，虽然隔三差五地出各种问题，可大家竟然能一直容忍下去，可能这就是爱吧。直到后来引进 Saga 做分布式事务，我意识到这是通过柔性事务来实现最终一致性，而如此前后矛盾的做法，只能成为此刻用来调侃的谈资，其实人更是如此，世间的一切你都可以去追逐，可你终将会失去它，这大概是人生的最终一致性。\n好了，言归正传，事实上，Redis 的主从复制可以分为连接建立、数据同步、命令传播三个阶段，下面我们来分别讲解各个部分的相关细节。\n连接建立阶段 连接建立阶段，主要目的是主从双方建立 Socket 连接，此时，双方都需要知道对方的 IP 地址和端口号，其基本交互流程如下图所示：\nRedis 主从复制：连接建立阶段\r可以注意到，在连接建立阶段，首先，由从节点发出指令slaveof \u0026lt;IP\u0026gt; \u0026lt;Port\u0026gt;，这里的 IP 地址和端口号都是指主节点的 IP 地址和端口号。事实上，该指令还可以有下面两种形式，即：\n服务器启动参数：在redis-server命令后附加参数，即 redis-server -slaveof \u0026lt;masterip\u0026gt; \u0026lt;masterport\u0026gt; 服务器配置文件：在 redis.conf 文件中配置主节点信息，即 slaveof \u0026lt;masterip\u0026gt; \u0026lt;masterport\u0026gt; 接下来，一旦主节点收到了该指令，就会对从节点做出响应，此时，从节点就可以获得主节点的 IP 地址和端口号，在 redis-cli 环境下，我们可以通过 info Replication 来验证这个观点。如图所示，是某个 Redis 从节点中存储的主节点信息：\nRedis 从节点中存储的主节点信息\r此时，从节点和主节点会打一场“乒乓球”，从节点会定期发送PING指令，此时，如果主节点返回了PONG指令，则表示连接到主节点的 Socket 可用，你可以将其理解为一种健康检查或者心跳机制，目的是确定主节点还“活”着，否则，作为客户端的从节点就会断开 Socket 连接并尝试重连。如果主节点要求提供密码，那么从节点还需要发送以下指令：auth password，这个时候主节点会对从节点提供的身份信息进行验证，一旦验证通过，从节点就会开始监听主节点的端口，与此同时，主节点会保存从节点的 IP 地址和端口号。同样地，我们可以在主节点中通过 info Replication 来验证这个观点：\nRedis 主节点中存储的从节点信息\r至此，主从双方的互相“试探”结束，双方正式建立连接，是为连接建立阶段。\n数据同步阶段 如果我们把建立连接看作是两个人“握手”，也许，你的脑海中此刻会浮现出诺基亚的经典开机画面，毫无疑问，下面两个人的“会话”才是真正的重头戏。于是，我们来到了数据同步阶段，这一阶段的主要目的是，完成从节点的数据初始化。在连接建立阶段，从节点是主节点的客户端；而到了这一阶段以及命令传播阶段，双方互为彼此的客户端，因为，此时主节点需要主动向从节点发送命令。按照主从节点的状态不同，可以分为：全量复制 和 部分复制。\n如果你接触过 Kafka，应该会有这样一种认知，即 Kafka 里面维护着一个始终追加的日志文件，而每一条消息则是这个日志文件中的一部分，Kafka 利用偏移量来定位某一条消息。在 Redis 的主从复制中，存在着类似的概念，它被称为：复制偏移量，事实上，参与复制的主从节点都会自身的复制偏移量，其中，主节点在处理完写入命令后，会将该命令的对应字节长度累加和记录，该信息可以在主节点的 info Replication 中的 master_repl_offset 字段上找到，我们还是用前面的图做例子来说明：\nRedis 主节点中存储的复制偏移量信息\r实际上，在数据同步阶段，主节点内部会维护一个固定长度的、先进先出(FIFO)的队列作为复制积压缓冲区，其默认大小为 1 MB，主节点在响应写命令的时候，不但会把命令发送给从节点，还会写入到复制缓冲区，如下图所示：\nRedis 复制缓冲区 和 复制偏移量累积\r这个复制缓存区的作用是保留主节点最近执行的写命令，因为它是一个先进先出的队列，所以，时间较早或者偏移量较大的命令会在一段时间后被挤出缓冲区，这样，我们就有了更进一步的结论，即：当主从节点 offset 的差距过大超过缓冲区长度时，将无法执行部分复制，只能执行全量复制。所以，选择全量复制还是部分复制，还是要有实际的使用场景来决定。主从配置第一次启用的时候，因为从库没有对应的复制偏移量，所以，第一次复制一定是全量复制，对于全量复制而言，其基本流程如下图所示：\nRedis 主从复制：全量复制\r而对于部分复制而言，其得以实施的前提是，复制偏移量之后的数据依然存在于复制缓冲区，那么，如何判断复制偏移量在不在复制缓冲区里呢？在 Redis 中主要是通过 PSYNC 命令来实现，请注意，在 Redis 2.8 之前的版本中，只有 SYNC 命令，而 PSYNC则是 Redis 2.8 版本之后推出的替代命令，它提供了 完整重同步 和 部分重同步 的功能，主要解决了老版本中断线以后重新复制带来的低效率问题。这里，我们以 PSYNC 命令为例来进行说明。事实上，无论主从节点，都会被分配一个唯一的 runid，所以，对于部分复制而言，实际上是从节点告诉主节点，它的 runid 和 offset，然后由主节点来判断这个 offset 是否在它的复制缓冲区里，如下图所示：\nRedis 主从复制：部分复制\r因为复制缓冲区中持久化了写入命令，所以此时我们只需要从复制缓冲区中找到对应区间的数据，发给从节点即可，在这种情况下，主节点不需要生成 RDB 快照，所以，部分复制的效率会比全量复制要高很多。以上就是 Redis 数据同步阶段的基本流程，其实在这这个过程中，我们还有很多问题没有说到，譬如 Redis 的两种持久化方案 RDB 和 AOF 应该如何去选择，全量复制过程中主要的性能损耗点，以及 从节点如何利用 RDB 快照更新数据等等，考虑到篇幅，这些话题等以后有机会了再专门来说吧！\n命令传播阶段 前面提到过，主节点内部会维护一个复制缓冲区，其主要作用是持久化最近执行的写命令，可是你把命令都放到这个复制缓冲区里了，那些从节点又怎么知道这个具体动作呢？实际上，这一工作是由命令传播程序来完成的，所以，命令传播阶段实际上就是指主节点在写入复制缓冲区以后，通知从节点的这个过程。下面是一个简单的示意图：\nRedis 主从复制：命令传播阶段\r我承认，这些偏理论的内容，不单单看起来挺费劲，我写起来更费劲，所以，我只能尽可能地去画这些流程图，这样能更好地帮助大家理解这些内容。其实，有很多的东西，你对它的理解，是一种潜移默化、层层递进的过程，就像这里的复制缓冲区，你压根说不上来，到底是 Kafka 帮助你理解了 Redis，还是 Redis 帮助你理解了 Kafka。有时候，我挺不理解人类变幻莫测的情感，那东西对我来说有时像一门玄学，显然，科学对我而言会更容易一点，那么，我不妨选择去相信科学、相信唯物主义。也许，屏幕前的你，会不置可否地批评我说“光说不练假把式”，好了，下面我们来看一个全量复制的简单实例。\n主从复制实战 这里，我们按照“一主两从”的方案来实施 Redis 的主从复制，前面提到过，我们有三种方式来配置主从复制。在这里，博主是使用第三种方式来进行配置的，如下图所示，准备三个文件夹及一个 docker-compose.yaml文件：\nMasterSlave |-- docker-compose.yaml |-- redis-1 | |-- redis.conf |-- redis-2 | |-- redis.conf |-- redis-3 | |-- redis.conf 其中，redis-1为主节点，对应端口号为：7001；redis-2 和 redis-3为从节点，对应端口号分别为：7002 和 7003。首先，主节点的配置项定义如下：\nbind 0.0.0.0\rprotected-mode no\rport 7001\rtimeout 5000\r# 900 秒内 1 个更改\rsave 900 1\r# 300 秒内 10 个更改\rsave 300 10\r# 60 秒内 10000 个更改\rsave 60 10000\rrdbcompression yes\rdbfilename dump.rdb\rdir /data\r# 开启 AOF 模式\rappendonly yes\r# 每秒一次 fsync\rappendfsync everysec\rrequirepass 12345678 接下来，对于redis-2 和 redis-3 这两个从节点而言，主要多了 slaveof 和 masterauth 这两行配置，这是因为我们在主节点中配置了密码：\nbind 0.0.0.0\rprotected-mode no\rport 7002\rtimeout 5000\rsave 900 1\rsave 300 10\rsave 60 10000\rrdbcompression yes\rdbfilename dump.rdb\rdir /data\rappendonly yes\rappendfsync everysec\rrequirepass 12345678\rslaveof 127.0.0.1 7001\rmasterauth 12345678 同理，可以对 redis-3 进行配置：\nbind 0.0.0.0\rprotected-mode no\rport 7003\rtimeout 5000\rsave 900 1\rsave 300 10\rsave 60 10000\rrdbcompression yes\rdbfilename dump.rdb\rdir /data\rappendonly yes\rappendfsync everysec\rrequirepass 12345678\rslaveof 127.0.0.1 7001\rmasterauth 12345678 以上配置文件，均可以在 Github 获取。接下来，我们会使用 Docker-Compose 进行服务编排：\nversion: \u0026#39;3.1\u0026#39; services: redis1: image: redis:latest container_name: redis-master restart: always network_mode: \u0026#34;host\u0026#34; volumes: - ./redis-1/redis.conf:/usr/local/etc/redis/redis.conf command: [\u0026#34;redis-server\u0026#34;, \u0026#34;/usr/local/etc/redis/redis.conf\u0026#34;] redis2: image: redis:latest container_name: redis-slave-1 restart: always network_mode: \u0026#34;host\u0026#34; volumes: - ./redis-2/redis.conf:/usr/local/etc/redis/redis.conf command: [\u0026#34;redis-server\u0026#34;, \u0026#34;/usr/local/etc/redis/redis.conf\u0026#34;] redis3: image: redis:latest container_name: redis-slave-2 restart: always network_mode: \u0026#34;host\u0026#34; volumes: - ./redis-3/redis.conf:/usr/local/etc/redis/redis.conf command: [\u0026#34;redis-server\u0026#34;, \u0026#34;/usr/local/etc/redis/redis.conf\u0026#34;] 此时，当我们执行docker-compose up命令，如下图所示，它就会自动建立主从关系，这意味着我们在主节点写入的值，均可以通过从节点来进行读取：\n通过 Docker-Compose 建立主从关系\r为了印证我们的想法，下面我们通过 Redis 的命令行工具redis-cli 来向主节点写入值，为此我们需要进入到容器内部并执行相应的命令。\n一主两从及对应的容器 ID # 进入主 Redis 容器 docker exec -it 8574d93eeaf4 sh # 通过 redis-cli 连接 master redis-cli -p 7001 -c # 身份认证 127.0.0.1:7001\u0026gt; auth 12345678 # 写入一个 key：name 127.0.0.1:7001\u0026gt; set name yuanpei 同样地，我们进入从节点，从 7002 和 7003 中任选一个即可：\n# 进入从 Redis 容器 $ docker exec -it 0b19237b7e58 sh # 通过 redis-cli 连接从节点 redis-cli -p 7002 -c # 身份认证 127.0.0.1:7002\u0026gt; auth 12345678 # 读取一个 key：name 127.0.0.1:7002\u0026gt; get name 可以注意到，我们从 7002 中读取出了 7001 中写入的值，这表明我们搭建的 Redis 主从复制起作用了，这样，如果主节点某一天遭遇不幸，这个时候从节点可以临时提供服务，而这恰好印证了我们一开始的观点，为什么需要主从复制呢？我们肉眼可见的效果就是数据冗余和故障恢复，而负载均衡和高可用更多的是一种战略上的考量，它完全取决于你所处的高度。截至到此刻，Redis 的主从复制，你学会了吗？\nRedis 主从复制效果展示\r本文小结 本文是 #分布式丛林探险系列# 的第一篇文章，主要分享了 Redis 主从复制模式的相关内容。首先，主从复制可以为 Redis 带来数据冗余、故障恢复、负载均衡以及高可用等方面的收益，单机版的 Redis 是一个符合 CP 的系统，而集群化的 Redis 则是一个符合 AP 的系统，其一致性正是由这篇文章中描述的复制来保证。根据主从节点状态的不同，Redis 中的主从复制，可以分为 全量复制 和 部分复制 两种，全量复制是主节点生成一个快照然后发送给从节点，而部分复制则是从复制缓冲区中筛选出命令然后发给从节点，在此基础上，我们用 Docker-Compose 构建了一个“一主两从”的主从复制方案。这个世界上没有 100% 完美的方案，Redis 的主从复制在实际使用中可能会遇到，诸如延迟与不一致、数据过期、故障切换等等的问题，特别是故障切换，虽然从节点可以在主节点挂了的时候临时顶上去，但这依赖于研发人员去切换项目中使用的连接字符串，如果希望更好的实现主从切换，可能还是需要大家去做进一步的工作，常言道，“实践出真知”，这篇文章充其量只能作为一个引子，更深入的话题、玩法，需要大家在实践中不断总结。好了，以上就是这篇文章的全部内容啦，如果大家对文章中的内容和观点有什么意见或者建议，欢迎大家在评论区留言，谢谢大家！\n","date":"2021-11-16T11:48:41Z","image":"/posts/1748863652/CAP-Theory.png","permalink":"https://qinyuanpei.github.io/posts/1748863652/","slug":"1748863652","tags":["分布式","Redis","主从复制"],"title":"分布式丛林探险系列之 Redis 主从复制模式"},{"categories":["数据分析"],"content":"突然间，十月以某种始料未及的方式结束了，也许是因为今年雨水变多的缘故，总觉得这个秋天过去得平平无奇，仿佛只有观音禅寺的满地银杏叶儿，真正地宣布着秋天的到来，直到看见朋友在朋友圈里借景抒怀，『 霜叶红于二月花 』，秋天终于没能迁就我的一厢情愿，我确信她真的来了。当然，秋天不单单会带来这些诗情画意的东西，更多的时候我们听到的是双十一、双十二，这些曾经由光棍节而催生出的营销活动，在过去的十多年间渐渐成为了一种文化现象，虽然我们的法定节日永远都只有那么几天，可这并不妨碍我们自己创造出无数的节日，从那一刻开始，每个节日都可以和购物产生联系，这种社会氛围让我们有了某种仪式感，比如，零点时为了抢购商品恨不得戳破屏幕。再比如，在复杂的满减、红包、优惠券算法中复习数学知识。可当时间节点来到 1202 年，你是否依然对剁手这件事情乐此不疲呢，在新一轮剁手行动开始前，让我们来试试通过 Python 预测一下今年的交易额，因为在这场狂欢过后，没有人会关心你买了什么，而那个朴实无华的数字，看起来总比真实的人类要生动得多。\n思路说明 其实，我一直觉得这个东西，完全不需要特意写一篇文章，因为用毕导的话说，这个东西我们在小学二年级就学过。相信只要我说出 最小二乘法 和 线性回归 这样两个关键词，各位就知道我在说什么了！博主从网上收集了从 2009 年至今历年双十一的交易额数据，如果我们将其绘制在二维坐标系内，就会得到一张散点图，而我们要做的事情，就是找到一条曲线或者方程，来对这些散点进行拟合，一旦我们确定了这样一条曲线或者方程，我们就可以预测某一年双十一的交易额。如图所示，是 2009 年至今历年双十一的交易额数据，在 Excel 中我们可以非常容易地得到对应的散点图：\n在 Excel 中绘制散点图\r如果有朋友做过化学或者生物实验，对接下来的事情应该不会感到陌生，通常我们会在这类图表中添加趋势线，由此得到一个公式，实际上这就是一个回归或者说拟合的过程，因为 Excel 内置了线性、指数型、对数型等多种曲线模型，所以，我们可以非常容易地切换到不同的曲线，而评估一个方程好坏与否的指标为 $R^2$，该值越接近 1 表示拟合效果越好，如图是博主在 Excel 中得到的一条拟合方程：\n在 Excel 中添加趋势线\r那么，在 Python 中我们如何实现类似的效果呢？答案是 scikit-learn，这是 Python 中一个常用的机器学习算法库，主要覆盖了以下功能：分类、回归、聚类、数据降维、模型选择 和 数据预处理，我们这里主要利用了回归这部分的 LinearRegression 类，顾名思义，它就是我们通常说的线性回归。事实上，这个线性并不是单指一元一次方程，因为我们还可以使用二次或者三次多项式，因为上面的 Excel 图表早已告诉我们，一元一次方程误差太大。\n实现过程 OK，具体是如何实现的呢？首先，我们从 CSV 文件中加载数据，这个非常简单，利用 Pandas 库中的 read_csv() 方法即可：\ndf = pd.read_csv(\u0026#39;./历年双十一交易额.csv\u0026#39;, index_col = 0) 接下来，我们使用下面的方法来获取 年份 和 交易额 这两列数据：\nyear = np.array(df.index.tolist()) sales = np.array(df.Sales.tolist()) 此时，我们可以非常容易地用 matplotlib 库绘制出对应的图表：\nplt.scatter(year, sales, marker=\u0026#39;o\u0026#39;) plt.xlabel(\u0026#39;年份\u0026#39;) plt.ylabel(\u0026#39;交易额(亿)\u0026#39;) plt.title(\u0026#39;历年双十一交易额变化趋势\u0026#39;) 通过 matplotlib 绘制散点图\r接下来，我们来看看如何对这组数据做线性回归，以一元一次方程为例：\nX = (year - 2008).reshape(-1, 1) X = np.concatenate([X], axis= -1) Y = sales lr = LinearRegression() lr.fit(X, Y) print(\u0026#39;方程系数：\u0026#39;, lr.coef_) print(\u0026#39;方程截距：\u0026#39;,lr.intercept_) 这里，首先我们需要把 2009 到 2020 这个范围内的年份转化为更方便计算的 1、2、3 \u0026hellip;，所以，每一个数都减去了 2008，这样我们就得到了横坐标 X 以及 纵坐标 Y。接下来，我们只需要将其传入 fit() 方法即可。有时候，你可能会看到下面这样的代码，即用于划分 训练集 和 测试集 的的代码片段：\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0) 考虑到我们这里的样本数据本来就不多，毕竟，这股消费主义的风潮在中国刚刚流行了十二年，所以，我们要珍惜这份福报，就不划分 训练集 和 测试集 了(逃。此时，我们会得到两个重要的参数，即方程系数 lr.coef_ 和 方程截取 lr.intercept_，而一旦有了这两个参数，我们就能确定一个线性方程，即传说中的 $y=ax+b$ :\nf = lambda x: lr.coef_[0] * x + lr.intercept_ print(\u0026#39;2021年交易额预测：\u0026#39;, f(13)) 此时，f(13) 就是 2021 年双十一的交易额，显然，这是一个预测值，大概是 3592 亿，这属实是有点离谱啦，都没能超过 2020 年的 4982 亿，我们前面提到过 $R^2$，在 scikit-learn 中我们使用下面的方法来计算它：\nprint(\u0026#39;R^2：\u0026#39;, lr.score(X_test, Y_test)) 没错，score() 函数需要这样一组测试数据，我们利用前面划分 训练集 和 测试集 的方法就可以得到它。我们发现，这个值大概是 0.73，这要不离谱都说不过去，这一点我们从图上就可以看出来：\n历年双十一交易额变化趋势预测：一次方程\r这样显然不行啊，那怎么办呢？遥想古人利用“割圆术”无限逼近圆形进而计算出圆周率的这种执着，我们是不是可以通过提高“次数”来解决欠拟合的问题呢？换句话说，我们来试试一元二次方程，此时，我们只需要调整下横坐标 X 的构造过程：\nX = (year - 2008).reshape(-1, 1) X = np.concatenate([X**2, X], axis= -1) 简单来说，如果你需要构造一个 N 次多项式，那么，只需要由高到底依次写出来即可，所以，你可以想象得到，对于三次多项式，我们可以这样构造：\nX = (year - 2008).reshape(-1, 1) X = np.concatenate([X**3, X**2, X], axis= -1) 对于二次多项式，我们可以得到下面的结果，其 $R^2$ 为 0.92，预测 2021 年双十一交易额为 5218 亿：\n历年双十一交易额变化趋势预测：二次方程\r同理，对于三次多项式，我们可以得到下面的结果，其 $R^2$ 为 0.96，预测 2021 年双十一交易额为 6213 亿：\n历年双十一交易额变化趋势预测：三次方程\r整体上看，三次方程的拟合效果更好一点，达到了 0.96，但这个和大学时候做实验追求的 0.99 相比，多少有一点尴尬，屏幕前的你，是否想到了更好的曲线模型呢，我们常常说某某行业取得了指数级的增长，难道说指数模型更适合这个问题？关于这一点博主没能去亲自验证，因为看起来在 scikit-learn 中没有指数型函数的计算模型，可当博主在 Excel 中添加了指数型的趋势线以后，发现结果有点出人意料，因为它的 $R^2$ 只有 0.8314，而按这个方程计算出的交易额则约为2万多亿，好家伙，这都赶上恒大目前拖欠的债务了，想想就觉得离谱，指数型函数对应的曲线如下：\n历年双十一交易额变化趋势预测：指数函数\r此时此刻，坐在屏幕前的你，更倾向于选择那种计算模型呢，个人觉得三次方程更好一点，因为它的 $R^2$ 是目前最好的，当然，scikit-learn 还有类似可解释的方差分数、最大误差、平均绝对误差、均方误差、均方误差对数等等不同的指标，大家可以参考 官方文档 来做更进一步的探索，一切都只有等到下个月双十一过后方能揭晓，就让我们拭目以待！本文中使用的源代码，已更新至 Github 供大家参考，如果大家对博客中的内容有什么意见或者建议，欢迎大家在评论区积极留言，谢谢大家！\n本文小结 恍惚中惊觉，十月份就这样结束了，这意味是 2021 年只剩下 60 多天。上了年纪的人总不免开始怀念，那些偷偷从树叶间隙里溜走的时光，去年的这个时候我在做些什么呢？大概刚刚追完「半泽直树」第二部吧！在疫情愈发常态化的日子里，人对于未来总有种难以言说的失控感，某君有言道，『不知道明天和意外哪一个先来』，虽然机器学习可以通过“训练”过去达到“预测”未来的目的，可人生并没有哪一个方程可以完美拟合，虽然这个世界早已为你规划好一条叫做“随大流”的路，一个人的过去、当下就像无数个离散的点，这些点构成了我们人生的轨迹，某种我们通常称之为回忆的东西，可这些点是否会向着预期的方向延伸，也许，永远没有人会知道答案，这个世界上有相当多的数字，譬如自然对数、圆周率、黄金分割比等等，都是前辈先哲从自然世界中寻找到的，可对于我们的人生而言，并没有这样明显的答案，所以，每次做数据分析的时候，我都有种奇怪的感觉，因为我不明白这些数字背后，到底和我们本身有着什么样的关联，可这个世界偶尔会感应到你的失落，当你听着「明年今日」，而心里想的是去年今日，我想，这一页可以翻过去，这篇文章可以结尾，这样就可以了吧\u0026hellip;\n","date":"2021-10-26T16:10:47Z","image":"https://i.loli.net/2021/10/27/DlHcQUIXf1pORob.png","permalink":"https://qinyuanpei.github.io/posts/735074641/","slug":"735074641","tags":["Python","Sklearn","双十一"],"title":"通过 Python 预测 2021 年双十一交易额"},{"categories":["生活感悟"],"content":"\r也许，在某一个不寻常的清晨，突然从睡梦中惊醒的你，开始重新审视周围的一切，这个习以为常的世界，是否是真实的呢？而这种疑问，《黑客帝国》里的 Neo 有过，《西部世界》里的 Dolores 有过，甚至，连庄周梦蝶这个故事，都在探讨同一个哲学命题，即：人应该如何认识真实。从某种意义上来讲，游戏世界是现实世界的一种映射，这种对人类文明进行解构，并尝试进行重新演绎的做法，其本身就有种浓厚的哲学意味，正如古希腊人刻在阿波罗神殿里的那句名言，“人啊，认识你自己”。关于真实，这是人从诞生那一刻起，就在不断思考着的终究问题：我是谁？我从哪里来？要到哪里去？\n《失控玩家》的切入点，源于一名自我意识觉醒的 NPC——盖，他每天过着一成不变的生活：起床、喝咖啡、到银行上班、等待劫匪上门、和黑人保安趴在地上配合演出……事实上，这可以视为早期的 AI —— 有限状态机，我想，接触过游戏开发的朋友们，看到这里都不免要会心一笑啦！可惜，这是一部以 NPC 作为主角的电影，在电影的设定中，盖是一个具备深度学习能力的 AI，当他偶然从玩家手中夺过墨镜以后，终于从玩家视角看见不一样的世界。和《头号玩家》中被游戏入侵真实生活的设定不同，《失控玩家》更像是游戏版的《楚门的世界》，如果用一句话来概括，“海的尽头是天空，天空的尽头是荧幕”，在某个时刻，当你发现周围的一切变得不再真实，甚至身边的所有人都是演员，而更为讽刺的是，你的人生其实是一场被现场直播着的真人秀，而这便是 “楚门” 所面对着的世界。暂且不论这种设定，是否隐隐约约预言了当下流行的真人秀节目。当不经意间发现生活在不断重复的时候，你是否会愿意男主盖一样跳出这个循环呢？从我们出生的那一刻起，姓名、身份、知识、道德、文化\u0026hellip;等等，无一不是我们的 “出厂设定” ，更不必说上学、工作、结婚、生子这种类似 RPG 游戏里打怪升级的任务系统。\n导演小策有一期视频，大意是说，我们这个世界，其实是高维度文明设计出的一个游戏，这个游戏制定了非常精细的规则，与此同时，它具备高度的自由性，这一切的一切，都让我们误以为我们可以自己来做决定。其实，这些高维度文明，早已设定好了基本的规则，他们唯一想做的事情，就是看我们能不能通过进化发现自己只是一个戏中人。正如电影中的美女咖啡师，难道她真的喜欢做 “两份奶一份糖” 的咖啡吗？还是冥冥中有某种接近命运的东西让她成为了这样一个咖啡师？也许，这样的理论是细思极恐的，可笛卡尔同样说过，“我思故我在”。既然，我们都解释不了，我们为何会在这里，我们又如何能相信，周围的一切就一定是真实的呢？如果说，盖因为爱情的鼓舞而快速升级进化，这多少沾点喜剧的色彩，那么，接下来，当整个世界即将不复存在的时候，整个故事就变成了彻头彻尾的悲剧，因为在那一刻，无论你等级有多高，装备有多好，全部都失去了意义，一如人死灯灭，名与利，转瞬化为烟云，假如真有高维度文明存在，对方的维度只比你多0.0000001，连一粒尘埃的质量都达不到，你拼命想要抓住、想要证明的人生，可能就只是一场游戏，一个实验，就像最近大火的韩国电视剧《鱿鱼游戏》，穷人的放手一搏，富人的博君一笑，荒诞而又真实。可以说，终其一生，我们都在追求意义和仪式感，可人生恰恰是没有意义的，就像盖每天都重复着同一句话醒来，在永无止境的循环中，意义始终逃不出这个作用域，你可以定义它，但你终将失去它。\n而这种感觉，在我玩《风之旅人》这款游戏时更为强烈，在广袤无垠的沙漠中，当我开始漫无目的地行走的时候，我突然意识到，有可以称之为起点的地方，便一定会有可以称之为终点的地方，我会在某个时刻消逝，而这个世界则不会结束，就好像这个世界从来没有过我一样。其实，在《风之旅人》这个游戏中，陈星汉老师通过壁画的形式，隐晦地讲述了白袍人文明从诞生到被黄沙掩埋的整个故事。主角拼尽全力穿过雪地，如同预言一般死在圣山脚下，然后化成一道光变成玩家一开始身着红袍的样子。你不能不说这是一种轮回，故事还是那个故事，红袍人还是红袍人，而你注定再难变回你自己。不管是游戏还是人生，我们都会碰见不止一个同伴，而它们永远只能陪伴我们走完一段旅程，接下来的人生还是需要自己去完成。所以，这是一种难以言说的感觉，虽然听起来像是某种虚无主义的观点，可其实你仔细一想，如果上帝真的会掷骰子，女娲真的会造人，这个世界本身又何尝不能是一款游戏，一个人从出生的那一刻起，就开始扮演各种角色，这是一个角色扮演游戏；从小到大，从分数到薪资再到地位，这是一个跑酷竞技游戏……而游戏规则永远掌握在少数人手中，而这个时代，成功或者是优秀的标准，早已令人高不可攀，甚至没有人会想到，证明你是一个真实的人要比想象中还要难。在人工智能的加持下，游戏里的 NPC 有了独立的意识，可如果你无法分辨出什么是真实，你就无法证明这个世界是真实存在的。正因为如此，《模仿游戏》中的警官，只有不无遗憾地对图灵说，“I can’t judge you”.\n回到电影本身的话，我想，它更能激发骨灰级电影爱好者发掘彩蛋的兴趣。因为，这部电影里的梗简直多得像一部百科全书。从主角盖的冒险经历来看，你能找到《楚门的世界》、《西部世界》、《黑客帝国》等多部电影的影子，而如果从 “自由城” 的氛围来看，你会发现这无非就是一个升级、高配版本的《侠盗猎车手》游戏，甚至于盖在对抗 Dude 这个游戏中的 Boss 的时候，更是接连使用美队盾牌、绿巨人、光剑这种颇具代表性的意象。当然，你还可以从研发人员的视角来调侃这部电影，无论是盖背后无与伦比的人工智能算法，还是尚未完成设计就仓促上线的 Dude，抑或者是模型岛屿下面的天空盒，即使你并不确定，游戏公司老板选择用斧头来砸游戏服务器，其中是否有致敬《闪灵》的嫌疑？这个世界的神奇之处在于，无论你试图用文学、戏剧、游戏、电影中的哪一种方式来进行描摹，你都能得到一个令你自己确信的故事，唯独在确定自身的真实性上不止一次地迟疑过。对于普通人而言，我们真的有那么多的选择吗？是我们在玩一款游戏，还是躲在游戏背后的人在愚弄我们？当讨论一个群体的时候，个体的独特性就会被忽略，可或许我们本就没有什么不同，完全是 AGCT 四种碱基的排列组合让我们相信自己与众不同，在这款被称为 “人生” 的游戏里，99.9999999 的玩家注定要成为 NPC，人人都笑工具人，可在这款游戏里，谁又不是工具人呢？人们步履不停、反反复复地跌入生老病死的无限循环，为了只在这次循环中被定义和声明的作用域而奔波一生，这是否可以算作某种献祭？\n如果这个世界是假的，你应该如何面对？对于这个问题，我想，我没有答案，因为它就像我一直都看不清、抓不着的未来一样，无论我有没有机会去做出这个选择，它始终就像写好的结局一样被放在哪里，而我步履蹒跚的每一个当下，从某种意义上来讲，它或许只是我走向未来的一个工具，就像《风之旅人》里漫无边际的沙漠一样，你追着风的足迹前往某个叫做远方的地点，没有人知道远方的远方到底是什么。可你生命中遇见的每一个闪光点是何其的珍贵，即使它只在这一次循环内存在，即使它终会在你走向某座雪山的时候失去，而这正是生命的意义所在，不妨去假设这个世界是一款虚拟的游戏，你我都是这个游戏里最普通不过的 NPC，我们来到这个世界的唯一目的，也许是为了在这个宇宙中传递某种讯息，正如屏幕前的你，曾经无意间捕获到这点微弱的信号。罗曼罗兰曾经说过，“世上只有一种英雄主义，就是在认清生活真相之后依然热爱生活”，而这款游戏的目的，或许正是要我们在窥破这个世界的真实面目的时候，依然能够勇敢地面对人类孤独的永恒本质，依然能够用热爱和虔诚去面对由别人定义的意义，依然能够让生命中的每个当下都变得不舍和难忘，无论快乐还是悲伤，我们总不愿意永远活在别人的期待里，成为一个被信息茧房和消费主义麻醉的人，而这，正是我想要的冒险。\n","date":"2021-10-18T20:42:47Z","image":"/posts/1005876321/「风之旅人」游戏截图.jpg","permalink":"https://qinyuanpei.github.io/posts/1005876321/","slug":"1005876321","tags":["感悟","影评","游戏","哲学"],"title":"从《失控玩家》中得到的启示"},{"categories":["编程语言"],"content":"有人说，程序员最讨厌两件事情，一件是写文档，一件是别人不写文档，这充分展现了人类双标的本质，所谓的“严于律人”、“宽于律己”就是在说这件事情。虽然这种听来有点自私的想法，是生物自然选择的结果，可一旦人类的大脑皮层在进化过程中产生了“理性”，就会试图去纠正这种来自动物世界的阴暗面。所以，人类双标的本质，大概还是因为这个行为本身就有种超越规则、凌驾于众人之上的感觉，毕竟每个人生来就习惯这种使用特权的感觉。回到写文档这个话题，时下流行的微服务架构，最为显著的一个特点是：仓库多、服务多、接口多，此时，接口文档的重要性就凸显出来，因为接口本质上是一种契约，特别在前后端分离的场景中，只要前、后端约定好接口的参数、返回值，就可以独立进行开发，提供一份清晰的接口文档就显得很有必要。在 RESTful 风格的 API 设计中，Swagger 是最为常见的接口文档方案，那么，当我们开始构建以 gRPC 为核心的微服务的时候，我们又该如何考虑接口文档这件事情呢？今天我们就来一起探讨下这个话题。\nprotoc-gen-doc 方案 当视角从 RESTful 转向 gRPC 的时候，本质上是接口的描述语言发生了变化，前者是 JSON 而后者则是 Protobuf，因此，gRPC 服务的文档化自然而然地就落在 Protobuf 上。事实上，官方提供了 protoc-gen-doc 这个方案，如果大家阅读过我以前的博客，就会意识到这是 Protobuf 编译器，即 protoc 的插件，因为我们曾经通过这个编译器来生成代码、服务描述文件等等。protoc-gen-doc 这个插件的基本用法如下：\nprotoc \\ --plugin=protoc-gen-doc=./protoc-gen-doc \\ --doc_out=./doc \\ --doc_opt=html,index.html \\ proto/*.proto 其中，官方更推荐使用 Docker 来进行部署：\ndocker run --rm \\ -v $(pwd)/examples/doc:/out \\ -v $(pwd)/examples/proto:/protos \\ pseudomuto/protoc-gen-doc 默认情况下，它会生成 HTML 格式的接口文档，看一眼就会发现，就是那种传统的 Word 文档的感觉：\n通过 protoc-gen-doc 生成的接口文档\r除此以外，这个插件还可以生成 Markdown 格式的接口文档，这个就挺符合程序员的审美，因为此时此刻，你眼前看到的这篇文章，就是通过 Markdown 写成的：\ndocker run --rm \\ -v $(pwd)/examples/doc:/out \\ -v $(pwd)/examples/proto:/protos \\ pseudomuto/protoc-gen-doc --doc_opt=markdown,docs.md 这个方案如果整合到 CI/CD 中还是挺不错的，传统的 Word 文档形式的接口文档，最主要的缺点是没有版本控制、无法实时更新，因此，对于团队间的协作是非常不利的，我本身挺讨厌这种 Word 文档发来发去的。有时候，只有接口文档是不完美的，因为懒惰的人类希望你能提供个调用示例，最好是直接Ctrl+C、Ctrl+V这种程度的，对此，博主只有仰天长叹：悠悠苍天，此何人哉\u0026hellip;\u0026hellip;\nSwagger 方案 考虑到，第一种方案没有办法对接口进行调试，所以，下面我们来尝试第二种方案，即整合 Swagger 的方案，可能有小伙伴会好奇，Swagger 还能和 Protobuf 这样混搭起来玩？目前，Swagger 是事实上的 OpenAPI 标准，我们只需要在 Protobuf 和 OpenAPI 规范间做一个适配层即可。还记得博主曾经为 ASP.NET MVC 编写的 Swagger 扩展吗？没错，我们要再次“整活”了，首先，这里给出的是 OpenAPI 规范的定义：\n{ \u0026#34;openapi\u0026#34;: \u0026#34;3.0.1\u0026#34;, \u0026#34;info\u0026#34;: { }, \u0026#34;servers\u0026#34;: [ ], \u0026#34;paths\u0026#34;: { }, \u0026#34;components\u0026#34;: { } } 其中，info 节点里存放的是接口文档的基本信息，例如标题、作者、许可证等。servers 节点里存放的是接口所属服务的主机名、端口号等。paths 节点里存放的是每个 API 端点的信息，例如路由、请求参数、返回值等。components 节点里存放的是类型信息，例如请求参数、返回值中每个属性或者字段的具体类型等。一旦搞清楚了这些内容，我们发现这个里面最关键的两个信息是：paths 和 components，如果我们回过头来看 Protobuf 的声明文件，就会发现这两个东西，分别对应的是 rpc 和 message，如下图所示：\nSwagger 与 Protobuf 的对应关系\r通常情况下，我们使用 Swashbuckle.AspNetCore.Swagger 这个库来为 ASP.NET Core 项目提供 Swagger 支持，其中最为关键的是ISwaggerProvider接口，这里我们来尝试为 Protobuf 提供一个具体的实现：\npublic class GrpcSwaggerProvider : ISwaggerProvider { private readonly ISchemaGenerator _schemaGenerator; private readonly SwaggerGeneratorOptions _options; private readonly IApiDescriptionGroupCollectionProvider _apiDescriptionsProvider; private readonly GrpcSwaggerSchemaGenerator _swaggerSchemaGenerator; public GrpcSwaggerProvider( SwaggerGeneratorOptions options, ISchemaGenerator schemaGenerator, IApiDescriptionGroupCollectionProvider apiDescriptionsProvider, GrpcSwaggerSchemaGenerator swaggerSchemaGenerator ) { _options = options; _schemaGenerator = schemaGenerator; _apiDescriptionsProvider = apiDescriptionsProvider; _swaggerSchemaGenerator = swaggerSchemaGenerator; } public OpenApiDocument GetSwagger(string documentName, string host = null, string basePath = null) { if (!_options.SwaggerDocs.TryGetValue(documentName, out OpenApiInfo info)) throw new UnknownSwaggerDocument(documentName, _options.SwaggerDocs.Select(d =\u0026gt; d.Key)); var schemaRepository = new SchemaRepository(documentName); // Swagger Document var swaggerDoc = new OpenApiDocument { Info = info, Servers = BuildOpenApiServers(host, basePath), Paths = new OpenApiPaths() { }, Components = new OpenApiComponents { Schemas = schemaRepository.Schemas, SecuritySchemes = new Dictionary\u0026lt;string, OpenApiSecurityScheme\u0026gt;(_options.SecuritySchemes) }, SecurityRequirements = new List\u0026lt;OpenApiSecurityRequirement\u0026gt;(_options.SecurityRequirements) }; // Swagger Filters var apiDescriptions = _apiDescriptionsProvider.GetApiDescriptions().Where(x =\u0026gt; x.Properties[\u0026#34;ServiceAssembly\u0026#34;]?.ToString() == documentName); var filterContext = new DocumentFilterContext(apiDescriptions, _schemaGenerator, schemaRepository); foreach (var filter in _options.DocumentFilters) { filter.Apply(swaggerDoc, filterContext); } // Swagger Schemas swaggerDoc.Components.Schemas = _swaggerSchemaGenerator.GenerateSchemas(apiDescriptions); var apiDescriptionsGroups = _apiDescriptionsProvider.ApiDescriptionGroups.Items.Where(x =\u0026gt; x.Items.Any(y =\u0026gt; y.Properties[\u0026#34;ServiceAssembly\u0026#34;]?.ToString() == documentName)); swaggerDoc.Paths = _swaggerSchemaGenerator.BuildOpenApiPaths(apiDescriptionsGroups); return swaggerDoc; } } 这里的OpenApiDocument对应着 OpenAPI 规范中的定义的结构，我们需要返回一个OpenApiDocument，并对其Components和Paths属性进行填充，这部分工作由GrpcSwaggerSchemaGenerator类来完成。我们这里不会直接去解析 Protobuf 文件，而是利用Google.Protobuf.Reflection这个包来反射 Protobuf 生成的类，然后将其转化为 OpenAPI 规范中定义的结构，更多的细节，大家可以参考这里。\n接下来，在实现了ISwaggerProvider以后，我们还需要替换掉默认的实现：\npublic static void AddGrpcGateway( this IServiceCollection services, IConfiguration configuration, Action\u0026lt;Microsoft.OpenApi.Models.OpenApiInfo\u0026gt; setupAction = null, string sectionName = \u0026#34;GrpcGateway\u0026#34; ) { var configSection = configuration.GetSection(sectionName); services.Configure\u0026lt;GrpcGatewayOptions\u0026gt;(configSection); var swaggerGenOptions = new GrpcGatewayOptions(); configSection.Bind(swaggerGenOptions); var swaggerGenSetupAction = BuildDefaultSwaggerGenSetupAction(swaggerGenOptions, setupAction); services.AddSwaggerGen(swaggerGenSetupAction); // Replace ISwaggerProvider services.Replace(new ServiceDescriptor( typeof(ISwaggerProvider), typeof(GrpcSwaggerProvider), ServiceLifetime.Transient )); // Replace IApiDescriptionGroupCollectionProvider services.Replace(new ServiceDescriptor( typeof(IApiDescriptionGroupCollectionProvider), typeof(GrpcApiDescriptionsProvider), ServiceLifetime.Transient )); // GrpcDataContractResolver services.AddTransient\u0026lt;GrpcDataContractResolver\u0026gt;(); // GrpcSwaggerSchemaGenerator services.AddTransient\u0026lt;GrpcSwaggerSchemaGenerator\u0026gt;(); // Configure GrpcClients services.ConfigureGrpcClients(swaggerGenOptions); // AllowSynchronousIO services.Configure\u0026lt;KestrelServerOptions\u0026gt;(x =\u0026gt; x.AllowSynchronousIO = true); services.Configure\u0026lt;IISServerOptions\u0026gt;(x =\u0026gt; x.AllowSynchronousIO = true); } 接下来，就是见证奇迹的时刻，gRPC 和 Swagger 牵手成功。从此，查阅和调试 gRPC 接口，我们有了更时尚的做法：\ngRPC 成功牵手 Swagger 调一下接口看看效果：\n通过 Swagger 调试 gRPC 接口\r可以注意到，此时，Swagger 中返回了我们期望的结果，事实上，只有 Swagger 还不足以令它运作起来，其中的诀窍是，博主利用终结点(Endpoints)动态创建了路由。关于这一点，博主曾在 ASP.NET Core gRPC 打通前端世界的尝试 这篇文章中提到过。最终，博主编写了一个更为完整的项目：FluentGrpc.Gateway，而关于 Swagger 的这部分内容则成为了这篇博客的内容，如果大家对这个项目感兴趣的话，欢迎大家去做进一步的探索，欢迎大家 Star 和 PR，而到这里，这篇博客差不多就可以结尾啦！\n本文小结 有时候，博主会不由地感慨，整个微服务架构的落地过程中，服务治理是花费时间和精力最多的环节，除了保证接口的稳定性，更多的时候，其实是不同的服务间相互打交道。那么，除了口头传达外，最好的管理接口的方式是什么呢？显然是接口文档。本文分享了两种针对 gRPC 的服务文档化的方案，第一种是由官方提供的 protoc-gen-doc，它可以从 Protobuf 生成 HTML 或者 Markdown 格式的接口文档。第二种是由博主实现的 FluentGrpc.Gateway，它实现了从 Protobuf 到 Swagger 的转换，只需要在项目中引入这个中间件，就可以把 gRPC 带进 Swagger 的世界，不管是查阅接口还是调试接口，都多了一种玩法，如果你还需要给非开发人员提供接口文档，那么，我觉得你还可以试试 YAPI，只需要导入 Swagger 格式的服务描述信息即可，而这一步，我们已经实现了。好了，以上就是这篇博客的全部内容啦，谢谢大家！\n","date":"2021-09-28T14:13:32Z","image":"https://grpc.io/img/landing-2.svg","permalink":"https://qinyuanpei.github.io/posts/4056800047/","slug":"4056800047","tags":["gRPC","Swagger","微服务","文档"],"title":"gRPC 搭配 Swagger 实现微服务文档化"},{"categories":["编程语言"],"content":"Hi，大家好，我是飞鸿踏雪，欢迎大家关注我的博客。近来，博主经历了一次服务器迁移，本以为有 Docker-Compose 加持，一切应该会非常顺利，没想到最终还是在证书上栽了跟头，因为它的证书是和 IP 地址绑定的。对，你没听错，这个世界上还真就有这么别扭的设定，尤其是你折腾了一整天，发现你需要到一个 CA 服务器上去申请证书的时候，那种绝望你晓得吧？数字证书、HTTPS、SSL/TLS、加密……无数的词汇在脑海中席卷而来，这都是些啥啊？为了解答这些困惑，经历了写字、画图、查资料的无数次轮回，终于在周末两天淅淅沥沥的雨声中，有了今天这篇文章，我将借此带大家走进 SSL/TLS 加密传输与数字证书的前世今生，希望从此刻开始，令人眼花缭乱的证书格式不会再成为你的困扰。\n证书与加密 对于数字证书的第一印象，通常来自于 HTTPS 协议。因为地球人都知道，HTTP 协议是不需要数字证书的。对于 HTTPS 协议的理解，可以简单粗暴的认为它约等于 HTTP + SSL，所以，从这个协议诞生的那一刻起，加密算法与数字证书就密不可分，因为从本质上来讲，HTTPS 协议就是为了解决如何在不安全的网络上、安全地传输数据的问题。事实上，HTTPS 协议的实现，背后依托 SSL/TLS、数字签名、对称/非对称加密等一系列的知识。也许，在读到这篇文章以前，你就像博主一样，对于 HTTPS 的理解，永远止步于 HTTP + SSL。那么，我希望下面的解释可以帮助到你，通常，HTTPS 认证可以分为 单向认证 和 双向认证 两种，这里我们以为以单向认证为例，来说明数字证书与加密算法两者间的联系：\nHTTPS 数字证书与加密传输间的关系\r如图所示，HTTPS 单向认证流程主要经历了下面 7 个步骤，它们分别是：\n客户端发起 HTTPS 请求 服务器返回证书信息，本质上是公钥 客户端/浏览器通过 CA 根证书验证公钥，如果验证失败，将会收到警告信息 客户端随机生成一个对称密钥 Key，并利用公钥对 Key 进行加密 服务器使用私钥解密获得对称密钥 Key 通过对称密钥 Key 对确认报文进行加密 双方开始通信 由此，我们可以看出，整个 HTTPS 单向认证流程，实际上是结合了 对称加密 和 非对称加密 两种加密方式。其中，非对称加密主要用于客户端、服务器双方的“试探”环节，即证书验证部分；对称加密主要用于客户端、服务器双方的“正式会话”阶段，即数据传输部分。关于 对称加密 和 非对称加密 两者的区别，我们可以从下面的图中找到答案：\n对称加密 与 非对称加密\r因为客户端持有服务器端返回的公钥，所以，两者可以使用 非对称加密 对随机密钥 Key 进行加/解密。同理，因为客户/服务器端使用相同的随机密钥，所以，两者可以使用 对称加密 对数据进行加/解密。有朋友可能会问，那照你这样说，任何一个客户端都可以向服务器端发起请求嘛，你这样感觉一点都不安全呢？我承认，大家的担心是有道理的。所以，在此基础上，我们还可以使用双向认证，就是不单单客户端要验证服务器端返回的证书，同样，服务器端要对客户端的证书进行验证。那么，客户端是如何验证服务器端返回的证书的呢？服务器返回的证书里都含有哪些信息呢？带着这些问题，我们来看看知乎这个网站：\n知乎的证书信息\r事实上，浏览器在对服务器端返回的证书进行校验时，主要关心下面这些信息：\n判断域名、有效期等信息是否正确：这些信息在证书中是公开的，可以非常容易地获得。 判断证书是否被篡改：需要由 CA 服务器进行校验。 判断证书来源是否合法：每一份签发的证书都可以按照证书链找到对应的根证书，所以，可以通过操作系统中安装的根证书对证书的来源进行验证。 判断证书是否被吊销：需要由 CRL（Certificate Revocation List，即 证书注销列表）和 OCSP（Online Certificate Status Protocol, 即 在线证书状态协议） 来实现。 这里引入了一个新的概念，即 CA（Certification Authority）。那么，什么是 CA 呢？ 通俗来讲，CA 就是一个负责签发、认证和管理证书的机构。可能有朋友会想，客户端和服务器端通过非对称加密相互校验证书就好了啊，为什么还需要这样一个第三方的机构呢？事实上，这相当于一种担保/信用体系，因为服务器端的公钥对任何人来说都是可见的，我们来考虑这样一种情形。假设客户端从服务器端获得了某个公钥，并且它认为这个公钥是可信的，此时，有一个不怀好意的中间人截获了这个公钥，它如法炮制伪造了一个相同的公钥并返回，那么，此时客户端会如何看待这个公钥呢？虽然这个中间人不可能伪造出与服务端相同的私钥，可这无疑会让客户端感到困惑，因为它没有办法判断这个证书的真假。\n证书的签发与认证\r其实，写到这里的时候，博主隐隐约约意识到，当下流行的比特币/数字人民币均与数字签名息息相关，因为 CA 使用私钥对证书进行了签名，这样就杜绝了证书被篡改的可能，从而可以为证书的真实性背书，这种基于信任制、拥有权威性的体系，就像现实生活中银行为货币的真实性、价值背书一样。因此，我们会注意到，在现实生活中，想要获得一份权威机构的数字证书，就需要向 CA 进行申请，例如，知乎的证书是从 DigiCert Inc 这个机构中购买的，不同的机构对于证书申请者的审核要求不同，这样就形成了不同价格甚至免费的数字证书。\nCA 组织树形结构\r当然，这个世界上有超过 1 亿个网站，如果每个网站都去向 CA 申请数字证书，那么，CA 一定会忙到崩溃。所以，实际的运行过程是，一个根 CA 会分成多个中间 CA，然后中间 CA 可以继续拆分为更小的中间 CA，这样做的好处是效率更高，同时保证了根 CA 中私钥的安全性。此时，我们会发现一个新的问题，就是当整个数字证书体系中突然多出来这么多“中介”以后，我们如何保证证书的权威性和真实性呢？类似地，数字证书世界里里有证书链的概念。所谓证书链，就是指证书可以追本溯源、在整个链路上都是可信任的，听起来是不是有区块链的味道了？事实上区块链正是利用了数字签名的不可伪造、不可抵赖、不可复制等一系列特性。说回到证书链，由根 CA 签发的证书称为根证书、由中间 CA 签发的证书称为中间证书，其关系如下图所示，假设 A 完全信任 B，B 完全信任 C，则 A 可以完全信任 C:\n证书链示意图\r证书创建 OK，现在我们已然理清了证书与加密两者间的联系，那么，在实际生活中，我们该如何获得一个证书呢？由上文可知，证书理论上应该由 CA 机构来签发。目前，全球主流的 CA 机构有Comodo、Symantec、GeoTrust、DigiCert、Thawte、GlobalSign、RapidSSL 等，其中 Symantec、GeoTrust 都是 DigiCert 机构的子公司，占据数字证书体系中的垄断地位，就连国内的互联网厂商都需要向这些机构来购买证书，所以，推广 HTTPS 并不是完全出于安全的考虑，实际上还有某种利益关系在里面，可以想象得到，假如你的证书信任度不高，不在浏览器的可信任机构列表中，那么，你的网站就会被浏览器认为是不安全的，随之而来的就是用户对网站的信任度的下降。当然，购买数字证书是需要花钱的，所以，实际操作中，通常有自签名证书 和 CA 证书 两种，两者唯一的差别就在于权威性不同，大概相当于一种互联网行业的“保护费”。\n自签名证书 所谓自签名证书，其实就是自建一个 CA，然后利用这个 CA 对证书进行签名。为什么说它没有权威性呢？大概这就像小时候试卷上要签署大人的名字一样，如果你照着大人的笔迹伪造了签名，那么，此时没有人能保证这份签名的真实性。更深层次的原因在于，由你自建的这个 CA 没有在互联网上备案，它产生的证书无法通过证书链追溯，这是自签名证书没有权威性的原因。我们通常说的创建/生成证书，其实都是指这种自签名证书，创建自签名证书最常见的方式是 OpenSSL：\n// 创建根证书 openssl genrsa -out ca.key 2048 openssl req -new -key ca.key -out ca.csr openssl x509 -req -days 365 -in ca.csr -signkey ca.key -out ca.crt 在这个过程中，OpenSSL 会要求我们提供下列信息：国家、省份、城市、组织 以及 全域名(FQDN)。在此之前，关于知乎的那个例子，实际上证书上的那些信息就是从这里来的。当我们有了这样一个自建的 CA 以后，我们就可以用这个自建的 CA 去签发证书，这就是自签名 CA 证书，如何生成这个证书呢？\n// 环境准备，下列路径在 openssl.conf 文件中定义 mkdir -p ./demoCA/newcerts cd ./demoCA/ touch index.txt echo \u0026#39;01\u0026#39; \u0026gt; serial cd .. // 签发证书 openssl genrsa -out server.key 2048 openssl req -new -key ca.key -out server.csr openssl ca -in server.csr -out server.crt -cert ca.crt -keyfile ca.key 同样的，我们需要再输入一次下列信息：国家、省份、城市、组织 以及 全域名(FQDN)，然后利用自建的 CA 进行签名。在 OpenSSL 中，它定义了证书申请方需要满足的“门槛”，这决定了你能不能向某个 CA 申请证书，其定义位于openssl.conf文件中：\nOpenSSL 策略配置\r例如，这里的策略表示，只有当证书申请方的国家、省份、组织相同的时候，CA 才会接受你的证书申请。所以，至此你明白证书为什么收费了吧？因为主流的 CA 机构都在国外，理论上 CA 机构可以去调整这个策略，可如果对方不愿意调整策略，那么你只能找别人帮你来申请，通过不断的调用openssl ca命令， 产生新的中间 CA，这样就形成了树状的 CA 组织。是不是觉得看人脸色非常地不舒服？除了这种方式以外，我们还可以按下面这种方式生成证书，这种方式像极了我们小时候模仿大人签字：\n// 签发证书 openssl genrsa -out server.key 4096 openssl req -new -key server.key -out server.csr openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt 如果是在 Windows 系统下，我们还可以搭建 CA 服务器，此时，证书申请者需要远程登陆到这台服务器进行操作，请参考：服务器证书部署。\nCA 证书 一旦理解了自签名证书，理解 CA 证书 就变得特别容易，这就是交了“保护费”的证书，过去总以为互联网世界里没有政治，后来发现互联网并不是“法外之地”，一切的自媒体、流量，最终都会转化为某种商品出售，只要人与人形成了某种圈子或者团体，这种政治就一定会存在。所以，你到腾讯云或者阿里云去购买证书，而腾讯和阿里则是某个 CA 机构的代理商，因为数字证书通常会和域名产生联系，所以，在供应商那里，两者往往是捆绑在一起销售，再加上网站备案、虚拟主机这些东西，在由资本绘制的商业版图里，你的钱包被安排得明明白白。或是为了打破这种垄断，或是为了某种利害关系，慢慢地出现了像 Let\u0026rsquo;s Encrypt 这样的提供免费证书的机构。所以，下面，我们以此为例来展示如何申请一个 CA 证书：\ngit clone https://github.com/acmesh-official/acme.sh.git cd ./acme.sh ./acme.sh --install acme.sh --register-account -m \u0026lt;Your E-Mail\u0026gt; acme.sh --issue -d \u0026lt;Your-Domain\u0026gt; --standalone 目前，Let\u0026rsquo;s Encrypt 的使用是通过 acme.sh 这个脚本来驱动的，其基本用法如上面脚本所示。不同于自签名证书，Let\u0026rsquo;s Encrypt 目前不支持使用公网 IP 来申请证书，所以，如果在开发阶段，可以使用自签名的证书；在生产阶段，则最好使用 CA 签发的证书。通过阅读 文档 可知，它支持 HTTP 和 DNS 两种验证方式，可以使用 Apache 、Nginx 和 Standalone 三种模式，个人推荐使用 Docker 来进行部署，因为前两种模式要求你安装对应的软件，第三种模式要求你的 80 端口是空闲的，这对于一名开发人员来说，简直是痴心妄想。如果你有一个域名，而恰好这个域名提供商在其支持的 列表 内，那么，你就可以使用下面的方式来申请证书。首先，准备一个docker-compose.yml文件，博主的域名是从 GoDaddy 申请的，大家可以结合实际情况进行调整：\nversion: \u0026#34;2.1\u0026#34; services: acme.sh: image: neilpang/acme.sh container_name: \u0026#34;acme.sh\u0026#34; volumes: - /docker/ssl:/acme.sh environment: - GD_Key=\u0026lt;GoDaddy Key\u0026gt; - GD_Secret=\u0026lt;GoDaddy Secret\u0026gt; command: daemon 接下来，我们只需要启动容器，然后在容器内部执行命令即可：\ndocker-compose up -d docker exec -it \u0026lt;ContainerId\u0026gt; sh acme.sh --register-account -m \u0026lt;Your E-Mail\u0026gt; acme.sh --issue --dns dns_gd -d \u0026lt;Your-Domain\u0026gt; 可以注意到，下面即为博主从 Let\u0026rsquo;s Encrypt 申请到的证书文件：\n从 Let‘s Encrypt 申请证书\r如果你的域名提供商在这个 列表 内，此时，你可以手动将其生成的值添加到域名记录中，这些在文档中均有提及，不再赘述。总而言之，你向 CA 机构申请证书需要一个有效的域名，像腾讯云、阿里云这种云服务提供商，早已提供好了完整的一条龙服务，只要你愿意花钱去买对方的产品。\n证书使用 一旦生成了证书，我们就可以在应用程序中使用这些证书啦，我注意到公司的每个项目都配置了证书文件，其实我一直不明白，为什么不能直接把证书安装到宿主机上？这样只需要折腾一次就好了啊，简直是一劳永逸。如果有小伙伴们知道这个问题的答案，欢迎大家在评论区留言。下面我们来看看，生成的证书如何在不同的环境中配置，这里以 ASP.NET Core 、Envoy 和 Nginx 为例来说明。\nASP.NET Core 在 ASP.NET Core 中配置 HTTPS 证书，最直接的方案是在通过 Kestrel 中间件来指定证书路径和密码：\nwebBuilder.ConfigureKestrel(options =\u0026gt; { // 方式 1 options.ConfigureHttpsDefaults(kestrel =\u0026gt; { kestrel.ServerCertificate = new X509Certificate2(\u0026#34;./path/to/your/example.com.pfx\u0026#34;,\u0026#34;\u0026lt;证书密码\u0026gt;\u0026#34;); }); // 方式 2 options.Listen(IPAddress.Loopback, 5001, kestrel =\u0026gt; { kestrel.UseHttps(new X509Certificate2(\u0026#34;./path/to/your/example.com.pfx\u0026#34;,\u0026#34;\u0026lt;证书密码\u0026gt;\u0026#34;)); }); }); 如果整个 ASP.NET Core 应用以容器方式运行，则还可以按下面这样的方式来配置证书：\ndocker run --rm -it -p 8000:80 -p 8001:443 \\ -e ASPNETCORE_URLS=\u0026#34;https://+;http://+\u0026#34; \\ -e ASPNETCORE_HTTPS_PORT=8001 \\ -e ASPNETCORE_ENVIRONMENT=Development \\ -e ASPNETCORE_Kestrel__Certificates__Default__Password=\u0026#34;\u0026lt;证书密码\u0026gt;\u0026#34; \\ -e ASPNETCORE_Kestrel__Certificates__Default__Path=/path/to/your/example.com.pfx -v /c/path/to/certs/:/https/ \u0026lt;镜像Id\u0026gt; 不得不说，这里的双下划线，总是让我不由地想起 Python 里的魔法方法：__init__。可能大家会疑惑，为什么博主这里要强调证书的扩展名，因为这实际上是数字证书里最让人迷惑的地方：\n数字证书编码格式与扩展名\r在整个数字证书体系中，X.509 是作为数字证书标准而存在的，按照编码格式的不同，可以分为 PEM 证书 和 DER 证书两类，前者是文本格式，而后者是二进制格式。不同的操作系统、开发语言，产生了不同的证书文件格式，但这些扩展名本身并不能说明什么，特别是像 .crt 或者 .cre 这种薛定谔的证书，唯一的判断标准，就是用记事本打开它，如果可读，说明它是 PEM 编码的证书，如果不可读，说明它是 DER 编码的证书。如果大家和 Java 系的技术或者产品做过对接，应该会对这种微妙的差别深有体会，此时，我们就需要通过 OpenSSL 来实现不同证书格式间的转换，ASP.NET Core 需要的 .pfx 证书是如何产生的呢？\n// x.509 -\u0026gt; .pfx openssl pkcs12 -export -in server.crt -inkey server.key -out server.pfx 同理，常见的 OpenSSL 转换命令如下：\n// .pem -\u0026gt; .pfx openssl pkcs12 -export -in cert.pem -out cert.pfx -inkey key.pem // .pfx -\u0026gt; .cer openssl pkcs12 -in server.pfx -out server.cer -nodes // .cer -\u0026gt; .pem openssl x509 -inform der -in server.cer -out server.pem // PEM -\u0026gt; DER openssl x509 -in server.pem -outform der -out server.der // DER -\u0026gt; PEM openssl x509 -in server.der -inform der -outform pem -out server.pem Envoy Envoy 中可以直接使用 .crt 文件 以及 .key 文件，这里出现了一个 TLS 协议，这个协议一直没机会来说，这里可以简单说一下，它可以视为 SSL 3.1，因为早期的 SSL 协议是由网景公司(Netscape)提出的，一共经历了 1.0、2.0 和 3.0 三个版本，后来标准化组织 IETE 在此基础上提出了增强版的 TLS 协议，一直沿用至今，所以，TLS 可以看做是 SSL 3.1，换句话讲，HTTPS = HTTP + SSL/TLS。\ntransport_socket: name: envoy.transport_sockets.tls typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext common_tls_context: alpn_protocols: - \u0026#34;h2\u0026#34; tls_certificates: certificate_chain: filename: \u0026#34;/path/to/your/example.com.crt\u0026#34; private_key: filename: \u0026#34;/path/to/your/example.com.key\u0026#34; 相信到了现在这个地步，大家终于能想明白 通过 HttpClient 调用第三方接口时，为什么要这这样一段堪称魔法的代码了吧？因为在推进 HTTPS 的过程中，大家使用的 SSL/TLS 协议版本都不一样，有时候客户端还提供不了可以通过验证的证书，所以，大家干脆无视协议的版本、证书的验证错误这些问题啦！\nSystem.Net.ServicePointManager.SecurityProtocol = SecurityProtocolType.Tls | SecurityProtocolType.Tls11 | SecurityProtocolType.Tls12 | SecurityProtocolType.Tls13; System.Net.ServicePointManager.ServerCertificateValidationCallback += (a, b, c, d) =\u0026gt; true; // RestSharp var client = new RestClient(); client.RemoteCertificateValidationCallback += (a, b, c, d) =\u0026gt; true; Nginx Nginx 就更不必说啦，不过我个人现在更喜欢 Envoy 一点，Nginx 可以用 .crt 证书 或者 .pem 证书，我们只需要简单配置一下就可以了：\nserver { listen 443 ssl; server_name example.com; ssl on; ssl_certificate /path/to/example.com.crt; ssl_certificate_key /path/to/example.com.key; } 本文小结 因为一次服务器迁移时被证书苦虐的经历，决定花点时间研究了一下数字证书，本文从 HTTPS 协议入手，引出了对称加密、非对称加密等加密相关的内容，然后讨论了什么是证书，什么是 CA，以及 为什么需要 CA 等内容，现实世界中需要一个为证书权威性、真实性提供担保的组织，这种组织可以签发证书、验证证书、管理证书，利用数字签名的不可篡改、不可抵赖、不可复制、不可伪造等特性，根 CA 可以授权中间 CA 去签发证书，因为整个证书链都是可以追溯的。有了这些知识作为背景，我们分享了如何获得一份自签名证书和 CA 证书，两者本质上没有什么不同，唯一的区别在于其信任度不同。故事的最后，博主分享了如何为 ASP.NET Core 、Envoy、Nginx 配置 证书，对于数字证书的理解，从道的层面到术的层面，我们全部都串联起来啦，好了，以上就是这篇博客的全部内容，欢迎大家在评论区积极留言、参与讨论，原创不易，写技术博客更不易，大家点个赞吧！\n参考链接 SSL 数字证书的标准、编码以及文件扩展名 Docker 使用 acme.sh 申请 SSL 证书 超文本传输安全协议 RingCentral Tech 丨证书，证书链，CA 的那些事 你知道，HTTPS 用的是对称加密还是非对称加密 ","date":"2021-09-05T14:13:32Z","image":"https://i.loli.net/2021/09/07/nkJFiPNdVc4ShAw.png","permalink":"https://qinyuanpei.github.io/posts/3163397596/","slug":"3163397596","tags":["证书","加密","签名","HTTPS"],"title":"SSL/TLS 加密传输与数字证书的前世今生"},{"categories":["生活感悟"],"content":"\r“古巴比伦王颁布了汉谟拉比法典，刻在黑色的玄武岩，距今已经三千七百多年”。多年以前，周杰伦在 《爱在西元前》 里这样喃喃道，古巴比伦、楔形文字、玄武岩石板、底格里斯河、汉谟拉比法典……千年以后，一切已无法考据，这些如图腾符号一般神秘的意象，留给后人的只有无限的遐想。据传，公元前 6 世纪，新巴比伦国王尼布甲尼撒二世，迎娶了米底的公主安美依迪丝为王后。公主美丽可人，深受国王宠爱，可没过多久，公主就因为思念家乡而满怀愁绪。为此，国王召集工匠依照米底山区的景致修建了空中花园。最终，这座被誉为“古代七大奇迹”之一的神秘建筑，凭借它巧夺天工的园林景色俘获了公主的欢心。当空中花园渐渐淹没在滚滚黄沙里，这段无人知晓的爱，大概会被永远淹没在史书文卷，直到一个叫做方文山的词人发现它，然后写出来。在几乎同时代的周朝，周幽王烽火戏诸侯，只为博褒姒一笑。故事总是相似的，只是我们更愿意相信，那就是爱。\n巴比伦“空中花园”\r在更为久远的公元前 4 世纪，彼时庞贝城刚刚开始兴建。这个背靠地中海的小渔村，依托着天然的港口优势，在短短几十年里，逐渐成为仅次于意大利古罗马城的第二大城市。它北距罗马 300 千米，西接著名的西西里岛，南通希腊与北非，有着丝毫不亚于古罗马的斗兽场、太阳神神庙、大剧院、巫师堂、蒸汽浴室、商铺以及娱乐场所，吸引了无数来自周中海周边城邦的贵族和富商。庞贝城以北，有一座维苏威火山，这座活火山千年来一直在不断喷发，甚至庞贝古城本身就是建筑在硬化的火山熔岩上面。公元 62 年 2 月 8 日，一次强烈的地震令庞贝古城中的大量建筑塌毁，人们重建了庞贝古城，比以前更加追求奢侈豪华。历史定格在公元 79 年 8 月 24 日这一天，维苏威火山突然爆发，厚约 6 米的火山灰完全将这座城市从地球上抹去。许嵩 对此发问，“如果火山喷发，是灾难还是壮美？”。也许，在那一瞬间，真的有人攥着新鲜的玫瑰，准备向喜欢的人求爱。火山喷发的刹那，庞贝是一颗千年的琥珀，时间自此被凝固和封印。\n重见天日的庞贝古城\r不管是神秘莫测的空中花园，还是重见天日的庞贝古城，某种程度上，我们更愿意相信，那些亦真亦假的美丽传说。也许，这是因为它比建筑本身更具有说服力。因此，巴黎圣母院有过卡西莫多与艾丝梅拉达的故事，沧浪亭有过《浮生六记》里的伉俪情深，泰姬陵有过沙·贾汗的一夜白头，布达拉宫有过松赞干布和文成公主的相敬如宾，埃菲尔铁塔有过古斯塔夫·埃菲尔的望眼欲穿……世上以爱为名的建筑不在少数，或为古堡、或为陵墓、或为高塔、或为教堂……即使随着时间的流逝，建筑会褪去自身的光芒，而透过这些故事所折射出的爱情的光华，则永远不会褪去，还有什么比历经风雨洗礼的建筑更能表达爱的深沉呢？电影 《夕雾花园》 同样讲述了一个关于建筑的故事，假如园林艺术可以当做一种建筑的话。于是，爱情、战争、艺术、救赎、悬疑……种种元素交织在一起，像极了这座园林里一草一木、一砖一瓦。透过石头和木头，去打量某段扑朔迷离的历史，这本身就是一种『借景』，谁还能记得起，在兴庆宫的花萼相辉楼，李白曾经写过某一首诗呢？\n从一方天地中借景\r妹妹云红，年轻时曾和姐姐云林一起前往日本旅行，并在那里见到了日式庭院，自此在心里埋下了一份庭院情结。后来，日本发动对马来西亚的侵略战争，姐妹俩被抓到同一个集中营里。姐姐云林，白天目睹日军对妹妹的暴行，夜晚聆听妹妹对日本园林的畅想。此时此刻，人生的苦难与艺术的理想，像极了一幅太极图，互不相溶而又紧紧相依。再后来，姐姐死里逃生，而妹妹却命丧矿井。带着这份深深的内疚感，云林决心替妹妹建造一座日式园林。就这样，女主结识了日本皇家园艺师中村有朋。中村并没有接受女主的园艺委托，而是让女主参与夕雾花园的修建工作，正是在这个过程中，中村教给了云林『借景』的智慧，所谓『借景』，是指在一个视觉范围内，借由远方的山，眼前的树，天上的云与雾，呼应人工打造的花园，创造出浑然天成的自然框景。所以，对于这个电影而言，云林是从过往的美好回忆、未来的无限期许中借景；中村是从和云林的相处中借景；导演是从时间、氛围、景致、阅历中借景。由此，云林和中村互相实现了自我救赎，云林能放下过去的执念努力生活下去，中村能够放下国家战争的罪恶感潜心钻研园艺，而我们能有幸看到这样一个故事。\n回忆中可爱的妹妹\r如果单纯用爱情来定义这部电影，显然是狭隘而片面的，因为我认为它表达了某种关乎艺术、心灵的东西。从无印良品到优衣库，日式的美学，始终让人联想到侘寂或者无用这样的字眼，甚至连同今年的东京奥运会开幕式，都被人们吐槽充满了阴间的味道。对于云林而言，中村是一个侵略者的身份，不管他是不是日军的间谍，不管他是不是知道集中营的位置，因为他们国家的铁蹄踏入自己的家园，亲人因此而走向毁灭，这是不折不扣的事实。可天使和魔鬼谁又真正分得清楚呢？云林曾经这样问妹妹云红，“日本人这样对待我们，为何你还喜欢他们的庭园”，云红说，“我爱的是花园，而不是建造它的人”。所以，对我而言，这部电影的基调是残酷而又不乏温柔的，战争带给人们的伤痛是真实而残酷的，而实现一种心灵层面上的宁静、尊重自然、尊重生命则是平静而温柔的。日式庭院，在战争时期对人们而言是毫无用处的，甚至修建这样一座庭院并不能让妹妹活过来，可正是这样一份来自侵略者国度的艺术理想，支撑着姐妹俩熬过那些人生中至暗的时刻。\n我们活在疯狂的世界\r透过云林被汗水浸透的衣服，中村第一次看见她背上的鞭痕，中村说，从那一刻开始，他就想为眼前的这个女人负责。刺青、折纸、浮世绘，这些非常日系的意象，仿佛故事里纠缠的线索一般，无一不在告诉观众，这是个谜一般的男人。正因为如此，他只能在隐秘的森林深处消失不见，恰如这个故事本身充满冲突和对立，只能选择戛然而止。有时候我会问自己，不能在当下验证和确认的爱，是否就要因此而阻断，甚至遥遥无期直至搁浅。我只知道，电影里是这样处理的，中村以一种私密而残忍的方式，将他对云林的爱，一针一针地钉在她的身上，那片由鞭痕演变而来的刺青。多年以后，经历过时间濯洗的云林，战争带给她的伤痛早已烟消云散，她终于能读懂那个男人的良苦用心。可正如那些被她扯下的折纸一般，当她像从前一样站在门口，凝视着这框架里嵌入的一方天地、一草一木、一花一叶，完成一次『借景』，可那个人此刻又在哪里呢？如果爱上一个来自侵略者国家的人，这样的爱算不算对妹妹、对战争带来的伤痛的背叛？而如果向殖民地的女人吐露国家的秘密，这样的爱又是否是对国家的背叛？年轻时，我们总以为爱情是奋不顾身，可爱与不爱，一个人说了不算啊……\n林深不见“中村”\r回过头看历史的时候，我们总以为“奋六世之余烈”的大秦帝国，理应是正义的一方，因为历史常常由赢家来书写，一旦你赢了，你怎么说都行，因为后人并不关心真相。一如历史上古罗马帝国横跨亚、欧、非三块大陆，蒙古帝国全盛时期版图甚至辐射到波兰，英国皇家海军一度在海上建立起“日不落”帝国……再后来，人们只记住了联合国五常，我们本以为这个世界不会再有战争，可塔利班还是在阿富汗打了起来。我不认为，战争与伤痛，就只能带来仇恨，就像朝鲜与韩国、印度和巴基斯坦，可能在我们有生之年都不会迎来和解，如果战争不能让人意识到爱与和平的珍贵，相反，它深化了人们内心的仇恨与愤怒，我以为，这才是对那些因为战争而死去的人的辜负和背叛。所以，身处乱世，一座庭院或者建筑，其本身是一种侘寂之美、无用之美，却能在痛苦与艰难中为内心寻得一处安宁，而这正是我看完 《夕雾花园》 后想表达的一种观点。无论爱在西元前还是西元后，站在庞贝古城前的你我，是否可以从此刻启程，回到人物饱满、情节充沛的某个瞬间，那一年，汉谟拉比用楔形文字刻下第一部法典，我静静地看着你，完全没想过防卫，拥抱的刹那，庞贝古城仿佛从未消失过一样……\n彼岸浮灯\r这世间的有些废墟相当壮观，而我对它们曾经的丰功伟绩知之甚少；这世间的有些故居格外亲切，而它们的主人对于历史进程的影响微乎其微。中国传统文学中的抒情主题，占比最多的怀古之情、兴亡之叹，所以，陈子昂登幽州台，发出前无古人、后无来者的慷慨悲歌；苏东坡赤壁遨游，感慨人生如梦，一樽还酹江月；辛弃疾登京口北固亭，梦回金戈铁马，气吞万里如虎……可能，从废墟中寻找某种感同身受的历史幻想，这才是中国文人群体如同着魔一般的集体症候，而将这一现象延伸到更大的世界，也许，全人类都是这样。谁能说刺客信条系列游戏，不是将人类几千年的历史，放在 Animus 这样一个沉浸式的 VR 设备中重新演绎呢？当艾吉欧·奥迪托雷漫步在古罗马的街道，人们总会不由自主地想到，凯撒在长老院遇刺身死、古罗马斗兽场里的三千斯巴达勇士……罗马不是一天建成的，而条条大路通罗马，这种由废墟而产生的莫名的情结，始终萦绕在人们的心头，帕特农神庙、圆明园、庞贝古城、乞力马扎罗雪山……世间有太多的风景，还未来得及亲眼去看就已经消失不见，无论是天然的景致，还是人造的景观，其实和人类本身一样，都要面临消逝的结局，唯一的不同的是，在时间尺度上它们显得更为永恒一点，相比六十年产权的现代化住宅，我还是更喜欢，这些废墟里慢悠悠地流淌着的故事。\n","date":"2021-08-26T09:13:32Z","image":"https://i.loli.net/2021/08/27/D2mPAnxp3Zlo6hv.jpg","permalink":"https://qinyuanpei.github.io/posts/3623891261/","slug":"3623891261","tags":["夕雾花园","影评","建筑","历史"],"title":"夕雾花园：从建筑中读出的爱情和美学"},{"categories":["编程语言"],"content":"这个月月初的时候，朋友兴奋地和我描述着他的计划——准备带孩子到宁夏自驾游。朋友感慨道，“小孩只在书本上见过黄河、见过沙漠，这样的人生多少有一点遗憾”，可正如新冠病毒会变异为德尔塔一样，生活里唯一不变的变化本身，局部地区疫情卷土重来，朋友为了孩子的健康着想，不得不取消这次计划，因为他原本就想去宁夏看看的。回想过去这一年多，口罩和二维码，是每天打交道最多的东西。也许，这会成为未来几年里的常态。在西安，不管是坐公交还是地铁，都会有人去检查防疫二维码，甚至由此而创造了不少的工作岗位。每次看到那些年轻人，我都有种失落感，因为二十九岁高龄的我，已然不那么年轻了，而这些比我更努力读书、学历更高的年轻人，看起来在做着和学历/知识并不相称的工作。也许，自卑的应该是我，因为国家刚刚给程序员群体定性——新生代农民工。可是，我这个农民工，今天想做一点和学历/知识相称的事情，利用 Python 来自动识别防疫二维码。\n原理说明 对于防疫二维码而言，靠肉眼去看的话，其实主要关注两个颜色，即标识健康状态的颜色和标识疫苗注射状态的颜色。与此同时，为了追踪人的地理位置变化，防疫/安检人员还会关注地理位置信息，因此，如果要自动识别防疫二维码，核心就是读出其中的颜色以及文字信息。对于颜色的识别，我们可以利用 OpenCV 中的 inRange() 函数来实现，只要我们定义好对应颜色的 HSV 区间即可；对于文字的识别，我们可以利用 PaddleOCR 库来进行提取。基于以上原理，我们会通过 OpenCV 来处理摄像头的图像，只要我们将手机二维码对准摄像头，即可以完成防疫二维码的自动识别功能。考虑到检测不到二维码或者颜色识别不到这类问题，程序中增加了蜂鸣报警的功能。写作本文的原因，单纯是我觉得这样好玩，我无意借此来让人们失业。可生而为人，说到底不能像机器一样活着，大家不都追求有趣的灵魂吗？下面是本文中使用到的第三方 Python 库的清单：\npyzbar == 0.1.8 opencv-contrib-python == 4.4.0.46 opencv-python == 4.5.3.56 paddleocr == 2.2.0.2 paddlepaddle == 2.0.0 图块检测 下面是一张从手机上截取的防疫二维码图片，从这张图片中我们看出，整个防疫二维码，可以分为三个部分，即：上方的定位信息图块，中间的二维码信息图块，以及下方的核酸检验信息图块。\n“西安一码通” 防疫二维码\r对于二维码的检测，我们可以直接使用 pyzbar 这个库来解析，可如果直接对整张图进行解析，因为其中的干扰项实在太多，偶尔会出现明明有二维码，结果无法进行解析的情况。所以，我们可以考虑对图片进行切分，而切分的依据就是图中的这三个图块。这里，我们利用二值化函数 threshold() 和 轮廓提取函数 findContours() 来实现图块的检测：\n# 灰度化 \u0026amp; 二值化 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) _, binary = cv2.threshold(gray, 135, 255, cv2.THRESH_BINARY) # 检测轮廓，获得对应的矩形 contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2] for i in range(len(contours)): block_rect = cv2.boundingRect(contours[i]) 这里有一个感触颇深的地方，在检测图块的过程中，博主发现中间和底部这两个图块，其检测要更为简单一点，因为它有明显的边界、属于规则的矩形，而上方的图块，因为带有装饰性的纹理，以及灰色的过渡区，二值化并不能检测到其边缘，如下图所示，地铁上使用的二维码，相比商场里使用的二维码，轮廓线要更为清晰一点。所以，这里选择一个什么样的阈值来做二值化，个人感觉是需要反复去尝试的。考虑到要兼容这种轮廓不规则的图块，实际上我使用了一点小技巧，即：在得到下面两个图块以后，利用高度的换算关系，人为地生成上方图块的矩形范围。\n“西安一码通” 灰度化 \u0026amp;amp; 二值化\r那么，这是否说明，代表美的设计，在代表绝对理性的算法面前，其实更像是一种噪音。也许，它们各自的领域不同、观点不同，可都一样在为这个世界发光发热，生活不止一种真相，世界不止一种回声，有微小的差异，同样有宏大的统一。\n二维码检测 好了，我们可以注意到，一旦完成图块的切分，此时，二维码位于中间这个图块，检测二维码在这里并不是重点，因为检测这个二维码是第一步，按照这个二维码所在的矩形去检测中心的的色彩，这是这里的重点，因为这个二维码解析以后就是一个 URL 地址，本身并没有包含任何信息，我们想要知道一个人是否健康，唯一的办法就是检测中间的色彩。其实，理论上剩余两个图块同样需要检测色彩，可考虑到三者在含义的表达上是一致的，即三者拥有相同的颜色，我们只需要处理其中一个即可。下面是利用 pyzbar 库对二维码区块进行解析，获取二维码信息、二维码所在的矩形等信息的代码片段：\n# 检测二维码 def detect_qrcode(image, block): block_image, block_rect, _ = block block_x, block_y, _, _ = block_rect gray = cv2.cvtColor(block_image, cv2.COLOR_BGR2GRAY) qrcodes = decode(gray, [ZBarSymbol.QRCODE]) if len(qrcodes) \u0026gt; 0: qrcode = qrcodes[0] qrcodeData = qrcode.data.decode(\u0026#34;utf-8\u0026#34;) x, y, w, h = qrcode.rect abs_x = block_x + x abs_y = block_y + y cv2.rectangle(image, (abs_x, abs_y), (abs_x + w, abs_y + h), color_marker, 2) return True, qrcodeData, (abs_x, abs_y, w, h) else: return False, None, None 可以注意到，通过 pyzbar 这个库，我们不单单可以获取到二维码的信息，同时还可以获得二维码在图块中的矩形范围，由此我们可以推算出，二维码在整张图片中的矩形范围，我们会绘制一个矩形来标识二维码的位置，这样使用者就可以清楚的知道，我们的的确确检测到了二维码。\n色彩检测 一旦我们确定了二维码的矩形范围，接下来的工作，就是在这个矩形范围里检测颜色啦！譬如一个人如果健康状态，二维码的中间部分会显示为绿色。如果一个人完成了疫苗的注射，二维码边上的区域会显示为金色。所以，基于这样的原理，我们只需要检测对应区域是否有对应的颜色即可，这里主要利用了HSV颜色模型，不同于RGB颜色模型，HSV颜色模型利用色相、饱和度和亮度三个指标来描述颜色，是一种把RGB色彩空间中的点放在倒圆锥体上的表示方法。其中：\nH，即 Hue，表示色相，它通过角度来度量，因此，它的取值范围是0 到 360 度，如下图所示，红色对应 0 度，绿色对应 120 度，蓝色对应 240 度： HSV 颜色模型：色相\rS，即 Saturation，表示饱和度，用 0 到 100% 之间的数值表示，如果用下面的倒圆锥体来表示，则 S 表示的是色彩点到所在圆形切面圆心的距离与该圆半径的比值： HSV 颜色模型：倒圆锥体\rV，即 Value，表示亮度，同样用 0 到 100% 之间的数值表示，参考上面的倒圆锥体，可以了解到，V 表示的是色彩点所在圆形切面圆心与该圆圆心在垂直距离上的比值： 此时此刻，你有没有回想起小时候调电视机画面时的经历呢？\n找不到合适的图，简单怀旧一下？\r对于HSV颜色模型，我们可以参考下面的取值范围：\nHSV 颜色模型：参考范围\r以红色为例，其 H 分量取值范围为：0 到 10；S 分量取值范围为：43 到 255；V 分量取值范围为：46 到 255。OpenCV 中的 inRange() 函数，可以判断某个 HSV 数组（此时图片使用一个数组来表示）是否在某个给定的区间范围内。于是，我们的思路就是：定义好目标颜色的 HSV 区间，同时提供一份 HSV 格式的图片数据。此时，其实现逻辑如下：\n# 颜色范围定义 color_dist = { \u0026#39;red\u0026#39;: {\u0026#39;Lower\u0026#39;: np.array([0, 60, 60]), \u0026#39;Upper\u0026#39;: np.array([6, 255, 255])}, \u0026#39;blue\u0026#39;: {\u0026#39;Lower\u0026#39;: np.array([100, 80, 46]), \u0026#39;Upper\u0026#39;: np.array([124, 255, 255])}, \u0026#39;green\u0026#39;: {\u0026#39;Lower\u0026#39;: np.array([35, 43, 35]), \u0026#39;Upper\u0026#39;: np.array([90, 255, 255])}, \u0026#39;golden\u0026#39;: {\u0026#39;Lower\u0026#39;: np.array([26, 43, 46]), \u0026#39;Upper\u0026#39;: np.array([34, 255, 255])}, } # 检测颜色 def detect_color(image, color): # gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # 灰度 gs = cv2.GaussianBlur(image, (5, 5), 0) # 高斯模糊 hsv = cv2.cvtColor(gs, cv2.COLOR_BGR2HSV) # HSV erode_hsv = cv2.erode(hsv, None, iterations=2) # 腐蚀 inRange_hsv = cv2.inRange(erode_hsv, color_dist[color][\u0026#39;Lower\u0026#39;], color_dist[color][\u0026#39;Upper\u0026#39;]) contours = cv2.findContours(inRange_hsv.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2] if len(contours) \u0026gt; 0: draw_color_area(image, contours) return True else: winsound.Beep(440, 5000) return False 这里，我们先对图片做了一次高斯模糊、然后将其转换为 HSV 格式，经过侵蚀以后传给 inRange()函数，这样我们就得到了所有符合这个区间范围的点。接下来，单单找到颜色还不行，我们还需要根据这些点得到一个轮廓，此时，findContours()函数再次登场，为了让使用者更直观地找到对应的颜色区域，我们这里使用下面的方法将其“画”出来：\n# 标记颜色区域 def draw_color_area(image, contours): max, index = 0, -1 for i in range(len(contours)): area = cv2.contourArea(contours[i]) if area \u0026gt; max: max = area index = i if index \u0026gt;= 0: rect = cv2.minAreaRect(contours[index]) cv2.ellipse(image, rect, color_marker, 2, 8) cv2.circle(image, (np.int32(rect[0][0]), np.int32(rect[0][1])), 2, color_marker, 2, 8, 0) 以中间部分的二维码图块为例，此时，我们可以得到下面的结果，这是做了两次颜色检测得到的，第一次检测绿色，第二次检测金色：\n“西安一码通” 防疫二维码：颜色检测\rOCR 识别 OCR识别没有太多悬念，因为我们直接使用 PaddleOCR 即可，因为我们已经完成对图块的切分，只需要依次对图片进行检验即可：\npython -m pip install paddlepaddle==2.0.0 -i https://mirror.baidu.com/pypi/simple python -m pip install paddleocr 在安装的过程中，可能会得到这样的错误信息：Microsoft Visual C++ 14.0 is required。如果你安装了 Visual Studio 依然提示错误，解决方案就是找到 Visual Studio 安装包，然后勾选那些和 Microsoft Visual C++ 14.0 相关的可选的安装项，再安装了这些必要组件以后，重新使用pip安装即可。\n“Microsoft Visual C\u0026#43;\u0026#43; 14.0 is required” 错误信息\r因为 PaddleOCR 接受的是PIL库中的Image类型，所以，在拆分图块的时候，实际上是为每个图块生成了一个对应的文件。此时，OCR 识别部分的代码实现如下。首先，我们需要初始化 PaddleOCR ，首次运行会自动下载训练好的模型文件:\n# PaddleOCR ocr = PaddleOCR() 这里，我们通过detect_text来检测每个图块的文字，并在原始图片中标记出文字位置：\n# 检测文字 def detect_text(image, block): _, block_rect, block_file = block block_x, block_y, _, _ = block_rect result = ocr.ocr(block_file) for line in result: boxes = line[0] texts = line[1][0] x = int(boxes[0][0]) y = int(boxes[0][1]) w = int(boxes[2][0]) - x h = int(boxes[2][1]) - y abs_x = block_x + x abs_y = block_y + y cv2.rectangle(image, (abs_x, abs_y), (abs_x + w, abs_y + h), color_marker, 2) yield texts 以底部图块的检测结果为例，其文字位置标记及文字识别结果如下图所示：\n通过 OCR 识别出来的文字位置\r通过 OCR 识别出来的文字信息\r成品展示 到现在为止，主要的部分我们已经编写完成，接下来，我们只需要接入摄像头，从摄像头捕捉图像即可。这里，请允许在下推荐一个非常好用的软件：iVCam，它可以让手机摇身一变成为摄像头，从而可以让我们模拟扫描二维码的场景。使用 OpenCV 捕捉来自摄像头的图片非常简单，大家可以参考我曾经的博客：视频是不能P的系列：OpenCV人脸检测，这里我们直接给出代码：\ndef handle_video(): cap = cv2.VideoCapture(0) while True: ret, image = cap.read() if ret: # 检测画面中的图块 blocks = list(detect_blocks(image)) # 处理每个图块 for block in blocks: image = handle_block(image, block) # 展示处理结果 cv2.imshow(\u0026#39;QRCode Detecting\u0026#39;, image) # 按 Q 退出 if cv2.waitKey(1) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break else: continue cap.release() cv2.destroyAllWindows() 此时，我们就可以看到下面的结果。可以注意到，在实际应用中，通过视频采集的图像会受到环境光照、拍摄角度等因素的影响，受此影响，我们的图块检测在这个环节表现不佳，它甚至把整张图片当成了一个图块，这直接导致最重要的二维码没有检测出来。百度的 PaddleOCR 表现倒是可圈可点，识别速度和准确性还是非常出色的。对于视频这种级别的输入，特别是在人流量较大的商场、车站等场所，对于识别准确性、可靠性都有着非比寻常的要求，如果要考虑这个思路的落地，应该在图像采集的预处理、图像检测的算法上去下功夫，特别是在拆分图块这个环节，识别的准确性还会受到二维码样式的影响，而这些显然是这篇博客背后的故事啦！正所谓，”路漫漫其修远兮，吾将上下而求索”，如果大家对这个项目感兴趣的话，可以到 Github 上做进一步的了解。\n通过摄像头检测防疫二维码\r本文小结 写完这篇博客的时候，我不由地会想，也许，屏幕前的某个人会在看完这篇博客以后，一脸鄙夷地说道，就这？可这的确就是基础性研究的现状，即：投入了时间和精力，并不一定能得到满意的结果。我们从小到大接受的关于成功的理念，无非都是“只要功夫深，铁杵磨成针”、“吃得苦中苦，方为人上人”\u0026hellip;\u0026hellip;可不知道为什么，这种理念在被一点一点的打破，某种意义上来讲，国家和个人在这个时代面对的选择是相似的，在选择挣快钱还是挣慢钱这个问题上。多年以前，在实验室里捣腾化学试剂的我，曾经一度认为做实验、分析数据、写报告这些事情是枯燥而无用的，因为在当时看来，这些东西距离实际应用都挺遥远的。可是，此刻我大概不得不承认，这些基础工作的重要性。的确，写算法、做模型，这些事情都是科学家去做的事情，我们普通人只要奉行“拿来主义”就好，可当 OpenCV 就放在你手里，而你依然做不好这件事情的时候，大概还是我输了罢，说“认真你就输了”的人，真的真的真的认真过吗？\n","date":"2021-08-19T14:13:32Z","image":"/posts/1509692610/sunflowers-g5d29e5eb6_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/1509692610/","slug":"1509692610","tags":["Python","OpenCV","健康码","防疫"],"title":"使用 Python 自动识别防疫健康码"},{"categories":["编程语言"],"content":"在团队内推广Docker Compose有段时间啦，值得庆幸的是，最终落地效果还不错，因为说到底，大家都不大喜欢，那一长串复杂而枯燥的命令行参数。对我而言，最为重要的一点，团队内使用的技术变得更加透明化、标准化，因为每个微服务的配置信息都写在docker-compose.yml文件中，任何人都可以快速地构建出一套可用的服务，而不是每次都要去找具体的某一个人。我想说，这其实是一个信息流如何在团队内流动的问题。也许，我们有文档或者Wiki，可新人能不能快速融入其中，这才是检验信息流是否流动的唯一标准。就这样，团队从刀耕火种的Docker时代，进入到使用服务编排的Docker Compose时代。接下来，能否进入K8S甚至是云原生的时代，我终究不得而知。今天我想聊聊，在使用Docker Compose的过程中，我们遇到的诸如容器的启动顺序、网络模式、健康检查这类问题，我有一点Docker Compose的进阶使用技巧想和大家分享。\n容器的启动顺序 使用服务编排以后，大家最关心的问题是，如果服务间存在依赖关系，那么如何保证容器的启动顺序？我承认，这是一个真实存在的问题，譬如，你的应用依赖某个数据库，理论上数据库要先启动，抑或者是像Redis、Kafka、Envoy这样的基础设施，总是要优先于应用服务本身启动。\n假如章鱼的这些脚互相影响会怎么样？\r熟悉Docker Compose的同学，也许会想到depends_on这个选项，可如果大家亲自去尝试过就会知道，这终究只是我们的一厢情愿。为什么呢？因为这个depends_on主要是看目标容器是不是处于running的状态，所以，在大多数情况下，我们会注意到Docker Compose并不是按我们期望的顺序去启动的，因为目标容器在某一瞬间的确已经是running的状态了，那这样简直太尴尬了有木有啊！我们从一个简单的例子开始：\nversion: \u0026#34;3.8\u0026#34; services: redis_server: image: redis:latest command: \u0026gt; /bin/bash -c \u0026#39; sleep 5; echo \u0026#34;sleep over\u0026#34;;\u0026#39; networks: - backend city_service: build: CityService/ container_name: city_service ports: - \u0026#34;8081:80\u0026#34; networks: - backend depends_on: - redis_server networks: backend: 可以注意到，为了证明city_service服务不会等待redis_server服务，我故意让子弹飞了一会儿，结果如何呢？我们一起来看看：\nDocker Compose 启动顺序：一厢情愿\r果然，我没有骗各位，city_service服务不会等待redis_server服务。我们知道，Redis提供的命令行接口中，有一个PING命令，当Redis可以正常连接的时候，它会返回一个PONG，也许，这就是乒乓球的魅力所在。基于这个想法，我们继续修改docker-compose.yml文件：\nversion: \u0026#34;3.8\u0026#34; services: redis_server: image: redis:latest networks: - backend city_service: build: CityService/ container_name: city_service ports: - \u0026#34;8081:80\u0026#34; networks: - backend depends_on: - redis_server command: \u0026gt; /bin/bash -c \u0026#39; while ! nc -z redis_server 6379; do echo \u0026#34;wait for redis_server\u0026#34;; sleep 1; done; echo \u0026#34;redis_server is ready!\u0026#34;; echo \u0026#34;start city_service here\u0026#34;; \u0026#39; networks: backend: 这里，我们用了一种取巧的方法，Ubuntu中的nc命令可以对指定主机、指定端口进行检测，换言之，我们简单粗暴的认为，只要6379这个端口可以访问，就认为Redis准备就绪啦，因为我们没有办法在city_service这个容器中调用redis-cli，这个做法本身并不严谨，我们这里更多的是验证想法：\nDocker Compose 启动顺序：检测 Redis\r可以注意到，此时，city_service服务会等待redis_server服务，直到redis_server服务就绪。所以，要解决服务编排时，容器的启动顺序的问题，本质上就是把需要等待的服务、端口以及当前服务的启动命令，统一到容器的入口中。为此，官方提供了 wait-for-it 这个方案，官方关于容器启动顺序的文档，可以参考：Startup Order。对于上面的例子，我们可以这样改写docker-compose.yml文件：\nversion: \u0026#34;3.8\u0026#34; services: redis_server: image: redis:latest networks: - backend city_service: build: CityService/ container_name: city_service ports: - \u0026#34;8081:80\u0026#34; networks: - backend depends_on: - redis_server command: [\u0026#34;/wait-for-it.sh\u0026#34;, \u0026#34;redis_server:6379\u0026#34;, \u0026#34;--\u0026#34;, \u0026#34;dotnet\u0026#34;, \u0026#34;CityService.dll\u0026#34;] networks: backend: 此时，启动容器时的效果如下，因为这个方案依赖 Netcat 这样一个工具，所以，我们的容器中还需要加入这个工具，此时，可以使用下面的脚本片段：\nFROM debian:buster-slim as wait-for-it RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \u0026#34;wait-for-it\u0026#34; COPY --from=wait-for-it /usr/bin/wait-for-it . 不过，不太明白为什么这里一直提示路径不对：\nDocker Compose 启动顺序：wait-for-it.sh\r个人建议，最好将这个语句写在Dockerfile，或者试提供一个类似于entrypoint.sh的脚本文件。关于这个方案的更多细节，大家可以参考官方文档，写这篇文章的时候，我不由得感慨：Shell脚本真的是太难学了(逃……。所以，点到为止。刚刚提到过，我个人觉得这种主机 + 端口号的检测方式不够严谨，因为一个端口可以PING通，并不代表服务一定是可用的，所以，在接下来的内容里，我会介绍基于健康检查的思路。\n容器的健康检查 不知道大家有没有这样的经历，就是你明明看到一个容器的状态变成Up ，可对应的微服务就是死活调不通。面对来自前端同事的戏谑与嘲讽，你不禁仰天长叹一声，开始在容器里翻箱倒柜，一通操作如虎。过了许久，你终于发现是容器内部出现了始料不及的错误。看来，容器状态显示为Up，并不代表容器内的服务就是可用的啊！果然，还是需要一种机制来判断容器内的服务是否可用啊！等等，这不就是传说中的健康检查？恭喜你，答对了！\nDocker 经典集装箱形象\r在Docker及Docker Compse中，均原生支持 健康检查 机制，一旦一个容器指定了HEALTHCHECK选项，Docker会定时检查容器内的服务是否可用。我们都知道，一个普通的 Docker 容器，无非是开始、运行中、停止这样三种状态，而提供了HEALTHCHECK选项的Docker容器，会在这个基础上增加健康(healthy)和非健康(unhealthy)两种状态，所以，我们应该用这两个状态来判断容器内的服务是否可用。下面是一个指定了HEALTHCHECK选项的容器示例：\nFROM FROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim EXPOSE 80 EXPOSE 443 WORKDIR /app COPY /app/publish . ENTRYPOINT [\u0026#34;dotnet\u0026#34;, \u0026#34;CityService.dll\u0026#34;] HEALTHCHECK --interval=5s --timeout=3s \\ CMD curl -fs http://localhost:80/city || exit 1 可以注意到，Docker原生的健康机制，需要通过CMD的方式来执行一个命令行，如果该命令行返回 0 ，则表示成功；返回 1，则表示失败。\n此处，我们还可以配置以下三个参数，--interval=\u0026lt;间隔\u0026gt;表示健康检查的间隔，默认为 30 秒；--timeout=\u0026lt;时长\u0026gt;表示健康检查命令超时时间，超过该时间即表示unhealthy，默认为 30 秒；--retries=\u0026lt;次数\u0026gt;表示连续失败的次数，超过该次数即表示unhealthy。对于我们这里的ASP.NET Core应用而言，如果程序正常启动，显然这个地址是可以调通的，我们可以用这个来作为一个“探针”。\nDocker 健康检查：healthy 我们可以注意到，在容器启动的第 14 秒，其状态为：health：starting。而等到容器启动的第 16 秒，其状态则为：healthy，这表明我们的服务是健康的。此时此刻，如果我们耍点小心思，让curl去访问一个不存在的地址会怎么样呢？可以注意到，此时状态变成了：unhealthy:\nDocker 健康检查：unhealthy HEALTHCHECK指令除了可以直接写在Dockerfile中以外，还可以直接附加到docker run命令上，还是以上面的项目作为示例：\ndocker run --name city_service -d -p 8081:80 city_service \\ --health-cmd=\u0026#34;curl -fs http://localhost:80/city || exit 1\u0026#34; \\ --health-interval=3s \\ --health-timeout=5s \\ --health-retries=3 甚至，我们还可以使用下面的命令来查询容器的健康状态：docker inspect --format='{{json .State.Health}}' \u0026lt;ContainerID\u0026gt;\n{ \u0026#34;Status\u0026#34;: \u0026#34;unhealthy\u0026#34;, \u0026#34;FailingStreak\u0026#34;: 5, \u0026#34;Log\u0026#34;: [{ \u0026#34;Start\u0026#34;: \u0026#34;2021-08-14T15:27:50.3325424Z\u0026#34;, \u0026#34;End\u0026#34;: \u0026#34;2021-08-14T15:27:50.3813102Z\u0026#34;, \u0026#34;ExitCode\u0026#34;: 1, \u0026#34;Output\u0026#34;: \u0026#34;\u0026#34; }] } 不过，我个人感觉这个curl的写法非常别扭，尤其是当我试图在docker-compose中写类似命令的时候，我觉得稍微复杂一点的健康检查，还是交给脚本语言来实现吧！例如，下面是官方提供的针对MongoDB的健康检查的脚本docker-healthcheck.sh：\n#!/bin/bash set -eo pipefail host=\u0026#34;$(hostname --ip-address || echo \u0026#39;127.0.0.1\u0026#39;)\u0026#34; if mongo --quiet \u0026#34;$host/test\u0026#34; --eval \u0026#39;quit(db.runCommand({ ping: 1 }).ok ? 0 : 2)\u0026#39;; then exit 0 fi exit 1 此时，HEALTHCHECK可以简化为：\nHEALTHCHECK --interval=5s --timeout=3s \\ CMD bin/bash docker-healthcheck.sh 更多的示例，请参考：docker-library/healthcheck/ 以及 rodrigobdz/docker-compose-healthchecks。\n其实，对于容器的启动顺序问题，我们还可以借助检查检查的思路来解决，因为depends_on并不会等待目标容器进入ready状态，而是等目标容器进入running状态。这样，就回到了我们一开始描述的现象：一个容器明明都变为Up状态了，可为什么接口就是死活调不通呢？因为我们无法界定这样一个ready状态。考虑到depends_on可以指定condition，此时，我们可以这样编写docker-compose.yml文件：\nversion: \u0026#34;3.8\u0026#34; services: redis_server: image: redis:latest healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;redis-cli\u0026#34;, \u0026#34;ping\u0026#34;] interval: 1s timeout: 3s retries: 30 networks: - backend city_service: build: CityService/ container_name: city_service ports: - \u0026#34;8081:80\u0026#34; networks: - backend depends_on: redis_server: condition: service_healthy networks: backend: 简单来说，我们使用了Redis内置的命令对redis_server服务进行健康检查，而city_service服务则依赖于redis_server服务的健康状态，只有当Redis准备就绪了以后，city_service才会开始启动。下面是实际启动过程的截图，看看是不是和我们想的一样：\nDocker 健康检查：容器启动顺序\r果然，奇怪的知识有增加了呢，我们唯一需要解决的问题，就是怎么给某一个服务做健康检查，以上！\n容器的网络模式 接下来，我们来说说Docker里的网络模式，特别是当我们使用docker-compose来编排一组服务的时候，假设我们有一个目录app，在这个牡蛎里我们放置了服务编排文件docker-compose.yml，默认情况下，Docker-Compose会创建一个一个名为app_default的网络，并且这个网络是bridge，即网桥模式的一个网络。什么是网桥模式呢？你可能会感到困惑，而这要从Docker中的网络模式开始说起，这里简单下常用的几种：\nhost 模式，或叫做主机模式，可以认为容器和主机使用相同的端口进行访问，因为容器和主机在同一个网络下，此模式下，意味着通过-p绑定的端口失效，因为所有容器都使用主机的网络，所以容器间可以相互通信，此模式通过--network=host指定。 bridge 模式，或叫做网桥模式，这是Docker中默认的网络设置，此模式下，容器和主机有各自的 IP/端口号，两者之间通过一个虚拟网桥进行通信，虚拟网桥的作用类似于物理交换机。因此，不同容器间的网络是相互隔离的，此模式通过--network=bridge指定。 none 模式，通俗讲就是无网络模式，意味着容器是一个封闭的环境，无法通过主机访问外部的网络，这种模式在那种讲究保密性质、封闭式开发的场合应该会有一点用，可这都 2021 年了，难道你还能把互联网上的软件全部下载下来吗？此模式通过--network=none指定。 container 模式，或叫做共享模式，通俗来讲，就是指一个容器共享某个已经存在的容器的Network Namespace，此时，该容器将不会拥有属于自己的 IP/端口号等资源，因为这种模式可以节约一定的网络资源，此模式通过--network=\u0026lt;Container_ID\u0026gt;/\u0026lt;Container_Name\u0026gt;指定。 为了帮助大家理解和区分这四种模式，博主绘制了下面的图示来补充说明：\n容器的网络模式(主机、容器、网桥)示意图\r通过以上的图文信息反复加深印象，相信大家可以找出点规律：\n如果你的容器网络与主机网络不需要隔离，那么选择主机模式(host) 如果你的应用运行在不同的容器里，并且这些容器间需要相互通信，那么选择网桥模式(bridge) 如果你的应用需要运行在一个隔绝外界网络的环境中，那么选择无网络模式(none) 如果你希望在节省网络资源的同时，实现不同容器间的通信，那么选择容器模式(container) 以上四种网络模式，除了可以在docker run的时候指定以外，我们还可以在docker-compose.yml文件中指定。例如，下面表示的是一个主机模式的容器：\nversion: \u0026#39;3.8\u0026#39; services: cache_server: build: . container_name: cache_server restart: always network_mode: host 大多数情况下，我们只需要连接到docker0这个虚拟网卡即可，而如果你想为某个容器或者一组容器单独建立这样一张网卡，此时，就不得不提到Docker中的自定义网络功能，我们一起来看下面的示例：\n// 创建一个网络：test-network docker network create test-network // 创建一个Nginx的容器：nginx_8087，使用网络：test-network docker run -d --name nginx_8087 --network test-network -p 8087:80 nginx:latest // 创建一个Nginx的容器：nginx_8088 docker run -d --name nginx_8088 -p 8088:80 nginx:latest // 连接容器：nginx_8088 至网络：test-network docker network connect test-network nginx_8088 接下来，通过下面的命令，我们可以拿到两个容器的 ID，在此基础上我们看一下两个容器各自分配的 IP 是多少：\ndocker ps -a docker inspect --format=\u0026#39;{{.NetworkSettings.IPAddress}}\u0026#39; \u0026lt;ContainerID\u0026gt; 此时，我们会发现一个有趣的现象，nginx_8087这个容器，可以获得 IP 地址172.17.0.2，而nginx_8088则无法获得 IP 地址，这是为什么呢？这其实就是我们前面提到过的容器模式(container)，此时，nginx_8088这个容器实际上是和nginx_8087共享一个Network Namespace，即使它们有各自的文件系统。同样地，我们可以使用下面的命令来让容器从某个网络中断开：\n// 断开容器：nginx_8088 至网络：test-network docker network disconnect test-network nginx_8088 // 删除网络 docker network rm test-network 是否觉得手动维护容器的网络非常痛苦？幸好，我们还有Docker-Compose可以用，上面两个Nginx的容器我们可以这样维护：\nversion: \u0026#34;3.8\u0026#34; services: nginx_8087: image: nginx:latest container_name: nginx_8087 ports: - 8087:80 networks: - test-network nginx_8088: image: nginx:latest container_name: nginx_8088 ports: - 8088:80 networks: - test-network networks: test-network: driver: bridge 此时，我们可以注意到，Docker Compose会创建两个网络，即network_mode_default和network_mode_test-network：\nDocker Compose 中使用自定义网络\r这说明默认网络依然存在，如果我们希望完全地使用自定义网络，此时，我们可以这样修改服务编排文件：\nnetworks: default: driver: host 这表示默认网络会采用主机模式，相应地，你需要修改nginx_8087和nginx_8088两个容器的network选项，使其指向default。\n除此之外，你还可以使用external指向一个已经存在的网络：\nnetworks: default: external: true name: a-existing-network 在Docker中，每个容器都会分配IP，因为这个IP总是不固定的，所以，如果我们希望像虚拟机那样使用一个静态IP的话，可以考虑下面的做法：\nversion: \u0026#34;3.8\u0026#34; services: nginx_8087: image: nginx:latest container_name: nginx_8087 ports: - 8087:80 networks: - test-network ipv4_address: 172.2.0.10 nginx_8088: image: nginx:latest container_name: nginx_8088 ports: - 8088:80 networks: - test-network ipv4_address: 172.2.0.11 networks: test-network: driver: bridge config: - subnet: 172.2.0.0/24 关于Docker及Docker Compose中的网络驱动，如 macvlan、overlay 等等，这些显然是更加深入的话题，考虑到篇幅，不在这里做进一步的展开，对此感兴趣的朋友可以参考官方文档：Networking Overview 以及 Networking in Compose。博主写这篇文章的想法，主要是源于团队内落地Docker-Compose时的一次经历，当时有台虚拟机偶尔会出现IP被篡改的情况，而罪魁祸首居然是Docker-Compose，虽然最终用主机模式勉强解决了这个问题，可终究留下了难以言说的疑问，此刻，大概能稍微对Docker的网络有点了解。果然，越靠近底层，就是越是抽象、越是难以理解。\n文本小结 本文分享了Docker及Docker-Compose中的进阶使用技巧，主要探索了服务编排场景下容器的启动顺序、健康检查、网络模式三类问题。默认情况下，Docker-Compose的depends_on选项，取决于容器是否处于running状态，因此，当我们有多个服务需要启动时，实际上启动顺序并不会受到depends_on选项的影响，因为此时容器都是running的状态。为了解决这个问题，官方提供了 wait-for-it 的方案，这是一种利用 Netcat 对TCP和UDP进行检测的机制，当检测条件被满足的时候，它会执行由用户指定的启动脚本。从这里看，其实已经有了一点健康检查的影子，而官方的健康检查，则允许用户使用更加自由的命令或者脚本去实现检测逻辑，所以，从这个角度上来讲，HEALTHCHECK结合depends_on，这才是实现容器启动顺序控制的终极方案。Docker的网络是一个相对复杂的概念，所以，这里就是简单的介绍了下常见的四种网络模式，更深入的话题比如网络驱动等，还需要花时间去做进一步的探索。本文示例以上传至Github，供大家参考。好了，以上就是这篇博客的全部内容啦，谢谢大家！\n","date":"2021-08-14T22:13:32Z","image":"https://i.loli.net/2021/08/15/fplPBvICiEOYsKR.jpg","permalink":"https://qinyuanpei.github.io/posts/172025911/","slug":"172025911","tags":["Docker","容器","服务编排","云原生"],"title":"你不可不知的容器编排进阶技巧"},{"categories":["编程语言"],"content":"在构建以 gRPC 为核心的微服务架构的过程中，博主曾经写过一篇名为 ASP.NET Core gRPC 打通前端世界的尝试 的文章，主要是希望打通 gRPC 和 前端这样两个异次元世界，因为无论我们构建出怎样高大上的微服务架构，最终落地的时候，我们还是要面对当下前后端分离的浪潮。所以，在那篇文章中，博主向大家介绍过 gRPC-Web 、gRPC-Gateway 、封装 API 、编写中间件 这样四种方案。我个人当时更喜欢编写中间件这种方案，甚至后来博主进一步实现了 gRPC 的 “扫描” 功能。\n当时，博主曾模糊地提到过，Envoy 可以提供容器级别的某种实现，这主要是指 Envoy 独有的 gRPC-JSON Transcoder 功能。考虑到 Envoy 是一个同时支持 HTTP/1.1 和 HTTP/2 的代理软件，所以，它天然地支持基于 HTTP/2 实现的 gRPC。所谓 gRPC-JSON Transcoder，其实指 Envoy 充当了 JSON 到 Protobuf 间互相转换的角色，而它利用的正是 Envoy 中的 过滤器 这一重要组件。好了，在今天这篇文章中，博主就为大家介绍一下这种基于 Envoy 的方案，如果大家困惑于如何把 gRPC 提供给前端同事使用，不妨稍事休息、冲一杯卡布奇诺，一起来探索这广阔无垠的技术世界。\n从 Envoy 说起 开辟鸿蒙，始有天地。上帝说，要有光，于是，就有了光。而故事的起源，则要追溯到我们最早提出的那个问题：假设我们有下面的 gRPC 服务，我们能否让它像一个 JSON API 一样被调用？ 通过查阅 Protobuf 的 官方文档，我们可以发现 Protobuf 与 JSON间存在着对应关系，这是两者可以相互转化的前提。博主在编写 中间件 时，同样借助了 Protobuf 暴露出来的接口 MessageParser：\nsyntax = \u0026#34;proto3\u0026#34;; option csharp_namespace = \u0026#34;GrpcService\u0026#34;; package greet; service Greeter { rpc SayHello (HelloRequest) returns (HelloReply); } message HelloRequest { string name = 1; } message HelloReply { string message = 1; } 接下来，这个 gPRC 服务如何和 Envoy 这个代理服务器产生关联呢？首当其冲的自然是一个路由啦：\nroutes: - match: prefix: \u0026#34;/greet\u0026#34; route: cluster: grpc_service timeout: seconds: 60 这表示以 /greet 开头的请求会被路由到 grpc_service 这个集群，如果按照一般的 Envoy 使用流程，接下来，我们只需要配置对应的集群节点即可。我们前面提到过，Envoy 的这个 gRPC-JSON Transcoder 功能，是通过过滤器来实现的，更确切地说，它是一个 HTTP 级别的过滤器，所以，我们继续耐心往下看：\nhttp_filters: - name: envoy.filters.http.grpc_json_transcoder typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.grpc_json_transcoder.v3.GrpcJsonTranscoder proto_descriptor: \u0026#34;/etc/descriptor/greet.pb\u0026#34; services: - \u0026#34;greet.Greeter\u0026#34; print_options: add_whitespace: true always_print_primitive_fields: true always_print_enums_as_ints: false preserve_proto_field_names: false auto_mapping: true - name: envoy.filters.http.router 可以注意到，这里使用了一个叫做 envoy.filters.http.grpc_json_transcoder 的过滤器。对于这个过滤器而言，核心的、需要注意的地方有两个：\nproto_descriptor 指向一个 Protobuf 的描述文件，这是一个二进制文件，可以由protoc编译器生成。 services 表示一组服务，必须按照 包名.服务名 的格式进行填写，这里的示例为：greet.Greeter。 关于如何生成二进制的 Protobuf 描述文件，我们专门放在下一节来讲，在此基础上，我们只要增加集群即可完成 Envoy 的配置：\nclusters: - name: grpc_service connect_timeout: 0.25s type: LOGICAL_DNS lb_policy: ROUND_ROBIN dns_lookup_family: V4_ONLY http2_protocol_options: {} upstream_connection_options: tcp_keepalive: keepalive_time: 300 load_assignment: cluster_name: grpc_service endpoints: - lb_endpoints: - endpoint: address: socket_address: address: grpc_service port_value: 80 完整的 Envoy 配置文件，请参考 这里，不再占用篇幅进行说明。\n准备描述文件 生成 Protobuf 的二进制描述文件，需要借助 protoc 这个命令行工具，此前我们介绍 gRPC 生态中的 gRPC-Web、gRPC-Gateway 时曾经接触过它。Envoy 正是通过这个描述文件来处理 JSON 和 Protobuf 的相互转换，博主猜测这里可能用到了类似 MessageParser 的东西，Envoy 从这个二进制的描述文件中获取 gRPC 的元数据信息，并由此从 JSON 构建出 Protobuf。这里，我们还是以本文开始的 .proto文件为例：\nprotoc --descriptor_set_out=./Protos/descriptor/greet.pb --include_imports Protos\\greet.proto 这条命令行的含义是，为 Protos\\greet.proto 生成对应的服务描述文件 /Protos/descriptor/greet.pb。下图即为博主生成的服务描述文件：\n通过命令行生成 Protobuf 描述文件\r此时，我们只需要将其放到 Envoy 的目录中即可，本文中的示例位于以下路径：/etc/descriptor/greet.pb。好了，现在 Envoy 和 gRPC 均已就绪，我们通过 docker-compose 对服务进行编排：\nversion: \u0026#39;3\u0026#39; services: envoygateway: build: Envoy/ ports: - \u0026#34;9090:9090\u0026#34; - \u0026#34;9091:9091\u0026#34; volumes: - ./Envoy/envoy.yaml:/etc/envoy/envoy.yaml grpcservice: build: GrpcService/GrpcService/ ports: - \u0026#34;8082:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; 启动服务后，如果我们像调用 gRPC 服务中的 SayHello()方法，此时，对应的路由为：/greet.Greeter/SayHello，即：包名.服务名/方法名。好了，我们用 Postman 或者 Apifox 对接口进行测试：\n像调用一个 JSON API 一样调用 gRPC 至此，我们实现一开始的目的，通过 Envoy 代理 gRPC 服务以后，对于前端而言，它已不再关心，这个服务背后的服务提供者到底是什么？因为对它而言，JSON API 还是 Protobuf 已经完全没有差别。博主曾经评价它是容器级别的方案，因为它可以将多个 gRPC 服务统一到一个入口中，非常适合充当整个微服务的网关，如果你正在使用 gRPC，相信我，这会是一条必由之路。\n目前，博主所在的公司，已经全面采用了这种方案，而博主则进一步在团队中推广了Docker-Compose，换言之，我们将多个微服务通过Docker-Compose进行编排，并通过 Envoy 为所有微服务提供统一入口，唯一的遗憾是，通过protoc生成服务描述文件这个过程没有纳入到 CI/CD 环节，靠手动生成、复制服务描述文件，到底还是会有点失落呢？如果结合前面分享过的 Envoy 身份认证，整个微服务架构终于看起来形成闭环啦！\n本文小结 本文分享了 Envoy 中的 gRPC-JSON Transcoder 功能，它可以将一个 gRPC 服务代理成一个 JSON API，从而方便前端或者是客户端去消费一个 gRPC 服务。其原理是，Envoy 中可以通过配置过滤器来实现 JSON 和 Protobuf 的相互转换，这一过程依赖 Protobuf 的元数据，故而，我们需要通过命令行工具protoc生成服务描述文件，我们只需要在 Envoy 中添加相关配置，就可以像调用一个 JSON API 一样调用 gRPC。至此， gRPC 与 Web 世界彻底打通，我们可以用我们熟悉的技术去消费一个 gRPC 服务。博主的 Grpc.Gateway 实现了类似的功能，如果大家感兴趣，欢迎大家前去体验一番。好了，以上就是这篇博客的全部内容啦，谢谢大家，祝各位晚安！\n","date":"2021-08-08T22:49:47Z","image":"https://grpc.io/img/landing-2.svg","permalink":"https://qinyuanpei.github.io/posts/3942175942/","slug":"3942175942","tags":["Envoy","微服务","gRPC","RESTful"],"title":"ASP.NET Core 搭载 Envoy 实现 gRPC 服务代理"},{"categories":["编程语言"],"content":"AOP，即：面向切面编程，关于这个概念，博主其实写过好几篇博客啦！从这个概念，我们可以引申出诸如代理模式、动态代理、装饰器模式、过滤器、拦截器等等相互关联的概念。从实现方式上而言，微软官方的 .NET Remoting 提供了真实代理和透明代理的支持，我们熟悉的 WebService 和 WCF 均和这项技术息息相关，作为最早的分布式 RPC 解决方案，其本身更是与客户端的动态代理密不可分。或许，各位曾经接触过 Unity、Castle、AspectCore、PostSharp 等等这些支持 AOP 特性的库，那么，我们是否已经抵达了 AOP 的边界呢？事实上，如果你仔细研究过 Stub 和 Mock 这样两个术语，你就发现 AOP 的应用范围远比我们想象的宽广。今天这篇文章，我不打算再介绍一遍这些第三方库的“奇技淫巧”，我更想聊聊，如何通过 AOP 来简化一个缓存操作。\n缓存，一个面试时命中率 100%的话题，曾记否？来自面试官的灵魂发问三连：缓存击穿、缓存穿透、缓存雪崩。与此同时，缓存是一个令人爱恨交加的东西，其一致性、持久化、高可用等等，均是实际应用中需要去考虑的东西。狭义的缓存主要指 Redis、Memcached 等分布式缓存系统，而广义的缓存则可以是 HTTP 响应缓存、EF/EF Core 查询缓存、二级缓存等等。我们都知道，使用缓存可以显著地提升软件性能，而究其本质，则是因为减少了和数据库交互的频次。于是，我们注意到，大多数的缓存代码，都是下面这样的风格：\nvar cacheKey = \u0026#34;GetAllStudents\u0026#34;; var students = new List\u0026lt;Student\u0026gt;(); var cacheValue = distributedCache.GetString(cacheKey); if (string.IsNullOrEmpty(cacheValue)) { // 未命中缓存：从数据库查询数据 + 写缓存 students = repository.GetAll().ToList(); var bytes = Encoding.UTF8.GetBytes(JsonConvert.SerializeObject(students)); distributedCache.Set(cacheKey, bytes); } else { // 命中缓存：读缓存 students = JsonConvert.DeserializeObject\u0026lt;List\u0026lt;Student\u0026gt;\u0026gt;(cacheValue); } return students; 正所谓：大道至简，“高端的食材，往往只需要最朴素的烹饪方式”。故而，最朴素的思想就是，首先从缓存中查询数据，如果数据存在则直接返回，否则从数据库中查询数据，并执行一次写缓存操作。这的确是个朴实无华的方案，因为我们每一次都要写这样的代码，其程度丝毫不亚于永远不会缺席的 xxx != null。写到这里，博主不由得陷入了沉思：难道真的没有更简单点的方案了吗？后来的故事大家都知道了，我们可以在方法的参数上附加 [NotNull] 特性。所以，接下来，我们会用类似的方案来解决缓存的问题，换言之，我们可以把我们经常写、写到不愿意再写的代码交给代理类来做，既然缓存本质上是为了查询数据，那我们就只需要关心查询数据这个行为本身。具体怎么实现的呢？我们一起来看下面的代码。\n此时此刻，假设我们有这样一个接口：IFakeService，它通过GetColors()方法返回一组颜色：\ninterface IFakeService { [Cacheable(CacheKeyPrefix = \u0026#34;Fake\u0026#34;, Expiration = 180)] List\u0026lt;string\u0026gt; GetColors(); } 我们希望，在调用这个方法的时候，可以对其返回值进行缓存，所以，可以注意到，这里添加了一个[Cacheable]的特性。其定义如下：\n[AttributeUsage(AttributeTargets.Method)] public class CacheableAttribute : Attribute { public string CacheKeyPrefix { get; set; } public int Expiration { get; set; } } 其中，CacheKeyPrefix用于指定缓存键名前缀，Expiration用于指定缓存过期时间，单位为秒。接下来，博主通过DispatchProxy来实现动态代理，它可以视为RealProxy在后.NET 时代的替代品：\npublic class CacheInterceptor\u0026lt;TCacheService\u0026gt; : DispatchProxy { private TCacheService _realObject =\u0026gt; ServiceProvider.GetRequiredService\u0026lt;TCacheService\u0026gt;(); private ICacheSerializer _cacheSerializer =\u0026gt; ServiceProvider.GetRequiredService\u0026lt;ICacheSerializer\u0026gt;(); private IDistributedCache _distributedCache =\u0026gt; ServiceProvider.GetRequiredService\u0026lt;IDistributedCache\u0026gt;(); public IServiceProvider ServiceProvider { get; set; } protected override object Invoke(MethodInfo targetMethod, object[] args) { byte[] cacheValue; var returnType = targetMethod.ReturnType; // void \u0026amp;\u0026amp; Task if (returnType == typeof(void) || returnType == typeof(Task)) return targetMethod.Invoke(_realObject, args); if (IsAsyncReturnValue(targetMethod)) returnType = targetMethod.ReturnType.GetGenericArguments()[0]; var cacheableAttribute = targetMethod.GetCustomAttribute\u0026lt;CacheableAttribute\u0026gt;(); if (cacheableAttribute != null) { var cacheKey = GetCacheKey(cacheableAttribute, targetMethod); cacheValue = _distributedCache.Get(cacheKey); if (cacheValue != null) { // Task\u0026lt;T\u0026gt; if (IsAsyncReturnValue(targetMethod)) return Task.FromResult(_cacheSerializer.Deserialize(cacheValue, returnType)); return _cacheSerializer.Deserialize(cacheValue, returnType); } dynamic returnValue = targetMethod.Invoke(_realObject, args); cacheValue = _cacheSerializer.Serialize(returnValue); // Task\u0026lt;T\u0026gt; if (IsAsyncReturnValue(targetMethod)) cacheValue = _cacheSerializer.Serialize(returnValue.Result); var cacheOptions = new DistributedCacheEntryOptions() { AbsoluteExpirationRelativeToNow = TimeSpan.FromSeconds(cacheableAttribute.Expiration) }; _distributedCache.Set(cacheKey, cacheValue, cacheOptions); return returnValue; } return targetMethod.Invoke(_realObject, args); } } 这里，最为关键的地方是Invoke()方法，它负责对被代理对象的方法进行拦截，这里的被代理对象，其实就是_realObject，即真实对象，因为，我们最终调用的，实际上是真实对象上对应的方法。因为DispatchProxy在创建代理对象时，要求这个代理基类，即这里的拦截器，必须要有一个无参的构造函数。所以，我们这里用属性注入的方式来注入IServiceProvider。说回这个方法，首先，我们会判断它的返回值类型是不是void或者Task，因为无返回值的方法本身就不需要缓存。接下来，我们会检查当前方法上是否附加了[Cacheable]特性，因为我们只需要处理有这个特性的方法。接下来，通过GetCacheKey()方法来生成一个唯一的键名，通过这个键名我们就可以在缓存中查询数据啦，该方法的实现细节如下：\nprivate string GetCacheKey(CacheableAttribute cacheableAttribute, MethodInfo methodInfo) { var segments = new List\u0026lt;string\u0026gt;(); if (!string.IsNullOrEmpty(cacheableAttribute.CacheKeyPrefix)) segments.Add(cacheableAttribute.CacheKeyPrefix); segments.Add(methodInfo.DeclaringType.FullName.Replace(\u0026#34;.\u0026#34;, \u0026#34;_\u0026#34;)); segments.Add(methodInfo.Name); methodInfo.GetParameters().ToList().ForEach(x =\u0026gt; segments.Add(x.Name)); return string.Join(\u0026#34;_\u0026#34;, segments); } 对于分布式缓存，博主这里使用的是微软提供的IDistributedCache这个接口，接下来的事情就变得朴实无华起来，因为它和我们一开始写的代码一脉相承，唯一的不同是，这里考虑了Task\u0026lt;T\u0026gt;这种异步的返回值类型，同时对序列化/反序列化进行了抽象，即这里注入的ICacheSerializer接口，注意到IDistributedCache接口的Set()方法需要传入一个byte[]，显然二进制的序列化方案如 Protobuf 、MessagePack 会更加得心应手一点。所以，我们将这一层单独抽象出来。至此，我们已经完成了最核心的部分。\n对于一开始的IFakeService，我们提供一个简单的实现，并通过让线程阻塞的方式来模拟一个耗时操作：\npublic class FakeService : IFakeService { public List\u0026lt;string\u0026gt; GetColors() { Thread.Sleep(500); return new List\u0026lt;string\u0026gt; { \u0026#34;Red\u0026#34;, \u0026#34;Yellow\u0026#34;, \u0026#34;Green\u0026#34; }; } } 下面是一个简单的示例：\n// 注入IFakeService、ICacheSerializer、IDistributedCache var services = new ServiceCollection(); services.AddTransient\u0026lt;IFakeService, FakeService\u0026gt;(); services.AddTransient\u0026lt;ICacheSerializer, JsonCacheSerializer\u0026gt;(); services.AddStackExchangeRedisCache(options =\u0026gt; { options.Configuration = \u0026#34;localhost:6379\u0026#34;; options.InstanceName = \u0026#34;Caching.AOP.Test\u0026#34;; }); // 生成代理对象 var serviceProvider = services.BuildServiceProvider(); var fakeServiceProxy = DispatchProxy.Create\u0026lt;IFakeService, CacheInterceptor\u0026lt;IFakeService\u0026gt;\u0026gt;(); (fakeServiceProxy as CacheInterceptor\u0026lt;IFakeService\u0026gt;).ServiceProvider = serviceProvider; // 调用代理对象 for (var i = 0; i \u0026lt; 5; i++) { var stopWatch = new Stopwatch(); stopWatch.Start(); var colors = fakeServiceProxy.GetColors(); stopWatch.Stop(); Console.WriteLine($\u0026#34; {i} - Invoke GetColors used {stopWatch.Elapsed.TotalMilliseconds} ms\u0026#34;); } 此时，我们可以得到下面的结果，可以注意到，第一次调用的时候，因为缓存不存在，调用的时间相对更长一点，而当缓存存在的时候，调用的时间会明显缩短。\n有无缓存对调用时长的影响\r虽然这个性能提升与缓存不无关系，可对于调用者来说，它完全不用关心缓存里有没有数据这件事情，它只需要像往常一样调用接口方法即可，这就是 AOP 之于缓存的意义所在，为了证明我没有说谎，我们可以看到 Redis 中对应的数据：\nRedis中对应的缓存数据\r需要说明的是，这个思路同样可以扩展到Unity、Castle、AspectCore、PostSharp 这些第三方库，实现方式上大同小异，大家可以结合自己的业务场景做相应的调整。其实，从业务上抽离出通用组件、功能作为公共库或者下沉到框架中，是及其自然而然的一件事情。这里面最关键的问题是，基础组件或者框架相对于业务方的职责范围，因为如果基础组件或者框架做得太多，业务上往往难以定制或者扩展；而如果基础组件或者框架做得太少，业务上就要写大量的辅助代码。写这篇文章的原因是，我对于一个缓存方案设计上的疑问，业务上想要缓存一张表中的数据，至少需要写 20 行代码，在下觉得这简直太离谱了，更不用说，业务方还要关心这个缓存是否可用。有人说，一个合格的前任就应该像死了一样，那么，我是不是可以说，一个合格的中间件，就应该像它从来没有来过一样，你甚至都感觉不到它的存在，可事实上它总是无所不在。也许，这听起来有点科幻的色彩，可这的确是我期待的某种自洽的、优雅的设计思路。\n本文小结 本文分享了通过 AOP 来简化缓存操作的一种思路，考虑到常规的缓存代码写法，读/写缓存与业务代码严重耦合在一起，而博主心目中的缓存应该像水、电、煤气一样普普通通，你只需要告诉我哪些数据需要缓存，而无需关心这些数据怎么缓存。基于这样一种考虑，博主基于DispatchProxy实现了一个针对缓存的 AOP 方案，我们只需要在接口上打上[Cachable]标签，它会自动对方法的返回值进行缓存，从而简化我们平时使用缓存的流程。Catcher Wong 大佬在其缓存框架 EasyCaching 同样集成了这一特性，如果大家有类似的使用场景，可以直接使用这个框架。如果大家对此有更好的想法或者思路，欢迎大家在评论区留言，本文示例已上传至 Github，供大家学习或者参考。好了，以上就是这篇博客的全部内容啦，谢谢大家！\n","date":"2021-08-04T20:49:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2126762870/","slug":"2126762870","tags":["缓存","AOP","动态代理","Redis"],"title":"再话 AOP，从简化缓存操作说起"},{"categories":["生活感悟"],"content":" 对我而言，洗衣服是周末的例行活动，尤其是在炎热的夏天。也许，你会自顾自地说，衣服不必攒到周末去洗，如果你愿意下班后腾出一点时间。可人的惰性，正如在太阳底下会流汗一般寻常，如果我愿意，你也许会早一点看到，这些只在周末显得安静的文字。\n换这部手机时，店家附赠了一只蓝牙音响，带着物尽其用的想法，先后用它来听网易云、听微信读书、听 TED。于是，在一刹那间，水龙头里的流水声、拧干衣服时的水花声、我脑海里的闪念的低吟声，都成为这只麦克风的伴奏。我开始黯然失色，这听来听去，大概是在听寂寞在唱歌。果然，微信听书的效果并不好，那机械而平静的合成音，甚至还不如地铁上的播报充满感情。最为致命的问题，微信听书像极了听老师讲课：书读完了吗？读完了！还记得讲了什么吗？完全不记得！\n现实生活可不像武侠世界，没有那么多无招胜有招的奇遇。所以，有一段时间，我总觉得用听书这种方式来读书，像极了姜太公钓鱼——愿者上钩。五柳先生，好读书而不求甚解，因为观其大略，而这连看都不愿意看一眼，简直就是自欺欺人。如此反复折磨自我，发现听演讲居然是最适合打发时间的方式，特别是洗衣服的这段时间。仔细一想，大概是演讲更能做到声情并茂，古人一桌、一椅、一扇、一抚尺，就能讲一个沉浸感十足的故事。而我们从文字到图片再到视频，仿佛都不足以表达自我。在百家争鸣的战国时代，不管是合纵/连横的策略，还是法、儒、墨、兵各家，我们能听见不同的声音，虽然表达方式不过是竹简。\n可今天，我们好像陷入了一个信息黑洞，网络上的信息越来越嘈杂，原本代表着开放与连接的互联网，在一个又一个的小圈子里，正在走向越来越封闭的局面。譬如，喝茶与喝咖啡，从生物学上来讲这两个行为间并无差异，可人与人之间就是会形成所谓的鄙视链，甚至连咖啡本身都会形成这种鄙视链，手冲和速溶，本质上有什么不一样吗？圈子文化的盛行，让圈子本身更加封闭，隐形门槛的提高，让圈子外的人更加不能理解圈内人的行为，比如汉服与 JK，本质上不就是一件衣服吗？可人定要分出个山(寨)与正(品)的差别。人与人的关系，大致可以理解为相互炫耀、相互鄙视，而不同群体间的互相鄙视，其实加速了整个互联网的割裂，男女对立、饭圈文化……，无一不是这种割裂感的具体产物。\n鲁迅先生写道，“人类的悲欢并不相通，我只是觉得他们吵闹”。人类想要互相理解彼此，除了感同身受以外，大概只有放下强烈的个人意识这一条路。可做一个精致的利己主义者，又有什么不好吗？只要我们不因为远方的声音让这个世界频频陷入大火，自私一点又有什么关系呢？罗曼蒂克不会消亡，只是我们对罗曼蒂克的要求变得越来越高。翻开历史，人类几千年的文明，一样是在这种割裂和封闭的状态下延续着，梁武帝从信佛到灭佛，汉武帝罢黜百家独尊儒术，商鞅因法而兴由法而灭……儒家与道家尚不能共治，巴基斯坦和印度更是势如水火……人们推倒了篱墙，再重新筑起篱墙，周而复始，反反复复。我由衷地想念那个百家争鸣的时代，人们有耻食周栗的觉悟、有伯牙绝弦的深情、有横槊赋诗的豪情……古人寿命不及我们、生活不如我们，可这几千年的精神世界，都是他们留给我们的。\n不同于被生活磨去棱角的年轻人，一开口就是房子、车子和孩子。要知道，子在过去可是一种敬词，孔孟不必多说，老庄无须多言，前有张子连横六国，后有苏子遨游赤壁，这是否意味着，古人的精神世界远比我们丰富，毕竟我们都太枯燥了。有时候，我在地铁上看到别人面无表情地刷着抖音，如果说圈子本身让我们变得狭隘，那么信息茧房无疑会让我们变得愚蠢。你说，这个地球是不是变得越来越小，可明明我们还没有走过所有地方，不曾见过亚马逊的热带雨林，不曾见过极地的奇幻光影，不曾见过东非的荒漠草原……我们实在太容易相信那就是全部了，因为别人都这样过每一天，因为随大流不需要花时间思考什么，因为只此一次的生命实在过于短暂……越来越觉得，结婚就是用高昂的沉没成本，来阻止人们试错，每一步都走得小心翼翼的人，步步生莲，莲是三寸金莲的莲，虽然我们有耐克、有鸿星尔克、有美特斯邦威。\n我喜欢逛西安这座城市的书店，因为传统书店愈发没落的今天，它需要找到一种物质和精神上的平衡，可人何尝又不是这样？每当我漫步在不同的商场，我忽然觉得，我们只是以为自己有那么多的选择，越来越多的餐饮像是流水线一般，我们寻找的那份属于自己的独特，早已在机器的转动声中消失殆尽。于是，人们开始复兴手工制作，越来越多的商家，开始在招牌上加上手工的字样。也许，现代和传统就是这样两个相互鄙视的圈子，它们互相鄙视，而又反复横跳。其实，单以甜点而论，我更喜欢中式的点心，大概是因为那些西式点心的名称，说起来要更绕口一点。泡芙、圣代、提拉米苏……像极了你学英语时的样子，每一个单词都认识，放到一起简直不知所云。以前，我在挑选饮料方面选择困难，因为总是记不住那些眼花缭乱的名字，后来手机里装了大众点评，忽然发现，每家商场里的店铺都差不多呢，这大概是一种进步，因为你没有选择。\n当年我有一位高中同学，特别喜欢郭德纲的相声，报菜名、说绕口令的技艺相当纯熟，毕业后留在苏州的学而思，据说是变成了一名老师。近来教育培训行业政策有变动，不知道他是否还有心情饶舌一番。说回听书这件小事，那时，听一位作家讲金庸先生的越女剑，联想到武侠的没落，大概有几分道理可言。为什么漫威的超级英雄在这个时代更受欢迎，因为超级英雄们获得能的方式更现代化一点，无论是神话、科技、变异，这都是我们这个时代可以理解的东西，所以，我们能接受通过蛛丝发射器飞檐走壁的蜘蛛侠，唯独接受不了同样靠轻功飞檐走壁的大侠们。因为，没有人能说得清武功的来源。在一个武术成为观赏性项目的时代，我们对武功的理解，不会比神话时代好多少，我们都听过卧薪尝胆的故事，听过博浪飞锥的故事，听过图穷匕首的故事……如果世上真的有武功，大概就像我们认为的战争，对于没有亲身经历的人而言，永远都只能活在想象里，那么，武侠的起源到底从哪里开始呢？\n在 B 站看到 30 多年前的西安，隐隐约约可以认出永宁门、大雁塔和钟楼，弹幕里有人打出无人机的字眼，原来，航拍这个词的含义已经等同于无人机，不管那个时候有没有无人机。同样地，现在的小孩会问，怎么通过座机打电话，我会不由得想起初/高中住校那几年，和家里联络基本都是靠座机。后来，我们有了直板手机、智能手机，再不必担心两百条短信会用完，再不必掐着时间给家里打电话，可再没有那样愿意陪你发短信的人，一个月下来甚至都打不了几个电话，流量从 5 块钱 30 兆一直涨到几十块钱，可对我来说，无非还是写写字、读读书，和过去相比并没有什么不同。人啊，总有些东西，在默默提醒着你：你在一天天地老去，永远都 18 岁，那比科幻电影还要科幻，除非你能从卷福手里拿到时间宝石。如果回到过去，你会如何和过去的自己谈判呢？我只知道，金庸先生穿越回吴越争霸的时代，他让阿青从白猿身上学到了武功。越王勾践卧薪尝胆、三千越甲吞吴的故事，父亲从小就同我讲过，可他也许不知道金庸先生的这个版本。\n阿青被范蠡带入宫中，传授越国剑士精妙剑法，自此帮助越王勾践打败吴国、洗雪前耻，范蠡得以与情人西施重逢，可偏偏阿青喜欢上来了范蠡，没有人能阻挡阿青手中的竹棒，除了西施绝世的容貌，原来她比范蠡描述的还要美。虽然阿青放弃了寻仇，可棒头的内劲儿还是伤到了西施，自此西施落下来心口疼痛的毛病。这大概就是金庸先生心目中武功的缘起，范蠡西施放舟太湖、悠游终生，自此世上有了江湖，果然，这个说法像雷神之锤一样相当有说服力，“那些都是很好很好的，可是我偏偏不喜欢”，某种意义上来讲，功成身退的范蠡比张仪、韩信、商鞅要幸运得多，而这正是历史的迷人之处。什么？你问我衣服洗完了没有？当然洗完了！因为这些闪念，对于一个双子座而言，就像穿衣吃饭一般寻常，唯一的困难在于，我要将它写出来、同时让你看懂，以上！果然是标准的日式结尾呢！\n","date":"2021-08-02T00:13:48Z","image":"/posts/3938682696/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/3938682696/","slug":"3938682696","tags":["随笔","感悟","生活"],"title":"洗衣随想曲"},{"categories":["编程语言"],"content":"在构建以 gRPC 为核心的微服务架构的过程中，得益于 Envoy 对 gRPC 的“一等公民”支持，我们可以在过滤器中对 gRPC 服务进行转码，进而可以像调用 Web API 一样去调用一个 gRPC 服务。通常情况下， RPC 会作为微服务间内部通信的信使，例如，Dubbo、Thrift、gRPC、WCF 等等更多是应用在对内通信上。所以，一旦我们通过 Envoy 将这些 gRPC 服务暴露出来，其性质就会从对内通信变为对外通信。我们知道，对内和对外的接口，无论是安全性还是规范性，都有着相当大的区别。博主从前的公司，对内的 WCF 接口，长年处于一种\u0026quot;裸奔\u0026ldquo;的状态，属于没有授权、没有认证、没有文档的“三无产品”。那么，当一个 gRPC 服务通过 Envoy 暴露出来以后，我们如何保证接口的安全性呢？这就是今天这篇博客的主题，即 Envoy 作为网关如何提供身份认证功能，在这里，我们特指通过JWT，即 Json Web Token 来对接口调用方进行身份认证。\n搭建 Keycloak 对于 JWT ，即 Json Web Token ，我想大家应该都非常熟悉了，它是目前最流行的跨域认证解决方案。考虑到，传统的 Session 机制，在面对集群环境时，扩展性方面表现不佳。在日益服务化、集群化的今天，这种无状态的、轻量级的认证方案，自然越来越受到人们的青睐。在 ASP.NET Core 中整合JWT非常简单，因为有各种第三方库可以帮助你生成令牌，你唯一需要做的就是配置授权/认证中间件，它可以帮你完成令牌校验这个环节的工作。除此以外，你还可以选择更重量级的 Identity Server 4，它提供了更加完整的身份认证解决方案。在今天这篇博客里，我们使用的 Keycloak，一个类似 Identity Server 4 的产品，它提供了一个更加友好的用户界面，可以更加方便的管理诸如客户端、用户、角色等等信息。其实，如果从头开始写不是不可以，可惜博主一时间无法实现 JWKS，所以，就请大家原谅在下拾人牙慧，关于 JWKS ，我们会在下一节进行揭晓。接触微服务以来，在做技术选型时，博主的一个关注点是，这个方案是否支持容器化。所以，在这一点上，显然是 Keycloak 略胜一筹，为了安装 Ketcloak ，我们准备了如下的服务编排文件：\nversion: \u0026#39;3\u0026#39; services: keycloak: image: quay.io/keycloak/keycloak:14.0.0 depends_on: - postgres environment: KEYCLOAK_USER: ${KEYCLOAK_USER} KEYCLOAK_PASSWORD: ${KEYCLOAK_PASS} DB_VENDOR: postgres DB_ADDR: postgres DB_DATABASE: ${POSTGRESQL_DB} DB_USER: ${POSTGRESQL_USER} DB_PASSWORD: ${POSTGRESQL_PASS} ports: - \u0026#34;7070:8080\u0026#34; postgres: image: postgres:13.2 restart: unless-stopped environment: POSTGRES_DB: ${POSTGRESQL_DB} POSTGRES_USER: ${POSTGRESQL_USER} POSTGRES_PASSWORD: ${POSTGRESQL_PASS} 其中，.env文件放置了服务编排文件中使用到的环境变量：\n# KEYCLOAK KEYCLOAK_USER=admin KEYCLOAK_PASS=admin # POSTGRESQL POSTGRESQL_DB=keycloak POSTGRESQL_USER=keycloak POSTGRESQL_PASS=keycloak 此时，我们运行docker compose up命令就可以得到一个 Keycloak 环境，它将作为我们整个微服务里的认证中心，负责对用户、角色、权限、客户端等进行管理。于此同时，接口消费方可以通过 Keycloak 获取令牌、JWKS，而 Envoy 正是利用 JWKS 来对令牌进行校验的。这个 JWKS 到底是何方神圣，我们暂且按下不表。在正式使用 Keycloak 前，我们需要做一点简单的配置工作，具体来说，就是指创建用户、角色和客户端，我们一起来看一下。\n首先，是创建一个用户，这里以《天龙八部》中壮志未酬的“慕容龙城”为例：\nKeycloak 创建用户\r《天龙八部》中提到，“慕容龙城”一心想光复大燕，可惜时不我与，正好遇上宋太祖建立宋朝，即使他创造出“斗转星移”的武功绝学，依然免不了郁郁而终的结局。慕容龙城算是第一代创业者，我们准备一个Developer的角色：\nKeycloak 创建角色\r在权限系统的设计中，角色总是需要和用户关联在一起。同样地，在 Keycloak 中，我们需要给“慕容龙城”分配一个Developer的角色：\nKeycloak 分配角色\r到了“慕容复”这一代，“慕容垂”假借死亡之名秘密活动，而活跃在台前的“慕容复”，实际上是作为慕容家族的“代理人”出现。在今天这篇文章中，Envoy 会充当认证服务的代理，因为我们希望 Envoy 可以对所有进站的 API 请求进行统一的认证。所以，这里，我们还需要创建一个客户端：envoy-client，并为其分配客户端角色：\nKeycloak 创建客户端\rOK，我们都知道，OAuth 2.0 有这样四种认证方式：密码模式、客户端模式、简化模式、授权码模式。这四种认证方式如何在 Keycloak 中实现呢？目前，博主基本搞清楚了前面两种。我们在创建完客户端以后，可以通过设置访问类型来决定客户端使用哪种认证方式，目前已知，当访问类型的取值为public时，表示密码模式。当访问类型的取值为confidential时，表示客户端模式。这里，我们以客户端模式为例：\nKeycloak 客户端模式\r此时，我们就可以拿到一个重要的信息：client_secret，如果大家使用过客户端模式，就会知道它是获取令牌的重要参数之一。好了，当我们有了这些信息以后，该怎么样去获取令牌呢？我们只需要用 POST 的方式，将grant_type、client_id、client_secret、username、password、scope传过去即可：\n从 Keycloak 获取令牌 如果需要刷新令牌，则只需要再追加一个refresh_token参数即可，它是我们第一次获取到的令牌：\n从 Keycloak 刷新令牌\r可能大家会疑惑，博主是从哪里知道这些 API 的端点地址的呢？其实，和 Identity Server 4 类似， Keycloak 提供了一个用于服务发现的接口地址：/auth/realms/master/.well-known/openid-configuration，通过这个接口地址，我们可以获得一份 API 列表：\nKeycloak 提供的 “服务发现” 能力\r可以注意到，图中有我们需要的换取令牌的接口，以及提供 JWKS 的接口：/auth/realms/master/protocol/openid-connect/certs\u0026quot;，尤其第二点，它对于对我们进行下一个步骤意义重大，Envoy 能不能承担起微服务认证的重担，就看它的啦，至此， Keycloak 的搭建工作已经完成。\n配置 Envoy 在上一节内容中，博主卖了一个关子，说要等到这一节再说 JWKS 是何方神圣？不过，博主以为，“饭要一口一口吃，步子迈太大，咔，容易扯着蛋”，我们还是先来说说 JWT ，因为只要你了解了它的结构，你才能了解如何去检验一个令牌。我们说，JWT，是 JSON Web Token 的简称，那这个 JSON 到底体现在哪里呢？而这要从 JWT 的结构开始说起。\nJSON Web Token 结构说明图\r这是一张来自 JWT 官网的截图，博主认为，这张图非常清晰地展示出了 JWT 的加密过程，我们熟悉的这个令牌，其实是由header、payload和signature三个部分组成，其基本格式为：header.payload.signature，细心的朋友会发现，图中生成的令牌中含有两个.。其中，header部分是一个 JSON 对象，表示类型(typ)及加密算法(alg)，常见的加密算法主要有 HMAC、RSA、ECDSA 三个系列。payload部分同样是一个 JSON 对象，主要用来存放实际需要传递的数据。目前，JWT 官方规定了以下7个备选字段：\niss，即 issuer，表示：令牌签发人 exp，即 expiration time，表示：令牌过期时间 sub，即 subject，表示：令牌主题 aud，即 audience，表示：令牌受众 nbf，即 Not Before，表示：令牌生效时间 iat，即 Issued At，表示：令牌签发时间 jti，即 JWT ID，表示：令牌编号 需要注意的是，header和payload这两部分，默认是不加密的，这意味着任何人都可以读到这里的信息，所以，一个重要的原则是，不要在payload中存放重要的、敏感的信息。无论是header还是payload，最终都需要通过 Base64URL 算法将其转化为普通的字符串，该算法和 Base64 算法类似，唯一的不同点在于它会对+、/ 和 = 这三个符号进行替换，因为这三个符号在网址中有着特殊的含义。\nBase64 \u0026amp;amp; Base64URL 算法对比\r第三部分，signature，即通常意义上的签名，主要是防止数据篡改。对于 HMAC 系列的加密算法，需要指定一个密钥，以 HMACSHA256 算法为例，其签名函数为：HMACSHA256(base64UrlEncode(header) + \u0026quot;.\u0026quot; + base64UrlEncode(payload), secret)。对于 RSA 和 ECDSA 这两个系列的加密算法，需要指定公钥和私钥，以 ECDSASHA512 算法为例，其签名函数为：ECDSASHA512(base64UrlEncode(header) + \u0026quot;.\u0026quot; + base64UrlEncode(payload), PublicKey, PrivateKey)。一旦计算出签名，就可以将这三部分合成一个令牌，而这就是 JWT 的产生原理，而如果我们对第一节中获得的令牌进行解密，我们就会得到下面的结果：\n解密 Keycloak 生成的令牌\r所以，JSON Web Token 中的 JSON，其实是指 header 和 payload 这两个 JSON 对象，并且我们可以注意到，Keycloak 中生成的令牌实际上携带了更多的信息，例如，客户端、IP 地址、realm_access 以及 resource_access等等，所以。 JWT 其实是一个相对宽松的规范，在实现payload这部分时，可以结合实际场景做更多的扩展，唯一的要求还是那句话，不要在payload中存放重要的、敏感的信息。至此，我们讲清楚了 JWT 的底层原理。\nOK，解释清楚了 JWT，我们再来说 JWKS，这位又是何方神圣呢？我们提到，JWT 至少需要一个密钥或者一对公/私钥来进行签名的校验，因为对于header和payload这两个部分而言，它的加密算法始终都是 Base64URL，所以，我们总是可以反推出原始的 JSON 字符串。接下来，我们只需要按签名函数计算签名即可，对于 HMAC 系列的加密算法，需要指定一个密钥；对于 RSA 和 ECDSA 这两个系列的加密算法，需要指定公钥和私钥。由此，我们就可以计算出一个签名，此时，我们只需要比较两个签名是否一致即可。\nJWT 校验过程示意图\r通过 JWKS 的 官网，我们可以了解到一件事情，那就是 JWKS 本质上是 Json Web Key Set 的简称，顾名思义，这是一组可以校验任意 JWT 的公钥，并且这些 JWT 必须是通过 RS256 算法进行签名的，RS256 则是我们上面这张图里的 RSA 非对唱加密算法，它需要一个公钥和一个私钥，通常强况下，私钥用来生成签名，公钥用来校验签名。这个 JWKS 呢？同样是一个 JSON 对象，它只有一个属性keys，以 Keycloak 中获得的 JWKS 为例：\nKeycloak 产生的 JWKS\r关于 JWKS 的规范，大家可以通过 RFC7515 来了解，作为一种通用的规范，Identity Server 4 和 Keycloak 都实现了这一规范，所以，就今天这篇博客而言，不管是哪一种方案，它都可以和 Envoy 配合得天衣无缝。为什么这样说呢？因为我们在 Envoy 中实现 JWT 认证，其核心还是 JWKS 这一套规范。博主没有选择从头开始实现这一切，就在于这个 JWKS 有特别多的细节。总之，我们只需要知道，通过 JWKS 可以对一个令牌进行验证，而 Envoy 刚好有这样一个过滤器，下面是 Envoy 中对应的配置项：\nhttp_filters: - name: envoy.filters.http.jwt_authn typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.jwt_authn.v3.JwtAuthentication providers: jwt_provider: issuer: \u0026#34;http://192.168.50.162:7070/auth/realms/master\u0026#34; audiences: - \u0026#34;account\u0026#34; forward: true remote_jwks: http_uri: uri: \u0026#34;http://192.168.50.162:7070/auth/realms/master/protocol/openid-connect/certs\u0026#34; cluster: keycloak timeout: 5s rules: - match: prefix: \u0026#34;/api/w\u0026#34; requires: provider_name: jwt_provider - match: prefix: \u0026#34;/api/c\u0026#34; requires: provider_name: jwt_provider - name: envoy.filters.http.router 可以注意到，我们这里配置了一个叫做envoy.filters.http.jwt_authn的过滤器，并为这个过滤器指定了一个叫做jwt_provider的认证提供者，其中的issuer和audiences，我们在讲解 JWT 结构的时候提到过，最为关键的是remote_jwks，我们通过 Keycloak 的服务发现功能，可以获得这个地址，我们将其配置到 Envoy 中即可，Envoy 可以通过它来验证一个 JWT 的令牌，而下面的规则，表示哪些路由需要认证，这里我们假设需要对/api/w和/api/c这两个端点进行认证。所以，可以预见的是，我们可以为整个网关配置统一的认证流程，无论我们有多少个微服务。以往我们都是通过 ASP.NET Core 里的过滤器来实现应用级的认证服务，而此时此刻，我们有了容器级别的认证服务，基础设施从框架提升到了容器层面。除此以外，我们还需要为 Envoy 定义一个集群，这样读取远程 JWKS 的请求才会被正确地转发过去：\nclusters: - name: keycloak connect_timeout: 0.25s type: STRICT_DNS lb_policy: ROUND_ROBIN load_assignment: cluster_name: keycloak endpoints: - lb_endpoints: - endpoint: address: socket_address: address: 192.168.50.162 port_value: 7070 如此，整个认证服务相关的基础设施均已准备就绪，所谓“万事俱备，只欠东风”，我们还需要定义资源 API 供调用者消费，所以，接下来，我们来看看 API 如何编写。\n编写 API 编写 API 非常简单，我们直接用 ASP.NET Core 创建两个项目即可，这里是两个服务：CityService 和 WeatherService。\n首先，是 CityService：\n[ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class CityController : ControllerBase { private static readonly string[] Cities = new[] { \u0026#34;中卫\u0026#34;, \u0026#34;西安\u0026#34;, \u0026#34;苏州\u0026#34;, \u0026#34;安庆\u0026#34;, \u0026#34;洛阳\u0026#34;, \u0026#34;银川\u0026#34;, \u0026#34;兰州\u0026#34; }; private readonly ILogger\u0026lt;CityController\u0026gt; _logger; public CityController(ILogger\u0026lt;CityController\u0026gt; logger) { _logger = logger; } [HttpGet] public dynamic Get() { var rnd = new Random(); var city = Cities[rnd.Next(Cities.Length)]; return new { City = city, Now = DateTime.Now }; } } 接下来，是 WeatherService：\n[ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class WeatherController : ControllerBase { private static readonly string[] Summaries = new[] { \u0026#34;Freezing\u0026#34;, \u0026#34;Bracing\u0026#34;, \u0026#34;Chilly\u0026#34;, \u0026#34;Cool\u0026#34;, \u0026#34;Mild\u0026#34;, \u0026#34;Warm\u0026#34;, \u0026#34;Balmy\u0026#34;, \u0026#34;Hot\u0026#34;, \u0026#34;Sweltering\u0026#34;, \u0026#34;Scorching\u0026#34; }; private readonly ILogger\u0026lt;WeatherController\u0026gt; _logger; public WeatherController(ILogger\u0026lt;WeatherController\u0026gt; logger) { _logger = logger; } [HttpGet] public IEnumerable\u0026lt;WeatherForecast\u0026gt; Get() { var rng = new Random(); return Enumerable.Range(1, 5).Select(index =\u0026gt; new WeatherForecast { Date = DateTime.Now.AddDays(index), TemperatureC = rng.Next(-20, 55), Summary = Summaries[rng.Next(Summaries.Length)] }) .ToArray(); } } 关于这两个服务如何实现容器化、反向代理等等的细节，大家可以参考博主前面几篇文章，本文示例已托管到 Github，供大家做进一步的参考。\n服务编排 这段时间最大的收获便是，学会了通过docker-compose对服务进行编排，虽然目前还有点悬而未决的东西，可一旦接触了这种略显“高端”的技巧，便再不愿回到刀耕火种、敲命令行维护docker环境的时代。等有时间了，博主会考虑写一点docker或者docker-compose使用技巧的文章，当然这些都是以后的事情啦！我们要活在当下啊，还是看看这个docker-compose.yaml文件：\nversion: \u0026#39;3\u0026#39; services: envoy_gateway: build: Envoy/ ports: - \u0026#34;6060:9090\u0026#34; - \u0026#34;6061:9091\u0026#34; volumes: - ./Envoy/envoy.yaml:/etc/envoy/envoy.yaml city_service: build: CityService/ ports: - \u0026#34;8081:80\u0026#34;![Envoy-Jwt-Keycloak-16.png](https://i.loli.net/2021/07/24/rCcUBWDyJVtOxkd.png) environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; weather_service: build: WeatherService/ ports: - \u0026#34;8082:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; keycloak: image: quay.io/keycloak/keycloak:14.0.0 depends_on: - postgres environment: KEYCLOAK_USER: ${KEYCLOAK_USER} KEYCLOAK_PASSWORD: ${KEYCLOAK_PASS} DB_VENDOR: postgres DB_ADDR: postgres DB_DATABASE: ${POSTGRESQL_DB} DB_USER: ${POSTGRESQL_USER} DB_PASSWORD: ${POSTGRESQL_PASS} ports: - \u0026#34;7070:8080\u0026#34; postgres: image: postgres:13.2 restart: unless-stopped environment: POSTGRES_DB: ${POSTGRESQL_DB} POSTGRES_USER: ${POSTGRESQL_USER} POSTGRES_PASSWORD: ${POSTGRESQL_PASS} 等所有的服务都启动起来以后，我们来验证下这个网关，是不是真的像我们期待的那样。注意到，Envoy 对外暴露出来的端口是6060，这里我们以CItyService为例：\n首先，是不带令牌直接访问接口，我们发现接口返回了401状态码，并提示：Jwt is missing。\n不携带令牌，Envoy 认证失败\r我们带上令牌会怎么样呢？可以注意到，接口成功地返回了数据，这表示我们的目的达到了，这些经由 Envoy 代理的 API 接口，今后都必须携带令牌进行访问：\n携带令牌，Envoy 返回数据\r因为 Keycloak 这个认证中心是独立于我们的应用单独存在的，所以，我们可以直接在 Keycloak 中设置令牌的过期时间、为用户分配角色、为不同的资源设置范围等等，而这一切都不需要应用程序或者 Envoy 做任何调整，开发者只需要认真地写好每一个后端服务即可，这是否就是传说中的基础设施即服务呢？\n本文小结 本文主要分享了如何利用 Envoy 实现容器级别的 JWT 认证服务，在实现过程中，我们分别了解了 JWT 和 JWKS 这两个概念。其中，JWT 即JSON Web Token，是目前最为流行的跨域认证方案，一个 JWT 通常由 header、payload 和 signature 三个部分组成，JWT 的 JSON 主要体现在header和payload这两个 JSON 对象上，通过 Base64Url 算法实现串化，而 signature 部分则是由header和payload按照签名函数进行生成，主要目的是防止数据篡改。JWKS 可以利用密钥或者公/私钥对令牌进行验证，利用这一原理，Envoy 中集成了 JWKS ，它表示一组可以校验任意 JWT 的公钥，同样是一个 JSON 对象。为了获得可用的 JWKS，我们可以通过 Identity Server 4 或者 Keycloak 中提供的地址来获得这一信息，方便起见，本文选择了更为便捷的 Keycloak。最终，我们实现了一个通用的、容器级别的认证网关，调用方在消费这些 API 资源时都必须带上从认证中心获得的令牌，进而达到保护 API 资源的目的，更好地保障系统和软件安全。\n","date":"2021-07-25T09:41:24Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/731808750/","slug":"731808750","tags":["微服务","Envoy","JWT","Keycloak","认证"],"title":"ASP.NET Core 搭载 Envoy 实现微服务身份认证(JWT)"},{"categories":["生活感悟"],"content":"\r近日，Netflix 官方宣布，浪客剑心·最终章：追忆篇 将于 7 月 30 日上线，这意味着这部横跨十年时间、被誉为漫改巅峰的系列电影，终于要迎来它的落幕。人对于时间的感觉，难免会相对迟钝一点。如果将思绪拉回到 2011 年，对我来说，人生中无数闪光的时刻，无一不是从这一刻开始：第一次拥有互相喜欢的人、第一次拥有属于我的电脑、第一次在图书馆里借满 100 本书……每次听别人说到漫威十年，我总觉得一切无比陌生。回想起来，第一次到电影院看电影，始于 Lemon 同学请我吃石锅拌饭，对于漫威英雄们的了解，更多的是事后诸葛亮，而唯有浪客剑心系列，一直陪伴着我走过这兵荒马乱的十年。所以，当这个系列走向终点的时候，我果然还是想说点自以为是的话，因为在时代的波涛里，每一个小人物的命运，都不过是艰难挣扎着活下去。\n也许，不光是此刻屏幕前的你，就连我自己完全想象不出，有一天我会对日本的影视作品产生兴趣。过去的我，是一个被人称为“不适合在现代社会”的“怪人”。彼时，我喜欢苏轼烟雨任平生的豁达，喜欢稼轩气吞万里如虎的豪迈，喜欢纳兰容若秋风画扇的悲凉……更多的时候，我是一个偏理想化、偏浪漫主义的文艺青年。在我越来越模糊的时光回忆里，全然没有火影忍者、海贼王这些日漫作品的身影，因为在喜欢安静的我看来，这些动漫人物总是在互相“攻伐”，或许是因为日/韩的语言听起来更像是“吵架”，我一度认为这些东西是聒噪而喧嚣的。直到后来，接触到半泽直树、Legal High、白色巨塔这类影视作品，终于对日本人那种听起来像是“吵架”的表演风格有所了解，而像宫崎骏、新海诚、米林宏昌等动画导演的作品，则是后来一点点接触到的，甚至连鬼灭系列完全都是一个偶然，单纯是因为，我想找一个类似无皇刃谭的作品。\n绯村剑心与雪代缘战斗\r第一次看浪客剑心的时候，我全然不知它是一部漫改作品。当时，除了感觉人物造型有点 cosplay 以外，更多的时候，我喜欢把当作日式古装片/武侠片。庆幸的是，浪客剑心对于幕末/明治时期的社会氛围一直刻画地不错，即使后来浪客剑心更为人所称道的，是 垣谷健治 从中国功夫电影中借鉴到的动作设计。动画版的浪客剑心，或被称之为：明治剑客浪漫谭。也许，是因为 100 多年的历史不远不近，更适合人们去肆意想象。所以，当我们提起明治亦或者民国，我们总是期待，那是一个浪漫的时代。浪客剑心的开场，是鸟羽伏见之战，电影中我们记住的，或许是剑心傲娇地将刀插在地上，因为最终赢得这场战争的，是剑心背后的新政府军。可历史永远比想象更残酷，末代幕府将军德川庆喜，在苍茫夜色中逃往大阪城的时候，是否会不时想起，在大坂夏之阵中独自对抗德川大军的真田幸村。历史是何其地相似，可当你恍然间惊觉，原来泛黄的书页已翻过三百余年。\n鸟羽伏见战场\r有人说，日本幕府的毁灭始于黑船事件，自此以后便是让日本快速崛起的明治维新。如果把黑船(枪炮)看作西方工业革命的象征，明治维新无疑就是一场西洋枪炮与东瀛武士刀的角逐。所以，在这样一种背景下，浪客剑心里的矛盾冲突，其实都是新时代与旧时代的一次碰撞。禁刀令下，一个曾经双手沾满鲜血的刽子手——绯村拔刀斋，手握一柄逆刃刀，试图斩断一切囿于过去的亡魂，等到他终于放下心中的罪孽感，不再执着于过去发生的一切，脸上的十字刀疤终于消失不见。你告诉我，还有比听起来比这个更浪漫的故事吗？绯村剑心，本名心太，幼年时父母因混乱而死，在被人贩子运送途中遭山贼袭击，幸得飞天御剑流第十三代传人比古清十郎搭救并收为弟子，传授其飞天御剑流剑术。比古清十郎认为心太这个名字太过柔弱，不适合一名剑客，故将其改名为剑心。多年后，剑心与师父的意见向左，师父认为，剑是凶器，剑术就是杀人伎俩，无论是用多么华丽的词藻去粉饰终究是事实。\n剑心与师傅比古清十郎\r而在一个动荡的乱世，剑术固然可以锄强扶弱，可更多的或许是成为政客手里的杀人工具。一心想亲手拯救人民于水火的剑心，在下山后遇到了桂小五郎、高杉晋作等维新志士组成的奇兵队，自此成为专门暗杀幕府政要的刽子手，其出众的剑术令幕府闻风丧胆，人称刽子手拔刀斋。其实，在时代的洪流里，不管是作为刽子手的拔刀斋，还是作为浪人的剑心，其实都是一个时代的牺牲品。志志雄真实和剑心，本质上都属于同一类人，不同的是，志志雄是在新时代建立后被抛弃的人，因为身体被大面积烧伤而无法正常排汗，内心燥热的火焰终于要随无限刃而喷薄欲出，他从国外购买了铁甲舰、手下集结了十本刀，决心将这个新时代变成炼狱。明治维新，是两个新旧时代的碰撞，在这样一个大背景下，传统的武士、刽子手大量被抛弃。不管是鹈堂刃卫，还是志志雄真实，都失去了存在下去的意义，他们被鲜血和执念吞噬，试图用最极端的方式来证明自己的存在，在武侠的世界里，追求武功天下第一，是每一个习武之人的毕生追求，可禁刀令一出，大家都生活在充满法制、文明的时代，曾经的一切都仿佛不复存在。\n武士刀 \u0026amp;amp; 警棍\r这种失落感相当真实，多年以前，徐克拍摄《黄飞鸿》时，曾用“铁布衫”严振东的死，表达过这种在坚船利炮面前的无力感。纵观整个浪客剑心系列，除了第一部的反派武田观柳以外，几乎没有绝对的反派。有一个人，和这些囿于过去、不愿放过自己的人形成强烈对比，那就是斋藤一，这个被称为“壬生狼”的前新选组成员，永远奉行着“恶即斩”的主观标准。在每个被人潮推着向前走的时代，没有人能独善其身，可毫无疑问，斋藤一会是适应能力最强的那一类人。显然，剑心是那种愿意向前看，可依然对过去无法释怀的那一类人。有时候，我们会在文学作品中遇到隐形主角，譬如袁崇焕之于碧血剑，而浪客剑心的隐形主角，我以为应该是替剑心打造逆刃刀的新井赤空，一个铸剑师以匠人的心态打造神兵利器，结果这些刀剑都被用作凶器去杀人。同样地，一名剑客以济世救人的心态加入维新志士这一方，结果在迎接新时代到来的过程中夺去了别人的生命。\n新井赤空 \u0026amp;amp; 剑心\r可以说，剑心手上的逆刃刀，其实就是新井赤空的化身，两个人在赎罪这一心理上是高度一致的，甚至剑心内心的挣扎，早已和这把逆刃刀融为一体，逆刃刀固然会伤到自己，而一个人敢于直视自己的内心，未尝不会被这份鲜血淋漓灼伤，当剑心面对一个又一个的敌人，当剑心身上的秘密一点点被揭开，剑心面对的其实一条自我灵魂的救赎之路。电影中的逆刃刀一共有两把，第一把被称之为“影打”，属于试验品。在和“天剑”宗次郎对决的过程中被名刀虎彻斩断。第二把被称之为“真打”，属于千锤百炼的真品。在关键时刻让剑心打败十本刀之一的“刀狩”泽下条张。在京都大火篇中，当剑心准备从泽下条张手中救下伊织时，剑心有过一段阐述个人理念的独白，大意是说在新时代降生的孩子，都是真正的天选之子，值得他用生命去守护。从这里可以看出，即使曾经作为一个血债累累的刽子手，剑心灵魂深处的仁慈从未丢失过，多年后，他依然还是那个选择埋葬山贼和人贩子尸体的少年心太。\n剑心对战 “天剑” 宗次郎\r可这个角色让人着迷的地方就在于，剑心身上有着难以融合的关于救赎、杀念和仁慈的混合气质：手执逆刃刀，是为不杀之誓，是为自我救赎；在新时代拒绝传授飞天御剑流剑术，认为神谷活心流的“活人剑”更值得传承下去，是为武者之仁。在我的印象中，剑心只有两次真正动了杀心，一次是从鹈堂刃卫手中救下被“心之一方”麻痹肺部的神谷薰，一次是从“刀狩”下泽条张手中救下新井青空的孩子伊织。有时候，我会想，那个一直让剑心不要再杀人的女人可真狠心。直到后来，我终于明白，雪代巴和神谷薰，都是剑心的剑鞘，一个真正爱你的人，怎么会忍心看着你堕入修罗呢？历史的扑朔迷离，往往来自那些不经意间文过饰非的春秋笔法，浪客剑心的第三部，即传说的完结篇，在这一篇里，伊藤博文宣布，绯村拔刀斋已死，绯村剑心重生。后人已无法知晓，伊藤博文下令向铁甲站舰开炮时的心境，也许在某一瞬间，伊藤博文真的想让剑心，连同这些幕末的亡灵一起葬送于火海。如果是这样，每一个想成为时代弄潮儿的英雄，是否最后都变成了政治家的牺牲品呢？虽然我不得不承认，海滩上明治政府向武士们致敬的这一幕，一旦搭配上飞天的背景音乐，就会成为比少年热血漫还要沸腾的东西。\n明治政府向武士们致敬\r美国人曾经拍过一部电影《最后的武士》，描写社会变更时期的武士精神如何走向没落。历史的车轮呼啸而过，传统在飞扬的尘土中转瞬湮没。坂本龙马、大久保利通、西乡隆盛等维新志士，在历史的长河里惊鸿一瞥，人类面对滚滚历史长河时的渺小，大概就像大海中浮沉着的一叶孤舟，无论自身多么想要划向远方，最终亦不得不面对历史的进程。在这部电影结尾，明治天皇被阿汤哥的精神感动，从他手中接过胜元的武士刀，这大概是一种艺术加工。因为真实的历史是往往要更加残酷，此后的许多年里，武士道精神被偷换为军国主义，战争给这个世界带来的伤害可谓历历在目。我们说民国浪漫，是一种“为往圣继绝学，为万世开太平”的浪漫，是那种为了一个民族的未来，而甘愿做孺子牛、上下求索的浪漫。假如剥离这层浪漫的滤镜，将历史放大到一个普通人的生活。或许啊，我们看到的会是 《觉醒年代》 里的饿殍遍野、民生多艰。同样地，我们说明治浪漫，是那如夕阳一般绝美的最后的高光时刻。因为，在每一个时代，都有这样一群人，他们在新与旧，改革与保守，东方与西方的冲突中不断地挣扎。\n绯村剑心经典红白造型\r时至今日，年轻人对国家的未来充满希望，对个人的未来充满绝望，也许是因为，在时代的潮流中，普通人甚至比不上一朵小浪花，一如被剑心斩杀的雪代巴的未婚夫清里，本质上并无对错可言，无非是阵营不同。在浪客剑心里，年幼的心太对比古清十郎说，“人死了不过都是一具尸体”。多年后，我在日剧《Unnatural》 里找到了对应的答案，中堂医生在庭审时说过的话，“人这种生物，无论是谁，切开来剥开皮都只不过是一团肉，等你死了就知道了”。说到底，我们不过是碰巧活着啦，比古清十郎和高荷惠，都曾劝诫剑心，在救人前要先学会自保。或许，爱情更是如此，我们常说，“自爱沉稳而后爱人”，《仁医》 里穿越到幕末时代的医生南方仁，怀着对生命和历史的温情与敬意，不自觉地参与到幕末的各种历史事件中，并由此领悟到，“世间的一切都是先人赐予我们的，是历史中的每个人战斗、挣扎和牺牲所赢得的，更是由无数的生命奇迹编织而成，所以，我们必须用我们的双手，给予后人更加光明的未来”。剧中南方仁的仁是医者之仁，而坂本龙马的仁是以公义超越私爱，这两者共同构成这部电视剧的主题：仁，而剑心的仁在于止杀(戈)，这是真正的武者之仁。\n守护世界上最萌的剑心\r兔死狐悲的历史总在不断重复上演，历史上的白起、韩信、伍子胥，莫不如是。所以，对于志志雄真实这样一个悲情人物，总是会让人不由心生感慨。原著中的志志雄，不单单有蓄意谋国的野心，甚至开始探索“石油”这种属于未来的科技，在被明治政府抛弃以后，强忍着身心双重折磨，如丧尸一般存活下来。他建立起一套“弱肉强食”的社会达尔文主义理论，在手下十本刀的帮助下，意图颠覆刚建立不久的明治政府。每一个时代都有想成为“弄潮儿”的人，可更多的时候，不过是让这个世界频频陷入“大火”，时代的车轮呼啸着碾过的时候，牺牲的是无数细小的浪花、尘埃，每一个人都想成为英雄，可成为英雄的代价是什么呢？一将功成万骨枯，太阳从树叶的缝隙中穿过的时候，每一片叶子都合成了叶绿素，可难免会刺痛某个躺在树下乘凉的人的眼睛。不管是人诛篇的雪代缘，还是完结篇的志志雄，时代是需要英雄，可你不必非要成为那样的人，我还是想做一个普通人，因为，活着便能创造新的回忆。\n","date":"2021-07-12T08:53:48Z","image":"https://i.loli.net/2021/07/09/VtUchpjIKZeBrWL.jpg","permalink":"https://qinyuanpei.github.io/posts/673523131/","slug":"673523131","tags":["电影","浪客剑心","日本","感悟","佐藤健"],"title":"浪客剑心：一曲幕末时代的挽歌"},{"categories":["编程语言"],"content":"在构建微服务架构的过程中，我们会接触到服务划分、服务编写以及服务治理这一系列问题。其中，服务治理是工作量最密集的一个环节，无论是服务发现、配置中心、故障转移、负载均衡、健康检查……等等，这一切的一切，本质上都是为了更好地对服务进行管理，尤其是当我们面对数量越来越庞大、结构越来越复杂的集群化环境的时候，我们需要一种科学、合理的管理手段。博主在上一家公司工作的时候，每次一出现线上故障，研发都要第一时间对问题进行排查和处理，而当时的运维团队，对于微服务的监控止步于内存和CPU，无法系统而全面的掌握微服务的运行情况，自然无法从运维监控的角度给研发部门提供方向和建议。所以，今天这篇文章，博主想和大家聊聊，如何利用Envoy来对微服务进行可视化监控。需要说明的是，本文的技术选型为Envoy + ASP.NET Core + Prometheus + Grafana，希望以一种无侵入的方式集成到眼下的业务当中。本文源代码已上传至 Github ，供大家学习参考。\n从 Envoy 说起 在介绍 Envoy 的时候，我们提到了一个词，叫做可观测的。什么叫可观测的呢？官方的说法是， Envoy 内置了stats模块，可以集成诸如prometheus/statsd等监控方案，可以集成分布式追踪系统，对请求进行追踪。对于这个说法，是不是依然有种云里雾里的感觉？博主认为，这里用Metrics这个词会更准确点，即可度量的，你可以认为， Envoy 提供了某种可度量的指标，通过这些指标我们可以对 Envoy 的运行情况进行评估。如果你使用过 Elastic Stack 中的 Kibana，就会对指标(Metrics)这个词汇印象深刻，因为 Kibana 正是利用日志中的各种指标进行图表的可视化的。庆幸的是，Grafana 中拥有与 Kibana 类似的概念。目前， Envoy 中支持三种类型的统计指标：\nCounter：即计数器，一种只会增加不会减少的无符号整数。例如，总请求数 Gauge：即计量，一种可以同时增加或者同时减少的无符整数。例如，状态码为200的有效请求数 Timer/Hitogram：即计时器/直方图，一种无符号整数，最终将产生汇总百分位值。Envoy 不区分计时器（通常以毫秒为单位）和 原始直方图（可以是任何单位）。 例如，上游请求时间（以毫秒为单位）。 在今天的这篇文章中，除了 Envoy 以外，我们还需要两位新朋友的帮助，它们分别是Prometheus 和 Grafana。其中，Prometheus 是一个开源的完整监控解决方案，其对传统监控系统如 Nagios、Zabbix 等的测试和告警模型进行了彻底的颠覆，形成了基于中央化的规则计算、统一分析和告警的新模型。可以说，Prometheus 是完整监控解决方案中当之无愧的后起之秀，它最为人所称道的是它强大的数据模型，在 Prometheus 中所有采集到的监控数据吗，都以指标(Metrics)的形式存储在时序数据库中。和传统的关系型数据库中使用的 SQL 不同，Prometheus 定义一种叫做 PromQL 的查询语言，来实现对监控数据的查询、聚合、可视化、告警等功能。\nPrometheus \u0026amp;amp; Grafana 的奇妙组合\r目前，社区中提供了大量的第三方系统的采集功能的实现，这使得我们可以轻易地对MySQL、PostgresSQL、Consul、HAProxy、RabbitMQ， Redis等进行监控。而 Grafana 则是目前主流的时序数据展示工具，正是因为这个原因， Grafana 总是和 Prometheus 同时出现， Prometheus 中采集到监控数据以后，就可以由 Grafana 赖进行可视化。相对应地，Grafana 中有数据源的概念，除了 Prometheus 以外，它还可以使用来自 Elasticsearch 、InfluxDB 、MySQL 、OpenTSDB 等等的数据。基于这样一种思路，我们需要 Envoy 提供指标信息给 Prometheus ，然后再由 Grafana 来展示这些信息。所以，我们面临的主要问题，其实是怎么拿到 Envoy 中的指标信息，以及怎么把这些指标信息给到 Prometheus 。\n原理说明 首先，我们来简单阐述一下原理。在 Envoy 的早期版本中，通常是通过 statsd 来采集 Envoy 中的信息，这些信息会被存储在 Prometheus 中，然后由 Grafana 从 Prometheus 中读取数据并展示为图表。而在 Envoy 最新的版本中，Envoy 本身就可以输出 Prometheus 需要的数据格式，故而就不再需要 statsd 这样一个监控工具。关于第一种方案，大家可以参考这篇文章：Envoy Service Mesh、Prometheus和Grafana下的微服务监控。这里，为了简单起见，我们采用第二种方案来进行集成。在接下来的例子中，我们会部署下面四个服务，我们希望在调用 gRPC 服务的时候，可以在 Grafana 看到相关的监控指标：\nversion: \u0026#34;3\u0026#34; services: # prometheus prom: image: quay.io/prometheus/prometheus:latest volumes: - ./Prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:rw ports: - 2333:9090 # envoy_gateway envoy_gateway: build: Envoy/ ports: - \u0026#34;9090:9090\u0026#34; - \u0026#34;9091:9091\u0026#34; volumes: - ./Envoy/envoy.yaml:/etc/envoy/envoy.yaml # grpc_service grpc_service: build: GrpcService/GrpcService/ ports: - \u0026#34;8082:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; # grafana grafana: image: grafana/grafana ports: - \u0026#34;3000:3000\u0026#34; environment: - “GF_SECURITY_ADMIN_PASSWORD=Gz2020@” - “GF_INSTALL_PLUGINS=alexanderzobnin-zabbix-app” restart: always depends_on: - prom 接下来，为了让 Prometheus 可以直接读取 Envoy 中输出的指标数据，我们需要在其配置文件prometheus.yml中添加一个对应的任务：\nglobal: scrape_interval: 15s external_labels: monitor: \u0026#39;codelab-monitor\u0026#39; scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; scrape_interval: 5s static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] - job_name: \u0026#39;envoy\u0026#39; metrics_path: \u0026#39;/stats/prometheus\u0026#39; scrape_interval: 15s scrape_timeout: 15s static_configs: - targets: [\u0026#39;localhost:9091\u0026#39;] 大家还记得 Envoy 中提供的管理接口吗？我们说 Envoy 提供了 Prometheus 格式的指标数据，其实就是指 Envoy 管理接口中的 stats/prometheus 接口，它对应的地址为：http://localhost:9091/stats/prometheus，直接访问这个地址，我们就可以得到下面的结果，这就是 Prometheus 需要的指标数据格式：\nPrometheus 需要的指标数据格式\r数据源与可视化 现在，万事俱备，我们通过docker-compose启动服务即可，默认情况下，Prometheus 使用9090端口，Grafana 使用3000端口，其中，Grafana 默认的账号为admin/admin，建议大家第一次登录后，及时修改默认的账号密码。\nPrometheus 运行效果展示\rGrafana 运行效果展示\r接下来，我们可以注意到，Prometheus 中两个 target 都已正常启动，这表示它们开始采集数据，我们还可以通过 Graph 菜单来查看当前采集到的数据。\nPrometheus 采集数据\r那么，数据采集到 Prometheus 以后，如何在 Grafana 中进行图表的可视化展示呢？首先，我们需要在 Grafana 中添加一个数据源，点击左侧第6个图标就可以找到入口。显然，这里的数据源就是 Prometheus ：\nGrafana 添加数据源\r接下来，我们可以到官方的 社区 里找一个 Envoy 的模板，这是一个别人做好的 Dashboard，我们暂时用这个模板来看看效果。随着学习的深入，我们会先从自定义图表开始做起，最终，我们会拥有一个属于自己的 Dashboard 。这里，我们选择一个 Dashboard 模板后，复制其ID，并在 Grafana 中进行导入，导入的时候需要选择数据源，我们选择 Prometheus 即可。接下来，就是见证奇迹的时刻：\nEnvoy 监控面板效果展示\r自定义图表 好了，如果大家阅读过官方文档，就会知道，除了 Prometheus ，像常见的 MySQL、Nginx 等，都可以作为 Grafana 的数据源，如果你需要监控 Nginx 的某个指标，这会是个非常不错的思路。那么。如何按照个人/领导的要求，对 Dashboard 进行进一步的定制呢？这就要说到 Grafana 的自定义图表，这里，我们通过下面的例子来进行说明：\nrate(envoy_http_rq_total{envoy_http_conn_manager_prefix=\u0026#34;grpc_json\u0026#34;, instance=\u0026#34;192.168.6.120:9902\u0026#34;}[5m]) 在 Prometheus 中，采用的是与 OpenTSDB 类似的时序格式：\n\u0026lt;metric name\u0026gt;{\u0026lt;label name\u0026gt;=\u0026lt;label value\u0026gt;, ...} 可以注意到，每一个指标含有多个键值形式的标签。例如，http_requests_total{method=\u0026quot;POST\u0026quot;}表示的是所有 HTTP 请求中的 POST 请求。\n此外，除了上文中提到过的 Counter 、 Gauge 、Histogram 这三种类型，Prometheus 还支持一种叫做 Summary 的类型。和大多数语言类似，这门被叫做 PromQL 的语言，(1)：支持常见的运算符，例如算术运算符、比较运算符、逻辑运算符、聚合运算符等等。(2)：支持大量的内置函数，例如，由浮点型转换为整型的floor和ceil，计算平均速率的rate等等：\nfloor(avg(http_requests_total{code=\u0026#34;200\u0026#34;})) ceil(avg(http_requests_total{code=\u0026#34;200\u0026#34;})) rate(http_requests_total[5m]) 在这里，我们给出的示例，它表示的是5分钟内 HTTP 请求的平均数目。我们可以在 Prometheus 中的 Graph 菜单对其结果进行查看：\n在 Prometheus 中查询 Envoy 指标数据\r通常，我们可以在这里对查询语句做简单的调试，而如果需要将其集成到 Grafana 中，我们就需要在 Grafana 新建一个图表，可以注意到，两者的语法是完全相同的，这里唯一的不同点在于，时间间隔从固定的5分钟变成了一个变量：\n在 Grafana 中查询 Envoy 指标数据\r此时，我们就完成了一个自定义图表的制作，其中的关键有两点，其一是了解每一个指标的含义，其二是了解每一个内置函数的用法。革命尚未成功，同志仍须努力。这些内容无法在一篇博客里全部讲到，如果需要做进一步的探索，还是建议大家去看官方文档，这里博主可以给大家推荐一个不错的中文文档。\n文本小结 本文介绍了利用 Prometheus 和 Grafana 对 Envoy 进行监控预警的方案。在 Envoy 的早期版本中，主流的方案都是通过 statsd 来采集 Envoy 的指标信息，而在 Envoy 最新版本中，它本身就可以输出 Prometheus 需要的数据格式，我们只需要在 Prometheus 的配置文件中指定stats/prometheus这个地址即可。Prometheus 采用了和 OpenTSDB 类似的时序格式，每一个指标均含有多个键值形式的标签。Prometheus 在此基础上提供了 PromQL 查询语言，我们可以利用这个查询语言在 Grafana 中制作自定义图表，这些自定义图表可以是一个瞬时数据、可以是一个区间数据，或者是一个纯量数字，因此，我们可以按照自己的喜好去定制整个仪表盘，结合实际的业务场景来决定要关注哪些指标。除此以外，我们还可以在 Prometheus 定义告警规则，当业务系统出现问题时，可以第一时间通知运维或者研发团队。在后端研发越来越服务化、集群化的今天，我们不能永远都盯着 CRUD 这一亩三分地，更普遍的，可能是针对 Docker、K8S、Redis、MySQL 等等基础设施的监控，扁鹊见蔡桓公的故事大家耳熟能详，防微杜渐，无论过去还是现在甚至将来都是一样的。好了，以上就是这篇博客的全部内容啦，谢谢大家！\n","date":"2021-07-10T14:41:24Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1519021197/","slug":"1519021197","tags":["微服务","Envoy","Prometheus","Grafana","监控"],"title":"ASP.NET Core 搭载 Envoy 实现微服务的监控预警"},{"categories":["编程语言"],"content":"如果说，我们一定要找出一个词来形容这纷繁复杂的世界，我希望它会是熵。有人说，熵增定律是宇宙中最绝望的定律，所谓熵，即是指事物混乱或者无序的程度。在一个孤立系统下，熵是不断在增加的，当熵达到最大值时，系统就会出现严重混乱，直至最终走向死亡。从某种意义上来讲，它揭示了事物结构衰退的必然性，甚至于我们的人生，本来就是一场对抗熵增的旅程。熵增的不可逆性，正如时光无法倒流一般，古人说，“覆水难收”正是这个道理。同样地，当我们开始讨论微服务的划分/编写/治理的时候，当我们使用服务网格来定义微服务架构的时候……我们是否有意或者无意的增加了系统中的熵呢？**一个孤立的系统尚且会因为熵增而最终走向死亡，更何况是相互影响和制约的复杂系统呢？**现代互联网企业都在追求4个9(即99.99%)的高可用，这意味着年平均停机时长只有52.56分钟。在此之前。我们介绍过重试和熔断这两种故障转移的策略，而今天我们来介绍一种更朴素的策略：负载均衡。\n什么是负载均衡 负载均衡，即Load Banlancing，是一种计算机技术，用来在多个计算机(计算机集群)、网络连接、CPU、磁盘驱动器或其它资源中分配负载，以达到最优化资源使用、最大化吞吐率、最小化响应时间、避免过载的目的。\n我们可以注意到，在这个定义中，使用负载均衡技术的直接原因是避免过载，而根本原因则是为了优化资源使用，确保最大吞吐量、最小响应时间。所以，这本质上是一个局部最优解的问题，而具体的手段就是\u0026quot;多个\u0026quot;。有人说，技术世界不过是真实世界的一个镜像，联系生活中实际的案例，我们发现负载均衡比比皆是。譬如车站在面对春运高峰时增加售票窗口，银行通过多个业务窗口来为客户办理业务……等等。这样做的好处显而易见，可以大幅度地减少排队时间，增加\u0026quot;窗口\u0026quot;这个行为，在技术领域我们将其称为：水平扩展，因为有多个\u0026quot;窗口\u0026quot;，发生单点故障的概率就会大大降低，而这正是现在软件追求的三\u0026quot;高\u0026quot;：高性能、高可用、高并发。\n银行柜员窗口示意图\r每次坐地铁经过小寨，时常听到地铁工作人员举着喇叭引导人们往不同的出口方向走动。此时，工作人员就是一个负载均衡器，它要做的就是避免某一个出口人流量过载。**从熵的角度来看，人流量过载，意味着无序/混乱状态加剧，现代社会通过道德和法律来对抗熵增，人类个体通过自律来对抗熵增。**有时候，我会忍不住去想，大人与小孩儿愈发内卷的恶性竞争，除了给这个世界带来更多的熵以外，还能带来什么？如果参考社会达尔文主义的理论，在这个弱肉强食的世界里，增加熵是人为的选择，而同样的，你亦可以选择\u0026quot;躺平\u0026quot;。\n负载均衡器示意图\rOK，将思绪拉回到负载均衡，它所做的事情，本质上就是控制信息或者说流量流动的方向。一个网站，以集群的方式对外提供服务，你只需要输入一个域名，它就可以把请求分发到不同的机器上面去，而这就是所谓的负载均衡。目前，负载均衡器从种类上可以分为：基于DNS、基于MAC地址(二层)、基于IP(三层)、基于IP和Port(四层)、基于HTTP(七层)。\nOSI七层模型与TCP/IP五层模型\r譬如，博主曾经参与过伊利的项目，它们使用的就是一个四层的负载均衡器：F5。而像更常见Nginx、HAProxy，基本都是四层和七层的负载均衡器，而Envoy就厉害了，它可以同时支持三/四/七层。负载均衡器需要配合负载均衡算法来使用，典型的算法有：轮询法、随机法、哈希法、最小连接数法等等，而这些算法都可以结合加权算法引出新的变式，这里就不再一一列举啦。\nEnvoy中的负载均衡 通过上一篇博客，我们已经了解到，Envoy中一个HTTP请求的走向，大致会经历：客户端、侦听器(Listeners)、集群(Clusters)、终结点(Endpoints)、服务(ervices)这样几个阶段。其中，一个集群可以有多个终结点(Endpoints)。所以，这里天然地就存在着负载均衡的设计。因为，负载均衡本质上就是告诉集群，它应该选择哪一个终结点(Endpoints)来提供服务。而之所以我们需要负载均衡，一个核心的原因，其实是因为我们选择了分布式。\nEnvoy架构图：负载均衡器连接集群和服务\r如果类比RabbitMQ、Kafka和Redis，你就会发现，这些产品中或多或少地都会涉及到主(Leader)、从(Follower)以及推举Leader的实现，我个人更愿意将其看作是更广义的负载均衡。最直观的，它可以分担流量，简称分流，不至于让某一台服务器满负荷做运行。其次，它可以作为故障转移的一种方案，人生在世，多一个B计划，就多一种选择。同样地，多一台服务器，就多一分底气。最后，它可以指导某一个产品或者功能的推广，通过给服务器设置不同的权重，在必要的时候，将流量局部地导入某一个环境，腾讯和阿里这样的大厂，经常利用这种方式来做灰度测试。\nEnvoy中支持常用的负载均衡算法，譬如：ROUND_ROBIN(轮询)、LEAST_REQUEST(最少请求)、RING_HASH(哈希环)、RANDOM(随机)、MAGLEV(磁悬浮)、CLUSTER_PROVIDED等等。因为一个集群下可以有多个终结点，所以，在Envoy中配置负载均衡，本质上就是在集群下面增加终结点，而每个终结点则会对应一个服务，特殊的点在于，这些服务可能是通过同一个Dockerfile或者Docker镜像来构建的。所以，一旦理解了这一点，Envoy的负载均衡就再没有什么神秘的地方。例如，下面的代码片段展示了，如何为WeatherService这个集群应用负载均衡：\nclusters: # Weather Service - name: weatherservice connect_timeout: 0.25s type: STRICT_DNS # ROUND_ROBIN(轮询） # LEAST_REQUEST(最少请求) # RING_HASH(哈希环) # RANDOM(随机) # MAGLEV(磁悬浮) # CLUSTER_PROVIDED lb_policy: LEAST_REQUEST load_assignment: cluster_name: weatherservice endpoints: - lb_endpoints: - endpoint: address: socket_address: address: weatherservice1 port_value: 80 - endpoint: address: socket_address: address: weatherservice2 port_value: 80 是不是觉得特别简单？我想说，也许是Envoy更符合人的直观感受一些，理解起来本身没有太大的心智负担。最近看到一个缓存设计，居然还要依赖Kafka，使用者为了使用缓存这个功能，就必须先实现三个丑陋的委托，这就是所谓的心智负担，违背人类的直觉，使用缓存为什么要了解Kafka？到这里，你大概就能了解利用Envoy实现负载均衡的思路，首先是用同一个Dockerfile或者Docker镜像启动多个不同容器(服务)，然后将指定集群下面的终结点指定不同的服务，再告诉集群要用哪一种负载均衡策略即可。\n邂逅 ASP.NET Core OK，说了这么多，这里我们还是用ASP.NET Core写一个例子。可以预见到的是，我们需要一个Envoy网关，一个ASP.NET Core的服务。这里，我们还是用Docker-Compose来编排这些服务，下面是对应的docker-compose.yaml文件：\nversion: \u0026#39;3\u0026#39; services: envoygateway: build: Envoy/ ports: - \u0026#34;9090:9090\u0026#34; - \u0026#34;9091:9091\u0026#34; volumes: - ./Envoy/envoy.yaml:/etc/envoy/envoy.yaml weatherservice1: build: WeatherService/ ports: - \u0026#34;8082:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; weatherservice2: build: WeatherService/ ports: - \u0026#34;8084:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; 而对于Envoy来说，主要的工作是维护集群下的终结点信息这块儿。其实，这段配置在本文的上一节就出现过啦，你有多少个服务实例，就配置多少个终结点，请求落在哪一个实例上，就交给Envoy来决定好啦。故而，这里我们不再做更多的解释：\nstatic_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 9090 filter_chains: - filters: - name: envoy.filters.network.http_connection_manager typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager codec_type: AUTO stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: backend domains: - \u0026#34;*\u0026#34; routes: - match: prefix: \u0026#34;/api/w\u0026#34; route: auto_host_rewrite: true prefix_rewrite: /Weather cluster: weatherservice http_filters: - name: envoy.filters.http.router clusters: # Weather Service - name: weatherservice connect_timeout: 0.25s type: STRICT_DNS lb_policy: LEAST_REQUEST load_assignment: cluster_name: weatherservice endpoints: - lb_endpoints: - endpoint: address: socket_address: address: weatherservice1 port_value: 80 - endpoint: address: socket_address: address: weatherservice2 port_value: 80 admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 0.0.0.0, port_value: 9091 } 使用docker compose up启动容器，我们可以注意到熟悉的ASP.NET Core的身影，这意味着，我们需要的服务都成功地跑起来了。简单调用下API看看，果然，可以正确地返回结果呢(逃……果然，我是一个喜新厌旧的人，自打用了Encoy以后，再不想用Nginx来做类似的事情(逃……\n通过管理接口查看集群的请求情况\r好了，如何验证我们选择的负载均衡策略呢？这是一个问题。不知道大家还记不记得Envoy的管理接口，它里面可以统计请求的次数，所以，我们只要看看两个服务各自请求了多少次里有好啦！这里以RANDON这个策略为例：\n两个实例拥有不同的请求次数\r可以注意到，两个的请求次数果然是随机的呢？而如果我们换成是ROUND_ROBIN，你就会发现这两个数值一前一后相互追赶。选择哪一种负载均衡策略，这个按大家实际的场景去决定就好。我倒觉得，按最少请求数的策略会好一点。虽然计算机永远不知疲倦地接收人们的指令，可考虑能者多劳这种人类世界里的策略，本身充满道德绑架的意味，使用负载均衡，站在计算机的角度来看，这是避免计算机内卷的一种方案。所以啊，老板们，请不要逮着一个老实人就拼了命的剥削，多学学负载均衡算法，卓别林心目中的摩登时代，我一点都不期待，技术世界总告诉我们要严谨、冷静，可充满人情味的世界，同样值得你去热爱啊。\n本文小结 本文结合物理学中的熵增定律引出了负载均衡这个话题，而**负载均衡的本质，就是在多个计算机(计算机集群)、网络连接、CPU、磁盘驱动器或其它资源中分配负载，以达到最优化资源使用、最大化吞吐率、最小化响应时间、避免过载的目的。**对于这一点，我们可以结合现实生活中的\u0026quot;窗口\u0026quot;排队来理解。负载均衡可以和水平扩展完美结合，通过降低单点的故障率，来达到提升整个系统可用性的目的。接下来，我们介绍了常见的负载均衡器分类及其算法，Envoy中的负载均衡配置，并结合ASP.NET Core编写了一个实际的案例。好了，以上就是这篇博客的全部内容啦，如果大家对于熵增定律或者负载均衡器有任何的看法，欢迎大家在评论区留言，谢谢大家！\n","date":"2021-07-05T22:49:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3599307336/","slug":"3599307336","tags":["Envoy","微服务","负载均衡","熵增定律"],"title":"ASP.NET Core 搭载 Envoy 实现微服务的负载均衡"},{"categories":["编程语言"],"content":"回想起来，博主第一次接触到Envoy，其实是在微软的示例项目 eShopOnContainers，在这个示例项目中，微软通过它来为Ordering API、Catalog API、Basket API 等多个服务提供网关的功能。当时，博主并没有对它做深入的探索。此刻再度回想起来，大概是因为那个时候更迷恋领域驱动设计(DDD)的理念。直到最近这段时间，博主需要在一个项目中用到Envoy，终于决定花点时间来学习一下相关内容。所以，接下来这几篇博客，大体上会以记录我学习Envoy的历程为主。考虑到Envoy的配置项特别多，在写作过程中难免会出现纰漏，希望大家谅解。如对具体的配置项存在疑问，请以官方最新的 文档 为准，本文所用的示例代码已经上传至 Github，大家作为参考即可。对于今天这篇博客，我们来聊聊 ASP.NET Core 搭载 Envoy 实现微服务的反向代理 这个话题，或许你曾经接触过 Nginx 或者 Ocelot，这次我们不妨来尝试一点新的东西，譬如，通过Docker-Compose来实现服务编排，如果对我说的这些东西感兴趣的话，请跟随我的脚步，一起来探索这广阔无垠的技术世界吧！\n走近 Envoy Envoy 官网对Envoy的定义是：\nEnvoy 是一个开源边缘和服务代理，专为原生云应用设计。\n而更进一步的定义是：\nEnvoy 是专为大型现代服务导向架构设计的 L7 代理和通讯总线。\n这两个定义依然让你感到云里雾里？没关系，请看下面这张图：\nEnvoy架构图\r注：图片来源\n相信从这张图中，大家多少能看到反向代理的身影，即下游客户端发起请求，Envoy对请求进行侦听(Listeners)，并按照路由转发请求到指定的集群(Clusters)。接下来，每一个集群可以配置多个终结点，Envoy按照指定的负载均衡算法来筛选终结点，而这些终结点则指向了具体的上游服务。例如，我们熟悉的 Nginx ，使用listen关键字来指定侦听的端口，使用location关键字来指定路由，使用proxy_pass关键字来指定上游服务的地址。同样地，Ocelot 使用了类似的上下游(Upstream/Downstream)的概念，唯一的不同是，它的上下游的概念与这里是完全相反的。\n你可能会说，这个Envoy看起来“平平无奇”嘛，简直就像是“平平无奇”的古天乐一般。事实上，Envoy强大的地方在于：\n非侵入式的架构： 独立进程、对应用透明的Sidecar模式 Envoy 的 Sidecar 模式\rL3/L4/L7 架构：Envoy同时支持 OSI 七层模型中的第三层(网络层, IP 协议)、第四层(传输层，TCP / UDP 协议)、第七层(应用层，HTTP 协议) 顶级 HTTP/2 支持： 视 HTTP/2 为一等公民，且可以在 HTTP/2 和 HTTP/1.1间相互转换 gRPC 支持：Envoy 支持 HTTP/2，自然支持使用 HTTP/2 作为底层多路复用协议的 gRPC 服务发现和动态配置：与 Nginx 等代理的热加载不同，Envoy 可以通过 API 接口动态更新配置，无需重启代理。 特殊协议支持：Envoy 支持对特殊协议在 L7 进行嗅探和统计，包括：MongoDB、DynamoDB 等。 可观测性：Envoy 内置 stats 模块，可以集成诸如 prometheus/statsd 等监控方案。还可以集成分布式追踪系统，对请求进行追踪。 Envoy配置文件 Envoy通过配置文件来实现各种各样的功能，其完整的配置结构如下：\n{ \u0026#34;node\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;static_resources\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;dynamic_resources\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;cluster_manager\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;hds_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;flags_path\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;stats_sinks\u0026#34;: [], \u0026#34;stats_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;stats_flush_interval\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;watchdog\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;tracing\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;runtime\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;layered_runtime\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;admin\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;overload_manager\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;enable_dispatcher_stats\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;header_prefix\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;stats_server_version_override\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;use_tcp_for_dns_lookups\u0026#34;: \u0026#34;...\u0026#34; } 这里我们主要对侦听器(Listeners)、集群(Clusters) 和 管理(Admin)这三个常用的部分来进行说明。其中，(Listeners)、集群(Clusters) 均位于 static_resources 节点下，而 管理(Admin) 则有一个单独的admin节点。\n侦听器(Listeners) 侦听器，顾名思义就是侦听一个或者多个端口：\nstatic_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 9090 在这里，它表示的是侦听9090这个端口，这里的listeners是一个数组，所以，你可以同时侦听多个端口。\n过滤器(Filters) 我们知道，单单侦听一个或者多个端口，是无法完成一个HTTP请求的，因为它还不具备处理HTTP请求的能力。\nEnvoy Filters 架构图\r在 Envoy 中，这一工作由一个或者多个过滤器组成的过滤器链(Filter Chains) 来完成：\nstatic_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 9090 filter_chains: - filters: - name: envoy.filters.network.http_connection_manager typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager codec_type: AUTO stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: backend domains: - \u0026#34;*\u0026#34; routes: - match: prefix: \u0026#34;/api/w\u0026#34; route: auto_host_rewrite: true prefix_rewrite: /Weather cluster: weatherservice - match: prefix: \u0026#34;/api/c\u0026#34; route: auto_host_rewrite: true prefix_rewrite: /City cluster: cityservice http_filters: - name: envoy.filters.http.router 在这段配置中，Http Connection Manager表示的是位于 L3(网络层)/L4(传输层) 的过滤器，这个过滤器连接的下一个过滤器是envoy.filters.http.router，表示的是 L7(应用层) 的关于路由的过滤器。个人感觉，这和我们通常所说的中间件相当接近。注意到，我们在 L3/L4 这个层级上做了什么事情呢？其实应该是 TCP/IP 层面上请求转发，这里定义的路由规则如下：\n当外部调用者访问/api/w时，请求会被转发到WeatherService。 当外部调用者访问/api/c时，请求会被转发到CityService。 集群(Clusters) 在过滤器部分，我们定义了路由，那么，请求的最终去向是哪里呢？这里 Envoy 将其称之为 集群：:\nstatic_resources: clusters: # City Service - name: cityservice connect_timeout: 0.25s type: STRICT_DNS lb_policy: ROUND_ROBIN 集群本身，其实只是一个名字，并没有实际的意义，真正工作的其实是指向上游服务的终结点，我们可以为每一个集群指定一个负载均衡策略，让它决定选择哪一个终结点来提供服务。\n负载均衡(Load Assignment) 目前，Envoy支持以下负载均衡算法：\nROUND_ROBIN：轮询 LEAST_REQUEST：最少请求 RING_HASH：哈希环 RANDOM：随机 MAGLEV：磁悬浮 CLUSTER_PROVIDED 下面的示例展示了如何为某个集群配置负载均衡：\nstatic_resources: clusters: # Weather Service - name: weatherservice connect_timeout: 0.25s type: STRICT_DNS lb_policy: LEAST_REQUEST load_assignment: cluster_name: weatherservice endpoints: - lb_endpoints: - endpoint: address: socket_address: address: weatherservice1 port_value: 80 - endpoint: address: socket_address: address: weatherservice2 port_value: 80 其中，weatherservice1和weatherservice2是同一个服务的两个容器实例，当我们使用Docker-Compose进行构建的时候，不需要显式地去指定每一个服务的IP地址，只要对应docker-compose.yaml文件中的服务名称即可。\n管理(Admin) 管理(Admin)这块儿相对简单一点，主要用来指定Envoy的的管理接口的端口号、访问日志存储路径等等：\nadmin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 0.0.0.0, port_value: 9091 } 服务编排 好了，在正确配置Envoy以后，我们来考虑如何对这些服务进行编排，在本文的 例子 中，我们有两个后端服务，WeatherService 和 CityService，它们本质上是两个ASP.NET Core应用，我们希望通过Envoy来实现反向代理功能。这样做的好处是，后端服务的架构不会直接暴露给外部使用者。所以，你会注意到，在微服务架构的设计中，网关经常扮演着重要的角色。那么，如何对服务进行编排呢？这里我们使用Docker-Compose来完成这个工作：\nversion: \u0026#39;3\u0026#39; services: envoygateway: build: Envoy/ ports: - \u0026#34;9090:9090\u0026#34; - \u0026#34;9091:9091\u0026#34; volumes: - ./Envoy/envoy.yaml:/etc/envoy/envoy.yaml cityservice: build: CityService/ ports: - \u0026#34;8081:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; weatherservice: build: WeatherService/ ports: - \u0026#34;8082:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; 可以注意到，这里需要部署3个服务，其中，Envoy负责监听9090端口，即对外的网关。而两个后端服务，WeatherService 和 CityService则被分别部署在8082和8081端口。这里最重要的是envoy.yaml这个配置文件，我们在上一节编写的配置文件会挂在到容器目录：/etc/envoy/envoy.yaml。Envoy将如何使用这个配置文件呢？事实上，这个Dockerfile是这样编写的：\nFROM envoyproxy/envoy-alpine:v1.16-latest COPY ./envoy.yaml /etc/envoy.yaml RUN chmod go+r /etc/envoy.yaml CMD [\u0026#34;/usr/local/bin/envoy\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/etc/envoy.yaml\u0026#34;, \u0026#34;--service-cluster\u0026#34;, \u0026#34;reverse-proxy\u0026#34;] 除此之外，Envoy通过9091端口提供管理功能，我们可以通过这个端口来获得集群、请求、统计等信息，这一特性我们将会在接下来的文章里用到：\nEnvoy 管理界面功能一览\r这里，想分享一个关于Envoy的小技巧，当我们在指定集群的地址时，可以使用docker-compose.yaml中定义的服务的名称，这会比填入一个固定的IP地址要更优雅一点，理由非常简单，我们不希望每次都来维护这个地址。\n# Weather Service - name: weatherservice connect_timeout: 0.25s type: STRICT_DNS lb_policy: ROUND_ROBIN load_assignment: cluster_name: weatherservice endpoints: - lb_endpoints: - endpoint: address: socket_address: # 建议使用 docker-compose.yaml 文件中对应的服务名称来代替IP地址 address: weatherservice port_value: 80 接下来，如果每一个服务的Dockerfile都编写正确的话，我们就可以通过docker compose up命令启动这一组服务，通过命令行下打印出来的信息，我们可以确认，基于Envoy的网关服务、两个基于ASP.NET Core的后端服务都已经成功运行起来：\n在 Docker-Compose 中成功启动服务\r还记得我们的路由规则是是如何定义的吗？\n当外部调用者访问/api/w时，请求会被转发到WeatherService。 当外部调用者访问/api/c时，请求会被转发到CityService。 实际的情况如何呢？我们不妨来验证一下：\n通过 Envoy 调用 WeatcherService 可以注意到，不管是浏览器返回的结果，还是容器内部输出的日志，都表明请求确实被转发到对应的服务上面去了，这说明我们设计的网关已经生效。至此，我们实现了基于Envoy的反向代理功能，有没有觉得比Nginx要简单一点？重要的是，基于Docker-Compose的服务编排使用起来真的舒适度爆棚，这意味着你会有更多的属于程序员的贤者时间。前段时间热映的电视剧《觉醒时代》，鲁迅先生写完《狂人日记》后那一滴眼泪令人动容。也许，这种如匠人一般反复雕琢、臻于完美的心境是互通的，即使人类的悲欢并不相通，对美和极致的追求竟然出奇的相似。\n本文小结 本文主要介绍了 ASP.NET Core 搭载 Envoy 实现反向代理这一过程。对于 Envoy，有两个重要的定义：第一、Envoy 是一个开源边缘和服务代理，专为原生云应用设计。第二、Envoy 是专为大型现代服务导向架构设计的 L7 代理和通讯总线。相比 Nginx 和 Ocelot ，Envoy 提供了 L7 级别的代理服务，支持 HTTP/2 和 gRPC，无侵入式的Sidecar模式可以提供与应用进程完全隔离的代理服务。接下来，博主对 Envoy 配置文件的结构及主要的配置项进行了说明，对于常见的 API 网关，我们应该重点关注侦听器(Listeners) 和 集群(Clusters)。最终，我们结合Docker-Compose对服务进行了编排，并由此构建出了一个基本的反向代理的方案。本文的源代码已托管至 Github ，大家可以在此基础上做进一步的探索。好了，以上就是这篇博客的的全部内容啦，欢迎大家就本文中提出的方案、代码等进行讨论，如果大家有任何意见或者建议，欢迎在评论区进行留言，谢谢大家！\n参考文档 Envoy中文指南 Envoy官方文档 eShopOnContainers 知多少[12]：Envoy Gateways ","date":"2021-07-01T22:49:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3599307335/","slug":"3599307335","tags":["Envoy","微服务","网关","反向代理"],"title":"ASP.NET Core 搭载 Envoy 实现微服务的反向代理"},{"categories":["编程语言"],"content":"在构建以 gRPC 为核心的微服务架构的过程中，我们逐渐接触到了 gRPC 的过滤器、健康检查、重试等方面的内容。虽然， Protocol Buffers 搭配 HTTP/2 ，在整个传输层上带来了显著的性能提升，可当这套微服务方案面对前后端分离的浪潮时，我们能明显地有点“水土不服”。其实，如果单单是以 Protocol Buffers 来作为 HTTP 通信的载体，通过 protobuf.js 就可以实现前端的二进制化。考虑到 gRPC 实际的通信过程远比这个复杂，同时还要考虑.proto文件在前/后端共享的问题，所以，我们面对的其实是一个相当复杂的问题。现代的前端世界，是一个React、Angular和Vue三足鼎立的世界，如果这个世界不能和微服务的世界打通，我们面对的或许并不是一个真实的世界。因为博主注意到，项目中有一部分 gRPC 服务被封装为Web API并提供给前端，这说明大家都意识到了这个问题。所以，这篇博客想和大家分享的是，如何打通 gRPC 和 前端 两个不同的世界，这里介绍四种方式：gRPC-Web、gRpc-Gateway、封装 Web API、编写中间件，希望能给大家带来一点启发。\ngRPC-Web gRPC-Web 是官方提供的一个方案，它的原理是利用命令行工具ptotoc及其插件protoc-gen-grpc-web来生成.proto对应的客户端代码，这些代码经过webpack这类打包工具处理以后，就可以在前端使用。所以，对于 gRPC-Web ，你可以从两个方面来考虑它：第一，它支持生成强类型的客户端代码；第二，它支持在非 HTTP/2 环境下使用 gRPC 。下面是一个基本的使用流程：\n首先，我们需要下载命令行工具：protoc 及其插件：protoc-gen-grpc-web。\n此时，我们可以使用下面的命令来生成JavaScript版本的 gRPC 代码：\nprotoc greetjs.proto \\ --js_out=import_style=commonjs:. \\ --grpc-web_out=import_style=commonjs,mode=grpcwebtext:. \\ --plugin=protoc-gen-grpc-web=C:\\Users\\Payne\\go\\bin\\protoc-gen-grpc-web.exe 其中：\n--js_out 和 --grpc-web_out 分别指定了我们要生成的JavaScript代码的模块化标准，这里使用的是 CommonJS 规范。 mode=grpcwebtext 指定 gRPC-Web 的数据传输方式。目前：支持两种方式，application/grpc-web-text(Base64 编码，文本格式) 和 application/grpc-web+proto(二进制格式)，前者支持 Unary Calls 和 Server Streaming Calls，后者只支持 Unary Calls。 在这个例子中，会生成下面两个文件，它们分别定义了客户端和消息这两个部分：\n利用 protoc 生成 JavaScript 代码\r此时，我们可以这样编写我们的逻辑代码：\nvar client = new proto.greet.GreeterClient(\u0026#39;http://localhost:8000\u0026#39;); var request = new proto.greet.HelloRequest(); var metadata = { } request.setName(\u0026#39;长安书小妆\u0026#39;); client.sayHello(request, metadata, function(error, response) { if (error) { console.log(error); } else { console.log(response.getMessage()); } }); 如果你更倾向于使用类型安全的 TypeScript，你还可以按下面的方式来生成代码：\nimport_style=commonjs+dts: CommonJS \u0026amp; .d.ts typings import_style=typescript: 100% TypeScript 更多的细节请参考官方文档：https://hub.fastgit.org/grpc/grpc-web#typescript-support\n接下来，对于 .NET 开发者而言， gRPC-Web 意味着我们只需要简单地配置下 ASP.NET Core 的中间件管道，就可以享受到上面提供的这些便利。因为 Visual Studio 会在编译.proto文件时，自动帮你生成这个客户端代码，我们可以将这一技术应用到单页面应用(SPA) 和 WebAssembly 中，最典型的例子莫过于微软的 Blazor，它使得 gRPC 可以充当客户端与服务端间的信使。同样地，这里准备了相关的示例代码：\npublic void ConfigureServices(IServiceCollection services) { services.AddGrpc(); } public void Configure(IApplicationBuilder app, IWebHostEnvironment env) { // ... app.UseGrpcWeb(); app.UseEndpoints(endpoints =\u0026gt; { endpoints.MapGrpcService\u0026lt;GreeterService\u0026gt;().EnableGrpcWeb(); ); } 如果大家留意一下微软官方的 示例项目，就会发现和这里类似的东西，因为原理上一脉相承：\nconst { HelloRequest, HelloReply } = require(\u0026#39;./greet_pb.js\u0026#39;); const { GreeterClient } = require(\u0026#39;./greet_grpc_web_pb.js\u0026#39;); var client = new GreeterClient(window.location.origin); var nameInput = document.getElementById(\u0026#39;name\u0026#39;); var sendInput = document.getElementById(\u0026#39;send\u0026#39;); var streamInput = document.getElementById(\u0026#39;stream\u0026#39;); var resultText = document.getElementById(\u0026#39;result\u0026#39;); // Unary call sendInput.onclick = function () { var request = new HelloRequest(); request.setName(nameInput.value); client.sayHello(request, {}, (err, response) =\u0026gt; { resultText.innerHTML = htmlEscape(response.getMessage()); }); }; gRPC-Web 在将 gRPC 带入前端世界的过程中，其实是牺牲了一部分重要特性的，譬如浏览器中无法实现 HTTP/2，相对应地，gRPC-Web 不再支持客户端流和双向流，依然支持服务端流，博主猜测可能是利用了服务端发送事件(Server Sent Event)。不过，这并不影响我们对这个项目的敬意，感谢它将 gRPC 带入了前端的世界。\ngRPC-Gateway gRPC-Gateway 同样是命令行工具protoc的一个插件，其原理是，读取 gRPC 服务定义，并生成一个反向代理服务器，将 RESTful JSON API 转换为 gRPC 。而两者间的对应关系，则是通过.proto文件中的自定义选项来维护的。简单来说，就是在我们定义 gRPC 服务的同时，增加一组选项来表明这是一个 RESTful JSON API 。目前，这个插件只支持Go语言的代码生成。所以，如果想玩一玩这个插件，需要大家安装好Go的环境。\n首先，我们从 Github 下载 Protocol Buffers 的编译器，它负责从从.proto文件生成代码。\n这里我们选择 Windows 版本，直接将其解压到一个非中文的路径下即可。\nProtocol Buffers 的编译器\r这里，我们需要配置下面两个环境变量：\nPATH：C:\\Program Files\\Protobuf\\bin PROTOC_INCLUDE：C:\\Program Files\\Protobuf\\include 接下来，在Go环境中进行以下设置：\ngo env -w GO111MODULE=on go env -w GOPROXY=https://goproxy.cn,direct go install github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway go install github.com/grpc-ecosystem/grpc-gateway/protoc-gen-swagger go install github.com/golang/protobuf/protoc-gen-go 这样，我们就通过Go完成了protoc的插件的安装。此时，我们可以通过下面的命令来生成Go代码：\n# 生成Go的客户端代码 protoc --proto_path=. \\ --go_out=. \\ --plugin=protoc-gen-go=C:\\Users\\Payne\\go\\bin\\protoc-gen-go.exe \\ ./greet.proto # 生成Go的反向代理服务器端代码 protoc \\ -I C:\\Users\\Payne\\go\\pkg\\mod\\github.com\\grpc-ecosystem\\grpc-gateway@v1.9.0\\third_party\\googleapis\\ \\ --proto_path=. \\ --grpc-gateway_out=. \\ --plugin=protoc-gen-grpc-gateway=C:\\Users\\Payne\\go\\bin\\protoc-gen-grpc-gateway.exe \\ ./greet.proto 此时，我们可以得到下面两个.go格式的文件：\n通过 grpc-gateway 生成 Go 代码\r关于反向代理服务器的观点的验证，大家可以从生成的第二个文件中去发现。\n而关于 gRPC-Gateway 这个插件的使用，最直观的用法，其实应该来自.proto文件：\nsyntax = \u0026#34;proto3\u0026#34;; // Go里面的包名，必选 option go_package = \u0026#34;grpc-gateway/hello-word\u0026#34;; package greet; // Google的API注解相关的.proto文件，必选 import \u0026#34;Protos/google/api/annotations.proto\u0026#34;; service Greeter { rpc SayHello (HelloRequest) returns (HelloReply) { option (google.api.http) = { post: \u0026#34;/v1/greet/sayHello\u0026#34; body: \u0026#34;*\u0026#34; }; }; } message HelloRequest { string name = 1; } message HelloReply { string message = 1; } 考虑到博主并不擅长Go这门语言，这里我们就不再对它做进一步的探索啦！事实上，我觉得这个方案非常糟糕，因为只要修改了.proto文件，这个代理服务器就要重新生成，更不用说只支持Go这一显著的缺点啦！\n封装 Web API 封装 Web API，这是一个非常朴实无华的方案，博主目前的公司就是采用这种方案，所以，你能想象得到，基本就是在控制器中调用客户端。唯一的弊病在于，这是一个非常低效的工作。当年，博主的前公司，就是风风火火地要这样替换掉 WCF，结果最终还是不了了之。所以说，世间没有银弹，历史不过是一次次地重复上演。下面是一个简单的示例：\npublic async Task\u0026lt;ActionResult\u0026gt; SayHello(HelloRequestDTO requestDTO) { var request = requestDTO.Adapt\u0026lt;HelloRequest\u0026gt;(); var client = _serviceProvider.GetService\u0026lt;Greeter.GreeterClient\u0026gt;(); var replay = await client.SayHelloAsync(request); return new JsonResult(replay); } 而一旦做到这一层，其实我们是把一个未知的问题转化成一个已知的问题，这是数学家最常用的思路。\nvar headers = new Headers(); headers.append(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;); var options = { method: \u0026#39;POST\u0026#39;, headers: headers, body: JSON.stringify({name: \u0026#39;长安书小妆\u0026#39;}), }; fetch(\u0026#34;https://localhost:44372/Greet/SayHello\u0026#34;, options) .then(response =\u0026gt; response.json()) .then(result =\u0026gt; console.log(result)) .catch(error =\u0026gt; console.log(error)); 那么，下一个问题，你打算用 Fetch API 还是 Axios 呢？这个问题就交给前端的朋友啦！因为，我是一个伪全栈工程师(逃。\n编写中间件 其实，读到这里，你就会明白，这才是我真正要分享的内容，而此前种种，不过是我为了丰富这个话题而抛出的它山之石。既然觉得手写 Web API 太麻烦，那么我们能不能用一种新的思路来解决这个问题呢？这里说一下博主的思路，用户传入 JSON，经过中间件反序列化为.proto对应的类型，我们将这个类型传递给 gRPC 的客户端作为请求参数，等拿到结果以后，我们再将它序列化为 JSON 即可。这样，我们就实现了将一个 gRPC 服务转化为 Web API 的想法。下面是具体的代码，其实这个代码并不复杂，我最初打算用反射来解决，可惜 gRPC 生成的这个客户端方法重载实在太多啦，所以，我最后决定用下面的这种方式。当然啦，缺点就和 gRPC-Gateway 一样，每一个接口都要单独写，好处大概是代码量减少了好多。\n// 定义扩展方法：AddGrpcGateway public static void AddGrpcGateway\u0026lt;TClient,TRequest,TResponse\u0026gt;( this IApplicationBuilder app, string route, Func\u0026lt;string, TRequest\u0026gt; requestBuilder, Func\u0026lt;TClient,TRequest,TResponse\u0026gt; responseBuilder ) { app.UseEndpoints(endpoints =\u0026gt; endpoints.MapPost(route, async context =\u0026gt; { using (var streamReader = new StreamReader(context.Request.Body)) { var client = (TClient)app.ApplicationServices.GetService(typeof(TClient)); var payload = await streamReader.ReadToEndAsync(); var request = requestBuilder(payload); var reply = responseBuilder(client, request); var response = JsonConvert.SerializeObject(reply); await context.Response.Body.WriteAsync(Encoding.UTF8.GetBytes(response)); context.Response.StatusCode = 200; context.Response.ContentType = \u0026#34;application/json\u0026#34;; } })); } 从代码中可以看出，这个方案依赖 gRPC 的客户端代码，同时需要读取 HTTP 的请求体，所以，我们还需要下面的代码作为辅助：\npublic void ConfigureServices(IServiceCollection services) { services.Configure\u0026lt;KestrelServerOptions\u0026gt;(x =\u0026gt; x.AllowSynchronousIO = true); services.Configure\u0026lt;IISServerOptions\u0026gt;(x =\u0026gt; x.AllowSynchronousIO = true); services.AddGrpcClient\u0026lt;Greeter.GreeterClient\u0026gt;(opt =\u0026gt; { opt.Address = new Uri(\u0026#34;https://localhost:8001\u0026#34;); }); } 接下来，我们通过中间件配置一个路由即可：\n// 建议放在 UseEndpoints() 方法下面 app.AddGrpcGateway\u0026lt;Greeter.GreeterClient, HelloRequest, HelloReply\u0026gt;( route: \u0026#34;greet/SayHello\u0026#34;, requestBuilder: json =\u0026gt; new MessageParser\u0026lt;HelloRequest\u0026gt;(() =\u0026gt; new HelloRequest()).ParseJson(json), responseBuilder: (client, request) =\u0026gt; client.SayHelloAsync(request).ResponseAsync.Result ); 为了证明这个中间件真的有用，我们用 Apifox 或者 Postman 测试一下看看。\n自定义中间件实现 gRPC 转 API 效果\r此时，可以看到，这就真的和调用一个 Web API 一样，我们完全意识不到，这是一个 gRPC 服务。你觉得，这样子算是达到目的了吗？\n本文小结 其实，本文完全是临时想起来决定要写的一篇文章，起因就是看到了项目中有人在手动地封装 gRPC 服务为 RESTful 服务，当时就在想有没有一种方案，可以让这个过程稍微好一点点。所以，你可以认为，我写这篇博客的初衷，原来就是为了炫耀我写的那几行代码。不过，人到了一定的阶段以后，不管是写作还是思考，都似乎越来越喜欢某种框架结构，这种体验就有点像是上学时候写论文一样，虽然你明确地知道自己在做什么，可当你真正要把你的思路或者过程复述出来的时候，你还是需要有一个“文献综述”的环节。我个人以为，这是一种由外及内的认知方法，通过内外世界的对比来寻找自我提升的突破口。对于本文而言，不管是 gRPC-Web 还是 gRPC-Gateway，从本质上来讲，它们都是 Protocol Buffers 工具链中的插件，在这个过程中发现了平时使用 gRPC 过程中被隐藏了的一部分细节，这些细节如果能和开发工具完美结合的话，就可以极大地提升我们在 gRPC 方面的开发效率，譬如 gRPC-Web 在 .NET 中的实现就利用了 MSBuild 的自定义编译任务，这就让底层的 Protocol Buffers 工具链、前端构建工具等对使用者来说是无感知的，从开发体验上就给人心旷神怡的感觉。我个人还是倾向于结合 ASP.NET Core 或者容器级别的 Envoy 来解决这个问题，我觉得应该还有更好的方案，希望大家可以在评论区写下你的想法。好啦，这篇博客就先写到这里，谢谢大家！\n更新说明 截至 2021 年 6 月 25 日，基于中间件的方案已支持以下特性：自动注入客户端、自动配置路由。详情请参考：https://hub.fastgit.org/Regularly-Archive/2021/tree/master/src/GRPC.Logging/Grpc.Gateway。在此方案下，只需要 4 行代码：\npublic void ConfigureServices(IServiceCollection services) { services.Configure\u0026lt;KestrelServerOptions\u0026gt;(x =\u0026gt; x.AllowSynchronousIO = true); services.Configure\u0026lt;IISServerOptions\u0026gt;(x =\u0026gt; x.AllowSynchronousIO = true); services.AddGrpcClients(opt =\u0026gt; opt.Address = new Uri(\u0026#34;https://localhost:8001\u0026#34;)); } public void Configure(IApplicationBuilder app, IWebHostEnvironment env) { // ... app.AddGrpcGateway(); } 以上！\n","date":"2021-06-20T21:37:36Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2167892202/","slug":"2167892202","tags":["gRPC","微服务","前端","Web"],"title":"ASP.NET Core gRPC 打通前端世界的尝试"},{"categories":["数据存储"],"content":"在软件开发过程中，数据库永远都是绕不开的一个话题。有时候，我们甚至会因此而获得一个名字——“CURD Boy”。虽然不过是朴实无华的“增删查改”，可隐隐然早已分出了无数的流派。在这些不同的流派中，有的人坚持“我手写我心”，认为手写SQL才是真正的王道，没有读过/写过成百上千行的存储过程，便不足以谈论程序员的人生。而有的人喜欢ORM的清晰、整洁，认为数据库和面向对象存在着天然抗阻，ORM更有利于推进DDD和微服务的落地。相信大家都听说过Java里的SSH框架，从Hibernate到Mybatis再到Spring Data JPA，可以说这种争论一直没有停止过。这里我们不打算讨论这个问题，我们平时使用EF或者EFCore的过程中，作为连接数据库和面向对象两个异世界的桥梁，ORM需要我们来告诉它，实体数据与数据库表字段的映射关系，所以，经常需要通过数据注解或者Fulent API来写各种配置。那么，有没有什么方案可以让我们偷这个懒呢？下面隆重请出本文的主角：EFCore.NamingConventions。\n使用方法 EFCore. NamingConventions，目前由一个非官方的组织进行维护，代码托管在 Github 上，100％的开源项目。\n如果你希望直接使用它的话，可以直接通过NuGet进行安装：\nInstall-Package EFCore.NamingConventions 接下来，我们只需要在DbContext的 OnConfiguring()方法中，调用它提供的扩展方法即可：\nprotected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) =\u0026gt; optionsBuilder .UseSqlite(\u0026#34;Data Source=Chinook.db\u0026#34;) .UseSnakeCaseNamingConvention(); 或者，你可以使用依赖注入的方式：\nservices.AddDbContext\u0026lt;ChinookContext\u0026gt;(options =\u0026gt; options.UseSqlite(\u0026#34;Data Source=Chinook.db\u0026#34;) .UseSnakeCaseNamingConvention() ); 这里我以SQLite数据库为例，来展示它的具体使用细节。事实上，它提供了 4 种命名约定的策略：\nUseSnakeCaseNamingConvention: FullName -\u0026gt; full_name UseLowerCaseNamingConvention: FullName -\u0026gt; fullname UseCamelCaseNamingConvention: FullName -\u0026gt; fullName UseUpperCaseNamingConvention: FullName -\u0026gt; FULLNAME 简单来说，就是当我们的实体中存在一个属性FullName时，它会告诉EF或者EFCore，这个属性FullName对应的表字段是什么。\n虽然，在大多数的场景中，我们都希望属性名称和表字段一致，可你要知道，像Oracle这种对大小写敏感的数据库，特别喜欢自作聪明地帮你全部改成大写。\n所以，在上家公司工作的时候，为了兼容Oracle这病态的癖好，公司里有个不成文的规定，那就是：所有实体的属性名称最好都大写。\n本来大家用驼峰命名就是为了好认单词，好家伙！这下全部大写了，一眼望过去简直就是灾难，因为没有办法做到“望文生义”，如果那个时候知道这个库的存在，是不是就能解决这个问题了呢？\n第一个示例 下面我们以UseSnakeCaseNamingConvention为例，结合SQLite来做一个简单的例子。\n首先，我们定义必要的实体，并为DbContext配置实体命名约束规则：\n// Album public class Album { public int AlbumId { get; set; } public string Title { get; set; } public int ArtistId { get; set; } public string TenantId { get; set; } } // Artist public class Artist { public int ArtistId { get; set; } public string Name { get; set; } public string TenantId { get; set; } } 接下来，通过迁移命令来生成数据库架构：\nAdd-Migration \u0026#34;Init-Database\u0026#34; -Context ChinookContext Update-Database 可以注意到，生成的数据库表字段会以小写+下划线的方式命名。这就是所谓的实体命名约束。\n通过实体命名约束生成的 Album 表\r只要大家都看着这个约定来写实体的属性，这套机制就可以完美工作。它和MVC里的默认路由一样，都是属于一种“约定大于配置”的方案。\n在我看来，不管是配置还是约定。当以团队为单位进行协作时，最好还是以文档的形式记录下来，否则会出现两种结局，其一是没人知道怎么配置，其二是新人不知道有这个约定。\n以上就是EFCore.NamingConventions的基本用法，更多的细节大家可以去阅读它的README，因为这个库需要结合迁移功能来使用，所以，如果要在已存在的表上应用这套约束规则时，建议大家还是小心谨慎一点。\n我个人觉得，它可以方便团队去制定一套数据库规范，进而去约束开发人员写出更规范的命名。\n本文小结 本文主要介绍了可用于EFCore的实体命名约束库：EFCore.NamingConventions。这是一个由社区维护的、开源的项目，它可以在创建DbContext的时候，指定一个实体命名约束规则，即实体属性如何与数据库表字段进行对应，这是一种约定大于配置的方案，一旦团队形成了属于自己的数据库命名风格，那么，研发人员只需要按照规范为实体属性命名，例如开发人员可以使用驼峰风格的命名，而数据库管理员则可以使用下划线风格的命名。这样，就可以省略一部分字段映射的配置代码，从而提高团队研发的效率。值得说明的一点是，不管是配置还是约定。当以团队为单位进行协作时，最好还是以文档的形式记录下来，否则会出现两种结局，其一是没人知道怎么配置，其二是新人不知道有这个约定。好了，以上就是这篇博客的全部内容啦，谢谢大家！\n","date":"2021-06-17T16:37:11Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3219639636/","slug":"3219639636","tags":["EF","实体",".NET","数据库"],"title":"EFCore 实体命名约定库：EFCore.NamingConventions"},{"categories":["编程语言"],"content":"在上一篇 博客 中，我们一起探索和实现了gRPC的健康检查。从服务治理的角度来看，健康检查保证的是被调用的服务“健康”或者“可用”。可即使如此，我们依然会遇到，因为网络不稳定等原因而造成的服务调用失败的情形，就如同我们赖以生存的这个真实世界，本身就充满了各种不确定的因素一样，“世间唯一不变的只有变化本身”。不管是面对不稳定的服务，还是面对不确定的人生，任何时候我们都需要有一个 B 计划，甚至我们人生中的一切努力，本质上都是为了多一份自由，一份选择的自由。在微服务的世界里，我们将这种选择称之为“降级(Fallback)”，如果大家有接触过 Hystrix 或者 Polly 这类框架，就会明白我这里的所说的“降级”具体是什么。在众多的“降级”策略中，重试是一种非常朴素的策略，尤其是当你调用一个不稳定的服务的时候。\n重试\r引言 在此之前，博主曾经介绍过 HttpClient 的重试。所以，今天这篇博客我们来聊聊gRPC的客户端重试，因为要构建一个高可用的微服务架构，除了需要高可用的服务提供者，同样还需要高可用的服务消费者。下面，博主将由浅入深地为大家分享 4 种重试方案的实现，除了 官方 内置的方案，基本上都需要搭配 Polly 来使用，所以，到这里你可以理解这篇博客的标题，为什么博主会 毁人不倦 地尝试不同的重试方案，因为每一种方案都有它自身的局限性，博主想要的是一种更优雅的方案。具体来讲，主要有：基于 gRPC RetryPolicy、基于 HttpClientFactory、基于 gRPC 拦截器 以及 基于 CallInvoker 4 种方案。如果大家还有更好的思路，欢迎大家在博客评论区积极留言、参与讨论。\n基于 gRPC RetryPolicy 所谓的 gRPC RetryPolicy，其实是指 官方 提供的暂时性故障处理方案，它允许我们在创建GrpcChannel的时候，去指定一个重试策略：\nvar defaultMethodConfig = new MethodConfig { Names = { MethodName.Default }, RetryPolicy = new RetryPolicy { MaxAttempts = 5, InitialBackoff = TimeSpan.FromSeconds(1), MaxBackoff = TimeSpan.FromSeconds(5), BackoffMultiplier = 1.5, RetryableStatusCodes = { StatusCode.Unavailable } } }; var channel = GrpcChannel.ForAddress(\u0026#34;https://localhost:5001\u0026#34;, new GrpcChannelOptions { ServiceConfig = new ServiceConfig { MethodConfigs = { defaultMethodConfig } } }); 在上面的代码中，MethodConfig可以为指定的方法配置一个重试策略，当传入的方法名为MethodName.Default时，它将应用于该通道下的所有 gRPC 方法。如你所见，在重试策略中我们可以指定重试次数、重试间隔等参数。这个方案本身没有太多心智上的负担，唯一的缺点是，它没有预留出可扩展的接口，以至于我们想要验证它到底有没有重试的时候，居然要通过Fiddler抓包这种方式，换句话讲，我们没有办法自定义整个重试行为，譬如你想在重试过程中记录日志，这种方案就会鸡肋起来，对使用者来说，这完全就是一个黑盒子。\n官方自带的 “黑盒子” 重试机制\r除此之外，官方还提供了一种成为 Hedging 重试策略作为备选方案。类似地，它通过 HedgingPolicy 属性来指定重试策略。对比 RetryPolicy，它可以同时发送单个 gRPC 请求的多个副本，并使用第一个成功的结果作为返回值，所以，一个显而易见的约束是，它要求这个 gRPC 方法是无副作用的、幂等的函数。其实，这是所有重试方案都应该考虑的一个问题，而不单单是 HedgingPolicy。由于这两种策略有着本质上的不同，请记住：RetryPolicy 不能与 HedgingPolicy 一起使用。\nvar defaultMethodConfig = new MethodConfig { Names = { MethodName.Default }, HedgingPolicy = new HedgingPolicy { MaxAttempts = 5, NonFatalStatusCodes = { StatusCode.Unavailable } } }; var channel = GrpcChannel.ForAddress(\u0026#34;https://localhost:5001\u0026#34;, new GrpcChannelOptions { ServiceConfig = new ServiceConfig { MethodConfigs = { defaultMethodConfig } } }); 世间的一切都是双刃剑， HedgingPolicy 同样打不破这铁笼一般的人间真实，虽然它可以一次发送多个gRPC请求，可毫无疑问的是，这是一种相当浪费的策略，因为不管有多少个请求，它始终都取第一个结果作为返回值，而剩余的结果都将会被直接抛弃。想想每一年的高考状元，大家是不是都只记住了第一名。也许，人生正是如此呢，程序世界固然是由 0 和 1 构成的虚幻世界，可何尝就不是真实世界的某种投影呢？这里请允许博主安利一部动漫《你好世界》，它用视觉化的方式表达了真实世界与程序世界的某种特殊联系。\n基于 HttpClientFactory 接下来，我们要介绍的是基于 HttpClentFactory 的重试方案。也许，大家会感到困惑，明明这篇博客说的是 gRPC ，为什么 HttpClientFactory 会出现在这里呢？其实，很多时候，我们看到的只有表面，而出奇制胜的招式往往出自你对于本质的理解。如果大家阅读过 gRPC 客户端部分的源代码，就会意识到这样一件事情，即，gRPC 底层依然用到了 HttpClient 这套所谓“管道式”的体系，你可以理解为，最终传输层还是要交给 HttpClient 来处理，而 HttpClientFactory 本来就支持结合 Polly 进行重试，所以，我们其实是针对同一个问题的不同阶段进行了切入处理。一旦想清楚这一点，下面的代码理解起来就没有难度啦：\nvar services = new ServiceCollection(); services.AddGrpcClient\u0026lt;Greeter.GreeterClient\u0026gt;(opt =\u0026gt; { opt.Address = new Uri(\u0026#34;https://localhost:8001\u0026#34;); }) .ConfigurePrimaryHttpMessageHandler(() =\u0026gt; new HttpClientHandler { ClientCertificateOptions = ClientCertificateOption.Manual, ServerCertificateCustomValidationCallback = (httpRequestMessage, cert, cetChain, policyErrors) =\u0026gt; true }) .AddPolicyHandler( HttpPolicyExtensions.HandleTransientHttpError() .OrResult(res =\u0026gt; res.StatusCode != System.Net.HttpStatusCode.OK) .WaitAndRetryAsync( 6, retryAttempt =\u0026gt; TimeSpan.FromSeconds(Math.Pow(2, retryAttempt)) + TimeSpan.FromMilliseconds(new Random().Next(0, 100)), (result, timeSpan, current, context)=\u0026gt; { Console.WriteLine($\u0026#34;StatusCode={result.Result?.StatusCode}\u0026#34;); Console.WriteLine($\u0026#34;Exception={result.Exception?.Message}\u0026#34;); Console.WriteLine($\u0026#34;正在进行第{current}次重试，间隔{timeSpan.TotalMilliseconds}秒\u0026#34;); } ) ); var serviceProvider = services.BuildServiceProvider(); await serviceProvider.GetService\u0026lt;Greeter.GreeterClient\u0026gt;().SayHelloAsync(new HelloRequest() { Name = \u0026#34;长安书小妆\u0026#34; }); 在这里，为了模拟网络不畅的这种场景，我们故意指定了一个错误的终结点信息。此时，我们会得到下面的结果：\n基于 HttpClientFactory 的重试方案\r不过话又说回来，因为我们选择切入的阶段是“传输层”，所以，相对于整个 RpcException 而言，我们其实是找到了一个问题的子集，这意味着这个方案并不能覆盖到所有的场景，如果是在非“传输层”引发了某种异常，我们就没有办法通过这种方式去做重试处理。所以，我在一开始就说过，没有 100% 完美的解决方案，每一种方案都有它自身的局限性，这句话在这里得到了第一次印证。如果大家再回过头去看第一种方案，是不是就会发现，它里面还是使用了HTTP状态码作为是否重试的判断依据。所以，大家觉得呢？欢迎大家在评论区留下你的想法。\n基于 gRPC 拦截器 关于 gRPC 的拦截器，博主专门写过一篇 博客 来介绍它，所以，在一开始考虑重试方案的时候，拦截器其实是最容易想到的一种方案，主要思路是利用 Polly 中Policy的Execute()方法，对拦截器中获取gRPC调用结果的过程进行包装，我们一起来看下面的例子：\npublic override AsyncUnaryCall\u0026lt;TResponse\u0026gt; AsyncUnaryCall\u0026lt;TRequest, TResponse\u0026gt;( TRequest request, ClientInterceptorContext\u0026lt;TRequest, TResponse\u0026gt; context, AsyncUnaryCallContinuation\u0026lt;TRequest, TResponse\u0026gt; continuation ) { var retryPolicy = Policy\u0026lt;AsyncUnaryCall\u0026lt;TResponse\u0026gt;\u0026gt; .Handle\u0026lt;RpcException\u0026gt;(s =\u0026gt; s.StatusCode == StatusCode.Internal) .Or\u0026lt;WebException\u0026gt;() .OrResult(r =\u0026gt; { var awaiter = r.GetAwaiter(); if (awaiter.IsCompleted) return r.GetStatus().StatusCode == StatusCode.OK; try { r.ResponseAsync.Wait(); } catch (AggregateException) { return true; } return false; }) .WaitAndRetryAsync(3, x =\u0026gt; TimeSpan.FromSeconds(5), (result, timeSpan, current, context) =\u0026gt; { Console.WriteLine($\u0026#34;正在进行第{current}次重试...\u0026#34;); }); return retryPolicy.ExecuteAsync(() =\u0026gt; Task.FromResult(continuation(request, context))).Result; } } 基于 gRPC 拦截器的这种方案，它最大的问题在于异常的颗粒度太大，这句话是什么意思呢？简单来讲就是在拦截器这个层面上，你能捕捉到的只有RpcException，这样就使得我们难以捕获更小粒度的异常，譬如网络异常、超时异常等等。其次，gPRC 拦截器中大量使用了，类似AsyncUnaryCall\u0026lt;TResponse\u0026gt;这样的异步的返回值类型，这让我们在编写 Policy 的时候，多多少少会有一点不自在。综上所述，这个最容易想到的方案，本身是没有太大的问题的，最关键的问题是我们能接受什么样的异常颗粒度。而像异步返回值这种问题，只要写过一次以后，博主以为，它并不会成为我们继续探索的阻碍，这一点大家可以自己去体会。\n在尝试基于拦截器的重试方案的过程中，博主发现，指定一个错误的终结点信息，gRPC会在进入拦截器前就引发异常。这意味着这种基于拦截器的重试方案，在面对“传输层”的异常时略显乏力，所以，从某种程度上来讲，这个方案同样是一个不完美的方案。可这世上人来人往、本无完人，我们实在没有必要耽于技术方案的绝对完美而不可自拔，当求真、莫求执，所谓“大成若缺”，可以欣赏得来缺憾之美，同样是一种幸福。\n基于 CallInvoker 如果说，前面的 3 种方案都属于“见招拆招”的外家功夫。那么，接下来我要分享的思路，绝对可以称得上是“打通任督二脉”的玄门内功。\ngRPC客户端底层原理说明\r首先，博主想用一张图来讲解 gRPC 客户端的工作原理。从这张图中，我们可以看出，初始化一个gRPC的客户端，主要有GrpcChannel和CallInvoker两种构造形式，而GrpcChannel中的CreateCallInvoker()方法会返回HttpClientCallInvoker的一个实例。此时，我们就会发现，HttpClientCallInvoker是CallInvoker的一个子类。所以，我们基本可以判定CallInvoker是一个扮演着重要角色的类。继续探索，我们就会发现，GrpcCallInvokerFactory内部通过构造GrpcChannel，进而实现了CreateCallInvoker()方法，换句话说，本质上依然是调用了GrpcChannel中的CreateCallInvoker()方法。最终，这个CallInvoker实例会作为参数，传递给DefaultClientActivator的CreateClient()方法，至此我们就完成了整个gRPC客户端的创建工作。\n好了，相信现在大家都有一个疑问，这个CallInvoke到底是个什么东西呢？为什么它在整个gRPC的底层中是如此的重要呢？其实，它就是一个平平无奇的抽象类啦，可是一旦配合着gRPC中的Calls类来使用，这个CallInvoker简直就是扩展gRPC的一个重要的桥梁，因为我们不用关心底层是如何处理gRPC请求/响应的，而这丝毫不影响我们对这个过程进行自定义重写。因此，按照这样的思路，我们有了下面的实现：\nclass GrpcCallInvoker : CallInvoker { private readonly Channel _channel; private readonly GrpcPollyPolicyOptions _pollyOptions; public GrpcCallInvoker( Channel channel, GrpcPollyPolicyOptions pollyOptions ) { _channel = channel; _pollyOptions = pollyOptions; } public override AsyncClientStreamingCall\u0026lt;TRequest, TResponse\u0026gt; AsyncClientStreamingCall\u0026lt;TRequest, TResponse\u0026gt;( Method\u0026lt;TRequest, TResponse\u0026gt; method, string host, CallOptions options ) { var policy = CreatePollyPolicy\u0026lt;AsyncClientStreamingCall\u0026lt;TRequest, TResponse\u0026gt;\u0026gt;(); return policy.Execute(() =\u0026gt; Calls.AsyncClientStreamingCall(CreateCall(method, host, options))); } public override AsyncDuplexStreamingCall\u0026lt;TRequest, TResponse\u0026gt; AsyncDuplexStreamingCall\u0026lt;TRequest, TResponse\u0026gt;( Method\u0026lt;TRequest, TResponse\u0026gt; method, string host, CallOptions options ) { var policy = CreatePollyPolicy\u0026lt;AsyncDuplexStreamingCall\u0026lt;TRequest, TResponse\u0026gt;\u0026gt;(); return policy.Execute(() =\u0026gt; Calls.AsyncDuplexStreamingCall(CreateCall(method, host, options))); } public override AsyncServerStreamingCall\u0026lt;TResponse\u0026gt; AsyncServerStreamingCall\u0026lt;TRequest, TResponse\u0026gt;( Method\u0026lt;TRequest, TResponse\u0026gt; method, string host, CallOptions options, TRequest request ) { var policy = CreatePollyPolicy\u0026lt;AsyncServerStreamingCall\u0026lt;TResponse\u0026gt;\u0026gt;(); return policy.Execute(() =\u0026gt; Calls.AsyncServerStreamingCall(CreateCall(method, host, options), request)); } public override AsyncUnaryCall\u0026lt;TResponse\u0026gt; AsyncUnaryCall\u0026lt;TRequest, TResponse\u0026gt;( Method\u0026lt;TRequest, TResponse\u0026gt; method, string host, CallOptions options, TRequest request ) { var policy = CreatePollyPolicy\u0026lt;AsyncUnaryCall\u0026lt;TResponse\u0026gt;\u0026gt;(); return policy.Execute(() =\u0026gt; Calls.AsyncUnaryCall(CreateCall(method, host, options), request)); } public override TResponse BlockingUnaryCall\u0026lt;TRequest, TResponse\u0026gt;( Method\u0026lt;TRequest, TResponse\u0026gt; method, string host, CallOptions options, TRequest request ) { var policy = CreatePollyPolicy\u0026lt;TResponse\u0026gt;(); return policy.Execute(() =\u0026gt; Calls.BlockingUnaryCall(CreateCall(method, host, options), request)); } } 我想，经过连续三篇文章的洗礼，大家对这些方法应该都不陌生了吧！下面我们来着重讲解下CreateCall()和CreatePollyPolicy()这两个方法。其中，CreateCall()这个方法会相对简单一点，因为它完全就是返回gRPC的内置类型CallInvocationDetails。\nprotected CallInvocationDetails\u0026lt;TRequest, TResponse\u0026gt; CreateCall\u0026lt;TRequest, TResponse\u0026gt;( Method\u0026lt;TRequest, TResponse\u0026gt; method, string host, CallOptions options ) where TRequest : class where TResponse : class { return new CallInvocationDetails\u0026lt;TRequest, TResponse\u0026gt;(_channel, method, options); } 接下来，CreatePollyPolicy()这个方法就非常的明确啦，通过注入的GrpcPollyPolicyOptions来构造一个Policy。考虑到我们要做的是一个通用的方案，这里预留了断路器、重试、超时三种不同策略的参数。如果希望对构建 Policy 的过程进行自定义，则可以通过重写该方法来实现：\npublic virtual Policy\u0026lt;TResult\u0026gt; CreatePollyPolicy\u0026lt;TResult\u0026gt;() { Policy\u0026lt;TResult\u0026gt; policy = null; ; // 构造断路器策略 if (_pollyOptions.CircuitBreakerCount \u0026gt; 0) { var policyBreaker = Policy\u0026lt;TResult\u0026gt; .Handle\u0026lt;Exception\u0026gt;() .CircuitBreaker(_pollyOptions.CircuitBreakerCount, _pollyOptions.CircuitBreakerTime); policy = policy == null ? policyBreaker : policy.Wrap(policyBreaker) as Policy\u0026lt;TResult\u0026gt;; // 断路器降级 var policyFallBack = Policy\u0026lt;TResult\u0026gt; .Handle\u0026lt;Polly.CircuitBreaker.BrokenCircuitException\u0026gt;() .Fallback(() =\u0026gt; { return default(TResult); }); policy = policyFallBack.Wrap(policy); } // 构造超时策略 if (_pollyOptions.Timeout \u0026gt; TimeSpan.Zero) { var policyTimeout = Policy.Timeout(() =\u0026gt; _pollyOptions.Timeout, Polly.Timeout.TimeoutStrategy.Pessimistic); policy = policy == null ? (Policy\u0026lt;TResult\u0026gt;)policyTimeout.AsPolicy\u0026lt;TResult\u0026gt;() : policy.Wrap(policyTimeout); // 超时降级 var policyFallBack = Policy\u0026lt;TResult\u0026gt; .Handle\u0026lt;Polly.Timeout.TimeoutRejectedException\u0026gt;() .Fallback(() =\u0026gt; { return default(TResult); }); policy = policyFallBack.Wrap(policy); } // 构造重试策略 if (_pollyOptions.RetryCount \u0026gt; 0) { var retryPolicy = Policy\u0026lt;TResult\u0026gt;.Handle\u0026lt;Exception\u0026gt;().WaitAndRetry( _pollyOptions.RetryCount, x =\u0026gt; _pollyOptions.RetryInterval, (result, timeSpan, current, context) =\u0026gt; { Console.WriteLine($\u0026#34;正在进行第{current}次重试，间隔{timeSpan.TotalSeconds}秒\u0026#34;); }); policy = policy == null ? retryPolicy : policy.Wrap(retryPolicy) as Policy\u0026lt;TResult\u0026gt;; } return policy; } 因为我们无法修改DefaultGrpcClientFactory中关于CallInvoker这部分的逻辑，所以，我们采取了下面的“迂回战术”：\nservices.AddGrpc(); services.AddTransient\u0026lt;GrpcCallInvoker\u0026gt;(); services.AddTransient\u0026lt;Channel\u0026gt;(sp =\u0026gt; new Channel(\u0026#34;localhost\u0026#34;, 5001, ChannelCredentials.Insecure)); services.AddTransient\u0026lt;GrpcPollyPolicyOptions\u0026gt;(sp =\u0026gt; { return new GrpcPollyPolicyOptions() { RetryCount = 10, RetryInterval = TimeSpan.FromSeconds(1), CircuitBreakerCount = 5, CircuitBreakerTime = TimeSpan.FromSeconds(6), Timeout = TimeSpan.FromSeconds(10) }; }); var callInvoker = services.BuildServiceProvider().GetService\u0026lt;GrpcCallInvoker\u0026gt;(); var client = (Greeter.GreeterClient)Activator.CreateInstance(typeof(Greeter.GreeterClient), callInvoker); client.SayHello(new HelloRequest() { Name = \u0026#34;长安书小妆\u0026#34; }); 此时，如果我们故意写一个错误的终结点地址，我们将会得到下面的结果：\n基于 CallInvoker 的重试方案\r因为重试 5 次后就会启动断路器，所以，这个接口在重试 5 次后就立即停止了调用，这证明我们设想的这个方案是可以完美工作的！\n本文小结 写完以后，突然发现这一篇的信息量有点爆炸，尤其是CallInvoker这一部分，需要花点时间去阅读 gRPC 的源代码。可对于博主而言，其实更加享受的是，探索 gRPC 重试方案的这个过程。起初，因为对拦截器更熟悉一点，所以，我最先想到的是基于拦截器的重试方案。经过博主一番验证以后，发现这是一个有缺陷的方案。这时候，我意外发现，官方提供了重试策略，可这个重试策略对于使用者来说是一个黑盒子。再后来，发现可以在 HttpClient 上做一点文章，虽然它针对的是“传输层”这个阶段。直到从网上查资料，意识到可以重写CallInvoker这个抽象类，这个时候终于找到了最完美的方案。所以，通过这个过程，大家可以发现，我这篇博客的写作过程，其实与我思考过程有着明显的不同。思考的过程中带入“先入为主”的意识，这让我的思考过程走了不少的弯路，而写作过程则是一个由浅入深、由表及里的顺序。也许，下一次遇到类似的问题，我会先了解一下官方有没有提供标准方案，这是我在写完这篇博客以后最大的一个感悟。好了，这篇博客就先写到这里啦，如果大家对文中的内容由意见或者建议，欢迎大家在评论区给我留言，谢谢大家！\n","date":"2021-06-07T15:19:11Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2742255459/","slug":"2742255459","tags":["gRPC","Polly","重试",".NET"],"title":"ASP.NET Core gRPC 集成 Polly 实现优雅重试"},{"categories":["编程语言"],"content":"各位朋友，大家好，欢迎大家关注我的博客。在上一篇 博客 中，博主和大家分享了gRPC的拦截器在日志记录方面的简单应用，今天我们继续来探索gRPC在构建微服务架构方面的可能性。其实，从博主个人的理解而言，不管我们的微服务架构是采用RPC方式还是采用RESTful方式，我们最终要面对的问题本质上都是一样的，博主这里将其归纳为：服务划分、服务编写 和 服务治理。首先，服务划分决定了每一个服务的上下文边界以及服务颗粒度大小，如果按照领域驱动设计(DDD)的思想来描述微服务，我认为它更接近于限界上下文(BoundedContext)的概念。其次，服务编写决定了每一个服务的具体实现方式，譬如是采用无状态的RESTful风格的API，还是采用强类型的、基于代理的RPC风格的API。最后，服务治理是微服务架构中永远避不开的话题，服务注册、服务发现、健康检查、日志监控等等一切的话题，其实都是在围绕着服务治理而展开，尤其是当我们编写了一个又一个的服务以后，此时该如何管理这些浩如“星”海的服务呢？所以，在今天这篇博客中，博主想和大家一起探索下gRPC的健康检查，希望能给大家带来一点启发。\n健康检查-服务注册-服务发现示意图\r关于“健康检查”，大家都知道的一点是，它起到一种“防微杜渐”的作用。不知道大家还记不记得，语文课本里的经典故事《扁鹊见蔡桓公》，扁鹊一直在告知蔡桓公其病情如何，而蔡桓公讳疾忌医，直至病入骨髓、不治而亡。其实，对应到我们的领域知识，后端依赖的各种服务譬如数据库、消息队列、Redis、API 等等，都需要这样一个“扁鹊”来实时地“望闻问切”，当发现问题的时候及时地采取相应措施，不要像“蔡桓公”一样病入骨髓，等到整个系统都瘫痪了，这时候火急火燎地去“救火”，难免会和蔡桓公一样，发出“悔之晚矣”的喟叹。当我们决定使用gRPC来构建微服务架构的时候，我们如何确保这些服务一直是可用的呢？所以，提供一种针对gRPC服务的健康检查方案就会显得非常迫切。这里，博主主要为大家介绍两种实现方式，它们分别是：基于IHostedService的实现方式 以及 基于Consul的实现方式。\n基于 IHostedService 的实现方式 第一种方式，主要是利用IHostedService可以在程序后台执行的特点，搭配Timer就可以实现定时轮询。在 gRPC 的 官方规范 中，提供了一份Protocol Buffers的声明文件，它规定了一个健康检查服务必须实现Check()和Watch()两个方法。既然是官方定义好的规范，建议大家不要修改这份声明文件，我们直接沿用即可：\nsyntax = \u0026#34;proto3\u0026#34;; package grpc.health.v1; message HealthCheckRequest { string service = 1; } message HealthCheckResponse { enum ServingStatus { UNKNOWN = 0; SERVING = 1; NOT_SERVING = 2; } ServingStatus status = 1; } service Health { rpc Check(HealthCheckRequest) returns (HealthCheckResponse); rpc Watch(HealthCheckRequest) returns (stream HealthCheckResponse); } 接下来，我们需要实现对应的HealthCheckService:\npublic class HealthCheckService : Health.HealthBase { public override Task\u0026lt;HealthCheckResponse\u0026gt; Check( HealthCheckRequest request, ServerCallContext context ) { // TODO: 在这里添加更多的细节 return Task.FromResult(new HealthCheckResponse() { Status = HealthCheckResponse.Types.ServingStatus.Serving }); } public override async Task Watch( HealthCheckRequest request, IServerStreamWriter\u0026lt;HealthCheckResponse\u0026gt; responseStream, ServerCallContext context ) { // TODO: 在这里添加更多的细节 await responseStream.WriteAsync(new HealthCheckResponse(){ Status = HealthCheckResponse.Types.ServingStatus.Serving }); } } 接下来，我们需要实现HostedHealthCheckService，它实现了IHostedService接口，并在其中调用HealthCheckService:\npublic class HostedHealthCheckService : IHostedService { private Timer _timer = null; private readonly ILogger\u0026lt;HostedHealthCheckService\u0026gt; _logger; public HostedHealthCheckService(ILogger\u0026lt;HostedHealthCheckService\u0026gt; logger) { _logger = logger; } public Task StartAsync(CancellationToken cancellationToken) { _logger.LogInformation($\u0026#34;{nameof(HostedHealthCheckService)} start running....\u0026#34;); _timer = new Timer(DoCheck, null, TimeSpan.Zero, TimeSpan.FromSeconds(5)); return Task.CompletedTask; } public Task StopAsync(CancellationToken cancellationToken) { _logger.LogInformation($\u0026#34;{nameof(HostedHealthCheckService)} stop running....\u0026#34;); _timer?.Change(Timeout.Infinite, 0); return Task.CompletedTask; } private void DoCheck(object state) { using var channel = GrpcChannel.ForAddress(\u0026#34;https://localhost:5001\u0026#34;); ; var client = new Health.HealthClient(channel); client.Check(new HealthCheckRequest() { Service = \u0026#34;https://localhost:5001\u0026#34; }); } } 接下来，是大家非常熟悉的依赖注入环节：\n// ConfigureServices public void ConfigureServices(IServiceCollection services) { services.AddGrpc(options =\u0026gt; options.Interceptors.Add\u0026lt;GrpcServerLoggingInterceptor\u0026gt;()); services.AddHostedService\u0026lt;HostedHealthCheckService\u0026gt;(); } // Configure public void Configure(IApplicationBuilder app, IWebHostEnvironment env) { app.UseEndpoints(endpoints =\u0026gt; { endpoints.MapGrpcService\u0026lt;HealthCheckService\u0026gt;(); }); } 如果大家对上一篇博客中的拦截器还有印象，对于下面的结果应该会感到非常亲切：\n基于 IHostedService 的 gRPC 健康检查\r除此以外，我们还可以直接安装第三方库：Grpc.HealthCheck。此时，我们需要继承HealthServiceImpl类并重写其中的Check()和Watch()方法:\npublic class HealthCheckService : HealthServiceImpl { public override Task\u0026lt;HealthCheckResponse\u0026gt; Check( HealthCheckRequest request, ServerCallContext context ) { // TODO: 在这里添加更多的细节 return Task.FromResult(new HealthCheckResponse() { Status = HealthCheckResponse.Types.ServingStatus.Serving }); } public override async Task Watch( HealthCheckRequest request, IServerStreamWriter\u0026lt;HealthCheckResponse\u0026gt; responseStream, ServerCallContext context ) { // TODO: 在这里添加更多的细节 await responseStream.WriteAsync(new HealthCheckResponse() { Status = HealthCheckResponse.Types.ServingStatus.Serving }); } } 接下来，我们只需要在HostedHealthCheckService调用它即可，这个非常简单。\n故，无需博主多言，相信屏幕前的你都能写得出来，如果写不出来，参考博主给出得实现即可(逃！\n基于 Consul 的实现方式 Consul 是一个由 HashiCorp 提供的产品，它提供了服务注册、服务发现、健康检查、键值存储等等的特性。这里，我们通过集成它的SDK来实现gRPC服务的服务注册、服务发现、健康检查，从某种程度上来讲，它无形中帮助我们实现了客户端的负载均衡，因为我们可以将每一个服务的终结点都注册到Consul中，而Consul的健康检查则可以定时移除那些不可用的服务。所以，客户端获得的终结点实际上都是可用的终结点。\n首先，我们需要安装第三方库：Consul。接下来，我们可需要通过Docker安装一下Consul:\ndocker pull consul docker run --name consul -d -p 8500:8500 consul 默认情况下，Consul的端口号为：8500，我们可以直接访问：http://localhost:8500：\nConsul 界面效果展示\r接下来，为了让Startup类看起来清爽一点，首先，我们先来写一点扩展方法：\n// 为指定的gRPC服务添加健康检查 public static void AddGrpcHealthCheck\u0026lt;TService\u0026gt;(this IServiceCollection services) { var configuration = services.BuildServiceProvider().GetService\u0026lt;IConfiguration\u0026gt;(); // 注册ConsulClient services.AddSingleton\u0026lt;IConsulClient, ConsulClient\u0026gt;(_ =\u0026gt; new ConsulClient(consulConfig =\u0026gt; { var baseUrl = configuration.GetValue\u0026lt;string\u0026gt;(\u0026#34;Consul:BaseUrl\u0026#34;); consulConfig.Address = new Uri(baseUrl); })); // 注册gRPC服务 RegisterConsul\u0026lt;TService\u0026gt;(services).Wait(); } 其中，RegisterConsul()方法负责告诉Consul，某个服务对应的 IP 和端口号分别是多少，采用什么样的方式进行健康检查。\n不过，由于Consul默认不支持gRPC的健康检查，所以，我们使用了更为常见的基于TCP方式的健康检查。你可以认为，只要服务器连接畅通，gRPC服务就是健康的。\n// 注册指定服务到Consul private static async Task RegisterConsul\u0026lt;TService\u0026gt;(IServiceCollection services) { var serverHost = GetLocalIP(); var serverPort = services.BuildServiceProvider().GetService\u0026lt;IConfiguration\u0026gt;().GetValue\u0026lt;int\u0026gt;(\u0026#34;gRPC:Port\u0026#34;); await RegisterConsul\u0026lt;TService\u0026gt;(services, serverHost, serverPort); } // 注册指定服务到Consul private static async Task RegisterConsul\u0026lt;TService\u0026gt;( IServiceCollection services, string serverHost, int serverPort ) { var client = services.BuildServiceProvider().GetService\u0026lt;IConsulClient\u0026gt;(); var registerID = $\u0026#34;{typeof(TService).Name}-{serverHost}:{serverPort}\u0026#34;; await client.Agent.ServiceDeregister(registerID); var result = await client.Agent.ServiceRegister(new AgentServiceRegistration() { ID = registerID, Name = typeof(TService).Name, Address = serverHost, Port = serverPort, Check = new AgentServiceCheck { TCP = $\u0026#34;{serverHost}:{serverPort}\u0026#34;, Status = HealthStatus.Passing, DeregisterCriticalServiceAfter = TimeSpan.FromSeconds(10), Interval = TimeSpan.FromSeconds(10), Timeout = TimeSpan.FromSeconds(5) }, Tags = new string[] { \u0026#34;gRpc\u0026#34; } }) ; } 对于Consul中的健康检查，更常用的是基于HTTP的健康检查，简单来说，就是我们提供一个接口，供Consul来调用，我们可以去设置请求的头(Header)、消息体(Body)、方法(Method)等等。所以，对于这里的实现，你还可以替换为更一般的实现，即提供一个 API 接口，然后在这个接口中调用gRPC的客户端。除此以外，如果你擅长写脚本，Consul同样支持脚本级别的健康检查。\n在这里，博主水平扩展(复制)了两套服务，它们分别被部署在5001和6001两个端口上，通过Consul能达到什么效果呢？我们一起来看一下：\n// ConfigureServices public void ConfigureServices(IServiceCollection services) { services.AddGrpc(options =\u0026gt; options.Interceptors.Add\u0026lt;GrpcServerLoggingInterceptor\u0026gt;()); services.AddGrpcHealthCheck\u0026lt;GreeterService\u0026gt;(); services.AddGrpcHealthCheck\u0026lt;CalculatorService\u0026gt;(); } // Configure public void Configure(IApplicationBuilder app, IWebHostEnvironment env) { app.UseEndpoints(endpoints =\u0026gt; { endpoints.MapGrpcService\u0026lt;GreeterService\u0026gt;(); endpoints.MapGrpcService\u0026lt;CalculatorService\u0026gt;(); }); } OK，此时，我们注意到Consul中有两个服务注册进去，它们分别是：GreeterService 和 CalculatorService：\ngRPC 服务成功注册到 Consul 中\r以其中一个CalculatorService为例，我们可以注意到，它的确注册了5001和6001两个实例：\nCalculatorService 的两个实例\r至此，我们就完成了基于Consul的健康检查，在这里，图中的绿色标记表示服务可用。\n关于 gRPC 的引申话题 其实，写到这里的时候，这篇博客就该接近尾声啦，因为对于 gRPC 健康检查的探索基本都已找到答案，可我还是想聊一聊关于 gRPC 的引申话题。理由特别简单，就是在我看来，接下来要讲的这点内容，完全撑不起一篇博客的篇幅，索性就在这篇博客里顺带一提。我打算分享两个话题，其一，是 gRPC 客户端的负载均衡；其二，是 gRPC 接口的测试工具。\ngRPC 客户端的负载均衡 截止到目前为止，结合Consul我们已经实现了服务注册和服务发现两个功能。通过调研我们可以发现，针对服务器端的gRPC的负载均衡，目前主要有Nginx和Envoy两种方案，这两种相方案对要更复杂一点，博主目前所在的公司，在gRPC的负载均衡上感觉是个空白，这算是博主想要研究gRPC的一个主要原因。而在这里，由于Consul里注册了所有gRPC服务的终结点信息，所以，我们更容易想到的，其实是客户端的负载均衡，具体怎么实现呢？我们一起看一下：\n// 从Consul中获取服务终结点信息 var consulClient = serviceProvider.GetService\u0026lt;IConsulClient\u0026gt;(); var serviceName = typeof(TGrpcClient).Name.Replace(\u0026#34;Client\u0026#34;, \u0026#34;Service\u0026#34;); var services = await consulClient.Health.Service(serviceName, string.Empty, true); var serviceUrls = services.Response.Select(s =\u0026gt; $\u0026#34;{s.Service.Address}:{s.Service.Port}\u0026#34;).ToList(); if (serviceUrls == null || !serviceUrls.Any()) throw new Exception($\u0026#34;Please make sure service {serviceName} is registered in consul\u0026#34;); // 构造Channel和Client var serviceUrl = serviceUrls[new Random().Next(0, serviceUrls.Count - 1)]; var channel = GrpcChannel.ForAddress($\u0026#34;https://{serviceUrl}\u0026#34;); var client = new var client = new Calculator.CalculatorClient(channel); await client.CalcAsync(new CalculatorRequest() { Num1 = 10, Op = \u0026#34;+\u0026#34;, Num2 = 12 }); 可以看出，基本思路就是从Consul里拿到对应服务的终结点信息，然后构造出GrpcChannel，再通过GrpcChannel构造出 Client 即可。\n不过，博主觉得这个过程有一点繁琐，我们有没有办法让这些细节隐藏起来呢？于是，我们有了下面的改进方案：\npublic static async Task\u0026lt;TGrpcClient\u0026gt; GetGrpcClientAsync\u0026lt;TGrpcClient\u0026gt;( this IServiceProvider serviceProvider ) { var consulClient = serviceProvider.GetService\u0026lt;IConsulClient\u0026gt;(); var serviceName = typeof(TGrpcClient).Name.Replace(\u0026#34;Client\u0026#34;, \u0026#34;Service\u0026#34;); var services = await consulClient.Health.Service(serviceName, string.Empty, true); var serviceUrls = services.Response.Select(s =\u0026gt; $\u0026#34;{s.Service.Address}:{s.Service.Port}\u0026#34;).ToList(); if (serviceUrls == null || !serviceUrls.Any()) throw new Exception($\u0026#34;Please make sure service {serviceName} is registered in consul\u0026#34;); var serviceUrl = serviceUrls[new Random().Next(0, serviceUrls.Count - 1)]; var channel = GrpcChannel.ForAddress($\u0026#34;https://{serviceUrl}\u0026#34;); var constructorInfo = typeof(TGrpcClient).GetConstructor(new Type[] { typeof(GrpcChannel) }); if (constructorInfo == null) throw new Exception($\u0026#34;Please make sure {typeof(TGrpcClient).Name} is a gRpc client\u0026#34;); var clientInstance = (TGrpcClient)constructorInfo.Invoke(new object[] { channel }); return clientInstance; } 现在，有没有觉得简单一点？完美！\nvar client = await serviceProvider.GetGrpcClientAsync\u0026lt;CalculatorClient\u0026gt;(); await client.CalcAsync(new CalculatorRequest() { Num1 = 1, Num2 = 2, Op = \u0026#34;+\u0026#34; }); gRPC 接口的测试工具 我猜，大多数看到这个标题会一脸鄙夷，心里大概会想，就测试工具这种东西值得特地写出来吗？诚然，以前写 API 接口的时候，大家都是用 Postman 或者 Apifox 这样的工具来进行测试的，可是突然有一天你要调试一个gRPC的接口，你总不能每次都调用客户端啊，所以，这里要给大家推荐两个gRPC接口的测试工具，它们分别是: grpcurl 和 grpcui，它们都出自同一个人 FullStory 之手，基于 Go 语言开发，简单介绍下使用方法：\n// 建议使用国内源 go env -w GO111MODULE=on go env -w GOPROXY=https://goproxy.cn,direct // grpcurl brew install grpcurl // grpcui go get github.com/fullstorydev/grpcui/... go install github.com/fullstorydev/grpcui/cmd/grpcui // 安装后的路径为：C:\\Users\\\u0026lt;User\u0026gt;\\go\\bin\\grpcui.exe grpcui -bind \u0026lt;Your-IP\u0026gt; -plaintext \u0026lt;Your-gRPC-Service\u0026gt; 虽然这个说明简单而直白，可我还是没能装好，我不得不祭出 Docker 这个神器，果然它不会令我失望：\ndocker pull wongnai/grpcui docker run -e GRPCUI_SERVER=localhost:5001 -p 8080:8080 wongnai/grpcui 这里有两个重要的参数，其中，8080是grpcui的服务地址，可以按个人喜好进行修改，GRPCUI_SERVER是gRPC服务地址，该工具运行效果如下：\ngRPCUI 接口测试工具\r对于使用者来说，我们只需要选择服务(service)、方法(rpc)、然后填入参数即可，个人感觉非常方便。\n本文小结 本文探索并实现了gRPC服务健康检查，主要提供了两种思路：基于IHostedService + Timer的轮询的方案 以及 基于Consul的集服务注册、服务发现、健康检查于一身的方案。特别地，对于后者而言，我们可以顺理成章地联想到客户端的负载均衡，其原理是：Consul中注册了所有gRPC服务的终结点信息，通过IConsulClient可以拿到所有可用的终结点信息，只要以此为基础来构建GrpcChannel即可。根据这个原理，我们引申出了gRPC客户端负载均衡的相关话题，这里我们采用的是随机选择一个终结点信息的做法，事实上，按照一般负载均衡的理论，我们还可以采取轮询、加权、Hash 等等的算法，大家可以按照自己的业务场景来选择合适的方法。最后，我们简单介绍了下gRPC接口测试方面的内容，它可以帮助我们更高效地编写、验证gRPC接口。好了，以上就是这篇博客的全部内容啦，欢迎大家在评论区留言、参与讨论，谢谢大家！\n","date":"2021-06-01T11:37:36Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1657075397/","slug":"1657075397","tags":["gRPC","微服务","健康检查","Consul"],"title":"ASP.NET Core gRPC 健康检查的探索与实现"},{"categories":["编程语言"],"content":"gRPC是微软在.NET Core 及其后续版本中主推的 RPC 框架，它使用 Google 的 Protocol Buffers 作为序列化协议，使用 HTTP/2 作为通信协议，具有跨语言、高性能、双向流式调用等优点。考虑到，接下来要参与的是，一个以gRPC为核心而构建的微服务项目。因此，博主准备调研一下gRPC的相关内容，而首当其冲的，则是从 .NET Core 3.1 开始就有的拦截器，它类似于ASP.NET Core中的过滤器和中间件，体现了一种面向切面编程(AOP)的思想，非常适合在 RPC 服务调用的时候做某种统一处理，譬如参数校验、身份验证、日志记录等等。在今天这篇博客中，博主主要和大家分享的是，利用 .NET Core gRPC 中的拦截器实现日志记录的简单技巧，希望能给大家带来一点启发。\n开源、多语言、高性能的 gRPC\r关于 Interceptor 类 Interceptor类是 gRPC 服务拦截器的基类，它本身是一个抽象类，其中定义了下面的虚方法：\npublic virtual AsyncClientStreamingCall\u0026lt;TRequest, TResponse\u0026gt; AsyncClientStreamingCall\u0026lt;TRequest, TResponse\u0026gt;(); public virtual AsyncDuplexStreamingCall\u0026lt;TRequest, TResponse\u0026gt; AsyncDuplexStreamingCall\u0026lt;TRequest, TResponse\u0026gt;(); public virtual AsyncUnaryCall\u0026lt;TResponse\u0026gt; AsyncUnaryCall\u0026lt;TRequest, TResponse\u0026gt;(); public virtual TResponse BlockingUnaryCall\u0026lt;TRequest, TResponse\u0026gt;(); public virtual Task\u0026lt;TResponse\u0026gt; ClientStreamingServerHandler\u0026lt;TRequest, TResponse\u0026gt;(); public virtual AsyncServerStreamingCall\u0026lt;TResponse\u0026gt; AsyncServerStreamingCall\u0026lt;TRequest, TResponse\u0026gt;(); public virtual Task DuplexStreamingServerHandler\u0026lt;TRequest, TResponse\u0026gt;(); public virtual Task ServerStreamingServerHandler\u0026lt;TRequest, TResponse\u0026gt;(); public virtual Task\u0026lt;TResponse\u0026gt; UnaryServerHandler\u0026lt;TRequest, TResponse\u0026gt;(); 整体而言，如果从通信方式上来划分，可以分为：流式调用 和 普通调用；而如果从使用方来划分，则可以分为：客户端 和 服务端。进一步讲的话，针对流式调用，它还分为：\u0026quot;单向流\u0026quot; 和 \u0026ldquo;双向流\u0026quot;。关于这些细节上的差异，大家可以通过 gRPC 的 官方文档 来了解，这里我们给出的是每一种方法对应的用途：\n方法名 描述 AsyncClientStreamingCall 拦截异步客户端流式调用 AsyncDuplexStreamingCall 拦截双向流式调用 AsyncUnaryCall 拦截异步普通调用 BlockingUnaryCall 拦截阻塞普通调用 AsyncServerStreamingCall 拦截异步服务端流式调用 ClientStreamingServerHandler 拦截客户端流式调用的服务端处理程序 DuplexStreamingServerHandler 拦截双向流式调用的服务端处理程序 ServerStreamingServerHandler 拦截服务端流式调用的服务端处理程序 UnaryServerHandler 拦截普通调用的服务端处理程序 实现一个拦截器 好了，下面我们一起实现一个拦截器。这里，我们使用的是微软官方的例子：\npublic class GreeterService : Greeter.GreeterBase { private readonly ILogger\u0026lt;GreeterService\u0026gt; _logger; public GreeterService(ILogger\u0026lt;GreeterService\u0026gt; logger) { _logger = logger; } public override Task\u0026lt;HelloReply\u0026gt; SayHello(HelloRequest request, ServerCallContext context) { return Task.FromResult(new HelloReply { Message = \u0026#34;Hello \u0026#34; + request.Name }); } } 服务器端 实现服务器端的普通调用拦截，我们需要重写的方法是UnaryServerHandler:\npublic class GRPCServerLoggingInterceptor : Interceptor { private readonly ILogger\u0026lt;GRPCServerLoggingInterceptor\u0026gt; _logger; public GRPCServerLoggingInterceptor(ILogger\u0026lt;GRPCServerLoggingInterceptor\u0026gt; logger) { _logger = logger; } // 重写 UnaryServerHandler() 方法 public override Task\u0026lt;TResponse\u0026gt; UnaryServerHandler\u0026lt;TRequest, TResponse\u0026gt;( TRequest request, ServerCallContext context, UnaryServerMethod\u0026lt;TRequest, TResponse\u0026gt; continuation ) { var builder = new StringBuilder(); // Call gRPC begin builder.AppendLine($\u0026#34;Call gRPC {context.Host}/{context.Method} begin.\u0026#34;); // Logging Request builder.AppendLine(LogRequest(request)); // Logging Response var reply = continuation(request, context); var response = reply.Result; var exception = reply.Exception; builder.AppendLine(LogResponse(response, exception)); // Call gRPC finish builder.AppendLine($\u0026#34;Call gRPC {context.Host}/{context.Method} finish.\u0026#34;); _logger.LogInformation(builder.ToString()); return reply; } // 记录gRPC请求 private string LogRequest\u0026lt;TRequest\u0026gt;(TRequest request) { var payload = string.Empty; if (request is IMessage) payload = JsonConvert.SerializeObject( (request as IMessage) .Descriptor.Fields.InDeclarationOrder() .ToDictionary(x =\u0026gt; x.Name, x =\u0026gt; x.Accessor.GetValue(request as IMessage)) ); return $\u0026#34;Send request of {typeof(TRequest)}:{payload}\u0026#34;; } // 记录gRPC响应 private string LogResponse\u0026lt;TResponse\u0026gt;(TResponse response, AggregateException exception) { var payload = string.Empty; if (exception == null) { if (response is IMessage) payload = JsonConvert.SerializeObject( (response as IMessage) .Descriptor.Fields.InDeclarationOrder() .ToDictionary(x =\u0026gt; x.Name, x =\u0026gt; x.Accessor.GetValue(response as IMessage)) ); return $\u0026#34;Receive response of {typeof(TResponse)}:{payload}\u0026#34;; } else { var errorMsgs = string.Join(\u0026#34;;\u0026#34;, exception.InnerExceptions.Select(x =\u0026gt; x.Message)); return $\u0026#34;Receive response of {typeof(TResponse)} throws exceptions: {errorMsgs}\u0026#34;; } } } 对于gRPC而言，每一个由.proto声明文件生成的类，都带有一个叫做Descriptor的属性，我们可以利用这个属性获得gRPC请求和响应的详细信息。所以，在LogRequest()和LogResponse()两个方法中，我们均使用了这一思路来记录gRPC的报文信息，因为传输层的gRPC使用了二进制作为数据载体，这可以说是一种用可读性换取高效率的做法，不过幸运的是，我们在这里实现了这个小目标。\n接下来，为了让这个拦截器真正生效，我们还需要修改一下Startup类中注册gRPC这部分的代码：\nservices.AddGrpc(options =\u0026gt; options.Interceptors.Add\u0026lt;GRPCServerLoggingInterceptor\u0026gt;()); 此时，我们可以得到下面的结果：\ngRPC服务器端拦截器效果展示\r客户端 实现客户端的普通调用拦截，我们需要重写的方法是AsyncUnaryCall()，依样画葫芦即可：\npublic class GRPCClientLoggingInterceptor : Interceptor { // 重写 AsyncUnaryCall() 方法 public override AsyncUnaryCall\u0026lt;TResponse\u0026gt; AsyncUnaryCall\u0026lt;TRequest, TResponse\u0026gt;( TRequest request, ClientInterceptorContext\u0026lt;TRequest, TResponse\u0026gt; context, AsyncUnaryCallContinuation\u0026lt;TRequest, TResponse\u0026gt; continuation ) { var builder = new StringBuilder(); // Call gRPC begin builder.AppendLine($\u0026#34;Call gRPC {context.Host}/{context.Method} begin.\u0026#34;); // Logging Request builder.AppendLine(LogRequest(request)); // Logging Response var reply = continuation(request, context); var response = reply.ResponseAsync.Result; var exception = reply.ResponseAsync.Exception; builder.AppendLine(LogResponse(response, exception)); // Call gRPC finish builder.AppendLine($\u0026#34;Call gRPC {context.Host}/{context.Method} finish.\u0026#34;); Console.WriteLine(builder.ToString()); return reply; } } 类似地，为了让拦截器在客户端生效，我们需要这样：\nusing Grpc.Core.Interceptors; var channel = GrpcChannel.ForAddress(\u0026#34;https://localhost:5001\u0026#34;); // 简化写法 channel.Intercept(new GRPCClientLoggingInterceptor()); // 完整写法 var invoker = channel.CreateCallInvoker().Intercept(new GRPCClientLoggingInterceptor()); var client = new Greeter.GreeterClient(invoker); await client.SayHelloAsync(new HelloRequest() { Name = \u0026#34;长安书小妆\u0026#34; }); 此时，我们可以得到下面的结果：\ngRPC客户端拦截器效果展示\r客户端感觉不太好的一点就是，这个Interceptor传入的必须是一个实例，考虑到拦截器内部可能会依赖类似ILogger等等的组件，建议还是通过IoC容器来取得一个拦截器的实例，然后再传入Intercept()方法中。博主所在的项目中，则是非常“土豪”地使用了PostSharp，直接走动态编织的方案，果然，“这次第，怎一个羡字了得”。当然，gRPC的客户端，其实提供了日志相关的支持，不过，我个人感觉这个有一点无力：\nvar loggerFactory = LoggerFactory.Create(logging =\u0026gt; { logging.AddConsole(); logging.SetMinimumLevel(LogLevel.Debug); }); var channel = GrpcChannel.ForAddress( \u0026#34;https://localhost:5001\u0026#34;, new GrpcChannelOptions { LoggerFactory = loggerFactory } ); 本文小结 本文主要分享了gRPC拦截器的使用技巧，gRPC支持一元调用(UnaryCall)、流式调用(StreamingCall)、阻塞调用(BlockingCall)，因为区分客户端和服务器端，所以，实际上会有各种各样的组合方式。gRPC的拦截器实际上就是选择对应的场景去重写相应的方法，其中，拦截器的基类为Interceptor类，这里我们都是以普通的一元调用为例的，大家可以结合各自的业务场景，去做进一步的调整和优化。这里，我们使用IMessage类的Descriptor属性来“反射”报文中定义的字段，这样就实现了针对gRPC服务请求/响应的日志记录功能。关于gRPC中日志和诊断的更进一步的话题，大家可以参考微软的 官方文档 。好了，以上就是这篇博客的全部内容啦，谢谢大家！\n","date":"2021-05-26T09:03:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1679688265/","slug":"1679688265","tags":[".NET","gRPC","AOP","日志"],"title":"ASP.NET Core gRPC 拦截器的使用技巧分享"},{"categories":["数据分析"],"content":"SnowNLP 是一个功能强大的中文文本处理库，它囊括了中文分词、词性标注、情感分析、文本分类、关键字/摘要提取、TF/IDF、文本相似度等诸多功能，像隐马尔科夫模型、朴素贝叶斯、TextRank等算法均在这个库中有对应的应用。如果大家仔细观察过博主的博客，就会发现博主使用了摘要提取这一功能来增强博客的SEO，即通过自然语言处理(NLP)技术，提取每一篇文章中的摘要信息。因为 SnowNLP 本身使用的语料是电商网站评论，所以，当我们面对不同的使用场景时，它自带的这个模型难免会出现“水土不服”。因此，如果我们希望得到更接近实际的结果，最好的方案是使用自定义语料进行模型训练。值得庆幸的是，这一切在 SnowNLP 中实施起来非常简单，并不需要我们去钻研那些高深莫测的算法。至此，就引出了今天这篇博客的主题，即 SnowNLP 使用自定义语料进行模型训练。\n不知道大家是否还有印象，博主曾经在 《通过 Python 分析 2020 年全年微博热搜数据》 这篇文章中提到过 SnowNLP 的模型训练。当时，博主采集了整个 2020 年的微博热搜话题，因为要体现整个一年里的情感变化，博主特意找了两份微博语料，并以此为基础训练出了一个模型文件。\n2020全年微博热搜情感变化趋势\r那么，具体是怎么样做的呢？我们一起来看一下：\nfrom snownlp import sentiment sentiment.train(\u0026#39;./train/neg60000.txt\u0026#39;, \u0026#39;./train/pos60000.txt\u0026#39;) sentiment.save(\u0026#39;weibo.marshal\u0026#39;) 千万不要怀疑你的眼睛，因为它真的只有短短的三行代码。简单来说，我们只需要准备一个“积极”的语料文件，一个“消极”的语料文件，它就可以训练出一个模型文件。特别注意的是，如果是在Python 3.X的版本下，最终生成的模型文件的扩展名将会是.3，下图是博主这里训练出的模型文件：\nSnowNLP 使用自定义语料进行模型训练\r好了，一旦训练出这个模型文件，我们就可以考虑替换掉 SnowNLP 的默认模型文件，我们可以在以下位置：\\Lib\\site-packages\\snownlp\\sentiment 找到下列文件。为了安全起见，我们首先将原来的模型文件重命名，然后再放入我们自己的模型文件。\nSnowNLP 使用自定义模型替换默认模型\r此时，我们就可以利用训练好的模型，分析某一条微博的情感倾向。这里我选取了几条我的微博，看看这个情感倾向预测的结果如何：\nfrom snownlp import SnowNLP s = SnowNLP(u\u0026#39;我爱你，并不期待回声\u0026#39;) s.sentiments # 0.8760737296091975 s = SnowNLP(u\u0026#39;想找一个人，一起做老爷爷、老奶奶才做的事情，比如，替我拔一拔头上的白头发……[二哈] ​​\u0026#39;) s.sentiments # 0.001629297651780881 s = SnowNLP(u\u0026#39;如果两个人都不爱了，一别两宽，各生欢喜，其实是挺好的结局；可如果还有一个人爱着，对那个人来说，爱又是什么呢？\u0026#39;) s.sentiments # 0.809651945221708 s = SnowNLP(u\u0026#39;为了发张自拍，特意出来跑步，还有谁？[doge] ​​​\u0026#39;) s.sentiments # 0.4041057894917053 有人说，双子座是一个白天自愈、晚上孤独的星座，我确信这是真的，因为从我出生的那一刻起，那种宏大宇宙中的孤独感就一直笼罩着我，用一句话来形容，大概就是“热闹是人家的，我什么都没有”，因为内心世界里的两个灵魂，从来没有一刻闲歇地在纠缠和撕裂。我一直都想了解一件事情，如果这些基于概率或者是公式的算法，都可以琢磨出人类某个时刻的心境，我们期望别人能懂自己是不是太过矫情，我们是真的了解自己吗？\nOK，说完微博话题这个场景，我们再来说说电影评论这个场景。回想今年过年的时候，一部《你好，李焕英》，成为贺岁档电影中的一匹黑马，而相比之下，《唐人街探案 3》则有点“滑铁卢”的感觉。为了搞清楚某一部电影真实的评价情况，此时，我们可以考虑使用 SnowNLP ，对影评的情感趋向进行打分。同样地，这里我们找了一部分影评语料，为 SnowNLP 训练一个单独的模型。接下来，我们不妨从豆瓣上抓取一定数量的影评，来验证下我们这里训练好的模型，这里以《唐人街探案 3》为例：\n从豆瓣上抓取到的电影评论\r可以发现，这些影评的情感趋向介于 0 到 0.1 这个区间的数量最多，占到 160 以上，这意味着约有 30%的观众认为这部电影是个不折不扣的烂片。\n唐人街探案3豆瓣影评情感分布\r目前，《唐人街探案 3》 在豆瓣上的评分只有 5.5 分，其中，2 星和 3 星的评价占到 70%以上。由于豆瓣接口的限制，我们大概只能抓到 500 条左右的影评信息，可即使如此，可以看出大家对这部电影的情绪多少有一点不满。博主当时看这个电影，最大的感受是里面充斥着太多强行搞笑的东西，例如开篇机场那一场打砸抢的戏份，我完全不明白它存在的意义是什么，虽然日本演员们的表演可圈可点，可在这样一个推理和叙事都非常脆弱的故事里，大概就剩下翻来覆去重复使用的搞笑伎俩啦，你敢说医院这场戏和第一部阿香家那场戏没有相似的地方吗？更不用说，医院这场戏大家都在评论里无限吐槽啦！\n豆瓣电影-唐人街探案3\r其实，对于情感，我一直不知道该怎么来讲，可能是程序员的这份理性，让我在维系亲密关系或者说的情感的时候，有时候会生出一种近乎漠然的、置身事外的错觉，换句话说，也许是那种被人称为“天性凉薄”的东西。前任同我讲，我最爱的人其实是我自己，并不是她。因为站在她的角度上来讲，她并没有感受到我给予她的爱。我该怎么回答这个问题呢？在一切看似理性的数学计算背后，人类这些极为在乎的情感到底又是什么形式？也许有一天，两个人的感情说变淡就突然变淡，不管我们曾经说过什么样的话，在那一刻都会变得苍白无力，逐年攀升的离婚率触目惊心，可我们每个人都像扑向火焰的飞蛾，在这爱与欲望无法随心所欲的世界里，被欲望裹挟着不断向前。人会变的绝情、冷漠，我们自以为那是成长，可那不过是心变硬了，可这是我们当初期待的长大吗？\n欢迎来到无法随心所欲的爱与欲望的世界\r今天，听到袁隆平爷爷去世的消息，除了不断地提醒我们这代人已然老去这个事实以外，也许最大的体会应该是，我们在这个世界上追求的名利、身份和爱，最终都会无可避免地走向消亡，就如同我们身上这具躯壳一样，而真正能流传下去、泽被后世地，永远都是思想、是文化、是技术、是精神。佛家云：人死身灭，大概我们都不得不去接受这个残酷的事实，所以，请放下那些爱而不得、求而不得的执念吧，你一辈子不管遇见多少人，在某一个时候也许就会荡然无存，爱会消失、身会毁灭，这一切都是宇宙间的自然法则，与其去纠结那些“薛定谔态”的事物，不如多为这个世界做一点有意义的事情，正如尼采的那句名言，“对待生命你不妨大胆冒险一点, 因为无论如何你都要失去它”，我也许并不真正懂得人类的情感，因为它在理性面前毫无意义，世间万物毫无例外地走向那个坍塌的奇点，这难道不是一种荒凉的美感吗？\n人的心情难道不是一个黑洞\r嘘，如果你读到这里，意外发现这是一篇水字数的博客，而这或许说明了一件事情，我的确是一个会懈怠、会疲倦的活生生的人。关于 SnowNLP 使用自定义语料进行模型训练的话题，这次我们就先写到这里，做数据挖掘的时候，有的人在乎的是最终的结果，而有的人享受的是整个过程，人类的情感或许是相似的，所以，学着去接受这个多样性有点多到奇葩的世界，学着去和平凡而普通的自己和解吧，欢迎大家在评论区交换想法或者观点，谢谢大家！\n","date":"2021-05-19T21:22:41Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1772340994/","slug":"1772340994","tags":["NLP","训练","模型","情感"],"title":"SnowNLP 使用自定义语料进行模型训练"},{"categories":["生活感悟"],"content":" 我一直在想，世事无常，该是一种什么样的感觉。直到我读到夏目先生的《我是猫》，先生在书中不无感慨地写道，“世事变迁就像猫的眼珠一样变幻莫测”。可此时此刻，我会不由得觉得，世事无常，更像是时间突然间有了温度，“春观夜樱，夏望繁星，秋赏满月，冬会初雪”，拥有这般温度的时间毫无疑问是浪漫的，可世事无常所带来的时间的温度，更像是某种意义上的极致，譬如从地球两极瞬移到赤道，或者是一场大爆炸后突兀着的宁静。也许，四季还是那个四季，无非是我一厢情愿的自以为是，时间真的有温度吗？\n这次终于可以乘坐高铁回家，可当列车以每小时 250 公里的速度呼啸而过时，我已来不及仔细留意车窗外的风景。我隐隐约约地觉得，外面的山丘变得平缓，时不时穿过漆黑悠长的隧道，平原上点缀着麦田和葡萄架，等到列车横跨着黄河驶过的时候，我终于确信我回到了故乡。而我不得不说，人生的境遇里实在有太多的似曾相识，正如此刻窗外的风，兀自呼啸着撼动着那棵我自小便认识的树。回家后收到的第一个消息是，家族中一位叔叔的儿子，在工作时不慎从高处摔落下来，送到医院以后终于还是没能抢救过来，听长辈们讲，彼时他们正在参加某个人的婚礼，一时间百感交集。\n可以说，这是我这么多年来，第一次以一个成年人的身份去面对一个人的离开。因为逝者与我为同一辈人，所以于情于理我都要去吊唁一番。于是，快三十岁的人，第一次有了买花圈、写挽联的经历，甚至我在去见这位叔叔的时候，在脑海中浮现了多次的“还请您节哀顺变”，终于还是没能说出口来。或许是因为事出突然，有太多的身后事需要料理，留给悲伤的时间并不多。在逝者面前焚香、叩拜、鞠躬，虽然有长辈从旁指点，可整套动作还是显得有点僵硬。我终于还是想起来，这个只有 27 岁的年轻人，在我某次回家探亲的时候，自顾自走上前来，面带微笑的自我介绍道，“我是某某某，你不认得我了吗？”\n有时候想想，我喜欢怀旧，喜欢念念不忘，或许就是因为我怕，怕生命中每一次告别都是永诀。同样可以认为是第一次的，也许是公墓，是陵园，这种从前只有在电视上见到过的东西。于是，在夕阳的映照下，半边天空被染成金黄色，而在这一片荒凉中，一座六角形的塔静静地矗立着。站在一个高坡上一眼望去，满眼都是密密麻麻的墓碑。我在想，有一天人们会不会建成更加极致的地下宫殿，就如同城市中越来越多的高楼大厦一样，唯一的不同，或许是那具比单人床还要小一点的棺木，或者是和小酒坛差不多大的骨灰盒。独自站在旷野中，风吹着塔角的铃铛不时发出响声，我敲击不锈钢柱子时，它竟然发出了沉钟一般的轰鸣，难道人真的有灵魂吗？\n对于死亡，从小到大，我着实经历了不少，小学时爷爷去世，中学时有位同学被歹人杀害，大学时有位同学患白血病不治而亡，工作以后有一位同事因意外而溺水身亡……有时候想想，虽然我的人生，可能并不如别人那般精彩绝伦，可比起失去生命的他们，我能见到更多的人，见到更多的事情，这实在是幸运中的幸运。可或许是因为故事的视角发生了改变，所以，此刻比往常有了更多不由分说的感慨，就好像从前的我，虽然一样是某个事件的亲历者，但那时的我，还不大懂得死亡的意义，都说是人死灯灭，可只有你自己知道，一旦别人彻底地忘记了你，忘记了你在这世上的故事，你就大概的确真的死了罢！我们终其一生，不论记忆以文字还是影像的形式存在，所求者不过是记住别人和被别人记住，人生如朝露也好，如雪泥鸿爪也罢，也许，珍惜此时此刻，方能无惧参商永隔的痛苦吧……\n很多年前，作为长孙的我，举着高过我头顶的引魂幡走在前面，风裹挟着引魂幡的纸穗呼呼作响，那时，我还不知道再也见不到一个人，将会是多么难过的一件事情。后来，我偶尔会回想起，夏天做完农活回来，坐在凉席上吃西瓜的情形，就是在那个时候，爷爷开始埋怨头上有白头发，而我则被拉去帮爷爷找白头发。再后来，我偶尔会想有个人帮我找白头发，可明明我还没到三十岁啊，直到我看到三叔后脑勺开始变白，我终于惊觉，这是二十年前的事情了。有时候想想，我人生中最美好的那几年，同这二十年的长度相比，何尝不是沧海一粟呢？人生时常如此，你觉得几十年特别漫长，可二十年你还不是就这样“弹指一挥间”，而人生又特别短暂，短暂到我们怕这次见了就再见不着彼此。这样想来，拉黑或者删除一个人，成本简直低廉到无法想象，因为失去得太容易，大家就不会有这种看似突兀的想法。浮生倥偬，失散在风里的是沙，而失散在水里的是萍，失散的人们，会有引魂幡前来招魂，然后各自相认吗？\n所以，时间有温度吗？我想，该是有的，因为我们会在时间的长河里放下一盏浮灯，它承载着我们记忆深处最温暖的回忆。可也许这只是我们的一厢情愿，时间自顾自地往前走，从来不在乎人的记忆到底如何，就如同窗外呼啸而过的风，它并不懂得人类内心深处的那些情感，所以，更多的时候，我以为，时间是没有温度的，是冰冷的，是荒凉的，就像我在陵园里看到的夕阳一般冰冷，即使它被晚霞映得金黄。有时候，我会期待时间走得稍微慢一点，出于我的自私，我希望我此刻爱着的、曾经爱过的人们，都能老去地稍微慢一点，因为我怕再见不到那个人，因为我怕时间凝固成冰，因为我怕我终有一天要忘记，因为我怕我永远都赶不上时间，这或许是我想在此时此刻赋予时间的温度，如同人的正常体温 37 度，或许，它是如此的平静甚至是普通，可是啊，活着真的很好啊。\n","date":"2021-05-03T14:00:41Z","image":"/posts/2136925853/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/2136925853/","slug":"2136925853","tags":["时间","生死","随笔","感悟"],"title":"假如时间有温度"},{"categories":["编程语言"],"content":"最近，博主偶然间在 博客园 看到一篇文章：ASP.NET Core 扩展库之 Http 请求模拟，它里面介绍了一种利用 HttpMessageHandler 来实现 Http 请求模拟的方案。在日常工作中，我们总是不可避免地要和第三方的服务或者接口打交道，尤其是当我们需要面对“联调”这样一件事情的时候。通常，我们可以通过类似 YAPI 这样的工具来对尚在开发中的接口进行模拟。可是，因为这种方式会让我们的测试代码依赖于一个外部工具，所以，从严格意义上讲，它其实应该属于“集成测试”的范畴。在接触前端开发的过程中，对于其中的 Mock.js 印象深刻。故而，当看到 .NET 中有类似实现的时候，好奇心驱使我对其中的核心，即 HttpMessageHandler 产生了浓厚的兴趣。平时，我们更多的是使用 Moq 这样的库来模拟某一个对象的行为，而对一个 Http 请求进行模拟，可以说是开天辟地头一遭。带着这些问题出发，就有了今天这篇博客，通过 HttpMessageHandler 实现 HttpClient 请求管道的自定义。\n什么是 HttpMessageHandler？ 相信大家读过我提到的文章以后，都能找到这里面最核心的一个点：HttpMessageHandler。于是，我们今天要面对的第一个问题就是，什么是 HttpMessageHandler？此时，我们需要一张历久弥新的示意图，来自 微软官方。这里，我们重点关注的是 DelegatingHandler，它继承自 HttpMessageHandler。通过这张图，我们能够获得哪些信息呢？\n我认为，主要有以下几点：第一，HttpMessageHandler 处于整个 Http 请求管道的第一梯队，每一个路由匹配的请求都会从这里“进入”和“离开”；第二，HttpMessageHandler 可以是全局配置或者针对某个特定的路由，只要这个路由被匹配到就会执行；第三，HttpMessageHandler 可以直接构造 Http 响应并且返回，跳过剩余的管道流程。不知道大家看到这里会想到什么？坦白讲，我联想到了.NET Core 中的中间件，而唯一不同的地方或许是，中间件是 ASP.NET Core 里的概念，这里则是 ASP.NET Web API 里的概念。尤其是第三点，它对于我们的意义非常重大，因为它，我们才可以做到对一个 Http 请求进行模拟。\nHttpMessageHandler 与 ASP.NET Web API\r而事实上，在 ASP.NET Web API 的设计中，它是由一组 HttpMessageHandler 经过“首尾相连”而成，这种管道式的设计使得框架本身具有很高的扩展性。虽然，作为一个服务端框架，ASP.NET Web API 最主要的作用是就是“处理请求、响应回复”，可具体采用的处理策略会因具体场景的不同而不同。所以，管道式设计的本质，就是让某一个 Handler 只负责某个单一的消息处理功能，在根据具体场景的不同，选择需要的 Handler 并将其串联成一个完整的消息处理通道。而在这里，这个负责单一的消息处理功能的 Handler 其实就是 HttpMessageHandler，因为它不单单可以对请求消息(HttpRequestMessage)进行处理，同时还可以对响应消息(HttpResponseMessage)进行处理。此时，我们就不难理解 HttpMessageHandler 的定义：\npublic abstract class HttpMessageHandler : IDisposable { protected HttpMessageHandler(); public void Dispose(); protected virtual void Dispose(bool disposing); protected internal virtual HttpResponseMessage Send( HttpRequestMessage request, CancellationToken cancellationToken ); protected internal abstract Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ); } 也许，你会忍不住问这样一个问题：DelegatingHandler 和 HttpMessageHandler 的区别是什么？ 其实，只要你稍微仔细一点，你就会发现，两者最大的区别是 DelegatingHandler 里新增一个叫做 InnerHandler 的成员，它本身就是一个 HttpMessageHandler。所以，聪明的你又联想到什么呢？我想，或许是一个叫做 RequestDelegate 的委托，还记得我们写中间件是一直都少不了的 Next 吗？不得不说，这里越来越有中间件的味道了。你可以立马想到的一件事情是，除了最后一个 Handler 是 HttpMessageHandler 以外，剩下的前面的所有的 Handler 都是 DelegatingHandler。为什么这样说呢？因为前面的 n-1 个 Handler 都需要串联下一个 Handler，只有第 n 个 Handler可以允许短路，所以，大概就相当于 Use() 和 Run() 的区别？\npublic abstract class DelegatingHandler : HttpMessageHandler { protected DelegatingHandler(); protected DelegatingHandler(HttpMessageHandler innerHandler); // InnerHandler是实现管道式设计的关键 public HttpMessageHandler? InnerHandler { get; set; } protected override void Dispose(bool disposing); protected internal override HttpResponseMessage Send( HttpRequestMessage request, CancellationToken cancellationToken ); protected internal override Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ); } 所以，此时此刻，你能否为 HttpMessageHandler 下一个清晰的定义呢？我想，或许可以这样理解，一种可以对 请求消息(HttpRequestMessage) 和 响应消息(HttpResponseMessage) 进行处理，同时多个 HttpMessageHandler 可以组成一个完整的消息处理通道的中间件。屏幕前的你又是如何理解的呢？欢迎大家在评论区留言，留下你对于 HttpMessageHandler 的想法或者认识。\n实现自定义请求管道 好了，搞清楚 HttpMessageHandler 是什么以后，我们就可以考虑自定义请求管道的实现啦！让我们从一个最简单的示例开始，假设我们这里定义了两个自定义的 Handler，它们分别是： HandlerA 和 HandlerB，我们应该如何将其应用到具体的 HttpClient上呢？\n// Handler A public class HandlerA : DelegatingHandler { private readonly ILogger\u0026lt;HandlerA\u0026gt; _logger; public HandlerA(ILogger\u0026lt;HandlerA\u0026gt; logger) { _logger = logger; } protected override Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ) { _logger.LogInformation(\u0026#34;This is Handler A\u0026#34;); return base.SendAsync(request, cancellationToken); } } // Handler B public class HandlerB : DelegatingHandler { private readonly ILogger\u0026lt;HandlerB\u0026gt; _logger; public HandlerB(ILogger\u0026lt;HandlerB\u0026gt; logger) { _logger = logger; } protected override Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ) { _logger.LogInformation(\u0026#34;This is Handler B\u0026#34;); return base.SendAsync(request, cancellationToken); } } 这里，我们考虑两种场景，依赖注入 和 非依赖注入。对于依赖注入的场景，我们只需要调用AddHttpMessageHandler()方法按顺序注册即可，不需要处理InnerHandler，这里遵循先注册后使用的原则；对于非依赖注入的场景，需要处理InnerHandler，并在构造HttpClient的时候作为参数传入。\n// 依赖注入 var services = new ServiceCollection(); services.AddTransient\u0026lt;HandlerA\u0026gt;(); services.AddTransient\u0026lt;HandlerB\u0026gt;(); services.AddHttpClient(\u0026#34;MyClient\u0026#34;, options =\u0026gt; { options.BaseAddress = new Uri(\u0026#34;https://blog.yuanpei.me/\u0026#34;); }) .AddHttpMessageHandler\u0026lt;HandlerA\u0026gt;() .AddHttpMessageHandler\u0026lt;HandlerB\u0026gt;(); // 非依赖注入 var handler = new HandlerA() { InnerHandler = new HandlerB() }; var client = new HttpClient(handler) 此时，我们就可以得到下面的结果，可以注意到的是，两个Handler的执行顺序与注册顺序一致：\nHandler执行顺序与注册顺序\r好了，热身环节到此结束！下面，我们来开始实战，这里展示的是 HttpMessageHandler 在日志记录、请求重试 和 接口模拟等方面的应用。\n日志记录 对于 Http 请求的日志，我们希望记录请求的Url、Http动词、请求时长等信息，而这一点，在一个大量接入第三方接口的系统或者是以 Http 驱动的微服务架构中，常常是不可或缺的一环，对于我们排查故障、监控服务非常有用。\nprotected override async Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) { var correlationId = GetCorrelationId(request); using (_logger.BeginScope($\u0026#34;correlationId={correlationId}\u0026#34;)) { var sw = Stopwatch.StartNew(); _logger.LogInformation($\u0026#34;Start Processing HTTP Request {request.Method} {request.RequestUri} [Correlation: {correlationId}]\u0026#34;); var response = base.Send(request, cancellationToken); _logger.LogInformation($\u0026#34;End Processing HTTP Request in {sw.ElapsedMilliseconds}ms {response.StatusCode}, [Correlation: {correlationId}]\u0026#34;); return response; } } // GetCorrelationId private string GetCorrelationId(HttpRequestMessage request) { if (request.Headers.TryGetValues(\u0026#34;X-Correlation-ID\u0026#34;, out var values)) return values.First(); var correlationId = Guid.NewGuid().ToString(); request.Headers.Add(\u0026#34;X-Correlation-ID\u0026#34;, correlationId); return correlationId; } 此时，我们可以得到下面的结果：\nHttpMessageHandler 实现日志记录\r请求重试 我们知道，一个系统中接入的外部因素越多，则整个系统的稳定性越低。而国内的产品通常都喜欢\u0026quot;大而全\u0026quot;的\u0026quot;万物互联\u0026quot;，所以，最实际的问题，其实就是调用一个第三方的接口，如何保证其可靠性。所以，考虑请求的故障恢复就显得非常有意义，为此，我们可以引入Polly，在实现SendAsync()方法的时候，通过Polly中的超时、重试等机制对其做一层包装：\npublic class RetryableHttpMessageHandler : DelegatingHandler { private readonly ILogger\u0026lt;RetryableHttpMessageHandler\u0026gt; _logger; private readonly IAsyncPolicy\u0026lt;HttpResponseMessage\u0026gt; _retryPolicy; public RetryableHttpMessageHandler( ILogger\u0026lt;RetryableHttpMessageHandler\u0026gt; logger ) { _logger = logger; _retryPolicy = Policy\u0026lt;HttpResponseMessage\u0026gt; .Handle\u0026lt;HttpRequestException\u0026gt;() .Or\u0026lt;TimeoutException\u0026gt;() .OrResult(x =\u0026gt; (int)x.StatusCode \u0026gt;= 400) .RetryAsync(3, (ret, index) =\u0026gt; { _logger.LogInformation($\u0026#34;调用接口异常：{ret.Exception?.Message}，状态码：{ret.Result.StatusCode}, 正在进行第{index}次重试\u0026#34;); }); } protected override Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ) { return _retryPolicy.ExecuteAsync(() =\u0026gt; base.SendAsync(request, cancellationToken)); } } 同样地，我们这里通过HttpClient来请求指定的接口。因为，下面的接口实际上是不存在的。所以，理论上它会返回404这个状态码。而我们的重试策略是，在发生HttpRequestException或者TimeoutException异常以及 Http 响应的状态码大于 400 时，自动触发 3 次重试。\nvar client = _clientFactory.CreateClient(\u0026#34;ApiMock\u0026#34;); var response = await client.GetAsync(\u0026#34;/api/fail\u0026#34;); 此时，我们可以得到下面的结果：\nHttpMessageHandler 实现请求重试\r可以发现，不多不少刚好是 3 次。除了重试以外，Polly还支持类似超时、断路器等等不同的策略，甚至可以将它们组合起来使用，这些都属于Polly的内容，不作为本文的重点内容来讲解，感兴趣的朋友可以查阅这篇文章：.NET 开源项目 Polly 介绍。需要说明的是，微软官方提供的 Microsoft.Extensions.Http.Polly，它在IHttpClientBuilder上添加了一个名为AddPolicyHandler()的扩展方法，这里的例子可以被简化为下面这样，它和我们这里举的例子是完全一致的：\n// 定义重试策略 var retryPolicy = Policy\u0026lt;HttpResponseMessage\u0026gt; .Handle\u0026lt;HttpRequestException\u0026gt;() .Or\u0026lt;TimeoutException\u0026gt;() .OrResult(x =\u0026gt; (int)x.StatusCode \u0026gt;= 400) .RetryAsync(3, (ret, index) =\u0026gt; { Console.WriteLine($\u0026#34;调用接口异常：{ret.Exception?.Message}，状态码：{ret.Result.StatusCode}, 正在进行第{index}次重试\u0026#34;); }); // 注册HttpClient并指定重试策略 services.AddHttpClient(\u0026#34;ApiMock\u0026#34;, options =\u0026gt; { options.BaseAddress = new Uri(\u0026#34;https://blog.yuanpei.me\u0026#34;); }) .AddPolicyHandler(retryPolicy); 接口模拟 在集成第三方接口时，在双方确定好接口以后，接口消费方会有一段时间的“黒写”时期。因为在接口提供方的接口没有正式提供前，接口消费方始终只能通过“模拟”的方式来进行测试。考虑到单元测试对 YAPI 存在耦合，所以，接口模拟同样是一件意义非凡的事情。这里的思路是利用 HttpMessageHandler 的“短路”功能，即构造一个 HttpResponseMessage 并返回。\n首先，我们定义一个MockItem类型，它含有两个委托类型的属性RouteSelector和Executor。其中，前者用来匹配路由，而后者则用来处理接口返回值。\npublic class MockItem { public Func\u0026lt;HttpRequestMessage, bool\u0026gt; RouteSelector { get; set; } public Func\u0026lt;HttpRequestMessage, HttpResponseMessage, Task\u0026gt; Executor { get; set; } } 接下来，我们需要定义相应的Handler，这里是ApiMockHttpMessageHandler：\npublic class ApiMockHttpMessageHandler: DelegatingHandler { private readonly ILogger\u0026lt;ApiMockHttpMessageHandler\u0026gt; _logger; private readonly IEnumerable\u0026lt;MockItem\u0026gt; _routes; public ApiMockHttpMessageHandler( ILogger\u0026lt;ApiMockHttpMessageHandler\u0026gt; logger, IEnumerable\u0026lt;MockItem\u0026gt; routes) { _logger = logger; _routes = routes; } protected override async Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ) { // 匹配路由并调用其Executor属性 var route = _routes.FirstOrDefault(x =\u0026gt; x.RouteSelector?.Invoke(request)); if (route != null) { var response = new HttpResponseMessage(); await route.Executor?.Invoke(request, response); return response; } return base.Send(request, cancellationToken); } } 我们的思路是，对于所有注入到Ioc容器中的MockItem，检查其路由是否匹配，如果路由匹配，则通过其指定的Executor对HttpResponseMessage进行加工并返回。为了更加方便地在Ioc容器中进行注入，我们为IServiceCollection编写了相应的扩展方法：\npublic static IServiceCollection AddMock\u0026lt;TReturn\u0026gt;( this IServiceCollection services, string url, HttpMethod method, TReturn @return ) { var mockItem = new MockItem(); mockItem.Executor = BuildExecutor\u0026lt;TReturn\u0026gt;(@return); mockItem.RouteSelector = BuildRouteSelector(url, method); return services.AddTransient\u0026lt;MockItem\u0026gt;(sp =\u0026gt; mockItem); } public static IServiceCollection AddMock\u0026lt;TReturn\u0026gt;( this IServiceCollection services, Func\u0026lt;HttpRequestMessage, bool\u0026gt; routeSelector, Func\u0026lt;HttpRequestMessage, HttpResponseMessage, Task\u0026gt; executor ) { var mockItem = new MockItem(); mockItem.Executor = executor; mockItem.RouteSelector = routeSelector; return services.AddTransient\u0026lt;MockItem\u0026gt;(sp =\u0026gt; mockItem); } private static Func\u0026lt;HttpRequestMessage, bool\u0026gt; BuildRouteSelector( string url, HttpMethod method ) { Func\u0026lt;HttpRequestMessage, bool\u0026gt; selector = request =\u0026gt; { if (url == \u0026#34;*\u0026#34;) return true; return url.ToLower() == res.RequestUri.AbsolutePath.ToLower() \u0026amp;\u0026amp; method == res.Method; }; return selector; } private static Func\u0026lt;HttpRequestMessage, HttpResponseMessage, Task\u0026gt; BuildExecutor\u0026lt;TReturn\u0026gt;(TReturn @return) { Func\u0026lt;HttpRequestMessage, HttpResponseMessage, Task\u0026gt; executor = (request, response) =\u0026gt; { response.StatusCode = System.Net.HttpStatusCode.OK; if (@return is HttpStatusCode) response.StatusCode = (HttpStatusCode)Enum.Parse( typeof(HttpStatusCode), @return.ToString() ); else if (@return is Exception) throw @return as Exception; else if (@return is string) response.Content = new StringContent(@return as string); else response.Content = new StringContent(@return == null ? \u0026#34;\u0026#34; : JsonConvert.SerializeObject(@return) ); return Task.CompletedTask; }; return executor; } 此时，我们就可以在单元测试中对接口进行模拟，这样就实现了真正意义上的单元测试：\nvar services = new ServiceCollection(); // 添加 HttpClient并注册ApiMockHttpMessageHandler services.AddHttpClient(\u0026#34;ApiMock\u0026#34;, options =\u0026gt; { options.BaseAddress = new Uri(\u0026#34;https://blog.yuanpei.me\u0026#34;); }) .AddHttpMessageHandler\u0026lt;ApiMockHttpMessageHandler\u0026gt;(); // 添加3个模拟接口 services.AddMock(\u0026#34;/api/status\u0026#34;, HttpMethod.Get, HttpStatusCode.OK); services.AddMock(\u0026#34;/api/query\u0026#34;, HttpMethod.Post, new Exception(\u0026#34;帅哥你谁啊\u0026#34;)); services.AddMock(\u0026#34;/api/order\u0026#34;, HttpMethod.Get, new { OrderId = \u0026#34;OR09874\u0026#34;, CreatedBy = \u0026#34;张三\u0026#34; }); var serviceProvider = services.BuildServiceProvider(); var httpClientFactory = serviceProvider.GetRequiredService\u0026lt;IHttpClientFactory\u0026gt;(); var httpClient = httpClientFactory.CreateClient(\u0026#34;ApiMock\u0026#34;); // 调用/api/order接口 var response = await httpClient.GetAsync(\u0026#34;/api/order\u0026#34;); 下图是模拟接口返回的结果，与我们期望的完全一致：\nHttpMessageHandler 实现接口模拟\r本文小结 古人云：他山之石，可以攻玉。原本被接口模拟(Mock)所吸引的博主，意外地收获了 HttpMessageHandler 这个令人兴奋的知识点。博主认为，它是一种可以对 请求消息(HttpRequestMessage) 和 响应消息(HttpResponseMessage) 进行处理，同时多个 HttpMessageHandler 可以组成一个完整的消息处理通道的中间件。在此基础上，我们实现了诸如日志记录、请求重试、接口模拟等等的扩展性功能。除此以外，它还可以应用到 Http认证头处理 、客户端负载均衡等方面。\n其实，从 ASP.NET、OWIN、Nancy、ASP.NET Core 这样一路走过来，你会发现，管道的概念一直都存在，无非是以不同的形式存在着，譬如 ASP.NET Core 里的中间件，其实是替代了曾经的 HttpHandler 和 HttpModule，就像时间一直都在那里，不快不慢，觉得物是人非、喜新厌旧的多半还是我们。对我而言，写到这里，最大的感慨或许是，曾经试图实现的类似 Servlet 的 Http Server ，现在想起来还是太年轻、太朴实了，可年轻或者朴实，难道不好吗？好了，以上就是这篇博客的全部内容了，如果你觉得这篇博客对你有所帮助或者启发，希望你可以毫不吝啬地给个一键三连。如果你对这篇博客里的内容有意见或者建议，欢迎你评论区留下你的足迹和声音，谢谢大家！\n","date":"2021-04-28T20:25:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2070070822/","slug":"2070070822","tags":["HttpClient","Mock","管道","扩展"],"title":"使用 HttpMessageHandler 实现 HttpClient 请求管道自定义"},{"categories":["编程语言"],"content":"使用 ABP vNext 有一个月左右啦，这中间最大的一个收获是：ABP vNext 的开发效率真的是非常好，只要你愿意取遵循它模块化、DDD 的设计思想。因为官方默认实现了身份、审计、权限、定时任务等等的模块，所以，ABP vNext 是一个开箱即用的解决方案。通过脚手架创建的项目，基本具备了一个专业项目该有的“五脏六腑”，而这可以让我们专注于业务原型的探索。例如，博主是尝试结合 Ant Design Vue 来做一个通用的后台管理系统。话虽如此，我们在使用 ABP vNext 的过程中，还是希望可以针对性地对 ABP vNext 进行扩展，毕竟 ABP vNext 无法 100% 满足我们的使用要求。所以，在今天这篇博客中，我们就来说说 ABP vNext 中的扩展技巧，这里主要是指实体扩展和服务扩展这两个方面。我们经常在讲“开闭原则”，可扪心自问，我们每次修改代码的时候，是否真正做到了“对扩展开放，对修改关闭”呢？ 所以，在面对扩展这个话题时，我们不妨来一起看看 ABP vNext 中是如何实践“开闭原则”。\n扩展实体 首先，我们要说的是扩展实体，什么是实体呢？这其实是领域驱动设计(DDD)中的概念，相信对于实体、聚合根和值对象，大家早就耳熟能详了。在 ABP vNext 中，实体对应的类型为Entity，聚合根对应的类型为AggregateRoot。所以，你可以片面地认为，只要继承自Entity基类的类都是实体。通常，实体都会有一个唯一的标识(Id)，所以，订单、商品或者是用户，都属于实体的范畴。不过，按照业务边界上的不同，它们会在核心域、支撑域和通用域三者间频繁切换。而对于大多数系统而言，用户都将是一个通用的域。在 ABP vNext 中，其用户信息由AbpUsers表承载，它在架构上定义了IUser接口，借助于EF Core的表映射支持，我们所使用的AppUser本质上是映射到了AbpUsers表中。针对实体的扩展，在面向数据库编程的业务系统中，一个最典型的问题就是，我怎么样可以给AppUser添加字段。所以，下面我们以AppUser为例，来展示如何对实体进行扩展。\nDDD 中的实体、聚合根与值对象\r实际上，ABP vNext 中提供了2种方式，来解决实体扩展的问题，它们分别是：Extra Properties 和 基于 EF Core 的表映射。在 官方文档 中，我们会得到更加详细的信息，这里简单介绍一下就好：\nExtra Properties 对于第1种方式，它要求我们必须实现IHasExtraProperties接口，这样我们就可以使用GetProperty()和SetProperty()两个方法，其原理是，将这些扩展字段以JSON格式存储在ExtraProperties这个字段上。如果使用MongoDB这样的非关系型数据库，则这些扩展字段可以单独存储。参考示例如下：\n// 设置扩展字段 var user = await _identityUserRepository.GetAsync(userId); user.SetProperty(\u0026#34;Title\u0026#34;, \u0026#34;起风了，唯有努力生存\u0026#34;); await _identityUserRepository.UpdateAsync(user); // 读取扩展字段 var user = await _identityUserRepository.GetAsync(userId); return user.GetProperty\u0026lt;string\u0026gt;(\u0026#34;Title\u0026#34;); 可以想象得到，这种方式使用起来没有心智方面的困扰，主要问题是，这些扩展字段不利于关系型数据库的查询。其次，完全以字符串形式存在的键值对，难免存在数据类型的安全性问题。博主的上家公司，在面对这个问题时，采用的方案就是往数据库里加备用字段，从起初的5个，变成后来的10个，最后甚至变成20个，先不说这没完没了的加字段，代码中一直避不开的，其实是各种字符串的Parse/Convert，所以，大家可以自己去体会这其中的痛苦。\n基于 EF Core 的表映射 对于第2种方式，主要指 EF Core 里的“表拆分”或者“表共享”，譬如，当我们希望单独创建一个实体SysUser来替代默认的AppUser时，这就是表拆分，因为同一张表中的数据，实际上是被AppUser和SysUser共享啦，或者，你可以将其理解为，EF Core配置两个不同的实体时，它们的ToTable()方法都指向了同一张表。这里唯一不同的是，ABP vNext 中提供了一部分方法用来处理问题，因为牵扯到数据库，所以，还是需要“迁移”。下面，我们以给AppUser扩展两个自定义字段为例：\n首先，我们给AppUser类增加两个新属性，Avatar 和 Profile:\npublic class AppUser : FullAuditedAggregateRoot\u0026lt;Guid\u0026gt;, IUser { // ... public virtual string Profile { get; private set; } public virtual string Avatar { get; private set; } // ... 接下来，按照 EF Core 的“套路”，我们需要配置下这两个新加的字段：\nbuilder.Entity\u0026lt;AppUser\u0026gt;(b =\u0026gt; { // AbpUsers // Sharing the same table \u0026#34;AbpUsers\u0026#34; with the IdentityUser b.ToTable(AbpIdentityDbProperties.DbTablePrefix + \u0026#34;Users\u0026#34;); b.ConfigureByConvention(); b.ConfigureAbpUser(); // Profile b.Property(x =\u0026gt; x.Profile) .HasMaxLength(AppUserConsts.MaxProfileLength) .HasColumnName(\u0026#34;Profile\u0026#34;); // Avatar b.Property(x =\u0026gt; x.Avatar) .HasMaxLength(AppUserConsts.MaxAvatarLength) .HasColumnName(\u0026#34;Avatar\u0026#34;); }); 接下来，通过MapEfCoreProperty()方法，将新字段映射到IdentityUser实体，你可以理解为，AppUser和IdentityUser同时映射到了AbpUsers这张表：\n// Avatar ObjectExtensionManager.Instance.MapEfCoreProperty\u0026lt;IdentityUser, string\u0026gt;( nameof(AppUser.Avatar), (entityBuilder, propertyBuilder) =\u0026gt; { propertyBuilder.HasMaxLength(AppUserConsts.MaxAvatarLength); }); // Profile ObjectExtensionManager.Instance.MapEfCoreProperty\u0026lt;IdentityUser, string\u0026gt;( nameof(AppUser.Profile), (entityBuilder, propertyBuilder) =\u0026gt; { propertyBuilder.HasMaxLength(AppUserConsts.MaxProfileLength); }); 既然，连数据库实体都做了扩展，那么，数据传输对象(DTO)有什么理由拒绝呢？\nObjectExtensionManager.Instance .AddOrUpdateProperty\u0026lt;string\u0026gt;( new[] { typeof(IdentityUserDto), typeof(IdentityUserCreateDto), typeof(IdentityUserUpdateDto), typeof(ProfileDto), typeof(UpdateProfileDto), }, \u0026#34;Avatar\u0026#34; ) .AddOrUpdateProperty\u0026lt;string\u0026gt;( new[] { typeof(IdentityUserDto), typeof(IdentityUserCreateDto), typeof(IdentityUserUpdateDto), typeof(ProfileDto), typeof(UpdateProfileDto) }, \u0026#34;Profile\u0026#34; ); }); 经过这一系列的“套路”，此时，我们会发现，新的字段已经生效：\nABP vNext 实体扩展效果展示\r扩展服务 在 ABP vNext 中，我们还可以对服务进行扩展，得益于依赖注入的深入人心，我们可以非常容易地实现或者替换某一个接口，这里则指 ABP vNext 中的应用服务(ApplicationService)，例如，CrudAppService类可以帮助我们快速实现枯燥的增删改查，而我们唯一要做的，则是定义好实体的主键(Primary Key)、定义好实体的数据传输对象(DTO)。当我们发现 ABP vNext 中内置的模块或者服务，无法满足我们的使用要求时，我们就可以考虑对原有服务进行替换，或者是注入新的应用服务来扩展原有服务，这就是服务的扩展。在 ABP vNext 中，我们可以使用下面两种方法来对一个服务进行替换：\n// 通过[Dependency]和[ExposeServices]实现服务替换 [Dependency(ReplaceServices = true)] [ExposeServices(typeof(IIdentityUserAppService))] public class YourIdentityUserAppService : IIdentityUserAppService, ITransientDependency { //... } // 通过ReplaceService实现服务替换 context.Services.Replace( ServiceDescriptor.Transient\u0026lt;IIdentityUserAppService, YourIdentityUserAppService\u0026gt;() ); 这里，博主准备的一个示例是，默认的用户查询接口，其返回信息中只有用户相关的字段，我们希望在其中增加角色、组织单元等关联信息，此时。我们可以考虑实现下面的应用服务：\npublic interface IUserManageAppService { Task\u0026lt;PagedResultDto\u0026lt;UserDetailQueryDto\u0026gt;\u0026gt; GetUsersWithDetails( GetIdentityUsersInput input ); } 首先，我们定义了IUserManageAppService接口，它含有一个分页查询的方法GetUsersWithDetails()。接下来，我们来考虑如何实现这个接口。需要说明的是，在 ABP vNext 中，仓储模式的支持由通用仓储接口IRepository\u0026lt;TEntity, TKey\u0026gt;提供，ABP vNext 会在AddDefaultRepositories()方法中为每一个聚合根注入对应的仓储。同样地，你可以按照个人喜好为指定的实体注入对应的仓储。由于ABP vNext 同时支持 EF Core、Dapper 和 MongoDB，所以，我们还可以使用EfCoreRepository、DapperRepository 以及 MongoDbRepository，它们都是IRepository的具体实现类。在下面的例子中，我们使用的是EfCoreRepository这个类。\n事实上，这里注入的EfCoreIdentityUserRepository、EfCoreIdentityRoleRepository 以及 EfCoreOrganizationUnitRepository，都是EfCoreRepository的子类，这使得我们可以复用 ABP vNext 中关于身份标识的一切基础设施，来实现不同于官方的业务逻辑，而这就是我们所说的服务的扩展。\n[Authorize(IdentityPermissions.Users.Default)] public class UserManageAppService : ApplicationService, IUserManageAppService { private readonly IdentityUserManager _userManager; private readonly IOptions\u0026lt;IdentityOptions\u0026gt; _identityOptions; private readonly EfCoreIdentityUserRepository _userRepository; private readonly EfCoreIdentityRoleRepository _roleRepository; private readonly EfCoreOrganizationUnitRepository _orgRepository; public UserManageAppService( IdentityUserManager userManager, EfCoreIdentityRoleRepository roleRepository, EfCoreIdentityUserRepository userRepository, EfCoreOrganizationUnitRepository orgRepository, IOptions\u0026lt;IdentityOptions\u0026gt; identityOptions ) { _userManager = userManager; _orgRepository = orgRepository; _userRepository = userRepository; _roleRepository = roleRepository; _identityOptions = identityOptions; } [Authorize(IdentityPermissions.Users.Default)] public async Task\u0026lt;PagedResultDto\u0026lt;UserDetailQueryDto\u0026gt;\u0026gt; GetUsersWithDetails( GetIdentityUsersInput input ) { //Users var total = await _userRepository.GetCountAsync(input.Filter); var users = await _userRepository.GetListAsync( input.Sorting, input.MaxResultCount, input.SkipCount, input.Filter, includeDetails: true ); //Roles var roleIds = users .SelectMany(x =\u0026gt; x.Roles) .Select(x =\u0026gt; x.RoleId) .Distinct() .ToList(); var roles = await _roleRepository .WhereIf(roleIds.Any(), x =\u0026gt; roleIds.Contains(x.Id)) .ToListAsync(); //OrganizationUnits var orgIds = users .SelectMany(x =\u0026gt; x.OrganizationUnits) .Select(x =\u0026gt; x.OrganizationUnitId) .Distinct() .ToList(); var orgs = await _orgRepository .WhereIf(orgIds.Any(), x =\u0026gt; orgIds.Contains(x.Id)) .ToListAsync(); var items = ObjectMapper.Map\u0026lt;List\u0026lt;Volo.Abp.Identity.IdentityUser\u0026gt;, List\u0026lt;UserDetailQueryDto\u0026gt;\u0026gt;(users); foreach (var item in items) { foreach (var role in item.Roles) { var roleInfo = roles.FirstOrDefault(x =\u0026gt; x.Id == role.RoleId); if (roleInfo != null) ObjectMapper.Map(roleInfo, role); } foreach (var org in item.OrganizationUnits) { var orgInfo = orgs.FirstOrDefault(x =\u0026gt; x.Id == org.OrganizationUnitId); if (orgInfo != null) ObjectMapper.Map(orgInfo, org); } } return new PagedResultDto\u0026lt;UserDetailQueryDto\u0026gt;(total, items); } } 这里做一点补充说明，应用服务，即ApplicationService类，它集成了诸如ObjectMapper、LoggerFactory、GuidGenerator、国际化、AsyncExecuter等等的特性，继承该类可以让我们更加得心应手地编写代码。曾经，博主写过一篇关于“动态API”的博客，它可以为我们免去从 Service 到 Controller 的这一层封装，当时正是受到了ABP 框架的启发。当博主再次在 ABP vNext 中看到这个功能时，不免会感慨逝者如斯，而事实上，这个功能真的好用，真香！下面是经过改造以后的用户列表。考虑到，在上一篇博客里，博主已经同大家分享过分页查询方面的实现技巧，这里就不再展开讲啦！\n对“用户服务”进行扩展\r本文小结 我们时常说，\u0026quot;对修改关闭，对扩展开放\u0026quot;，\u0026quot;单一职责\u0026quot;，可惜这些原则最多就出现在面试环节。当你接触了真实的代码，你会发现\u0026quot;修改\u0026ldquo;永远比\u0026rdquo;扩展\u0026ldquo;多，博主曾经就见到过，一个简单的方法因为频繁地\u0026rdquo;打补丁\u0026quot;，最后变得面目全非。其实，有时候并不是维护代码的人，不愿意去\u0026quot;扩展\u0026quot;，而是写出可\u0026quot;扩展\u0026ldquo;的代码会更困难一点，尤其是当所有人都不愿意去思考，一味地追求短平快，这无疑只会加速代码的腐烂。在这一点上，ABP vNext 提供了一种优秀的范例，这篇文章主要分享了 ABP vNext 中实体和服务的扩展技巧，实体扩展解决了如何为数据库表添加扩展字段的问题，服务扩展解决了如何为默认服务扩展功能的问题，尤其是后者，依赖注入在其中扮演着无比重要的角色。果然，这世上的事情，只有你真正在乎的时候，你才会愿意去承认，那些你曾经轻视过的东西，也许，它们是对的吧！好了，以上就是这篇博客的全部内容，欢迎大家在评论区留言，喜欢的话请记得点赞、收藏、一键三连。\n","date":"2021-04-18T20:42:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3619320289/","slug":"3619320289","tags":["ABP","扩展","实体","服务"],"title":"ABP vNext 的实体与服务扩展技巧分享"},{"categories":["编程语言"],"content":"在 上一篇 博客中，博主和大家分享了如何在 EF Core 中实现多租户架构。在这一过程中，博主主要参考了 ABP vNext 这个框架。从上个月开始，我个人发起了一个项目，基于 ABP vNext 和 Ant Design Vue 来实现一个通用的后台管理系统，希望以此来推进 DDD 和 Vue 的学习，努力打通前端与后端的“任督二脉”。因此，接下来的这段时间内，我写作的主题将会围绕 ABP vNext 和 Ant Design Vue。而在今天的这篇博客中，我们来说说 ABP vNext 对接 Ant Design Vue 实现分页查询的问题，希望能让大家在面对类似问题时有所帮助。我不打算写一个系列教程，更多的是从我个人的关注点出发，如果大家有更多想要交流的话题，欢迎大家通过评论或者邮件来留言，谢谢大家！\nABP vNext中的分页查询 OK，当大家接触过 ABP vNext 以后，就会了解到这样一件事情，即，ABP vNext 中默认提供的分页查询接口，在大多数情况下，通常都会是下面这样的风格。这里以角色查询的接口为例，它对应的请求地址是：/api/identity/roles?SkipCount=0\u0026amp;MaxResultCount=10。此时，我们可以注意到，返回的数据结构中含有totalCount和items两个属性。其中，totalCount表示记录的总数目，items表示当前页对应的记录。\n{ \u0026#34;totalCount\u0026#34;: 2, \u0026#34;items\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Admin\u0026#34;, \u0026#34;isDefault\u0026#34;: false, \u0026#34;isStatic\u0026#34;: true, \u0026#34;isPublic\u0026#34;: true, \u0026#34;concurrencyStamp\u0026#34;: \u0026#34;cb53f2d7-159e-452d-9d9c-021629b500e0\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;39fb19e8-fb34-dfbd-3c70-181f604fd5ff\u0026#34;, \u0026#34;extraProperties\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;Manager\u0026#34;, \u0026#34;isDefault\u0026#34;: false, \u0026#34;isStatic\u0026#34;: false, \u0026#34;isPublic\u0026#34;: false, \u0026#34;concurrencyStamp\u0026#34;: \u0026#34;145ec550-7fe7-4c80-85e3-f317a168e6b6\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;39fb6216-2803-20c6-7211-76f8fe38b90e\u0026#34;, \u0026#34;extraProperties\u0026#34;: {} } ] } 事实上，ABP vNext 中自带的分页查询，主要是通过SkipCount和MaxResultCount两个参数来实现。假设MaxResultCount，即分页大小为m，则第n页对应的SkipCount应该为(n-1) * m。如果大家对于LINQ非常熟悉的话，应该可以自然而然地联想到Skip()和Take()两个方法，这是一个非常自然的联想，因为 ABP vNext 就是这样实现分页查询的。这里以博主的“数据字典”分页查询接口为例：\npublic async Task\u0026lt;PagedResultDto\u0026lt;DataDictionaryQueryDto\u0026gt;\u0026gt; GetCategories( GetDataDictionaryRequestInput input ) { var totalCount = (await _dataDictRepository.GetQueryableAsync()) .WhereIf(!string.IsNullOrEmpty(input.Name), x =\u0026gt; x.Name.Contains(input.Name) || x.Name == input.Name) .WhereIf(!string.IsNullOrEmpty(input.Description), x =\u0026gt; x.Description.Contains(input.Description) || x.Description == input.Description) .Count(); var items = (await _dataDictRepository.GetQueryableAsync()) .WhereIf(!string.IsNullOrEmpty(input.Name), x =\u0026gt; x.Name.Contains(input.Name) || x.Name == input.Name) .WhereIf(!string.IsNullOrEmpty(input.Description), x =\u0026gt; x.Description.Contains(input.Description) || x.Description == input.Description) .Skip(input.SkipCount) .Take(input.MaxResultCount) .ToList(); return new PagedResultDto\u0026lt;DataDictionaryQueryDto\u0026gt;() { TotalCount = totalCount, Items = ObjectMapper.Map\u0026lt;List\u0026lt;DataDictionary\u0026gt;, List\u0026lt;DataDictionaryQueryDto\u0026gt;\u0026gt;(items) }; } 可以注意到，在 ABP vNext 中我们只需要构造好TotalCount和Items这两个属性即可。\nSTable组件中的分页查询 接下来，在 Ant Design Vue 的 Pro 版本中，我们使用STable组件来展示列表类的数据，关于这个组件的使用方法，大家可以参考 官方文档。按照最小化可行产品(MVP)的理念，一个最简单的STable组件的使用，如下面所示：\n\u0026lt;template\u0026gt; \u0026lt;s-table ref=\u0026#34;table\u0026#34; size=\u0026#34;default\u0026#34; :rowKey=\u0026#34;(record) =\u0026gt; record.data.id\u0026#34; :columns=\u0026#34;columns\u0026#34; :data=\u0026#34;loadData\u0026#34; :rowSelection=\u0026#34;{ selectedRowKeys: selectedRowKeys, onChange: onSelectChange }\u0026#34; \u0026gt; \u0026lt;/s-table\u0026gt; \u0026lt;/template\u0026gt; 对于这个组件而言，其中最重要的地方当属data属性，它接受一个函数，该函数的返回值为Promise对象，并且有一个参数：\n\u0026lt;script\u0026gt; import STable from \u0026#39;@/components\u0026#39; export default { components: { STable }, data() { return { // 表格列名 columns: [], // 查询条件 queryParam: { }, // 加载数据方法，必须为 Promise 对象 loadData: parameter =\u0026gt; { return getRoles(Object.assign({}, this.queryParam, parameter)) .then(res =\u0026gt; { return res.result }) }, // ... selectedRowKeys: [], selectedRows: [] } } } \u0026lt;/script\u0026gt; 也许，你会好奇这个parameter到底是个什么东西？可如果我们将其打印出来，就会发现它其实是分页查询相关的参数：Object { pageNo: 1, pageSize: 10 }，而更进一步，如果深入到这个组件的源代码中，我们会注意到组件内部有一个loadData()方法：\nloadData (pagination, filters, sorter) { this.localLoading = true const parameter = Object.assign({ pageNo: (pagination \u0026amp;\u0026amp; pagination.current) || this.showPagination \u0026amp;\u0026amp; this.localPagination.current || this.pageNum, pageSize: (pagination \u0026amp;\u0026amp; pagination.pageSize) || this.showPagination \u0026amp;\u0026amp; this.localPagination.pageSize || this.pageSize }, (sorter \u0026amp;\u0026amp; sorter.field \u0026amp;\u0026amp; { sortField: sorter.field }) || {}, (sorter \u0026amp;\u0026amp; sorter.order \u0026amp;\u0026amp; { sortOrder: sorter.order }) || {}, { ...filters } ) const result = this.data(parameter) // 对接自己的通用数据接口需要修改下方代码中的 r.pageNo, r.totalCount, r.data 可以注意到，在STable组件内部，它会将分页、排序和过滤三种不同类型的参数，通过Object.assign()方法聚合到一个对象上，这个对象实际上就是我们刚刚打印出来的parameter。为什么这样说呢？因为它接下来就要调用data属性指向的方法啦！还记得这个data是什么吗？不错，它是一个函数，既然是一个函数，当然可以直接调用。到这里，我们可以获得第一个信息，即，ABP vNext 中的表格组件STable，本身封装了分页查询相关的参数，只要将这些参数传递给后端就可以实现分页查询。\n实现参数转换层 既然，这个参数和 ABP vNext 需要的参数不同，为了不修改已有的接口，我们考虑在这中间加一层转换。为此，我们定义下面的函数：\n// 默认列表查询条件 const baseListQuery = { page: 1, limit: 20 } // 查询条件转化 export function transformAbpListQuery (query) { query.filter = query.filter === \u0026#39;\u0026#39; ? undefined : query.filter if (window.isNaN(query.pageSize)) { query.pageSize = baseListQuery.limit } if (window.isNaN(query.pageNo)) { query.pageNo = baseListQuery.page } const abpListQuery = { maxResultCount: query.pageSize, skipCount: (query.pageNo - 1) * query.pageSize, sorting: \u0026#39;\u0026#39;, filter: \u0026#39;\u0026#39;, ...query } if (typeof (query.sortField) !== \u0026#39;undefined\u0026#39; \u0026amp;\u0026amp; query.sortField !== null) { abpListQuery.sorting = query.sortOrder === \u0026#39;ascend\u0026#39; ? query.sortField : `${query.sortField} Desc` } return abpListQuery } 代码非常简单，通过transformAbpListQuery函数，我们就实现了从STable到ABP vNext的参数转换。需要说明的是，这里的排序使用到了 System.Linq.Dynamic.Core 这个库，它可以实现IQueryable级别的、基于字符串的动态表达式构建功能，使用方法如下：\nvar resultSingle = queryable.OrderBy\u0026lt;User\u0026gt;(\u0026#34;NumberProperty\u0026#34;); var resultSingleDescending = queryable.OrderBy\u0026lt;User\u0026gt;(\u0026#34;NumberProperty DESC\u0026#34;); var resultMultiple = queryable.OrderBy\u0026lt;User\u0026gt;(\u0026#34;NumberProperty, StringProperty\u0026#34;); 所以，当它为降序排序时，我们在排序字段的后面添加DESC即可。关于filter参数，我准备做一套通用性更强的方案，所以，这里就暂时留空啦！接下来，如果大家足够细心的话，会发现STable组件对返回值同样有一定的要求，它要求返回值中至少含有pageNo、totalCount, data三个属性，而这，是我们获得的第二个信息：\n// 对接自己的通用数据接口需要修改下方代码中的 r.pageNo, r.totalCount, r.data // eslint-disable-next-line if ((typeof result === \u0026#39;object\u0026#39; || typeof result === \u0026#39;function\u0026#39;) \u0026amp;\u0026amp; typeof result.then === \u0026#39;function\u0026#39;) { result.then(r =\u0026gt; { this.localPagination = this.showPagination \u0026amp;\u0026amp; Object.assign({}, this.localPagination, { current: r.pageNo, // 返回结果中的当前分页数 total: r.totalCount, // 返回结果中的总记录数 showSizeChanger: this.showSizeChanger, pageSize: (pagination \u0026amp;\u0026amp; pagination.pageSize) || this.localPagination.pageSize }) || false this.localDataSource = r.data // 返回结果中的数组数据 this.localLoading = false }) } 依样画葫芦，我们继续编写转换层的代码，返回值格式参考了 Ant Design Vue 中Mock接口的返回值格式：\n// 查询结果转化 export function transformAbpQueryResult (data, message, code = 0, headers = {}) { const responseBody = { } responseBody.result = data if (message !== undefined \u0026amp;\u0026amp; message !== null) { responseBody.message = message } if (code !== undefined \u0026amp;\u0026amp; code !== 0) { responseBody.code = code responseBody._status = code } if (headers !== null \u0026amp;\u0026amp; typeof headers === \u0026#39;object\u0026#39; \u0026amp;\u0026amp; Object.keys(headers).length \u0026gt; 0) { responseBody._headers = headers } responseBody.timestamp = new Date().getTime() return responseBody } // 分页查询结果转化 export function buildPagingQueryResult (queryParam, data) { for (const item of data.items) { // Ant Design Vue 中要求每行数据中必须存在字段：key item.key = item.id } const pagedResult = { pageSize: queryParam.pageSize, pageNo: queryParam.pageNo, totalCount: data.totalCount, totalPage: data.totalCount / queryParam.pageSize, data: data.items } return transformAbpQueryResult(pagedResult) } 对于分页结果而言，我们会将分页大小、当前页数、总页数、总记录数及其对应的数据，统一封装到一个对象中，然后再将其传递给返回值中的result属性。\n最终对接效果 好了，写了这么多，我们到底实现了一个什么效果呢？对于一开始的角色查询接口，我们可以这样封装到前端的服务层：\nexport function getRoles (query) { const queryParam = transformAbpListQuery(query) return axios({ url: AppConsts.resourceService.baseUrl + \u0026#39;/api/identity/roles\u0026#39;, method: \u0026#39;get\u0026#39;, params: queryParam }).then(data =\u0026gt; { return buildPagingQueryResult(queryParam, data) }) } 接下来，我们只需要实现loadData()方法即可：\nimport { getRoles, updateRole, createRole, deleteRole } from \u0026#39;@/api/recipe/abp.role\u0026#39; loadData: parameter =\u0026gt; { return getRoles(Object.assign({}, parameter, this.queryParam)) .then(res =\u0026gt; { return res.result }) }, 此时，我们可以注意到，ABP vNext 与 Ant Design Vue 完美地集成在一起，并且参数的转换完全符合我们的预期。这样做的好处显而易见，我们只需要遵循 ABP vNext 的规范进行开发即可，考虑到 ABP vNext 可以直接将ApplicationService暴露为 API 接口，这意味着我们写完了接口，就可以立即开始前后端的联调工作，这无疑可以加快我们的研发效率！\nABP vNext 与 Ant Design Vue 完成整合\r好了，以上就是这篇博客的全部内容啦！这篇博客要实现的功能其实并不复杂，唯一的难点是，需要在前端和后端两个技术栈上频繁地切换上下文，这可能就是全栈开发者面临的最大挑战，因为技术世界浩如烟海，而一个人的精力终究有限，古人云：朝闻道，夕死可矣，人生百年，吾道不孤，还是请你继续努力哦！\n","date":"2021-04-07T21:07:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3670340170/","slug":"3670340170","tags":["ABP","Vue","分页","前端"],"title":"ABP vNext 对接 Ant Design Vue 实现分页查询"},{"categories":["数据存储"],"content":"各位朋友，大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是：https://blog.yuanpei.me。最近这段时间，我一直在学习 ABP vNext 框架，在整个学习过程中，我基本就是在“文档”和“源码”间来回横跳。我个人推荐大家，多去阅读一点优秀的代码，因为阅读 ABP vNext 的源代码简直就是一种享受，它可以暂时让你摆脱如泥沼一般的业务代码。言归正传，ABP vNext 是一个支持多租户架构的框架，在了解了其多租户的实现原理以后，从中收获一点微不足道的小技巧。正好前几天，刚刚同一位朋友讨论完分库、分表这类话题。因此，在今天这篇博客中，我想和大家一起探讨下 EF Core 关于分库、分表以及多租户架构的实现。此中曲折，可以说是初窥门径，或许我无法提供给你一个开箱即用的方案，至少它可以带给你一点启发。有读者朋友建议我，不要总是写这种“高深”、“复杂”的话题，适当地迎合读者写点不需要动脑子的东西。对此，我想说，我有我个人技术上的追求，希望大家理解！\n分库 首先，我们一起来探讨分库这个话题。从字面含义上了解，分库就是指应用程序拥有多个数据库，而这些数据库则拥有相同的表结构。你可能会问，为什么我们需要分库、分表？答案自然是性能，性能，还是TM的性能。我相信，大家都曾经或多或少地听到过垂直拆分、水平拆分这样的术语，下图展示了如何在数据库这一层级上进行拆分：\n数据库的垂直拆分与水平拆分\r其实，我们可以从索引存储、B+树高度、QPS 和 连接数 这四个不同的角度来审视这个话题。相关观点认为，当单表数据量达到一定量级(阿里巴巴Java开发手册中为500W)时，由于内存无法存储其索引，此时SQL查询会产生磁盘IO；行记录的大小决定了B+树的每个叶子节点能存储多少记录，所以，行记录的大小会影响B+树的高度；单个MySQL物理机实例写QPS峰值大概为1万，一旦业务量达到某个量级，这个瓶颈会逐步凸显出来；单个MySQL实例最大连接数有限，更多的访问量意味着需要更多的连接数。\n在谈论分库、分表的时候，我们忍不住会去想譬如“自动分表”和“路由”这样的问题，这些子库、子表，到底是提前在数据库里分好呢，还是在运行时期间自动去拆分呢，以及我对库/表进行拆分以后，我应该怎么样找到某条数据对应的库/表。我承认，这些问题并不简单，但当我们对问题进行简化以后，分库本质上就是动态地切换数据库，对不对？无非是拆分后的数据库可能会是类似db_0、db_1等等这样的序列。\n对 Chinook 进行水平拆分\r对于数据库的自动拆分，博主尝试过的一种方案是：首先，通过Add-Migration生成迁移。然后，通过循环修改连接字符串的方式，调用Context.Database.Migrate()方法为一个数据库迁移表结构和种子数据。当然，有些朋友不认同在生产环境使用迁移的做法，认为对数据库的操作权限还是应该交给 DBA 来管理，这当然无可厚非。我表达的一直都是一种思路，我不想一个工作六年的人，对技术的态度永远都停留在“能跑”、“能抄”这种水平。\n一旦想清楚这一层，实现起来还是非常简单的。我们在配置中准备多个数据库来模拟分库的场景，实际应用中到底是用范围、Hash 还是 配置，大家结合自己的场景来决定就好。其实，这个思路还可以用来做读写分离，无非是这个库更特殊一点，它是个从库。好了，我们一起来看下面的代码：\n// 这里随机连接到某一个数据库 // 实际应该按照某种方式获得数据库库名后缀 var shardings = _options.Value.MultiTenants; var sharding = shardings[new Random().Next(0, shardings.Count)]; _chinookContext.Database.GetDbConnection().ConnectionString = sharding.ConnectionString; Console.WriteLine(\u0026#34;--------分库场景--------\u0026#34;); Console.WriteLine(_chinookContext.Database.GetDbConnection().ConnectionString); Console.WriteLine(_chinookContext.Album.ToQueryString()); Console.WriteLine(_chinookContext.Artist.ToQueryString()); 事实上，如果选择性地忽略 “路由” 和 “自动分表” 这两个特性，我们已经在 EF 层面上局部的实现了 “分库” 功能：\n分库场景\r分表 好了，聊完分库，我们再来聊聊分表。分表就是指同一个数据库里拥有多张结构(Schema)相同的表。一个典型的例子是，Excel里的多张Sheet，只要它们拥有相同的结构(Schema)，就可以视为同一类型的数据，虽然它们拥有不同的表名。和分库类似，分表的着眼点是避免产生“大表”，从而达到提高查询性能的目的。而对应到 EF(EntityFramework) 的场景中，分表本质上就是在解决 EF 动态适配表名的问题。同样的，下面两张图展示了如何在表这个层级进行拆分：\n表的垂直拆分\r表的水平拆分\r图片援引自：雨点的名字 - 分库分表(1) \u0026mdash; 理论\n譬如，我们以年为单位，产生了Album_2020和Album_2021两张表。那么，在已经定义好了实体Album的情况下，有没有办法可以让实体Album动态地去适配这两张表呢？或许，熟悉 EF 的你，此刻正在心里暗笑道，这有何难，只要在对应实体的OnModelCreating()方法中，修改ToTable()方法的参数就好了啊。可如果你亲自试一试，就会知道这是你的一厢情愿啦！\n针对 Album 和 Artist 按年份进行拆分\r事实上，EF 针对实体和表的映射关系做了缓存，这意味着，一旦在OnModelCreating()方法中确定映射关系，这组映射关系将被缓存下来。在 EF 中，这组映射关系的缓存行为，由IModelCacheKeyFactory接口来决定，它提供了一个Create()方法，如果该方法的返回值与上一次相同，则不会调用OnModelCreating()方法。所以，我们的思路就是，让这个Create()方法返回不同的对象。为此，我们考虑实现IModelCacheKeyFactory接口，并用这个自定义实现来替换微软的默认实现。我们一起来看下面的代码：\npublic class DynamicModelCacheKeyFactory : IModelCacheKeyFactory { public object Create(DbContext context) { return context is ShardingContext shardingContext ? (context.GetType(), shardingContext.ShardingSuffix) : (object)context.GetType(); } } 为了配合DynamicModelCacheKeyFactory的使用，我们还需要定义用于分表的ShardingContext，它继承自DbContext，我们为其扩展了ShardingSuffix属性，并通过注入的IShardingPolicyProvider接口来获取一个分表后缀。比如，我们有Order表，经过拆分后获得Order_01、Order_02这样的子表，所以，这个分表后缀其实就是01、02。没错，我们还是要去修改ToTable()方法中的表名，不同的是，这里的表名是动态的。注意到，Create()方法返回的是一个元组，所以，不同的ShardingSuffix会产生不同的映射关系。\npublic class ShardingContext : DbContext { public DbSet\u0026lt;Artist\u0026gt; Artist { get; set; } public DbSet\u0026lt;Album\u0026gt; Album { get; set; } private readonly IShardingPolicyProvider _shardingPolicyProvider; public string ShardingSuffix { get; private set; } public ShardingContext( DbContextOptions\u0026lt;ShardingContext\u0026gt; options, IShardingPolicyProvider shardingPolicyProvider ) : base(options) { _shardingPolicyProvider = shardingPolicyProvider; ShardingSuffix = _shardingPolicyProvider.GetShardingSuffix(); } protected override void OnModelCreating(ModelBuilder modelBuilder) { base.OnModelCreating(modelBuilder); // Album // 动态映射表名，譬如：Album_2021 modelBuilder.Entity\u0026lt;Album\u0026gt;().ToTable($\u0026#34;Album_{ShardingSuffix}\u0026#34;); modelBuilder.Entity\u0026lt;Album\u0026gt;().HasKey(x =\u0026gt; x.AlbumId); modelBuilder.Entity\u0026lt;Album\u0026gt;() .Property(x =\u0026gt; x.AlbumId).HasColumnName(\u0026#34;AlbumId\u0026#34;); modelBuilder.Entity\u0026lt;Album\u0026gt;() .Property(x =\u0026gt; x.Title).HasColumnName(\u0026#34;Title\u0026#34;); modelBuilder.Entity\u0026lt;Album\u0026gt;() .Property(x =\u0026gt; x.ArtistId).HasColumnName(\u0026#34;ArtistId\u0026#34;); // Artist // 动态映射表名，譬如：Artist_2021 modelBuilder.Entity\u0026lt;Artist\u0026gt;().ToTable($\u0026#34;Artist_{ShardingSuffix}\u0026#34;); modelBuilder.Entity\u0026lt;Artist\u0026gt;().HasKey(x =\u0026gt; x.ArtistId); modelBuilder.Entity\u0026lt;Artist\u0026gt;() .Property(x =\u0026gt; x.ArtistId).HasColumnName(\u0026#34;ArtistId\u0026#34;); modelBuilder.Entity\u0026lt;Artist\u0026gt;() .Property(x =\u0026gt; x.Name).HasColumnName(\u0026#34;Name\u0026#34;); } } 关于分库、分表以后，怎么去匹配对应的库或者表，这类问题我们称之为路由问题。常见的策略主要有，范围、Hash 和 配置：\n范围最直观的就是按照时间来拆分，比如按年、按月、按天等等，主要的问题是分布不均匀；其次，可以按照Id的范围来划分，比如0到10万、10万到20万依次划分到不同的表里，主要的问题是热点数据带来的性能问题。 Hash主要指哈希取模。例如，可以针对用户Id做如下处理：HASH(userId) % N，其中，N表示当前拆分表的数目。可以预见的问题是，当N变化的时候，会产生数据迁移的需求，所以，这种方式并不利于扩容， 配置，顾名思义，就是用一张表来存储数据和子表间的映射关系，每次先按照数据的主键找到子表，然后再从子表中查询所需要的数据。好处是扩容灵活，而缺点同样明显，查询配置表，带来了额外的性能损耗。 在这里，我们是使用年份来作为分表后缀的。为了方便演示，在实现ShardingByYearPolicy类时，我们直接使用了当前时间，这意味着我们会将Album实体映射到Album_2021这张表，以此类推。在实际使用中，更推荐大家使用 雪花算法 生成Id，因为这样，我们就可以通过Id反推出具体的时间范围，进而决定要映射到哪一个库、哪一张表。关于子表的生成，博主这里是通过迁移来实现的，考虑到EF自动创建数据库/表，都需要先创建迁移，所以，这并不是一个开箱即用的方案。\nclass ShardingByYearPolicy : IShardingPolicyProvider { public string GetShardingSuffix() { return $\u0026#34;{DateTime.Now.ToString(\u0026#34;yyyy\u0026#34;)}\u0026#34;; } } 好了，现在我们可以编写简单的代码，来验证我们的这些想法是都正确，即使是最简单的控制台程序，我还是喜欢用依赖注入：\n// 注入ShardingContext services.AddDbContext\u0026lt;ShardingContext\u0026gt;(options =\u0026gt; { options.UseSqlite(config.GetValue\u0026lt;string\u0026gt;(\u0026#34;Database:Default\u0026#34;)); //替换默认实现 options.ReplaceService\u0026lt;IModelCacheKeyFactory, DynamicModelCacheKeyFactory\u0026gt;(); }); // 注入IShardingPolicyProvider services.AddTransient\u0026lt;IShardingPolicyProvider, ShardingByYearPolicy\u0026gt;(); 接下来，我们可以通过ShardingContext来匹配Album_2021表：\n// 这里应该连接到Album_2021表 // 实际应该按照某种方式获得表名后缀 Console.WriteLine(\u0026#34;--------分表场景--------\u0026#34;); Console.WriteLine(_shardingContext.Database.GetDbConnection().ConnectionString); Console.WriteLine(_shardingContext.Album.ToQueryString()); Console.WriteLine(_shardingContext.Artist.ToQueryString()); 此时，我们会得到下面的结果：\nEF Core 分表效果演示\r至此，如果选择性地忽略 “路由” 和 “自动分表” 这两个特性，我们已经在 EF 层面上局部的实现了 “分表” 功能。怎么样，是不是还行？\n多租户架构 最后，我们来聊聊多租户架构这个话题。可能有朋友觉得多租户架构和分库、分表没什么关系，不好意思啊，这是个非常合理的联想，因为还真就有关系，甚至我们还能继续发散到读写分离。你想想看，多租户架构中，如果一个租户一个数据库，这是不是就是分库的场景。而在分库的场景中，如果一个是主库，一个是从库，这是不是就是读写分离的场景。在学习数学的过程中，学会转化问题是一种重要的思维，即让一个不熟悉的问题变成一个熟悉的问题，在今天这篇博客中，从分库发散到多租户、读写分离，正是这一思路的体现，通常情况下，多租户架构有多数据库和单数据库两种实现方式。\n多数据库 多数据库，指每一个租户一个数据库。这种实现方式的好处是，租户间的数据天然隔离，数据库的访问压力天然隔离。可由于所有租户都共享一套应用程序，随着数据库越来越多，维护的成本亦越来越高。参考分库的实现，我们可以非常容易地实现租户数据库的切换。这里，我们的思路是，调用方在 HTTP 请求中加入自定义的首部字段X-TenantId，DbContext通过该字段来匹配对应的链接字符串，这样就可以实现多数据库的多租户架构：\npublic class TenantInfoProvider : ITenantInfoProvider { private const string X_TENANT_ID = \u0026#34;X-TenantId\u0026#34;; private readonly IHttpContextAccessor _httpContextAccessor; public TenantInfoProvider(IHttpContextAccessor httpContextAccessor) { _httpContextAccessor = httpContextAccessor; } public string GetTenantId() { var httpContext = _httpContextAccessor.HttpContext; if (httpContext != null \u0026amp;\u0026amp; httpContext.Request.Headers.ContainsKey(X_TENANT_ID)) return httpContext.Request.Headers[X_TENANT_ID].FirstOrDefault(); return null; } } 接下来，假设我们AppSettings.json文件维护各个租户的连接字符串信息。通常，在实际场景中，我们会将这些信息存储在数据库中：\n{ \u0026#34;Database\u0026#34;: { \u0026#34;Default\u0026#34;: \u0026#34;Data Source=Chinook.db\u0026#34;, \u0026#34;MultiTenants\u0026#34;: [ { \u0026#34;tenantId\u0026#34;: \u0026#34;01\u0026#34;, \u0026#34;ConnectionString\u0026#34;: \u0026#34;Data Source=Chinook01.db\u0026#34; }, { \u0026#34;tenantId\u0026#34;: \u0026#34;02\u0026#34;, \u0026#34;ConnectionString\u0026#34;: \u0026#34;Data Source=Chinook02.db\u0026#34; } ] } } 此时，我们可以通过下面的代码片段来实现租户切换：\nvar tenantId = _tenantInfoProvider.GetTenantId(); var database = _options.Value.MultiTenants.FirstOrDefault(x =\u0026gt; x.TenantId == tenantId); if (database == null) throw new Exception($\u0026#34;Invalid tenantId \\\u0026#34;{tenantId}\\\u0026#34;\u0026#34;); _chinookContext.Database.GetDbConnection().ConnectionString = database.ConnectionString; Console.WriteLine(\u0026#34;--------多租户 + 多数据库--------\u0026#34;); Console.WriteLine($\u0026#34;TenantId:{tenantId}\u0026#34;); Console.WriteLine(_chinookContext.Database.GetDbConnection().ConnectionString); Console.WriteLine(_chinookContext.Album.ToQueryString()); Console.WriteLine(_chinookContext.Artist.ToQueryString()); 可以注意到，一切如我们所预料的一样，程序自动切换到01这个租户：\n多租户 \u0026#43; 多数据库\r单数据库 单数据库，指所有租户都在一个数据库里，使用相同的表结构(Schema)，并通过TenantId字段进行区分。ABP vNext 中的多租户架构就是这种模式，而我之前的公司，则是单数据库 + 多数据库的混合模式。这种实现方式的好处是数据库非常精简，而缺点同样很明显，一旦某个租户出现问题，非常容易波及所有租户，因为所有租户都在一个数据库里，数据库的压力实际上是大家一起分担的，租户间相互影响的可能性非常大。\n同样地，我们依然需要用到X-TenantId这个请求头，由于所有租户都在一个数据库上，我们不会再试图去修改链接字符串。EF Core 中针对实体提供了HasQueryFilter()扩展方法，该访问允许我们传入一个 Lambda 表达式。此时，我们所有的请求都会自动带上类似Album.TenantId = 'xxxx'这样的条件，这样我们就实现了单数据库的多租户架构。\npublic class MulitiTenancyContext : DbContext { public DbSet\u0026lt;Artist\u0026gt; Artist { get; set; } public DbSet\u0026lt;Album\u0026gt; Album { get; set; } private readonly ITenantInfoProvider _tenantInfoProvider; public MulitiTenancyContext( DbContextOptions\u0026lt;MulitiTenancyContext\u0026gt; options, ITenantInfoProvider tenantInfoProvider ) : base(options) { _tenantInfoProvider = tenantInfoProvider; } protected override void OnModelCreating(ModelBuilder modelBuilder) { base.OnModelCreating(modelBuilder); modelBuilder.ApplyConfiguration(new ArtistMap()); modelBuilder.ApplyConfiguration(new AlbumMap()); // 利用 HasQueryFilter 进行租户间数据隔离 var tenantId = _tenantInfoProvider.GetTenantId(); if (!string.IsNullOrEmpty(tenantId)) { modelBuilder.Entity\u0026lt;Album\u0026gt;().HasQueryFilter(x =\u0026gt; x.TenantId == tenantId); modelBuilder.Entity\u0026lt;Artist\u0026gt;().HasQueryFilter(x =\u0026gt; x.TenantId == tenantId); } } } 为了在实体上应用这个过滤条件，参照 ABP vNext 中的实现，我们定义了IMulitiTenancy接口，所有实体均需要实现TenantId字段。为了简化设计，我们直接使用字符串类型来定义租户Id，而在 ABP vNext 中很多主键都被定义为 Guid，我们掌握核心原理即可，不用过分强求和 ABP vNext 的一致。\n// IMulitiTenancy public interface IMulitiTenancy { public string TenantId { get; set; } } // Album public class Album : IMulitiTenancy { public int AlbumId { get; set; } public string Title { get; set; } public int ArtistId { get; set; } public string TenantId { get; set; } } 此时，我们可以编写简单的测试代码，来验证我们的想法是否正确。同样地，我还是使用了依赖注入：\n// 这里应该查询01租户内的Album var tenantId = _tenantInfoProvider.GetTenantId(); Console.WriteLine(\u0026#34;--------多租户 + 单数据库--------\u0026#34;); Console.WriteLine($\u0026#34;TenantId:{tenantId}\u0026#34;); Console.WriteLine(_mulitiTenancyContext.Database.GetDbConnection().ConnectionString); Console.WriteLine(_mulitiTenancyContext.Album.ToQueryString()); Console.WriteLine(_mulitiTenancyContext.Artist.ToQueryString()); 可以注意到，打印出的 SQL 语句中自动带出了过滤条件：\n多租户 \u0026#43; 多数据库\r本文小结 这篇博客主要探讨了 EF 在分库、分表及多租户架构上实施的可能性。分库、分表的目的是为了提高数据库的查询性能，在这个过程中，我们可以考虑范围、Hash和配置三种路由策略，它们各自有自己的优缺点，需要使用者结合业务场景去衡量。虽然分库、分表在面对百万级别以上的数据时，不失为一种提高性能的方案，可世间万物都是双刃剑，它同样带来了一系列新的问题，譬如跨库写带来的分布式事务问题，跨库读带来的Join、Count()、排序、分页等问题，数据迁移问题等等，而如果希望通过Hash(Id)来进行拆分，还需要解决全局Id唯一的问题。所以说，这是一个没有标准答案的问题，需要使用者自己去进行取舍。多租户架构、读写分离均可以看作是特殊的分库场景，EF Core 中新增的HasQueryFilter()方法则帮助我们解决了单数据库的多租户架构问题。好了，以上就是这篇博客的全部内容啦，如果大家对文中的观点有建议或者意见，欢迎大家在评论区留言，谢谢！\n附本文源代码：https://github.com/Regularly-Archive/2021/tree/master/EF.Sharding\n","date":"2021-03-27T17:47:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2151871792/","slug":"2151871792","tags":["数据库","多租户","EF","架构"],"title":"浅议 EF Core 分库分表及多租户架构的实现"},{"categories":["编程语言"],"content":"本文是 #源代码探案系列# 第三篇，今天这篇博客，我们来一起解读下 ASP.NET Core 中的 CORS 中间件，熟悉这个中间件的的小伙伴们，想必都已经猜出本文的主题：跨域。这确实是一个老生常谈的话题，可我并不认为，大家愿意去深入探究这个问题，因为博主曾经发现，每当工作中遇到跨域问题的时候，更多的是直接重写跨域相关的 HTTP 头。博主曾经写过一篇关于跨域的博客：《聊聊前端跨域的爱恨情仇》，当时是完全以前端的视角来看待跨域。所以，在今天这篇博客里，博主想带领大家从一种新的视角来看待跨域，也许，可以从中发现不一样的东西。\n核心流程 关于 ASP.NET Core 中的 CORS，大家都知道的是，可以通过UseCors()方法在整个 HTTP 请求管道中启用跨域中间件，或者是通过AddCors()方法来定义跨域策略，亦或者通过[EnableCors]来显式地指定跨域策略，更多的细节大家可以参考微软的官方文档，而在这里，我想聊一点大家可能不知道的东西，譬如：服务器端如何处理来自浏览器端的跨域请求？而这一切在 ASP.NET Core 中又如何实现？带着这些问题来解读 CORS 中间件的源代码，我们能更快的找到我们想得到的答案。一图胜千言，请允许博主使用这张流程图来“开宗明义”，我们这就开始今天的“探案”：\n一张图览尽 CORS 中间件\r核心部件 对于整个 CORS 中间件而言，核心部件主要有：CorsPolicy、CorsService 以及 CorsMiddleware。\nCorsPolicy 整个 CORS 中间件中，首当其冲的是ICorsPolicy。这个接口的作用是定义跨域的策略，我们知道CORS中引入了Access-Control系列的 HTTP 头，所以，CorsPolicy 本质上是在定义允许哪些 HTTP 头、HTTP 方法、源(Origin) 可以访问受限的资源，以及当跨域请求是一个复杂请求的时候，预检请求的超时时间、是否支持凭据等等：\npublic class CorsPolicy { public bool AllowAnyHeader { get; } public bool AllowAnyMethod { get; } public bool AllowAnyOrigin { get; } public Func\u0026lt;string, bool\u0026gt; IsOriginAllowed { get; private set; } public IList\u0026lt;string\u0026gt; ExposedHeaders { get; } = new List\u0026lt;string\u0026gt;(); public IList\u0026lt;string\u0026gt; Headers { get; } = new List\u0026lt;string\u0026gt;(); public IList\u0026lt;string\u0026gt; Methods { get; } = new List\u0026lt;string\u0026gt;(); public IList\u0026lt;string\u0026gt; Origins { get; } = new List\u0026lt;string\u0026gt;(); public TimeSpan? PreflightMaxAge { get; set; } public bool SupportsCredentials { get; set; } 在整个中间件的设计中，与CorsPolicy接口产生直接联系的，是CorsPolicyBuilder和ICorsPolicyProvider。相信大家从命名上就可以了解到，前者是一个基于建造者模式的、针对 CorsPolicy进行“加工”的工具类，可以快速地对 跨域策略中允许的 HTTP 方法、HTTP 头、源(Origin)等信息进行修改。关于这一点，我们可以从CorsPolicyBuilder提供的方法签名中得到印证，而最终CorsPolicyBuilder通过Build()方法来返回一个“加工”好的CorsPolicy。\npublic class CorsPolicyBuilder { CorsPolicyBuilder WithOrigins(params string[] origins); CorsPolicyBuilder WithHeaders(params string[] headers); CorsPolicyBuilder WithExposedHeaders(params string[] exposedHeaders); CorsPolicyBuilder WithMethods(params string[] methods); CorsPolicyBuilder AllowCredentials(); CorsPolicyBuilder DisallowCredentials(); CorsPolicyBuilder AllowAnyOrigin(); CorsPolicyBuilder AllowAnyMethod(); CorsPolicyBuilder AllowAnyHeader(); CorsPolicyBuilder SetPreflightMaxAge(TimeSpan preflightMaxAge); CorsPolicyBuilder SetIsOriginAllowed(Func\u0026lt;string, bool\u0026gt; isOriginAllowed); CorsPolicyBuilder SetIsOriginAllowedToAllowWildcardSubdomains(); CorsPolicy Build(); } 除了通过CorsPolicyBuilder来生成跨域策略，我们还可以通过ICorsPolicyProvider来生成跨域策略。如果你经常使用ASP.NET Core中的配置系统和依赖注入，对于这种“套路”应该不会感到陌生。这里，微软提供了一个默认实现：DefaultCorsPolicyProvider。DefaultCorsPolicyProvider本身依赖CorsOptions，允许使用者传入一个CorsPolicy的实例 或者是一个委托，来自定义跨域策略的“加工”细节，并在其内部维护一个字典，来实现具名的跨域策略。如果使用者不为当前跨域策略指定名称，则会使用默认的跨域策略名称。在大多数场景下，我们并不会直接使用CorsPolicyBuilder，而是在Startup类中通过委托来定义跨域策略，两者可以说是不同层次上的跨域策略的“提供者”。\n// DefaultCorsPolicyProvider的GetPolicyAsync() public Task\u0026lt;CorsPolicy?\u0026gt; GetPolicyAsync(HttpContext context, string? policyName) { if (context == null){ throw new ArgumentNullException(nameof(context)); } policyName ??= _options.DefaultPolicyName; if (_options.PolicyMap.TryGetValue(policyName, out var result)) { return result.policyTask!; } return NullResult; } // CorsOptions public void AddDefaultPolicy(CorsPolicy policy); public void AddDefaultPolicy(Action\u0026lt;CorsPolicyBuilder\u0026gt; configurePolicy); public void AddPolicy(string name, CorsPolicy policy); public void AddPolicy(string name, Action\u0026lt;CorsPolicyBuilder\u0026gt; configurePolicy); public CorsPolicy? GetPolicy(string name); CorsService OK，说完了跨域策略的“定义”，现在我们来看看跨域策略是如何被中间件“执行”的，这部分代码被定义在CoreService类的EvaluatePolicy()方法中。可以注意到，如果受限资源允许任意源(Origin)访问，则服务器端会认为这是一个不安全的跨域策略。\n接下来，从HttpContext中提取客户端的源(Origin)，请求方法(HttpMethod)。此时，服务器端可以根据请求方法和 HTTP 头 判断当前请求是都为预检请求。按照CORS规范，当请求方法为OPTION且请求头中含有Access-Control-Request-Method时，即表示这是一个预检请求。\n至此，我们有了两种选择，预检请求会交给EvaluatePreflightRequest()方法去处理，非预检请求会交给EvaluateRequest()方法去处理。除了HttpContext和CorsPolicy这两个参数以外，它们都会接受第三个参数CorsResult，它里面封装了我们一开始判断出来的关于源和预检请求的信息。继续细看，我们会发现这两个方法，都调用了PopulateResult()方法，继续顺着这条线索下去，我们就会发现，这个方法的主要作用是，结合跨域策略设定的各种参数，进一步对上一步生成的CorsResult进行“加工”。\npublic CorsResult EvaluatePolicy(HttpContext context, CorsPolicy policy) { // ... if (policy.AllowAnyOrigin \u0026amp;\u0026amp; policy.SupportsCredentials) { throw new ArgumentException(Resources.InsecureConfiguration, nameof(policy)); } var requestHeaders = context.Request.Headers; var origin = requestHeaders[CorsConstants.Origin]; var isOptionsRequest = HttpMethods.IsOptions(context.Request.Method); var isPreflightRequest = isOptionsRequest \u0026amp;\u0026amp; requestHeaders.ContainsKey(CorsConstants.AccessControlRequestMethod); var corsResult = new CorsResult { IsPreflightRequest = isPreflightRequest, IsOriginAllowed = IsOriginAllowed(policy, origin), }; if (isPreflightRequest) { //预检请求 EvaluatePreflightRequest(context, policy, corsResult); } else { //非预检请求 EvaluateRequest(context, policy, corsResult); } return corsResult; } private static void PopulateResult(HttpContext context, CorsPolicy policy, CorsResult result ) { var headers = context.Request.Headers; if (policy.AllowAnyOrigin) { result.AllowedOrigin = CorsConstants.AnyOrigin; result.VaryByOrigin = policy.SupportsCredentials; } else { var origin = headers[CorsConstants.Origin]; result.AllowedOrigin = origin; result.VaryByOrigin = policy.Origins.Count \u0026gt; 1 || !policy.IsDefaultIsOriginAllowed; } // 支持凭据 result.SupportsCredentials = policy.SupportsCredentials; // 预检请求超时时间 result.PreflightMaxAge = policy.PreflightMaxAge; // https://fetch.spec.whatwg.org/#http-new-header-syntax AddHeaderValues(result.AllowedExposedHeaders, policy.ExposedHeaders); // 允许的HTTP方法 var allowedMethods = policy.AllowAnyMethod ? new[] { result.IsPreflightRequest ? (string)headers[CorsConstants.AccessControlRequestMethod] : context.Request.Method } : policy.Methods; AddHeaderValues(result.AllowedMethods, allowedMethods); // 允许的HTTP头 var allowedHeaders = policy.AllowAnyHeader ? headers.GetCommaSeparatedValues(CorsConstants.AccessControlRequestHeaders) : policy.Headers; AddHeaderValues(result.AllowedHeaders, allowedHeaders); } 那么，这些参数最终的走向是哪里呢？我们注意到CorsService里有一个叫做ApplyResult()的方法，观察方法签名可以发现，它负责把跨域检测的结果应用到 HTTP 响应上，相信大家都能想到，这里会设置各种Access-Control系列的头，比如Access-Control-Allow-Origin、Access-Control-Allow-Methods、Access-Control-Allow-Headers、 Access-Control-Max-Age\u0026hellip;等等。事实上，在CorsMiddleware中间件中，原本就是先调用EvaluateResult()方法，再调用ApplyResult()方法。当然，实际的代码中，还需要考虑[DisableCors]和[EnableCors]两个特性的影响，会多出一点判断的代码。关于跨域的代码层面的东西，我们就先讲到这里，在下一部分，我们会专门讲CORS里的简单请求和复杂请求。\npublic Task Invoke(HttpContext context, ICorsPolicyProvider corsPolicyProvider) { // ... if (!context.Request.Headers.ContainsKey(CorsConstants.Origin)) { return _next(context); } // [DisableCors] var corsMetadata = endpoint?.Metadata.GetMetadata\u0026lt;ICorsMetadata\u0026gt;(); if (corsMetadata is IDisableCorsAttribute) { var isOptionsRequest = HttpMethods.IsOptions(context.Request.Method); var isCorsPreflightRequest = isOptionsRequest \u0026amp;\u0026amp; context.Request.Headers.ContainsKey(CorsConstants.AccessControlRequestMethod); if (isCorsPreflightRequest) { // If this is a preflight request, and we disallow CORS, complete the request context.Response.StatusCode = StatusCodes.Status204NoContent; return Task.CompletedTask; } return _next(context); } // ... // [EnableCors] else if (corsMetadata is IEnableCorsAttribute enableCorsAttribute \u0026amp;\u0026amp; enableCorsAttribute.PolicyName != null) { // ... // Evaluate \u0026amp;\u0026amp; Apply return EvaluateAndApplyPolicy(context, corsPolicy); async Task InvokeCoreAwaited(HttpContext context, Task\u0026lt;CorsPolicy?\u0026gt; policyTask) { var corsPolicy = await policyTask; await EvaluateAndApplyPolicy(context, corsPolicy); } } } 再论CORS 好了，行文至此。既然这篇博客的主题是“跨域”，那么，我们不妨多说一点。我们知道，“跨域”产生的背景是，浏览器作为一个公共环境，它本身是不被信任的，所以，为了杜绝非当前域的资源，例如Cookie、API等等被“窃取”，浏览器便增加了“跨域”这一限制。而为了顺应“前后端分离”、“微服务”等等的开发思想，“跨域”这个问题开始频繁地出现在人们的视野中，从最初的JSONP，到如今成为事实标准的CORS，甚至从Vue里的代理服务器、Nginx里的反向代理，我们总是能窥出一点“跨域”的影子，“跨域”可谓是无处不在。\n那么，什么是 CORS 呢？ CORS ，即跨域资源共享，是一种利用 HTTP 头部来指示服务器端对除自身以外的源(域、协议、端口)是否可以访问指定的资源。你可能会联想到OAuth2、JWT等等关于认证授权的词汇，请注意，“跨域”始终发生在浏览器端，相对于浏览器，一般意义上的客户端都被视为可信任的。除此之外，CORS提供了一种被称之为“预检”的机制，它可以用来检测服务器端支持的 HTTP 请求头、HTTP 动词，在预检中，浏览器发送的头中标示有 HTTP 方法和真实请求中会用到的头。\n为什么会发生跨域？\r如上图所示，浏览器端，特别是XMLHttpRequest 、Fetch API 、Web字体 和 Canvas等始终遵循同源策略，domain-a.com和domain-b.com被视为两个不同域，因此，当domain-a.com试图访问domain-b.com下的资源时，就会被浏览器所限制，这就是我们所说的“跨域”。可能，这并不是一个特别好的例子，因为 HTML 中某些元素天生就被设计为允许跨域，例如：image、iframe、link、script等等。而如果我们通过“协商”来告诉domain-b，domain-a希望访问它下面的资源，这其实就是我们所说的 CORS 啦！这个“协商”过程呢，主要有两种，即 简单请求 和 复杂请求。\n简单请求 我们将不触发 CORS 预检 的请求称为简单请求，通常情况下，简单请求满足下列条件：\n使用下列方法之一：GET、HEAD 和 POST 除了被用户代理自动设置的首部字段(例如：Connection、User-Agent) 和 在 Fetch 规范中定义为 禁用首部名称 的其他首部，允许人为设置的字段为 Fetch 规范定义的 对 CORS 安全的首部字段集合。该集合为：Accept、Accept-Language、Content-Language、Content-Type、DPR、Downlink、Save-Data、Viewport-Width、Width Content-Type 的值仅限于下列三者之一：text/plain、multipart/form-data、application/x-www-form-urlencoded 请求中的任意 XMLHttpRequestUpload 对象均没有注册任何事件监听器；XMLHttpRequestUpload 对象可以使用 XMLHttpRequest.upload 属性访问。 请求中没有使用 ReadableStream 对象。 对于 简单请求 ，由于它的 HTTP 动词是确定的，故其跨域主要体现在服务器端返回的 HTTP 响应中，可能出现的响应头有：Access-Control-Allow-Origin、Access-Control-Allow-Headers等。所以，如果客户端请求的Origin被包含在服务器端返回的Access-Control-Allow-Origin中，则表示跨域被允许，反之则不被允许。所以，现在大家应该能想明白，为啥那些年里大家稀里糊涂地，把Access-Control-Allow-Origin和Access-Control-Allow-Headers设置为*就万事大吉了吧，而对照着中间件的代码，理解这层含义会更容易一点！\n复杂请求 与简单请求不同，复杂请求 要求必须首先使用 OPTIONS 方法发起一个预检请求到服务器，以获知服务器是否允许该实际请求。\u0026quot;预检请求\u0026ldquo;的使用，可以避免跨域请求对服务器的用户数据产生未预期的影响。\n预检请求\r当浏览器检测到，从JavaScript中发起的请求需要被预检。此时，可以注意到，预检请求中同时携带了下面两个首部字段：\nAccess-Control-Request-Method: POST\rAccess-Control-Request-Headers：X-PINGOTHER, Content-Type 服务器在接受预检请求后，会返回以下响应头：\nAccess-Control-Allow-Origin: http://foo.example\rAccess-Control-Allow-Methods: POST, GET, OPTIONS\rAccess-Control-Allow-Headers: X-PINGOTHER, Content-Type\rAccess-Control-Max-Age: 86400 其中：\n首部字段Access-Control-Allow-Methods表明服务器允许客户端使用 POST、GET 和 OPTIONS 方法发起请求。 首部字段Access-Control-Allow-Headers表明服务器允许请求中携带字段 X-PINGOTHER 与 Content-Type。 首部字段Access-Control-Max-Age表明该响应的有效时间为 86400 秒，即 24 小时。在有效时间内，浏览器无须为同一请求再次发起预检请求。 下面整理了 CORS 中常见的 Access-Control 系列头部字段：\nAccess-Control-Allow-Origin\rAccess-Control-Expose-Headers\rAccess-Control-Max-Age\rAccess-Control-Allow-Credentials\rAccess-Control-Allow-Methods\rAccess-Control-Allow-Headers\rOrigin\rAccess-Control-Request-Method\rAccess-Control-Request-Headers 本文小结 本文分别从 源代码 和 规范 两个角度探讨了 “跨域” 这个话题，两者可以说是相辅相成的存在，CORS 中间件实现了 CORS 规范，而通过 CORS 规范帮助我们理解了中间件。“跨域”产生的背景是，浏览器作为一个公共环境，它本身是不被信任的，所以，为了杜绝非当前域的资源，例如Cookie、API等等被“窃取”，浏览器便增加了 “跨域” 这一限制。最初我们通过 JSONP 这种方案来解决跨域问题，而后来我们有了CORS 这种事实上的标准，其原理上利用 Origin 及 Access-Control系列的头来标识服务器端可以允许哪些源、以什么样的 HTTP 动词 / 头来访问资源，按照 CORS 规范，浏览器端发起的请求被分为： 简单请求 和 复杂请求 两种，两者最大的区别是，复杂请求 必须首先通过 OPTIONS 方法发起一个预检请求到服务器，以获知服务器是否允许该实际请求。好了，以上就是这篇博客的全部内容啦，欢迎大家在博客评论中参与讨论，再次谢谢大家，晚安！\n","date":"2021-03-16T21:25:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1276287490/","slug":"1276287490","tags":["跨域","CORS","中间件","源码"],"title":"源代码探案系列之 .NET Core 跨域中间件 CORS"},{"categories":["编程语言"],"content":"在上一篇文章中，博主带领大家一起深入了解 ConcurrencyLimiter 这个中间件，正当我得意洋洋地向 Catcher Wong 大佬吹嘘这一点小收获时，大佬一脸嫌弃地说，一个单机版的方案有什么好得意的啊。大佬言下之意，显然是指，这个中间件在分布式环境中毫无用武之地。其实，你只需要稍微想一下，就能想明白这个问题。毕竟，它只是通过SeamphoreSlim控制线程数量而已，一旦放到分布式环境中，这个并发控制就被大大地削弱。所以，在今天这篇文章中，博主会带领大家一起“探案” ASP.NET Core 中的限流中间件 AspNetCoreRateLimite，希望大家可以从中感悟到不一样的东西。对我而言，这可能是人到中年的焦虑感所催生出来的一种源动力，同时亦是为了不让那些订阅专栏的同学失望。\n关于“限流”这个话题，我个人以为，它可以引申出非常多的东西，譬如“熔断”和“限流”，其实可以看作是同一类问题的“一体两面”。最早接触熔断，是源于 Spring Cloud 中的 Hystrix，它其实是指当服务不可用的时候，客户端应该采取什么样的措施去应对，实际使用中我们可能会考虑重试、超时、降级等策略。相应地，当服务端在面对来自客户端的异常流量时，就产生了“限流”这个概念，“限流”可以是线程隔离**(线程数 + 队列大小限制)，可以是信号量隔离(设置最大并发请求数目)，可以是限制QPS。这里，我们讨论的主要是第三种，而实现限流的常见算法主要有计数器算法、漏桶算法和令牌桶算法。这里，AspNetCoreRateLimit 这个中间件，则主要使用了计数器算法，并配合 IMemoryCache 和 IDistributedCache 分别实现了基于内存和基于分布式缓存的持久化逻辑。\n源代码解读 首先，使用者通过配置定义了一个或者多个规则，这些规则决定了每个客户端在访问特定终结点时，一段时间内可以访问的最大次数。 RateLimitMiddleware 通过注入的IRateLimitProcessor 来匹配规则，然后依次判断每个规则是否达到了限流条件。一旦达到限流条件，中间件会改变 HTTP 响应的状态码、响应头、返回值，告知使用者已达到最大调用次数。而针对每一种 IRateLimitProcessor ，主要通过ProcessRequestAsync() 方法来实现计数，如果上一次的请求对应的时间戳 + 规则中时间间隔 \u0026gt;= 当前时间，则说明请求没有过期，此时，就需要给这个计数增加1。好了，现在我们来针对 AspNetCoreRateLimit 中的核心部件逐个进行解读。\nRateLimitProcessor RateLimitProcessor，是一个抽象类，实现了IRateLimitProcessor接口，公开的方法有 3 个：ProcessRequestAsync()、IsWhitelisted() 和 GetRateLimitHeaders()。在此基础上，派生出ClientRateLimitProcessor和IpRateLimitProcessor两个子类。两者最大的不同在于，其所依赖的Store不同，前者为IClientPolicyStore，后者IIpPolicyStore，它们都实现了同一个接口IRateLimitStore：\npublic interface IRateLimitStore\u0026lt;T\u0026gt; { Task\u0026lt;bool\u0026gt; ExistsAsync(string id, CancellationToken cancellationToken = default); Task\u0026lt;T\u0026gt; GetAsync(string id, CancellationToken cancellationToken = default); Task RemoveAsync(string id, CancellationToken cancellationToken = default); Task SetAsync(string id, T entry, TimeSpan? expirationTime = null, CancellationToken cancellationToken = default ); } 可以注意到，这些都是典型的基于键-值的存储，所以，不管是基于内存的IMemeryCache，还是基于分布式缓存的IDistributedCache，都可以做到无缝切换。不同的Processor，本质上是它们生成缓存键的方式不同，例如，IpRateLimitProcessor是用一个前缀来表示一组IP，而ClientRateLimitProcessor则是用通过客户端前缀和客户端Id来作为区分：\n// src/AspNetCoreRateLimit/Core/IpRateLimitProcessor.cs public async Task\u0026lt;IEnumerable\u0026lt;RateLimitRule\u0026gt;\u0026gt; GetMatchingRulesAsync( ClientRequestIdentity identity, CancellationToken cancellationToken = default ) { var policies = await _policyStore.GetAsync( $\u0026#34;{_options.IpPolicyPrefix}\u0026#34;, cancellationToken ); var rules = new List\u0026lt;RateLimitRule\u0026gt;(); if (policies?.IpRules?.Any() == true) { // search for rules with IP intervals containing client IP var matchPolicies = policies.IpRules .Where(r =\u0026gt; IpParser.ContainsIp(r.Ip, identity.ClientIp)); foreach (var item in matchPolicies) { rules.AddRange(item.Rules); } } return GetMatchingRules(identity, rules); } // src/AspNetCoreRateLimit/Core/ClientRateLimitProcessor.cs public async Task\u0026lt;IEnumerable\u0026lt;RateLimitRule\u0026gt;\u0026gt; GetMatchingRulesAsync( ClientRequestIdentity identity, CancellationToken cancellationToken = default ) { var policy = await _policyStore.GetAsync( $\u0026#34;{_options.ClientPolicyPrefix}_{identity.ClientId}\u0026#34;, cancellationToken ); return GetMatchingRules(identity, policy?.Rules); } 对于RateLimitProcessor而言，其实现思路是，通过CounterKeyBuilder及其子类来生成计数器标识(CounterId)，然后再通过AsyncKeyLock来实现计数，最终通过IRateLimitCounterStore来实现存储：\npublic virtual async Task\u0026lt;RateLimitCounter\u0026gt; ProcessRequestAsync( ClientRequestIdentity requestIdentity, RateLimitRule rule, CancellationToken cancellationToken = default ) { var counter = new RateLimitCounter { Timestamp = DateTime.UtcNow, Count = 1 }; // 生成CounterId var counterId = BuildCounterKey(requestIdentity, rule); // 基于AsyncLock的计数器 // serial reads and writes on same key using (await AsyncLock.WriterLockAsync(counterId).ConfigureAwait(false)) { var entry = await _counterStore.GetAsync(counterId, cancellationToken); if (entry.HasValue) { // entry has not expired if (entry.Value.Timestamp + rule.PeriodTimespan.Value \u0026gt;= DateTime.UtcNow) { // increment request count var totalCount = entry.Value.Count + _config.RateIncrementer?.Invoke() ?? 1; // deep copy counter = new RateLimitCounter { Timestamp = entry.Value.Timestamp, Count = totalCount }; } } // 计数器存储 // stores: id (string) - timestamp (datetime) - total_requests (long) await _counterStore.SetAsync( counterId, counter, rule.PeriodTimespan.Value, cancellationToken ); } return counter; } AsyncKeyLock 在分析RateLimitProcessor类的时候，我们提到了AsyncKeyLock。对于AsyncKeyLock的实现，我个人认为这是整个中间件的精华，因为这里出现了，和SeamphoreSlim一样经典的东西，这里用到了自旋锁SpinLock。我个人理解，SpinLock 约等于 Interlocked + 内核级别的while。这部分代码本身并不复杂，难就难在这样一个精妙的想法上面。其中，AsyncKeyLockDoorman 这个类的实现，应该是参考了微软的一篇博客—— Building Async Coordination Primitives, Part 7: AsyncReaderWriterLock，因为ReaderLockAsync()、WriterLockAsync()、ReaderRelease() 和 WriterRelease() 这 4 个关键方法完全一样。结合限流这个场景来看，它是典型的“多写”场景，因为如果是相同的请求，那么就会产生相同的计数器标识(CounterId)，所以，这个AsyncLockDoorman这个类所定义的上下文边界，其实是“一读多写”的问题，所以，我们可以注意到，它里面定义了一个“写”操作的队列_waitingWriters，一个“读操作”的_waitingReader：\npublic AsyncKeyLockDoorman(Action\u0026lt;AsyncKeyLockDoorman\u0026gt; reset) { // 多个写入者 _waitingWriters = new Queue\u0026lt;TaskCompletionSource\u0026lt;Releaser\u0026gt;\u0026gt;(); // 单个读取者 _waitingReader = new TaskCompletionSource\u0026lt;Releaser\u0026gt;(); _status = 0; _readerReleaser = Task.FromResult(new Releaser(this, false)); _writerReleaser = Task.FromResult(new Releaser(this, true)); _reset = reset; } 对于“写”操作而言，当一个新的写入者希望进来的时候，如果此时锁没有被别人占用，那么这个新的写入者会获得这个锁，状态值m_status会被修改为-1。反之，如果此时这个锁已经被别人占用了，那么这个新的写入者将会进入等待队列。\npublic Task\u0026lt;Releaser\u0026gt; WriterLockAsync() { lock (_waitingWriters) { if (_status == 0) { _status = -1; return _writerReleaser; } else { var waiter = new TaskCompletionSource\u0026lt;Releaser\u0026gt;(); _waitingWriters.Enqueue(waiter); return waiter.Task; } } } 对于“读”操作而言，我们来思考这样一个问题，什么时候“读”操作会被允许呢？答案是这一时刻没有写入者正在“写”或者“等”，因为如果不这样的话，就会发生我们平常所说的“脏读”，所以，这种情况下，就必须强迫“读取者”去等待写入者“空闲”下来。此时，不难理解ReadLockAsync()的实现：\npublic Task\u0026lt;Releaser\u0026gt; ReaderLockAsync() { lock (_waitingWriters) { if (_status \u0026gt;= 0 \u0026amp;\u0026amp; _waitingWriters.Count == 0) { ++_status; return _readerReleaser; } else { ++_readersWaiting; return _waitingReader.Task.ContinueWith(t =\u0026gt; t.Result); } } } 现在，让我们把视线拉回到AsyncKeyLock，它负责维护一组AsyncKeyLockDoorman，其内部部通过一个字典来维护CounterId和AsyncKeyLockDoorman间的关系。与此同时，为了减少创建·AsyncKeyLockDoorman·带来的性能损耗，它使用一个栈来存储AsyncKeyLockDoorman。每次获取AsyncKeyLockDoorman的过程，本质上就是为指定的Key分配AsyncKeyLockDoorman的过程，同时会更新其引用数RefCount。相应地，释放AsyncKeyLockDoorman的过程，本质上就是减少其引用数RefCount，从字典中移除指定Key，“归还”对象池的过程：\n// GetDoorman() private static AsyncKeyLockDoorman GetDoorman(string key) { AsyncKeyLockDoorman doorman; bool lockTaken = false; try { _spinLock.Enter(ref lockTaken); if (!Keys.TryGetValue(key, out doorman)) { doorman = (Pool.Count \u0026gt; 0) ? Pool.Pop() : new AsyncKeyLockDoorman(ReleaseDoorman); doorman.Key = key; Keys.Add(key, doorman); } doorman.RefCount++; } finally { if (lockTaken) { _spinLock.Exit(); } } return doorman; } // ReleaseDoorman() private static void ReleaseDoorman(AsyncKeyLockDoorman doorman) { bool lockTaken = false; try { _spinLock.Enter(ref lockTaken); if (--doorman.RefCount == 0) { Keys.Remove(doorman.Key); if (Pool.Count \u0026lt; MaxPoolSize) { doorman.Key = null; Pool.Push(doorman); } } } finally { if (lockTaken) { _spinLock.Exit(); } } } RateLimitMiddleware OK，到这里，我们再回过头去看源代码解读这里的内容，大概就可以串起来整合中间件的调用链路，Middleware-\u0026gt;RateLimteProcessor-\u0026gt;AsyncKeyLock-\u0026gt;AsyncKeyLockDoorman，坦白来讲，我一直没能想明白为什么要用SpinLock？难道仅仅是为了减少等待时间、提高性能吗？经过精简，我们发现，整个中间件的Invoke()方法，大致要经历下面几个阶段：\npublic async Task Invoke(HttpContext context) { // 检查限流是否启用 if (_options == null) { await _next.Invoke(context); return; } // 获取用户身份 var identity = await ResolveIdentityAsync(context); // 检查白名单 if (_processor.IsWhitelisted(identity)) { await _next.Invoke(context); return; } //获取限流规则 var rulesDict = new Dictionary\u0026lt;RateLimitRule, RateLimitCounter\u0026gt;(); var rules = await _processor.GetMatchingRulesAsync( identity, context.RequestAborted ); foreach (var rule in rules) { // 获取计数器数目 var rateLimitCounter = await _processor.ProcessRequestAsync( identity, rule, context.RequestAborted ); if (rule.Limit \u0026gt; 0) { // 请求未过期 if (rateLimitCounter.Timestamp + rule.PeriodTimespan.Value \u0026lt; DateTime.UtcNow) { continue; } // 请求过期 if (rateLimitCounter.Count \u0026gt; rule.Limit) { // 各种记日志，告诉调用者多长时间后再重试 var retryAfter = rateLimitCounter.Timestamp.RetryAfterFrom(rule); // ... // 中止请求 await ReturnQuotaExceededResponse(context, rule, retryAfter); return; } } else { // Limit \u0026lt;= 0, 相当于直接不允许放行，中止请求 await ReturnQuotaExceededResponse( context, rule, int.MaxValue.ToString(System.Globalization.CultureInfo.InvariantCulture) ); } // ... } // 设置X-Rate-Limit头 // ... await _next.Invoke(context); } 本文小结 作为 并发限制 这一篇的“姊妹篇”，这一篇的难度相对上一篇堪称“高山仰止”，主要的难点是 SpinLock 、“一读多写”的异步读写锁 AsyncKeyLock 以及 AsyncKeyLockDoorman 。如果大家感兴趣的话，可以去搜索一下 AsyncKeyLock 这个关键字，大家就会发现在好多开源项目 中都能找到类似的代码片段，莫非这是某种神奇的算法吗？阅读源代码，其实是一个无法“立竿见影”的学习方法，有时候我们要通过叙述或者表达来输出我们对待一件事物的看法。这是因为，我们自以为是的“学会”和真正的“学会”，这两者间可能千差万别，就像我最近在用 ABP vNext 搭建一个小项目，阅读文档的时候，眼睛觉得它“学会”了，而实际需要需要扩展或者替换 ABP 的实体/服务的时候。我的手会告诉我，它真的“不会”。做一个知难行易的“调包”侠也许会非常容易，可正因为如此，你要凸显自我就会非常困难。世上的事情，“夫夷以近，则游者众；险以远，则至者少。而世之奇伟、瑰怪，非常之观，常在于险远，而人之所罕至焉，故非有志者不能至也”，哪怕就是增长一下见识呢，你说对吧……\n","date":"2021-03-10T21:52:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2396015802/","slug":"2396015802","tags":["限流",".NET Core","中间件","源码"],"title":"源代码探案系列之 .NET Core 限流中间件 AspNetCoreRateLimit"},{"categories":["编程语言"],"content":"打算开一个新的专栏——源代码探案系列，目的是通过源代码来探索更广阔的技术世界。因为我越来越意识到，我可能缺乏一个结构化的知识体系，虽然处在一个碎片化的时代，从外界接收了大量的信息，可这些碎片化的信息，到底能不能转化为自身可用的知识，其实是需要去认真思考一番。尤其是当我注意到，许多人工作多年，在经历过从“生手”到“熟练工”这种蜕变以后，居然还是会害怕原理性内容的考察。我承认，程序员这个职业更像是一个“手艺人”，可我更想说一句古人的话——君子不器。什么是器呢？“形而上者谓之道，形而下者谓之器”，用一句更直白的话来说，就是“不能知其然而不知其所以然”，这是我一个非CS科班出身的程序员，想去写这样一个专栏的初衷，因为在我看来，“器”是永远学不完的，而“道”虽然听起来虚无缥缈，实则“朝闻道，夕死可矣”。\n作为这个专栏的第一篇博客，我打算从 ASP.NET Core 中的 ConcurrencyLimiter 这个中间件开始。并发是一个爱恨交织的话题，我们喜欢高并发，因为这是程序员跻身高手行列的好机会；我们厌恶并发，因为它引入了多线程、锁、信号量这些复杂的东西。相信大家都曾被并发困扰过，古人云：他山之石，可以攻玉，还有什么比阅读源代码更朴实无华的“学习”呢？你找大牛，大牛可能忙着开会、做PPT；你找同事，同事里可能十个有八个都不知道啊。这个中间件的核心是 IQueuePolicy ，其位于以下位置，它定义了两个核心的方法：TryEnterAsync() 和 OnExit()：\npublic interface IQueuePolicy { ValueTask\u0026lt;bool\u0026gt; TryEnterAsync(); void OnExit(); } 在其默认实现QueuePolicy中，TryEnterAsync()方法，决定着一个请求是会被拒绝还是接受。具体是怎么做呢？它定义了一个最大的并发请求数目，如果实际数超过了最大的并发请求数目，那么请求将会被拒绝。反之，请求将被接受。再仔细看，我们就会发现，它内部使用了SeamphoreSlim和Interlocked，所以，聪明的小伙伴们应该立马会联想到，这两种锁各自的作用是什么。\n其中，Seamphore 是一个 Windows 内核中的一个同步信号量，适用于在多个有限的线程资源中共享内存资源，它就像一个栅栏，本身具有一定的容量，当线程数量达到这个容量后，新的线程就无法再通过，直到某个线程执行完成。SeamphoreSlim是Seamphore优化后的版本，在性能上表现更好一点，更推荐大家使用SeamphoreSlim。\n而 Interlocked 的则是我们熟悉的原子操作，它可以在多个线程中，对共享的内存资源进行原子加或者原子减操作。在这里，Interlocked主要用来控制并发请求数的加和减。如果当前的并发请求数小于最大的并发请求数，表示还可以允许新的请求进来，此时，TryEnterAsync()方法会返回true。如果此时的并发请求数大于最大的并发请求数，则需要对当前请求数进行减操作，此时，TryEnterAsync()方法会返回false。\n一旦搞清楚这一点，结合中间件的代码，我们可以非常容易地想明白,这个并发控制的实现思路。下面是QueuePolicy中TryEnterAsync()和OnExit()两个方法的实现，分别代表了“加锁”和“解锁”两个不同的阶段。某种程度上，Seamphore更像一个水闸，每次可以通过的“流量”是固定的，超出的部分会被直接“拒绝”：\n//“加锁” public ValueTask\u0026lt;bool\u0026gt; TryEnterAsync() { // a return value of \u0026#39;false\u0026#39; indicates that the request is rejected // a return value of \u0026#39;true\u0026#39; indicates that the request may proceed // _serverSemaphore.Release is *not* called in this method, // it is called externally when requests leave the server int totalRequests = Interlocked.Increment(ref _totalRequests); //当前请求次数 \u0026gt; 最大请求次数，返回false表示拒绝 if (totalRequests \u0026gt; _maxTotalRequest) { Interlocked.Decrement(ref _totalRequests); return new ValueTask\u0026lt;bool\u0026gt;(false); } Task task = _serverSemaphore.WaitAsync(); if (task.IsCompletedSuccessfully) { return new ValueTask\u0026lt;bool\u0026gt;(true); } return SemaphoreAwaited(task); } //“解锁” public void OnExit() { _serverSemaphore.Release(); Interlocked.Decrement(ref _totalRequests); } 揭秘 StackPolicy 除了QueuePolicy这种实现以外，官方还提供了StackPolicy的实现。从名称上，我们就能大致区分出它们的不同，因为我相信大家都能拎得清“队列”和“栈”。在实现StackPolicy的过程中，首先会判断是否还有访问请求次数_freeServerSpots，直接返回true，确保中间件可以继续执行。如果_queueLength和我们设置的队列最大容量相同，此时，表示队列已满，需要先取消之前的请求，并保留后来的请求。\npublic ValueTask\u0026lt;bool\u0026gt; TryEnterAsync() { lock (_bufferLock) { if (_freeServerSpots \u0026gt; 0) { _freeServerSpots--; return new ValueTask\u0026lt;bool\u0026gt;(true); } // 队列已满，则取消之前的请求，即_head if (_queueLength == _maxQueueCapacity) { _hasReachedCapacity = true; _buffer[_head].Complete(false); _queueLength--; } var tcs = _cachedResettableTCS ?? = new ResettableBooleanCompletionSource(this); _cachedResettableTCS = null; if (_hasReachedCapacity || _queueLength \u0026lt; _buffer.Count) { _buffer[_head] = tcs; } else { _buffer.Add(tcs); } _queueLength++; // increment _head for next time // 如果_head = 最大队列容量，则_head需要移动至首位 _head++; if (_head == _maxQueueCapacity) { _head = 0; } return tcs.GetValueTask(); } } public void OnExit() { lock (_bufferLock) { if (_queueLength == 0) { _freeServerSpots++; f (_freeServerSpots \u0026gt; _maxConcurrentRequests) { _freeServerSpots--; throw new InvalidOperationException(\u0026#34;OnExit must only be called once per successful call to TryEnterAsync\u0026#34;); } return; } // step backwards and launch a new task if (_head == 0) { _head = _maxQueueCapacity - 1; } else { _head--; } _buffer[_head].Complete(true); _queueLength--; } } 所以，现在，你可以感受到这两种策略的差异了，QueuePolicy是一个水闸，“多”出来的流量会被直接拒绝掉。StackPolicy是一个垂直的管道，每次都是先取消底部的请求，再让新的请求从顶部进来。此时，如果我们再回过头来看 ConcurrencyLimiterMiddleware 这个中间件的实现，就会有种恍然大悟的感觉。\n揭秘 Middleware public async Task Invoke(HttpContext context) { // Make sure we only ever call GetResult once on the TryEnterAsync ValueTask b/c it resets. // 以下代码片段，其实都是调用IQueuePolicy.TryEnterAsync() var waitInQueueTask = _queuePolicy.TryEnterAsync(); bool result; if (waitInQueueTask.IsCompleted) { ConcurrencyLimiterEventSource.Log.QueueSkipped(); result = waitInQueueTask.Result; } else { using (ConcurrencyLimiterEventSource.Log.QueueTimer()) { result = await waitInQueueTask; } } // 当result为true，表示请求被接收，此时，让中间件继续执行 // 切记：调用_queuePolicy.OnExit()来释放锁 if (result) { try { await _next(context); } finally { _queuePolicy.OnExit(); } } else { //这里就是请求被拒绝的情况，修改状态码以及输出错误信息 ConcurrencyLimiterEventSource.Log.RequestRejected(); ConcurrencyLimiterLog.RequestRejectedQueueFull(_logger); context.Response.StatusCode = StatusCodes.Status503ServiceUnavailable; await _onRejected(context); } } 至此，我们就理清了整个中间件的运作机制，ConcurrencyLimiterMiddleware 中注入了IQueuePolicy这个接口，当一个新的请求进来，中间件会调用IQueuePolicy接口的TryEnterAsync()方法，该方法决定了一个请求是会被接受还是拒绝。当请求被接受的时候，中间件会调用_next(context)让请求继续往下走；当请求被拒绝的时候，中间件会修改 HTTP 状态码(503) 和 返回值，保证调用者可以收到错误信息。这就是这个中间件全部的秘密。而如果要在项目中使用这个中间件，同样是非常简单的：\n// 中间件基本法，先注册后使用 // ConfigureServices() // 或者 services.AddQueuePolicy() services.AddStackPolicy(options =\u0026gt; { options.MaxConcurrentRequests = 2; options.RequestQueueLimit = 25; }) // Configure() app.UseConcurrencyLimiter(); 本文小结 这篇博客，主要揭秘了 ASP.NET Core 中的 ConcurrencyLimiter 中间件，这个中间件的主要功能是控制 ASP.NET Core 中的请求并发。作为这个中间件的核心，微软为 IQueuePolicy 接口提供了 QueuePolicy 和 StackPolicy 两种不同的策略实现。其中，QueuePolicy是一个水闸，“多”出来的流量会被直接拒绝掉。StackPolicy是一个垂直的管道，每次都是先取消底部的请求，再让新的请求从顶部进来。对于我们而言，这个中间件最值得学习的地方，其实是SeamphoreSlim和Interlocked，我们经常提到“锁”，其实，“锁”不单单是指 .NET 中Monitor的语法糖，即lock关键字，在同步信号量以及线程同步的相关话题中，我们还会接触到譬如 Mutex(互斥锁)、ReaderWriterLockSlim、Interlocked(原子操作)、SpinLock(自旋锁) 以及 SeamphoreSlim 等等不同的“锁”。除此之外，还有譬如AutoResetEvent、ManualResetEvent 和 ManualResetEventSlim 等等的同步信号量。如果有读者朋友对此感兴趣，可以到 MSDN 上去搜索相关的关键字，能让博主本人和大家从中有所收获，这是我坚持写下去的理由。好了，以上就是这篇博客的全部内容啦，欢迎大家在评论区留言、讨论。\n","date":"2021-03-04T20:13:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/18417412/","slug":"18417412","tags":["并发",".NET Core","中间件","源码"],"title":"源代码探案系列之 .NET Core 并发限制中间件 ConcurrencyLimiter"},{"categories":["编程语言"],"content":"重构我的 独立博客 ，是博主今年的计划之一，这个基于 Hexo 的静态博客，最早搭建于2014年，可以说是比女朋友更亲密的存在，陪伴着博主走过了毕业、求职以及此刻的而立之年。其间虽然尝试过像 Jekyll 和 Hugo 这样的静态博客生成器，可是考虑到模板、插件等周边生态，这个想法一直被搁置下来。直到最近，突然涌现出通过 Blazor 重写博客的想法，尤其是它对于 WebAssembly 的支持，而类似 Vue 和 React的组件化开发模式，在开发体验上有着同样不错的表现。所以，今天这篇博客就来聊聊在重写博客过程中的一点收获，即如何让 Blazor 访问本地的静态文件。\n从内嵌资源说起 首先，我们要引入一个概念，即：内嵌资源。我们平时接触的更多的是本地文件系统，或者是 FTP 、对象存储这类运行在远程服务器上的文件系统，这些都是非内嵌资源，所以，内嵌资源主要是指那些没有目录层级的文件资源，因为它会在编译的时候“嵌入”到动态链接库(DLL)中。一个典型的例子是Swagger，它在.NET Core平台下的实现是Swashbuckle.AspNetCore，它允许使用自定义的HTML页面。这里可以注意到，它使用到了GetManifestResourceStream()方法：\napp.UseSwaggerUI(c =\u0026gt; { // requires file to be added as an embedded resource c.IndexStream = () =\u0026gt; GetType().Assembly .GetManifestResourceStream(\u0026#34;CustomUIIndex.Swagger.index.html\u0026#34;); }); 其实，这里使用的就是一个内嵌资源。关于内嵌资源，我们有两种方式来定义它：\n在 Visual Studio 中选中指定文件，在其属性窗口中选择生成操作为嵌入的资源： 如何定义一个文件资源为内嵌资源\r在项目文件(.csproj)中修改对应ItemGroup节点，参考示例如下： \u0026lt;Project Sdk=\u0026#34;Microsoft.NET.Sdk.Web\u0026#34;\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;EmbeddedResource Include=\u0026#34;_config.yml\u0026#34;\u0026gt; \u0026lt;CopyToOutputDirectory\u0026gt;Always\u0026lt;/CopyToOutputDirectory\u0026gt; \u0026lt;/EmbeddedResource\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/Project\u0026gt; 这样，我们就完成了内嵌资源的定义。而定义内嵌资源，本质上还是为了在运行时期间去读取和使用，那么，自然而然地，我们不禁要问，该怎么读取这些内嵌资源呢？在Assembly类中，微软为我们提供了下列接口来处理内嵌资源：\npublic virtual ManifestResourceInfo GetManifestResourceInfo(string resourceName); public virtual string[] GetManifestResourceNames(); public virtual Stream GetManifestResourceStream(Type type, string name); public virtual Stream GetManifestResourceStream(string name); 其中，GetManifestResourceNames()方法用来返回所有内嵌资源的名称，GetManifestResourceInfo()方法用来返回指定内嵌资源的描述信息，GetManifestResourceStream()方法用来返回指定内嵌资源的文件流。为了方便大家理解，这里我们准备了一个简单的示例：\nvar assembly = Assembly.GetExecutingAssembly(); var resources = assembly.GetManifestResourceNames(); resources.ToList().ForEach(x =\u0026gt; Console.WriteLine(x)); //ConsoleApp.A.B.示例文档.txt //ConsoleApp.A._config.yml var fileInfo = assembly.GetManifestResourceInfo(resources[0]); var fileStream = assembly.GetManifestResourceStream(resources[0]); 此时，我们会发现，内嵌资源都是使用类似A.B.C.D这样的形式来表示资源路径的，因为内嵌资源本身是没有目录层级的。现在，如果我们再回过头去看Swagger的示例，就不难理解为什么会有CustomUIIndex.Swagger.index.html这样一个奇怪的值，因为它对应着实际的物理文件路径，如下图所示，示例代码中输出的资源路径和实际的物理路径存在着对应关系：\n项目中的物理路径与内嵌资源路径对照\rEmbededFileProvider OK，那么在了解了内嵌资源以后，接下来，我们需要关注的是EmbededFileProvider。需要说明的是，在ASP.NET Core中，微软是通过IFileProvider这个接口来解决文件读取问题的，典型的使用场景有静态文件中间件、Rozar模板引擎以及WWWRoot目录定位等等，通常情况下，我们使用PhysicalFileProvider更多一点，它和EmbededFileProvider一样，都实现了IFileProvider接口，所以，ASP.NET Core可以从不同的来源访问文件信息。\n显然，EmbededFileProvider正是为了内嵌资源而生，它在内部使用到了Assembly类中和内嵌资源相关的接口.所以，除了上面的方式，我们还可以通过下面的方式来访问内嵌资源，需要注意的是，使用EmbededFileProvider需要引用Microsoft.Extensions.FileProviders.Embedded，大家可以比较一下这两种方式地差异：\nvar assembly = Assembly.GetExecutingAssembly(); var provider = new EmbeddedFileProvider(assembly); //注意，这里写\u0026#34;.\u0026#34;或者\u0026#34;\u0026#34;都可以 var resouces = provider.GetDirectoryContents(\u0026#34;.\u0026#34;).ToList(); var fileInfo = provider.GetFileInfo(resouces[0]); var fileStream = fileInfo.CreateReadStream(); 除此以外，IFileProvider还有一个最重要的功能，即Watch()方法，它可以监听文件的变化，并返回一个IChangeToken。有没有一种似曾相识燕归来的感觉？没错，博主曾经在 基于选项模式实现.NET Core的配置热更新 这篇文章中介绍过它，它是实现配置热更新的关键。事实上，FileConfigurationSource这个类中有一个Provider属性，而它对应的类型恰好是IFileProvider，这难道是巧合吗？不，仔细顺着这条线，我们大概就能明白微软的良苦用心，我们的配置文件自然是来自文件系统，而考虑到内嵌资源的存在，我们面对的文件系统其实是一个广义的文件系统，它可以是物理文件、内嵌文件、Glob、对象存储(OSS)等等\nBlazor的奇妙缘分 好了，千呼万唤始出来，现在终于要讨论 Blazor 这个话题啦！众所周知，静态博客生成器里主要存在着两种配置，即站点配置和主题配置，Hexo 里甚至还支持从特定文件夹里加载自定义的数据。所以，对于静态博客而言，它需要有从外部加载数据这个特性。我们知道，Blazor 分为服务器和客户端两个版本，两者的区别主要在于 Rozar 模板由谁来渲染，前者相当于服务端渲染(SSR) + SignalR，而后者则是基于 WebAssembly，它可以直接在浏览器中加载。显然，后者更接近我们静态博客生成器的想法。由于 Hexo 使用 Yaml 作为配置语言，所以，为了读取原来 Hexo 博客的配置，参考 实现自己的.NET Core配置Provider之Yaml 这篇博客实现了一个YamlConfigurationProvider。\n在使用的过程中，遇到的问题是，它无法识别配置文件的路径。原因很简单，经过编译的 Blazor 会被打包为 WebAssembly ，而 WebAssembly 在前端加载以后，原来的目录层级早已荡然无存。此时，基于物理文件的 PhysicalFileProvider 将无法工作。解决方案其实大家都能想到，换一种IFileProvider的实现就好了啊！至此，奇妙的缘分产生了：\nclass YamlConfigurationProvider : FileConfigurationProvider { private readonly FileConfigurationSource _source; public YamlConfigurationProvider(FileConfigurationSource source) : base(source) { _source = source; } public override void Load() { var path = _source.Path; var provider = _source.FileProvider; using (var stream = provider.GetFileInfo(path).CreateReadStream()) { //核心问题就是这个Stream的来源发生了变化 var parser = new YamlConfigurationFileParser(); Data = parser.Parse(stream); } } 其实，官方文档中提到过，Blazor 的配置文件默认从 WWWRoot 下的appsettings.json加载，所以，对于像JSON这类静态文件，可以注入HttpClient，以API的方式进行访问。例如，官方文档中推荐的加载配置文件的方式为：\nvar httpClient = new HttpClient() { BaseAddress = new Uri(builder.HostEnvironment.BaseAddress) }; builder.Services.AddScoped(sp =\u0026gt; httpClient); //前方有语法糖，高甜:) using var response = await http.GetAsync(\u0026#34;cars.json\u0026#34;); using var stream = await response.Content.ReadAsStreamAsync(); builder.Configuration.AddJsonStream(stream); 而经过我们这样改造以后，我们还可以这样加载配置：\nbuilder.Configuration.AddYamlFile( provider:new EmbeddedFileProvider(Assembly.GetExecutingAssembly()), path: \u0026#34;_config.yml\u0026#34;, optional:false, reloadOnChange:true ); 一旦这些配置注入到 IoC 容器里，我们就可以纵享无所不在的依赖注入，这里以某个组件为例：\n@using Microsoft.Extensions.Configuration @inject IConfiguration Configuration \u0026lt;div class=\u0026#34;mdui-container-fluid\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;mdui-row DreamCat-content-header\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;mdui-container fade-scale in\u0026#34;\u0026gt; \u0026lt;h1 class=\u0026#34;title\u0026#34;\u0026gt;@Configuration[\u0026#34;title\u0026#34;]\u0026lt;/h1\u0026gt; \u0026lt;h5 class=\u0026#34;subtitle\u0026#34;\u0026gt;@Configuration[\u0026#34;subtitle\u0026#34;]\u0026lt;/h5\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 同样地，对于组件内的数据，在大多数场景下，我们可以这样来处理，还是因为有无所不在的依赖注入：\n@page \u0026#34;/\u0026#34; @layout MainLayout @inject HttpClient httpClient @using BlazorBlog.Core.Domain.Blog; @using BlazorBlog.Web.Shared.Partials; @if (posts != null \u0026amp;\u0026amp; posts.Any()) { foreach (var post in posts) { //这是一个自定义组件 \u0026lt;PostItem Model=post\u0026gt;\u0026lt;/PostItem\u0026gt; } } @code { private List\u0026lt;Post\u0026gt; posts { get; set; } protected override async Task OnInitializedAsync() { posts = await httpClient.GetFromJsonAsync\u0026lt;List\u0026lt;Post\u0026gt;\u0026gt;(\u0026#34;content.json\u0026#34;); await base.OnInitializedAsync(); } } 这里可以给大家展示下尚在开发中的静态博客：\n基于 Balzor 的静态博客\r理论上任何文件都可以这样做，主要是考虑到配置这种信息，用依赖注入会更好一点，这样每一个组件都可以使用这些配置，而如果是以 API 的形式集成，以目前 Blazor 打包以后加载的效果来看，页面会有比较大的“空白期”。我更加疑惑的是，如果 Blazor 打包后的体积过大，那么浏览器自带的存储空间是否够用呢？一句话总结的话， Blazor 是一个写起来非常舒服的框架，可未来是否会像当年的 Sliverlight 一样，这还要看大家对 WebAssembly 的接受程度，可谓是“路漫漫其修远兮”啊……\n本文小结 这篇博客，是博主由一个个“闪念”而串联起来的脑洞，作为一个实验性质的尝试，希望通过 Blazor 的客户端模式(WebAssembly) 实现一个静态博客，而在这个过程中，需要解决 Balzor 读取本地文件的问题，由此，我们引入了这篇博客的主题之一，即：EmbededFileProvider。顺着这条线索，我们梳理了内嵌的文件资源、IFileProvider接口、FileConfigurationProvider、FileConfigurationSource等等一系列看起来毫无关联的概念。事实上，“冥冥之中自有天意”，这一切怎么会毫无关联呢？我们最终从文件系统看到了配置系统，聊到了 Blazor 中的配置问题，这里我们熟悉的依赖注入、配置系统都得以延续下来。其实，单单就解决这个问题而言，完全不值得专门写一篇博客，可从一个点辐射到整个面的这种感悟，在人生的成长中更显得弥足珍贵，希望我们每一个人都能多多跳脱出自己的视角，去努力的看一看这个丰富多彩的世界，在多样性与多元化中去寻找整体上的统一，这是作为技术人员的我，一生都想去探索的哲学。好了，以上就是这篇博客的全部内容啦，欢迎大家在评论中留下你的想法或者建议，谢谢大家！\n","date":"2021-02-23T05:37:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3789745079/","slug":"3789745079","tags":["Blazor",".NET Core","文件","WebAssembly"],"title":"通过 EmbededFileProvider 实现 Blazor 的静态文件访问"},{"categories":["编程语言"],"content":"一直想写篇文章，聊一聊“低代码”这个话题。一方面，“低代码”这个概念确实非常火，其热度丝毫不亚于曾经的“中台”。有人说，2021 年是属于“云原生”的时代，看起来我们每一年都在被技术的“娱乐圈”抛弃，明明连 Kubernetes 都还没有入门呢？人们已然在欢呼雀跃般地声称要抛弃 Docker 。这个世界有时就是如此地魔幻，明明我们生活在一个拥有大量基础设施的时代，我们不必再像前辈们“刀耕火种”一般地去开发软件，可我们的生存空间为什么就越来越狭窄了呢？拼多多事件过去没有多久，腾讯的阳光普照奖再次让“打工魂”觉醒，也许果真像大鱼海棠里设定的一样，人的记忆只有 7 秒。而另一方面，我想结合我最近开发“工作流”的感受，来吐槽下这个看起来美好的“低代码”。也许，对企业而言，引入“低代码”的确能减少研发成本，可博主并不认为，它会降低业务本身的复杂性，如果所有声称“低代码”或者“无代码”的项目，最终依然需要研发人员来作为收场。对此，我想说，对不起，这不是我想要的“低代码”。\n低代码发展现状 或许，一个人成熟的标志就是，在面对一个未知的事物的时候，决不会不由分说地一通吐槽，就像一个人在职场上，你不能永远都只是学会抱怨，相对于抱怨，人们更希望听到的是解决方案。所以，一个人的成长，本质上就是不断学会为自己、为别人找解决方案的过程，前者是为了认识自我，而后者是为了交换资源。所以，在听我吐槽“低代码”前，不妨先一起来看看低代码的发展现状。\n低代码产品发展现状\r国外趋势 有人认为，“低代码”的兴起源于钉钉的低代码应用 易搭 的落地。诚然，巨头企业的每一个动向都引领着整个行业的风潮，可低代码这个概念最早要追溯到 1980 年。彼时，IBM 的快速应用程序开发工具(RAD)被冠以新的名字——低代码，这是低代码这个概念首次面向大众，此后的 40 年里，国外诞生了诸如 Outsystem 、Mendix 、 Zoho Creator 等等的产品，整体发展相对缓慢。直到 2015 年以后，AWS、Google、Microsoft 和 Oracle 等巨头开始入局低代码领域。2018 年，西门子更是宣布以 6 亿欧元收购低代码应用开发领域的领导者 Mendix 、快速应用开发的低代码平台 Outsystem 获得 3.6 亿美金的投资，低代码平台市场开始火爆起来，我们所熟悉的 Power Platform，其实就是微软的低代码开发平台，低代码领域通常都需要大量的积累和研发，需要有 10 到 20 年左右的技术沉淀。\n国内风云 国内的低代码领域，相比国外发展起步较晚，可依然涌现出像牛刀、APICloud、iVX、搭搭云、氚云、简道云、云表、宜搭云等等产品。从整体上而言，这类这类产品基本上都提供了可视化搭建环境，都声称无需编码即可完成业务系统的搭建。其实，从一名程序员的初心出发，我们所做的一切努力都是为了以后不写代码。经常有人问，怎么样可以做到零缺陷、零 Bug ，其实不写代码就好啦！我们并不担心低代码让我们失业，相反地，如果低代码可以消化掉 30% 的垃圾项目，那么，我们将会有更多的时间去做些有意义的事情，而不是在一个“劣币驱逐良币”的市场里，靠着 996 来争个你死我活。而从低代码的商业价值角度来看，Salesforce、Appian、Joget 这三家公司均已上市，Mendix 和 Outsystem 更是估值 10 亿美元以上的独角兽公司，这正是巨头们入局低代码的原因所在。\n低代码领域，目前关注的重点主要集中在：表单生成和处理、工作流生成和管理、办公协作、人力资源、客户关系、ERP 等企业应用上，就如同 SAP 、金蝶、 SCM 等企业软件一样，每一个软件都曾声称能帮助企业解决某一类问题，低代码领域同样遵循“二八原则”，即 80% 的场景，通过定义的方法论、方式、工具集能够实现；而剩下的 20% 的场景或许实现不了，需要使用者通过扩展的方式来自行解决。譬如，针对大多数企业都存在的 CRUD 的需求，通过在线的 Excel 表格来实现基于表的业务驱动。例如 SeaTable 就是这类主打协同工作的产品；针对大多数企业都存在的审批类的需求，则可以通过可视化的工作流设计系统来完成。例如 葡萄城 的 SpreadJS 和 活字格 ，同样可以视为低代码平台，甚至早期的 .NET 开发者被人“黑”只会拖控件，这难道不是广义上的低代码吗？\n低代码产品形态 搞清楚整个低代码的发展现状以后，那么，整个低代码领域主要的产品形态有哪些呢？了解其主要的产品形态，对于我们形成低代码的直观印象非常有帮助。在我看来，主要分为四类：\n表单生成类：以 宜搭云 和 JNPF 为代表，主张通过可视化的设计器来完成页面布局、编排、设计，即所谓的“所见即所得”，类似的还有 iVX。 工作流生成类：以 Mendix 和 Outsystems 为代表，提供组件式的服务，通过编排工作流来实现特定的业务，即通过流程图的方式来实现业务逻辑部分，不同的节点代表不同的功能，不同的线条代表不同的分支。 协同工作类：以 SeaTable 为代表，基于表的业务驱动开发平台，可以以不同的维度管理数据、对数据可视化、共享协作等等，同时具备自动化规则、脚本运行等能力。 服务聚合类：以 APICloud 为代表，基于 API 聚合的组件市场工具，通过流程管理工具，可以管理整个应用的开发周期，从产品、设计开始，到研发测试和运营。 所以，整体而言，低代码产品的核心是表单引擎 和 流程引擎(BPM)，外围支撑是BI 引擎、*协同工作、服务聚合等等，目前，市面上主流的低代码产品，表单引擎和流程引擎(BPM)基本是标配，所以，严格地说起来，上面的分类并不严谨，因为基本上都是混合式的产品形态。下面是部分低代码产品的截图：\n某“低代码”二维码应用\r某“低代码”人力资源管理系统\r某“低代码”可视化搭建系统\r低代码研发痛点 相信大家都知道了，接下来的内容是本文真正的重点。为什么要这样说呢？这主要和博主自身的工作有关系，简单来说，公司需要一个想象中的可视化设计器，业务人员只需要通过拖拽就可以完成业务逻辑的编排，而开发人员则需要负责对外输出组件供业务人员使用。这听起来特别像我们刚刚讨论的第二种产品形态对不对？听起来非常美好对不对？我承认这个想法真的符合潮流、非常的“低代码”。所以，我们前期采用了微软的 Windows Workflow Foundation 框架，使用以后的效果大概是下面这个样子：\nWindows Workflow Foundation 设计器\r多人协作不便 那么，我们在这个过程中到底遇到了哪些问题呢？首先，这种可视化编辑的场景，遇到的第一个问题就是多人协作，如果你使用过腾讯文档、钉钉文档这类在线文档类产品，你应该能领悟到我说的这个点。微软的这个框架是采用XMAL这种格式来储存数据的，虽然理论上可以通过 Git 实现多人协作，实际维护起来表示非常地麻烦，所以，我们最终由单人去维护这些工作流。那么，更广义上的低代码又该如何解决这个问题呢？流程图这种东西，就是一种看起来非常清晰，改起来非常麻烦的东西，就像一条锁链一样，你要不停地断开和接上。\n孱弱的表达能力 其次，是流程图这种表现方式的“表达”问题，就像你如果需要在SQL里表示循环要用到游标一样，这类工作流都无法表达程序三个结构中的循环，更不用说表达力孱弱的表达式啦，所以，这就造成一个非常尴尬的问题，你在流程图里写不了太复杂的表达式，一旦业务人员写不出来，就需要开发人员去写辅助性质的代码，类似正则、字符串插值、字符串处理、格式化等等的函数或者 API 非常缺乏。当然，我最无法忍受的，就是组件与组件间传值的方式，你除了返回 JSON 和写表再没有其它方式，更何况这个 JSON 返回给某个组件了，人家还未必能直接解析直接使用呢？因为编辑器无法绑定这种复杂的数据结构。\n混乱的变量和参数 接下来，我最想吐槽的是，关于全局变量和参数的问题，在流程图中你经常需要各个分支的标志位(Flag)或者是临时变量，然后你就看到了那种“变量满天飞”的混乱局面，简直像极了你刚开始写的代码，你需要顺着每个线条，逐个点开每个组件的属性面板，查看它都使用了哪些参数或者变量，至此，你终于明白了它的数据是如何流动的。从前，乡愁是成千上万行的代码；现在，乡愁是剪不断理还乱的“蜘蛛网”。多年前，我对虚幻引擎(Unreal)的蓝图功能有多么憧憬；多年后，我对这种基于流程引擎的低代码就有多排斥。尤其是，当我需要复用某一段逻辑的时候，我只能小心翼翼地选中节点和线条，然后再拷贝过去。\n动态计算/事件顺序/黑盒子 最后，我参考了一位被 Power Apps 所折磨的朋友的意见，除了上面提到的这些问题， 属性面板或者公式无法使用动态计算的值，类似Vue 里面的计算属性，从实际使用的体验来看，这类以流程引擎和表单引擎为主要卖点的低代码工具，其实都会存在这样的问题，而面对这种问题，一般只能通过trick的手段来解决。同样地，Power Apps 事件顺序的不确定问题，因为低代码实际上是框架提供了某种机制，可以帮你完成某个事情，所以，低代码内部对于使用者来说，完全就是一个黑盒子，譬如 Power Apps 在无网络的环境下使用会卡顿，调试起来非常不便等等。\n本文小结 坦白来讲，这篇博客实在没什么“技术含量”，无非是按照一个月前的计划在整理内容。我对“低代码”持一种中立的态度，作为程序员，我是希望有这样的技术来简化流程，可以让研发人员从枯燥的“增删改查”中解放出来，留出时间去做更多有意义、有价值的事情。当我了解了低代码和零代码的差异以后，我突然明白，我需要的其实是零代码，因为我希望那帮业务人员能自己搞定，这样就不用再来烦我，可经历这段时间的“低代码”，我清醒地认识到，这个想法根本不现实。一来业务人员并不像他们想象的那样，除了不会写代码以外无所不能；二来业务的复杂性满足守恒定律，它永远不会消失，只会从一种形式变成另一种形式。也许，低代码真的能帮企业省不少钱；也许，企业最喜欢做的事情，就是花点小钱招人外包做这种事情。但我依然想告诫开发者们，不要去追逐这些看起来美好的东西，对企业来说，它今天使用 A 技术，明天使用 B 技术，完全无关紧要。可对于个人而言，这个选择显得非常重要。看一看曾经的 SAP 咨询顾问就知道了，如果有一天 SAP 都倒闭了，你掌握着这些只有在 SAP 上能发挥作用的技术有什么用呢？对技术人员来说，学习通用型的知识和技能，永远比把鸡蛋放在一个篮子里要更保险。\n","date":"2021-02-15T12:37:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2637069146/","slug":"2637069146","tags":["低代码","行业","编程","感悟"],"title":"低代码，想说爱你不容易"},{"categories":["生活感悟"],"content":"年前朋友问我，要不要试试 ThoughtWorks 澳洲线的岗位。对于这家号称“世界上面试最难”的公司，多少还是有一点畏惧，直到朋友安慰我说，它们这次有中级的岗位，还是可以试一试的，梦想还是要有的，万一实现了呢？自此，我凑齐了西安. NET圈子里的四大“天花板”公司的面试：葡萄城、活跃网络、奥博杰天、ThoughtWorks ，而对于我来说，亦有幸见识到世界上最难的面试，虽然后来事实证明，这个世界上没有太多的逆袭，可我还是想分享一下我的这次面试经历，因为它让我知道，在过去的两年里，我在哪些方面取得进步，在哪些方面存在不足。当我写下这篇博客的时候，我即将在今年夏天迎来我的29岁，果然我还是希望自己能再努力一点，因为不想让平行世界里的某个人失望。\n面试流程 关于ThoughtWorks 的社招流程，大体上由HomeWork、Pair Programming 和 Face-to-face Interviews 3个部分组成，其中，HomeWork，即家庭作业，原则上给3天时间来完成，不过据说可以向 HR 申请更多的时间来完成。Pair Programming 和 Face-to-face Interviews 通常是安排到同一天来进行的，前者时间为1.5小时，即传说中的结对编程，面试时会有一左一右两名面试官看着你现场写代码。后者时间为1小时，即传说中的技术文化面试，考察技术的深度、广度以及对 Thought Works 敏捷文化的认同感。\nHomeWork 2月18日，下班以后接到HR小姐姐的电话，在明确了我投简历的意向以后，我收到了HR小姐姐的邮件，基本上就是一个家庭作业，三选一提交，需要在三天内完成。我选择了Conference Track Management 这道题目，因为白天要上班，所以，我为此而连续肝了三个晚上。\n坦白说，不同的阶段对这道题目的理解是不同的，在做家庭作业的阶段，你以为这道题考察的是职责分离和设计模式；而等到结对编程的阶段，你终于意识到，这其实是个背包问题。当然，这并不是说我会错了意，考虑到面试官有上帝视角，他们更容易看清楚问题的全貌。或许，面试官想最想看到的，恰恰就是你从冰山一角到目窥全牛这一瞬间的反应。\n当我接到HR小姐姐的通知，这份作业Review通过时，我内心是非常激动的，因为这意味着我获得了去ThoughtWorks面试的“入场券”。可当我事后再以上帝视角去看待这个题目，我内心又变得非常难过，因为无论怎么看这份作业，都会觉得它设计得并不好，尤其是当它引入弹性时间这个因素以后，我一直深陷于如何从Part 1 到 Part 2，是不是按 Part 2 重新设计会更好一点？此时此刻，终于能理解面试官反馈的，关于扩展性方面的问题。\n作业反馈01\r作业反馈02\r作业反馈03\r关于这部分，我个人建议多多关注：\n编程风格：编码规范、项目结构、代码坏味道等。 语言特性：澳洲线岗位需要熟悉 .NET Core，所以，我使用 .NET Core 完成整个项目的编写。 设计模式：选择合适的设计模式，遵循 SOLID 原则。 TDD：一定要有单元测试代码，这一点TW最为看中。如果写的好，一定是加分项。建议遵循AAA原则来编写用例。 程序满足要求：程序一定满足题目要求，可执行，运行结果满足题意，这是最基本的要求。 Pair Programming 提交作业后，等了一周多的时间，1月29日，HR 小姐姐终于联系我了，正如我上文所述，当时听到这个消息非常激动，因为终于有机会去 ThoughtWorks 这家世界上面试最难的公司去看看，ThoughtWorks 西安办公室位于环普产业园，这个地方相信大家都非常熟悉啦！当时算上周末，我给了自己 5 天时间去准备面试，因为我觉得面对 ThoughtWorks 的面试还是要重视一点，虽然后来好多问题都没有被问到。\n结对编程是基本上就是，两个面试官一左一右地坐在你旁边，采用聊天和探讨的方式一起写代码，刚开始本来是用电视投屏“直播”的方式，后来因为 HDMI 接口接触不良的缘故，两位面试官干脆就直接看我电脑屏幕啦！在这个环节，个人感觉解释编码思路花时间太多，重构完有一个用例没有通过。最重要的是，家庭作业阶段的设计不利于现场新需求的开展，所以，这些因素综合起来，导致我结对变成这个部分表现得不好，希望大家引以为戒啊。\n整个结对编程时长为一个半小时(1.5h)，在这段时间，你需要讲解编码思路、完成代码重构和完成现场作业，时间上还是非常紧凑的，回想起那天下午的两个半小时，有种像参加高考的感觉：你以为时间会很长，结果发现时间完全不够用。看起来轻松的氛围下，其实在不经意间考察你的沟通能力、工程能力和学习能力，ThoughtWorks 的面试，往往就是这样的朴实无华且“有趣”……\n对于这部分，我个人建议多多关注：\n工程能力：语言特性、调试能力、设计能力等。像TW非常重视快捷键的使用，频繁使用鼠标会拉低印象分。 沟通能力：善于倾听和表达、以及理解需求的能力，需要你在面试官的引导下完成需求确认，这个阶段一样可以展示你技术的深/广度，但建议最好长话短说。 学习能力：要求你对TDD、敏捷开发等有一定的好奇心，面试官教给你的新东西/思路，能否举一反三、学以致用，我是在重构的过程中得到了面试官的指导，对此我表示感谢。 适应能力：能否以开放的态度接受面试官的重构意见，当意见不一致时，能否有理有据地、自信地表达你的观点，我遇到的问题是，面试官认为我混淆了职责分离和组件依赖。 Face-to-face Interviews 结对编程环节结束以后，正当我还在关注那个失败用例的时候，两位新的面试官就走了进来，就这样，我迎来了那天下午的“技术文化面试”，考虑到天气的原因，我那天穿了一件鬼灭之刃的卫衣就去参加面试了，可那个小房间的闷热还是让人焦躁不安，一杯放凉的白开水，完全不足以缓解那种闷热的感觉。ThoughtWorks 的办公室和大多数外包公司的办公室没有什么区别，不同的是，它的办公室摆满了各种 O\u0026rsquo;Reilly 的动物书，至少在氛围上确实像它对外所展示的那样重视技术。\n在这个环节，我遇到了很多的开放型问题，譬如你经历过的、印象最深刻的项目是什么，你在项目中遇到问题以后都是怎么样去解决的，你所在项目的人员配置、研发流程是什么样的……等等，虽然一开始还是经典的“自我介绍”，可我感觉我在回答这些开放型问题的时候，缺乏一种系统性思考或者某种方法论，它和回答技术问题不同，有时候我们需要层层展开、关注细节，可是在这样的问题上，它需要的是简洁而准确的答案。面试期间，面试官不止一次提示我听清楚她的问题，难道真的是我的沟通能力出了问题吗？\n坦白来讲，这次我准备的很多面试题都没有被问到，我以为至少会问一下.NET Core、微服务 和 DDD 这些东西的。我同样不太明白的，是关于项目经历方面的，为什么面试官会认为，工作中主要负责的内容就是由我一个人单打独斗来的呢？我承认我这几年，性格上收敛了许多，没有了攻击性和对抗性，变成了一个非常随和的人，可我本质上并不是一个喜欢兜售或者推销的人，我并不觉得无法口若悬河是缺乏自信的表现。后来，面试官就考察了一下我的口语，本来就是口干舌燥，说到为什么选择 ThoughtWorks 的时候，大脑有一点卡壳，一边在组织中文，一边在想怎么翻译成英文，还有什么比结结巴巴地说完一段英语更让人难过的呢？\n对于这个部分，我个人建议多多关注：\n系统思考：结合工作经验，不断去提炼类似架构方向、敏捷开发、项目管理方面的内容，不要永远局限在一个点上看待问题，不管是表达还是编程，都采用系统性、结构化的思路来梳理，要做到清晰、准确、完整。 自信：ThoughtWorks 是一家咨询服务公司，所以，很多研发都是顶着咨询师的头衔，个人觉得还是自信一点，会就是会，正常交流，不会的话，就虚心接受，表现出后期愿意去学习的状态。 协作能力：能否影响和带动团队中的人一起学习、成长，ThoughtWorks 盛行学习和分享的文化，你一定听说过它们的技术雷达、洞见。 沟通能力：这体现在你能否和客户正常地沟通、能否和团队成员达成有效的协作，虽然程序员都不大喜欢说话，但你至少应该能传达出正确的声音、能理解来自别人的观点。 动机：对 ThoughtWorks 的意愿性/认同度，为什么会考虑 ThoughtWorks 等这些问题。 面试心得 其实，当天面试一结束，我就知道这次面试大概率是凉了。回去的路上，我和老大哥说了我面试的过程，老大哥说，“让我冷静，要对自己有信心”。果然，第二天下午，收到HR小姐姐的回复，说面试没有通过，看了下面试官反馈的意见，主要是在结对编程过程中重构做得不好，对重构的意义不太明确；其次是面试官觉得我在沟通方面还不够大胆，希望我可以在发展他人方面做出改变。\n听到这话，怎么突然就有种传销的感觉呢？说到影响别人，从12年开始写博客至今，我自认为我的博客还是帮助到了很多人，可能面试官一直觉得我在单打独斗吧，都2021年了，早就不是求伯君、雷军这些前辈们单打独斗写软件的时代啦，所以，果然还是我的表达出现了问题吗？我的朋友们经常批评说我沉迷于技术无法自拔，可我同样见过30多岁怕别人问原理的“中年”程序员，原本这个行业因为门槛低而越来越内卷，而这个圈子里的人又不以技术为重，有太多单纯为了钱而进入这个行业的人。可当整个行业都越来越“体力”劳动的时候，有很多浮躁的人跑来你面前说，技术并不重要类似的话，这个世界到底怎么了呢？\n我想说什么呢？我认为技术因素和非技术因素都很重要，其实写作一直是我练习表达的一种方式。也许，在那些能言善辩的人眼中，我们这些“闷葫芦”都是些内向的、不太会沟通的人吧！这次面试结束以后，我打算找点系统性思考方面的书来看看，继续背单词增加词汇量，利用空闲时间来练习口语。我从来不认为，一个技术人员努力钻研技术有什么不对，因为这是一个技术人员的基本功。沟通能力能做到妙语生花是一种艺术，而我，追求的目标非常简单，即有条理的、清晰的、结构化的表达，我不追求所谓“高情商”的话术，人类时常因为这些模棱两可的字眼而相互误会，因为信息失真，因为信息冗余。当然，此刻我的首要目标是，完成那个家庭作业的重构，因为它写得实在是太糟糕啦！\n","date":"2021-02-09T20:37:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2837181325/","slug":"2837181325","tags":["面试","ThoughtWorks","求职","感悟"],"title":"记一次失败的 ThoughtWorks 面试经历"},{"categories":["编程语言"],"content":"C# 版本历史记录 从 C# 1.0 到 C# 9.0，历代 C# 语言特性一览\r说明：因为Markdown下维护这样复杂的表格有一点麻烦，故，这里以图片形式展示出来，如后续内容有更新，请点击 这里 访问原始笔记链接。为知笔记 的表格渲染在移动端表现不佳，为了获得更好的阅读体验，请在电脑端访问查看。\nC# 版本特性说明 现在是 2021 年，相信 C# 7.0 以前的版本大家都应该没有什么问题，因为像博主这样的 90 后“中年”男人，接触的都是这个版本的 C#。所以，在这里我们主要讲解大家C# 7.0、8.0 以及 9.0 的语法特性。考虑到文章篇幅有限，这里选取的都是博主个人比较喜欢的语法特性，如果这里没有你喜欢的特性，请参考文章末尾的参考链接。如果这里的特性你都不喜欢，请你马上关掉这个网页，愿这个世界：Love \u0026amp; Peace。可能你会感觉到我说话变得小心翼翼起来，因为这个世界上有种叫做“杠精”的生物，当它从我的只言片语里读出那些挫败感的时候，终于有了嘲笑我们这批步入30岁行列的90后的底气，没错，我在最近的博客评论中被读者“嘲讽”了，让暴风雨来得更猛烈一些吧！\nC# 7.0 在 C# 7.0 中，我个人比较喜欢的特性主要有以下几个：元组和弃元、更多的 expression-bodied 成员、out 变量、异步 Main 方法、模式匹配 和 引发表达式。\n元组和弃元 这个概念乍听起来可能会有一点陌生，其实，按我的理解，这就是增强的元组语法，终于可以摆脱Item1、Item2\u0026hellip;\u0026hellip;啦：\n//示例1 (string Alpha, string Beta) namedLetters = (\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); Console.WriteLine($\u0026#34;{namedLetters.Alpha}, {namedLetters.Beta}\u0026#34;); //示例2 var alphabetStart = (Alpha: \u0026#34;a\u0026#34;, Beta: \u0026#34;b\u0026#34;); Console.WriteLine($\u0026#34;{alphabetStart.Alpha}, {alphabetStart.Beta}\u0026#34;); //示例3 int count = 5; string label = \u0026#34;Colors used in the map\u0026#34;; var pair = (count, label); Console.WriteLine(pair); 有一段时间，前端同事总和我吹嘘 ES6 里面的解构多么多么好用！对此，我想说，C# 一样可以解构，假设我们现在有下面的一个方法：\nstatic (string, double, double) GetLocation() { var city = \u0026#34;西安市\u0026#34;; var lat = 33.42d; var lon = 107.40d; return (city, lon, lat); } 这就是简化后的元组的用法，如果是以前，我们还需要返回一个Tuple\u0026lt;string, double, double\u0026gt;。此时，如果我们需要解析城市名称及其经纬度，可以这样做：\n//示例4 (string city, double lon, double lat) = GetLocation(); Console.WriteLine($\u0026#34;{city},({lon},{lat})\u0026#34;); OK，那么什么又是弃元呢？继续以上面的代码为例，如果我不关心经纬度，只需要城市名称又该怎么办呢？人家的方法返回的是一个3元的结果，而我们只需要其中的1元，此时，就有了所谓弃元的概念：\n(string city, _, _) = GetLocation(); Console.WriteLine($\u0026#34;{city}\u0026#34;); 在 C# 中可以使用下划线_来表示要舍弃的元，是为弃元，怎么样？你学会了吗？\n更多的 expression-bodied 成员 这部分同样是经过强化的 Lambda 表达式，之前我们可以在成员函数和 只读属性上使用 Lambda 表达式，而现在，我们可以将其运用在构造函数、终结器以及 get和set 访问器：\n// Expression-bodied constructor public ExpressionMembersExample(string label) =\u0026gt; this.Label = label; // Expression-bodied finalizer ~ExpressionMembersExample() =\u0026gt; Console.Error.WriteLine(\u0026#34;Finalized!\u0026#34;); private string label; // Expression-bodied get / set accessors. public string Label { get =\u0026gt; label; set =\u0026gt; this.label = value ?? \u0026#34;Default label\u0026#34;; } out变量 个人认为，这是一个非常不错的改进，终于不用再单独声明out变量啦：\nif (int.TryParse(input, out int result))\rConsole.WriteLine(result);\relse\rConsole.WriteLine(\u0026#34;Could not parse input\u0026#34;); 异步 Main 方法 顾名思义，Main 方法现在可以支持 async 关键字啦：\nstatic async Task\u0026lt;int\u0026gt; Main()\r{\r// This could also be replaced with the body\r// DoAsyncWork, including its await expressions:\rreturn await DoAsyncWork();\r} 在没有返回值的情况下，可以考虑返回Task:\nstatic async Task Main()\r{\rawait SomeAsyncMethod();\r} 模式匹配 主要是针对 is 和 switch 语句提供了增强的语法。在这里，对于前者来说，我们可以将判断和赋值两个步骤合二为一：\npublic static double ComputeAreaModernIs(object shape) { if (shape is Square s) return s.Side * s.Side; else if (shape is Circle c) return c.Radius * c.Radius * Math.PI; else if (shape is Rectangle r) return r.Height * r.Length; // elided throw new ArgumentException( message: \u0026#34;shape is not a recognized shape\u0026#34;, paramName: nameof(shape)); } 而对于后者来说，主要打破了传统 switch 语句的常量模式：\npublic static double ComputeArea_Version3(object shape) { switch (shape) { case Square s when s.Side == 0: case Circle c when c.Radius == 0: return 0; case Square s: return s.Side * s.Side; case Circle c: return c.Radius * c.Radius * Math.PI; default: throw new ArgumentException( message: \u0026#34;shape is not a recognized shape\u0026#34;, paramName: nameof(shape)); } } 引发表达式 这个主要是针对 throw 关键字的增强，当我看到微软的文档的时候，我突然意识到，这个语法其实我用了很久啦！\n//场景A：条件运算符 string arg = args.Length \u0026gt;= 1 ? args[0] : throw new ArgumentException(\u0026#34;You must supply an argument\u0026#34;); //场景B：Null合并运算符 public string Name { get =\u0026gt; name; set =\u0026gt; name = value ?? throw new ArgumentNullException( paramName: nameof(value), message: \u0026#34;Name cannot be null\u0026#34;); } //场景C：Lambda表达式 DateTime ToDateTime(IFormatProvider provider) =\u0026gt; throw new InvalidCastException(\u0026#34;Conversion to a DateTime is not supported.\u0026#34;); 以上，就是 C# 7.0 中我个人比较喜欢的语法特性。需要了解所有 C# 7.0 语法特性的小伙伴们，则可以参考这里：C# 7.0 - C# 7.3 中的新增功能。\nC# 8.0 在 C# 8.0 中，我个人比较喜欢的特性主要有以下几个：默认接口方法、异步流、索引和范围。\n默认接口方法 关于这个，我觉得有点多此一举，如果一定要有一个默认行为，那你用继承来实现不就好啦，接口本来就是用来实现的啊摔！\npublic class ChineseSayHello : ISayHello { public string Who { get; set; } } public interface ISayHello { private const string DefaultPersopn = \u0026#34;Anumouse\u0026#34;; string Who { get; set; } void SayHello() { Who = DefaultPersopn; Console.WriteLine($\u0026#34;Hello, {Who}\u0026#34;); } } 在上面这个例子里，ChineseSayHello没有实现SayHello()方法不影响编译，因为ISayHello有默认实现，可正因为如此，SayHello()方法属于ISayHello，不属于ChineseSayHello：\n//正确，可以编译 var sayHello = new ChineseSayHello() as ISayHello; sayHello.SayHello(); //错误，无法编译 var sayHello = new ChineseSayHello(); sayHello.SayHello(); 异步流 该特性可以看作是IEnumerable\u0026lt;T\u0026gt;的一个延伸，即IAsyncEnumerable\u0026lt;T\u0026gt;，主要有下面三个属性：\n它是用 async 修饰符声明的。 它将返回 IAsyncEnumerable。 该方法包含用于在异步流中返回连续元素的 yield return 语句。 下面是一个来自微软官方的基本示例：\n//生成异步流 public static async System.Collections.Generic.IAsyncEnumerable\u0026lt;int\u0026gt; GenerateSequence() { for (int i = 0; i \u0026lt; 20; i++) { await Task.Delay(100); yield return i; } } //枚举异步流 await foreach (var number in GenerateSequence()) { Console.WriteLine(number); } 和异步流相关的一个概念是：异步可释放，即 System.IAsyncDisposable，这个可以参考：实现 DisposeAsync 方法。\n索引和范围 关于这个，我们换一种说法，可能大家就能接受啦！是什么呢？答案是：切片。切片语法博主经常在 Python 中使用，想不到有生之年居然可以在 C# 里用到这个语法。不过，这个语法糖怎么看都不甜啊，因为没那味儿！\nvar words = new string[] { // index from start index from end \u0026#34;The\u0026#34;, // 0 ^9 \u0026#34;quick\u0026#34;, // 1 ^8 \u0026#34;brown\u0026#34;, // 2 ^7 \u0026#34;fox\u0026#34;, // 3 ^6 \u0026#34;jumped\u0026#34;, // 4 ^5 \u0026#34;over\u0026#34;, // 5 ^4 \u0026#34;the\u0026#34;, // 6 ^3 \u0026#34;lazy\u0026#34;, // 7 ^2 \u0026#34;dog\u0026#34; // 8 ^1 }; //取最后一个元素 Console.WriteLine($\u0026#34;The last word is {words[^1]}\u0026#34;); //获取第一个元素到第三个元素 var quickBrownFox = words[1..4]; //获取倒数第一个元素到倒数第二个元素 var lazyDog = words[^2..^0]; //获取全部元素 var all = words[..]; //获取开始到第三个元素 var firstPhrase = words[..4]; //获取结束到倒数第二个元素 var lastPhrase = words[6..]; 看起来这些东西在 Python 里都有啊，到底是哪里除了问题呢？我觉得更多的是符号上的不同吧， ^ 这个符号除了表示指数的意思以外，还有按位进行异或运算的意思，所以，这个语法糖加进来以后就会显得相当混乱，而 .. 这个符号显然没有 : 写起来方便啊，所以，虽然 C# 从 C# 8.0 开始有了切片语法，可这不是我想要的切片语法啊！\n以上，就是 C# 8.0 中我个人比较喜欢的语法特性。需要了解所有 C# 8.0 语法特性的小伙伴们，则可以参考这里：C# 8.0 中的新增功能。\nC# 9.0 在 C# 9.0 中，我个人比较喜欢的特性主要有以下几个：Record、顶级语句、模式匹配增强。\nRecord record 是 C# 9.0 中提供的一个新的关键字，地位上等同于 class 和 struct，中文翻译为：记录类型。这是一种引用类型，它提供合成方法来提供值语义，从而实现相等性。 默认情况下，记录是不可变的。简而言之，record 是不可变的引用类型。你可能会说，我们为什么要搞这么一个类型出来呢？难道 class 不香吗？\n我觉得如果要回答这个问题，可以借鉴 DDD 中的实体 和 值对象这两个概念。实体 通常都有一个唯一的标识并且在整个生命周期中具有连续性，这一类角色通过 class 来实现一直都工作得很好。例如，每一个 User 都会有一个唯一的UserId ，我们使用 UserId 来判断其相等性。而 值对象 则是指那些没有唯一的标识、不可变的、通过属性来判断相等性。例如，我们有一个地址 Address，它由省、市、区、县和详细地址组成，那么，问题来了，如果两个 Address 的省、市、区、县和详细地址都相同，这两个 Address 是不是同一个地址呢？常识告诉我们：不会，因为它们是不同的实例。\n这就是 record 出现的原因，对于上面的这个问题，我们可以来解决：\nrecord Address { public string Province { get; set; } public string City { get; set; } public string District { get; set; } public string County { get; set; } } var addr1 = new Address() { Province = \u0026#34;陕西省\u0026#34;, City = \u0026#34;西安市\u0026#34;, District = \u0026#34;雁塔区\u0026#34; }; var addr2 = new Address() { Province = \u0026#34;陕西省\u0026#34;, City = \u0026#34;西安市\u0026#34;, District = \u0026#34;雁塔区\u0026#34; }; Console.WriteLine($\u0026#34;addr1 == addr2：{addr1 == addr2}\u0026#34;); 想想以前我们是怎么做的呢？是不是要写类似下面这样的代码：\nif (addr1.Province == addr2.Province \u0026amp;\u0026amp; addr1.City == addr2.City) { //属性太多啦，我就不一个一个地比较啦，懂得都懂 } 所以，这就是 record 存在的意义。除此之外呢，这个关键字更多的是语法层面上的，实际上从编译出来的 IL 来看，它本质上依然是一个类，并且它是不可变的。定义记录类型时，编译器会合成其他几种方法：\n基于值的相等性比较方法 替代 GetHashCode() 复制和克隆成员 PrintMembers 和 ToString() 那么，你可能还会有疑问，假如我定义了两个不同的记录类型，它们都拥有相同的属性成员，如果按值相等来判断的话，岂不是这两个不同的记录类型变成相同的了？这么重要的问题，微软怎么可能没有想到呢？编译器会合成一个 EqualityContract 属性，该属性返回与记录类型匹配的 Type 对象。在这里，微软再一次发挥了元组的威力，对于上面定义的地址，我们可以继续使用解构语法：\n(province, city, district, county) = addr1; 当然，我相信哪怕到2090年，这个世界上依然会有“杠精”：你说这玩意儿不能变？我就想变怎么办？答案是使用with语法：\npublic record Person { public string LastName { get; } public string FirstName { get; } public Person(string first, string last) =\u0026gt; (FirstName, LastName) = (first, last); } var person = new Person(\u0026#34;Bill\u0026#34;, \u0026#34;Wagner\u0026#34;); Person brother = person with { FirstName = \u0026#34;Paul\u0026#34; }; // 修改FirstName的副本 Person clone = person with { }; // 空集副本 好了，关于记录类型就先为大家介绍到这里，更详细的说明可以参考这里：使用记录类型。\n顶级语句 顶级语句，这个又是一个听起来非常模糊的概念对不对？ 大家可以看一下这篇文章：26 种不同的编程语言的 “Hello World” 程序。怎么样，在众多解释型的语言中，C#、Java 甚至 C++ 的 “Hello World” 是不是都看起来有一点臃肿？\n好了，现在可以梦想成真啦！\nusing System; Console.WriteLine(\u0026#34;Hello World!\u0026#34;); 如果觉得这样还显得臃肿，可以省略 using 部分：\nSystem.Console.WriteLine(\u0026#34;Hello World!\u0026#34;); 当然啦，一个项目里显然只能有一个文件可以使用顶级语句，你可以理解为这些代码运行在一个看不见的Main()方法中，而Main()方法显然只能有一个，相比下来，Python 就自由多啦，不过if __name__ == '__main__'的老梗就不再这里展开啦！\n模式匹配增强 感觉微软在模式匹配的道路上越走越远啊，说好的语法糖呢？这简直是毒药，7.0 里面眼花缭乱的switch都还没学会呢！\npublic static bool IsLetter(this char c) =\u0026gt; c is \u0026gt;= \u0026#39;a\u0026#39; and \u0026lt;= \u0026#39;z\u0026#39; or \u0026gt;= \u0026#39;A\u0026#39; and \u0026lt;= \u0026#39;Z\u0026#39;; public static bool IsLetterOrSeparator(this char c) =\u0026gt; c is (\u0026gt;= \u0026#39;a\u0026#39; and \u0026lt;= \u0026#39;z\u0026#39;) or (\u0026gt;= \u0026#39;A\u0026#39; and \u0026lt;= \u0026#39;Z\u0026#39;) or \u0026#39;.\u0026#39; or \u0026#39;,\u0026#39;; if (e is not null) { // ... } 以上，就是 C# 9.0 中我个人比较喜欢的语法特性。需要了解所有 C# 9.0 语法特性的小伙伴们，则可以参考这里：C# 9.0 中的新增功能。\n参考链接 C# 发展历史 C# 7.0 - C# 7.3 中的新增功能 C# 8.0 中的新增功能 C# 9.0 中的新增功能 C# 版本与 .NET 版本对应关系以及各版本的特性 C# 语言历史版本特性（C# 1.0到C# 8.0汇总） ","date":"2021-02-01T22:36:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3918433482/","slug":"3918433482","tags":[".NET","CSharp","语言","总结"],"title":"从 C# 1.0 到 C# 9.0，历代 C# 语言特性一览"},{"categories":["数据分析"],"content":"几天前， Catcher Wong 大佬告诉我，他终于写完了 2020 年的年终总结。在看完大佬的年终总结以后，我有一种“前浪被后浪拍死在沙滩上”的感觉，正如当学生时都看“别人家的孩子”，工作以后看的都是“别人的年终总结”。我们的生活，其实就是由“别人”和“我们”交织在一起，而更多的时候，是成为“大多数”的“我们”，去关注成为“少数”的“别人”。我想说的是，世间万物互为装饰，就像卞之琳在《断章》里写道，“明月装饰了你的窗子，你装饰了别人的梦”。即便一个人在历史长河中，尤如一叶飘泊不定的孤舟在波涛中摇荡，可每一朵浪花都曾以自己的方式美丽过，所以，看“别人”的生活，联想“我们”的生活，这便是我同 2020 告别的一种方式，为此，博主决定抓取 2020 年全年 366 天的微博热搜，通过可视化的方式来串联起 2020 年的回忆。\n热搜抓取 首先，我们来考虑微博热搜的数据来源。 微博 官方提供了一个热搜排行榜的页面：https://s.weibo.com/top/summary，可惜这个网站只支持查看当天的热搜，显然这无法满足我们的需求。在搜索引擎的帮助下，找到了两个网站，它们分别是：微博时光机 和 热搜神器。经过一番权衡，决定选择页面结构更简单一点的 微博时光机 。\n通过抓包，可以快速获得两个关键的接口，它们分别是 获取 timeId 接口 和 获取历史热搜接口。\nFirefox抓包示意图\r简单来说，我们指定一个日期，第一个接口会返回timeId。接下来，通过这个timeId调用第二个接口就可以获得热搜数据。仔细观察的话，第一个接口传递的data参数像是一个BASE64加密后的结果，尝试解密后发现我的猜想是对的，加密前的内容如下：\n[\u0026#34;getclosesttime\u0026#34;,[\u0026#34;2021-01-20T23:08:02\u0026#34;]] 这意味着我们只需要改变这里的日期就可以啦，因此，我们的思路无非就是从 2020 年 1 月 1 日开始，依次请求热搜接口获取数据，直到 2020 年 12 月 31 日。这里想顺便吐槽下这个网站的接口设计，居然清一色地全部用数组来返回结果，难道是为了省掉这几个字段来节省流量吗？\n接口返回值说明-1\r接口返回值说明-2\r吐槽归吐槽，这里我们可以非常容易地写出对应的代码，由于日期和timeId的对应关系是固定的，为了减少后续的请求数量，我们使用MongoDB来对数据进行持久化。同样地，抓取热搜采用了类似的方式，因为历史热搜同样是确定的数据，这里只给出关键的代码，并不代表你可以无脑地复制、粘贴：\n# 获取指定日期对应的timeId def get_timeId(date, cookie): cacheKey = date.strftime(\u0026#39;%Y-%m-%d\u0026#39;) records = list(store.find(TABLE_TIME_ID, {\u0026#39;date\u0026#39;: cacheKey})) if len(records) \u0026gt; 0: return records[0][\u0026#39;timeId\u0026#39;] else: data = \u0026#34;[\\\u0026#34;getclosesttime\\\u0026#34;,[\\\u0026#34;{d}\\\u0026#34;]]\u0026#34;.format(d=cacheKey) data = base64.b64encode(data.encode(\u0026#39;utf-8\u0026#39;)) url = \u0026#39;https://www.weibotop.cn/apis/androidrouter/?versioncode=1\u0026amp;=\u0026amp;data=\u0026#39; + str(data, \u0026#39;utf-8\u0026#39;) data = request(url, cookie) timeId = json.loads(data)[0] store.insert(TABLE_TIME_ID, [{\u0026#39;date\u0026#39;: cacheKey, \u0026#39;timeId\u0026#39;: timeId }]) return timeId # 获取指定timeId对应的热搜 def get_weibo_trending(timeId, cookie): records = list(store.find(TABLE_TRENDING, {\u0026#39;timeId\u0026#39;: timeId})) if len(records) \u0026gt; 0: return records else: url = \u0026#39;https://www.eecso.com/test/weibo/apis/currentitems.php?timeid=\u0026#39; + timeId data = request(url, cookie) data = json.loads(data) trendings = list(map(lambda x:{\u0026#39;title\u0026#39;:x[0], \u0026#39;createdDate\u0026#39;:x[1], \u0026#39;updatedDate\u0026#39;:x[2], \u0026#39;rank\u0026#39;:int(x[3])}, data)) for trending in trendings: trending[\u0026#39;timeId\u0026#39;] = timeId trending[\u0026#39;href\u0026#39;] = \u0026#39;https://s.weibo.com/weibo?q=\u0026#39; + trending[\u0026#39;title\u0026#39;] trending[\u0026#39;createdDate\u0026#39;] = datetime.datetime.strptime(trending[\u0026#39;createdDate\u0026#39;], \u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) trending[\u0026#39;updatedDate\u0026#39;] = datetime.datetime.strptime(trending[\u0026#39;updatedDate\u0026#39;], \u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) store.insert(TABLE_TRENDING, trendings) return trendings 至此，我们就完成了微博热搜数据的抓取工作！\n热搜分析 好了，在采集到这些热搜数据以后，我们就可以着手准备热搜数据的分析工作啦！其实，目前这份热搜数据挺简陋的，它只有热搜话题、上榜时间、更新时间以及话题热度这样四个关键字段。而作为辅助，我们增加了热搜话题的链接，如果后续需要更详尽的信息，可能需要从这里来寻找突破口。在今天这篇博客里，我们主要从下面四个维度来分析和挖掘 2020 年全年的微博热搜：\n全年热搜热度分析 首先，我们要分析的是全年热搜的热度。何谓热度呢？我个人认为，可以从话题的使用频率和话题的持续时间两个方面来考虑，即，一个话题转发或者参与的人越多，话题持续的时间越长，则认为该话题越“热”。例如罗翔老师说 2020 年进入了“全民网课”的时代，因为“网课”是一个热门话题，而当时的背景则是因为疫情原因无法上学(班)，一时间远程会议/办公/教育变得炙手可热。所以，分析全年的热搜热度，可以让我们去关注每个月都发生了什么事情，而这样，我们就有了和这个世界建立联系的思绪，想想当时的你在做什么，心里又作何感想，这会是一件非常有趣的事情：\n2020全年微博热搜热度变化趋势\r首先，我们看到的是：2020 全年微博热搜热度变化趋势。通过这张图，我们可以注意到：在 3 月份左右国内疫情得到控制以后，大家都渐渐地回归到日常的工作和生活中，相应地，人们在社交媒体上的关注是逐渐下降的，直到 7 月份以后逐渐开始出现回升。我个人认为可能与下面这件事情有关，**第一，是腾讯公司因为一份虚假合同而起诉老干妈的事件；第二、因为疫情而姗姗来迟的高考推迟到了 7 月 7 日和 7 月 8 日这两天；第三、张一山、宋妍霏、阚清子、宋茜等一众明星频频登上热搜榜。**对于前两个因素，可以覆盖整个 7 月份的大多数时间段；对于第三个因素，更多的是从微博这样一个泛娱乐化的平台的属性去考虑，还有什么比吃明星的瓜更开心的事情吗？再往后，我们都知道，迎来了美国大选，不管这场大选闹出了多少风波，此时此刻，终于尘埃落定。\n2020全年微博热搜数量变化趋势\r接下来，我们看到的是：2020 全年微博热搜数量变化趋势。通过这张图，我们可以注意到：热搜数量的变化趋势整体上是吻合热搜热度的变化趋势的，两者的“低谷”都出现在 7 月份，不同的是热搜数量的变化要更为“缓和”一点，这可能和新浪微博的热搜榜单有一定的关系，不知道是不是因为微博的推荐算法，决定了每个月“吃瓜”的次数是差不多的，可如果没有算法来约束这一切，完全由用户及其粉丝自行主导，这会不会演变成现实版的美国大选呢？我特别心疼那位新浪微博的研发小哥@丁振凯，人生中三次遭遇热搜引发的“宕机”：结婚时撞上鹿晗公布恋情，海外度假时撞上双宋官宣、老婆待产撞上华晨宇承认和张碧晨未婚生有一女，简直永远都在扩容的路上，被誉为“史上最惨新浪程序员”一点都不冤枉啊……\n全年热搜情感分析 李诞在 2020 年年底策划了一期反跨年晚会，从头到尾都是脱口秀这种“语言类”节目，在这期节目里，有人以毛不易的“歌词”调侃了 2020 年大家的心境变化，从“像我这样优秀的人”到“消愁”，有时候打脸就是这么的猝不及防。坦白来说，我有段时间过得特别“丧”，“丧”到要靠《当幸福来敲门》来打鸡血。那么，整个 2020 年“活”在热搜里的人们的心态变化又是怎么样的呢？所以，接下来我们通过 SnowNLP 对 2020 年全年的热搜话题的情感倾向进行分析，到底大家是过得“积极”还是“消极”呢，让我们一起拭目以待，为了达到更好的效果，博主提前对 SnowNLP 进行了训练，因为 SnowNLP 自带的语料库主要是电商评论，与我们此刻的场景多少有一点差异。\n2020全年微博热搜情感变化趋势\r果然，2020 年真的是“丧”到家啦，366 天里平均置信概率在 0.5 以上的堪称寥寥啊。我有时候会想，我们常常希望在感情中有足够的安全感，希望对方可以“懂”我。诚然，我可以从一个人的朋友圈、微博去分析对方的情感变化，可身为人类的我们，并不是冷冰冰的计算机器。多年后，当我懊恼于曾经没有进行及时的沟通的时候，我静静地坐在电脑面前，你说这些字里行间没有透露出足够充足地信息，可我们依然有办法去反映过去一年里的喜怒哀乐。世事无常，每天都开开心心地面对，固然是心向往之，而生命中更朴实无华地大多数时刻，其实就是此刻如白开水一般索然无味，如果理性的思维最终还是要输给感性的直觉，我希望我可以两者兼有之，今年可能要在外地一个人度过春节啦，希望我的心情可以超过 0.5 呢……\n全年热搜词云分析 其实，在做这个分析的时候，我一直在想，也许“新冠”或者“疫情”这样的字眼会成为 2020 年的共同记忆吧！至少对博主这样即将步入中年的 90 后而言，这场疫情留下的深刻记忆丝毫不亚于 08 年的汶川地震。可转念间又安慰自己道，相比国外愈演愈烈的疫情，我们在三月份左右的时候就基本得到了控制，如果说互联网是没有记忆的，人们对这一切应该会遗忘地非常快，就像这热搜榜上的话题，简直是“你方唱罢我登场”。可惜，互联网的确是有记忆的，即使过去了整整一年，这一切还是通过数据被挖掘出来。这里，我们通过结巴分词对热搜话题进行分词，再通过这些关键词来绘制词云。对于这个结果，突然就变得感性起来，可能这就是所谓的“冥冥之中自有天意”吧，甚至对于 2021 年来说，疫情目前依然是人们关注的热搜话题：\n2020年全年微博热搜关键词词云\r全年热搜人物分析 曾经在知乎上读到过这样一句话，“人们宁愿去关心一个蹩脚电影演员的吃喝拉撒和鸡毛蒜皮，而不愿了解一个普通人波涛汹涌的内心世界”，这句话如果放到 2020 年的语境中，或许就是，人们在危难的时候会突然关心“国士无双”，而在安稳的时候则会更关注“娱乐八卦”，考虑到新浪微博是借鉴新浪博客的“名人效应”而起家，所以，我更关心在过去一年里有哪些人都登上过热搜。说实话，我挺怀念某位七十多岁高龄的老人，他和我奶奶差不多同龄，在这个“丧”如此普遍的年代，他带给了我们多少欢乐啊，虽然我预感到会有许多明星靠着“否认”、“道歉”、“心疼”、“回应”、“声明”等等字眼而登上热搜，可我还是想知道答案啊……\n2020全年微博热搜上榜人物分析.png\r果然，“说曹操曹操到”，2020 年以压倒性优势多次登上微博热搜的，居然真的是前美国总统特朗普。虽然说这位美国前总统喜欢孜孜不倦地发推特，史称“推特治国”，可在一个某明星代孕风波快速令“拼夕夕”事件烟消云散的社交平台上，这位老人能频频进入我们的视野，大概就能说明过去一年里国际形势的风起云涌。我们嘲笑他为“懂王”，甚至“亲切”地称之为“川建国”同志。有一段时间里，好像每一个人都觉得自己比这位老人更会做总统；同样地，好像每一个人都觉得自己比张小龙更懂得微信。我无意讨论政治相关的东西，可我依然感谢这位老人在疫情期间带给我们的欢乐，因为我并不觉得，他像媒体眼中的那样滑稽而愚蠢，一个能在商人、明星和总统多重身份中切换自如的人，无论如何会都有他的过人之处，疫情这件事情，换一个人来当这个总统未必会做得比他好。回过头来看，他在 2020 年都做了哪些事情呢？\n2020年特朗普的微博热搜\r本文小结 其实，在规划这篇博客的时候，我一直在想，该以一种什么样的心态去回顾 2020，因为当我看着“别人”的年终总结的时候，总有一种难以言说的失落感。一方面，时间在不经意间匆匆逝去，身边的一切都在刻意地想你强调着“物是人非”。而另一方面，你需要去面对诸如买房、结婚这种所谓“某某年龄应该去做的事情”。当我看到身边的同事，整天坐在一起讨论的无外乎是房子、车子、股票等一切所谓“投资”的事情的时候，我时而会觉得他们有一点枯燥，就是那种我们曾经都不愿意成为的“中年人”。等翻过年，我即将迎来我的 29 岁，可令人心动的 Offer 里的“背水辉”一样的被嫌弃的年纪，而距离 IT 行业所谓的“35 岁”门槛还剩下年时间。\n虽然给自己订了几个目标，可有时候难免会感到懈怠，尤其是当你意识到你再无法抓住某一样东西的时候，或许，你唯一的能做的事情，就是让自己永远不要忘记吧！写数据挖掘相关的内容，不管是在数据的抓取还是分析阶段，都需要投入大量的精力去试验，结合实际去调整写作的方向，在这篇博客中甚至还花了大量时间去训练 SnowNLP。“悟已往之不谏，知来者之可追”，2021 年 flag 我在心里记下来，我不想写出来，因为我怕到时候脸会疼，如果大家觉得这篇博客对你有帮助，欢迎点赞和收藏，如果可以一键三连，那就更好啦！2020，再见！\n","date":"2021-01-24T22:36:47Z","image":"https://i.loli.net/2021/01/26/gCcHX7vWlwsZhnI.jpg","permalink":"https://qinyuanpei.github.io/posts/2758545080/","slug":"2758545080","tags":["Python","热搜","微博","可视化"],"title":"通过 Python 分析 2020 年全年微博热搜数据"},{"categories":["数据分析"],"content":"最近一段时间，博主感觉到了某种危机感，或者说是每一个不再年轻的人都会面对的问题，即，怎么面对来自更年轻的“后浪”们的压力，自打国内 IT 行业有了 35 岁这个不成文的“门槛”以后，年轻的“后浪”们仿佛有了更多将“前浪”们拍死在岸上的勇气，我辈忍不住要叹一声后生可畏啊！我认识的 Catcher Wong 正是这样一位大佬，此君虽然比我小三岁，可在技术的广/深度以及经验的丰富程度上，足以令我这个”老人”汗颜，单单 EasyCaching 这一项，就令人望尘莫及啦！我看着他的时候，一如当年 Wesley 大哥看着我的时候，可能这就是某种轮回，姑且执浊酒一杯，致我们终将老去的青春。\n不正经的 Kimol 君 关注Kimol 君，最早源于他在我博客里留言，作为礼尚往来，我回访了他的博客，然后发现此人人如其名，非常的”不正经”，他的博客访问量出奇地高，在 CSDN 里写博客多年，深知现在不比从前有运营梦鸽和大白两位小姐姐帮忙推荐到首页，普通的内容很少有机会拥有这样的曝光机会，而像 郭霖 这种从 10 年前后开始写移动开发系列博客的“大神”或者是以图形学为主要写作方向的 诗人“浅墨” ，在通篇都是干货的情况下，长期保持着不错的人气。\n这萌萌哒求赞的表情我是做不来的\r起初，我以为此君的流量来自于标题党，譬如《学会这招，小姐姐看你的眼神将不一样》 和 《震惊！小伙竟然用 Python 找出了马大师视频中的名场面》这几篇，非常像 UC 编辑部和微信公众号的风格。我是一个擅长学习的人，主动去借鉴了他博客中的优点，比如尝试使用轻松、幽默的文风，在文章开头放入目录，适当“蹭”热点等等，我甚至专门致敬了一篇博客： 《厉害了！打工人用 Python 分析西安市职位信息》。而整个 1 月份，我就只有一篇博客流量高一点，就这还不是特别正经的”技术”博客，而此君的流量则是一个又一个的 1w+ ，可我实在想不通，一个不到 100 行的 Python 脚本，真就值得花那么多的流量，真就值得上百条的评论吗？这里放张图大家感受一下：\n不知道该说什么好\r仔细研究了他博客里评论的风格，发现有大量类似“夸夸群”风格的评论，就是那种读起来确实像对方读过了你的文章，可实际一想就觉得这是那种“放之四海而皆准”的话。我最近知道了一位大佬的博客，我惊奇地发现，此君居然在上面留过言，我顺着大佬的博客继续找，发现一个非常有意思的事情，此君曾经给我留言过的内容，居然出现在了别人的博客底下，而从这篇博客的评论里继续找，你会发现好像有一个团队专门在做这种事情，互相点赞、互相评论，甚至这些留言都是来自一篇博客都没有的”新人”，至此，基本可以断定，此君“不讲武德”，用作弊的方式在刷流量！当然，他自己都承认了：\n作弊实锤\r年轻人不讲”武德” OK，既然现在的年轻人都把心思用到这种事情上，作为一个老年人，必须要让他知道什么叫“耗子尾汁”，我们技术做一点正经事儿不行吗？其实，博客园的博客质量相比 CSDN 是要高出许多的，而正因为如此，CSDN 在全力转在线教育/课程以后，博客这个板块就再无往日的“生气”，如果每个人都像他一样，天天跑别人底下刷评论，发一点不痛不痒的话，甚至是推广某个小圈子里的 QQ 群，那真正优质的内容又如何能被大家看到呢？博主曾经加过这样的 QQ 群，你以为是交流技术的群吗？其实是为了推广某个 Python 课程，博主本想交流一下“半泽直树”，然后就被群管理员给删除了！此君大概是抓取 Python 板块排名靠前的博客，通过程序来刷存在感。\n对此，我想说，这玩意儿用 Selenium + Python 简直和闹着玩一样，毕竟在了解网页结构以后，直接上 jQuery 操作 DOM 即可，甚至连抓包都不需要，不信你看：\nimport requests from bs4 import BeautifulSoup import fake_useragent import os, json, time, random from selenium import webdriver from selenium.webdriver.support.ui import Select from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC class Proxy: def __init__(self, profile): os.environ[\u0026#39;TEMPDIR\u0026#39;] = os.path.join(os.path.abspath(\u0026#39;.\u0026#39;), \u0026#39;profile\u0026#39;) firefoxProfile = webdriver.FirefoxProfile(profile) fireFoxOptions = webdriver.FirefoxOptions() self.driver = webdriver.Firefox( firefox_options=fireFoxOptions, firefox_profile=firefoxProfile ) # 批量点赞 def vote(self, urls): \u0026#39;\u0026#39;\u0026#39; 对指定的一组博客地址批量进行点赞 \u0026#39;\u0026#39;\u0026#39; for url in urls: self.driver.get(url) time.sleep(3) flag = self.driver.execute_script(\u0026#34;return $(\u0026#39;#is-like-span\u0026#39;).text().trim()\u0026#34;) == \u0026#34;已赞\u0026#34; if not flag: self.driver.execute_script(\u0026#34;$(\u0026#39;#is-like-span\u0026#39;).click()\u0026#34;) time.sleep(1) # 批量收藏 def collect(self, urls): \u0026#39;\u0026#39;\u0026#39; 对指定的一组博客地址批量进行收藏 \u0026#39;\u0026#39;\u0026#39; for url in urls: self.driver.get(url) time.sleep(3) flag = self.driver.execute_script(\u0026#34;return $(\u0026#39;#is-collection\u0026#39;).text()\u0026#34;) == \u0026#34;已收藏\u0026#34; if not flag: self.driver.execute_script(\u0026#34;$(\u0026#39;#is-collection\u0026#39;).click()\u0026#34;) self.driver.execute_script(\u0026#34;$(\u0026#39;.csdn-collection-submit\u0026#39;).click()\u0026#34;) time.sleep(1) # 批量关注 def follow(self, urls): \u0026#39;\u0026#39;\u0026#39; 对指定的一组博客地址批量进行关注 \u0026#39;\u0026#39;\u0026#39; for url in urls: self.driver.get(url) time.sleep(3) flag = \u0026#39;已关注\u0026#39; in self.driver.execute_script( \u0026#34;return $($(\u0026#39;.toolbox-list\u0026#39;).children()[6]).find(\u0026#39;a\u0026#39;).text().trim()\u0026#34; ) if not flag: self.driver.execute_script(\u0026#34;$($(\u0026#39;.toolbox-list\u0026#39;).children()[6]).find(\u0026#39;a\u0026#39;).click()\u0026#34;) time.sleep(1) # 批量一键三连 def iloveyuou(self, urls): \u0026#39;\u0026#39;\u0026#39; 对指定的一组博客地址批量进行三连 \u0026#39;\u0026#39;\u0026#39; for url in urls: self.driver.get(url) time.sleep(3) self.driver.execute_script(\u0026#34;$($(\u0026#39;.toolbox-list\u0026#39;).children()[7]).find(\u0026#39;p\u0026#39;).click()\u0026#34;) time.sleep(1) # 批量留言 def comment(self, urls, texts): \u0026#39;\u0026#39;\u0026#39; 对指定的一组博客地址批量进行评论 \u0026#39;\u0026#39;\u0026#39; for url in urls: self.driver.get(url) time.sleep(3) text = random.choice(texts) self.driver.execute_script(f\u0026#34;$(\u0026#39;#comment_content\u0026#39;).text(\u0026#39;{text}\u0026#39;)\u0026#34;) self.driver.execute_script(f\u0026#34;$(\u0026#39;.btn-comment\u0026#39;).click()\u0026#34;) # CSDN对评论间隔有要求，那就再睡一会儿 time.sleep(5) # 热门文章 def hotRank(self, channel): \u0026#39;\u0026#39;\u0026#39; 抓取某个话题下的热门文章 \u0026#39;\u0026#39;\u0026#39; url = f\u0026#39;https://blog.csdn.net/phoenix/web/blog/hotRank?page=0\u0026amp;pageSize=25\u0026amp;child_channel={channel}\u0026#39; headers = { \u0026#39;User-Agent\u0026#39;:\u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko/20100101 Firefox/84.0\u0026#39;, \u0026#39;Cookie\u0026#39;:\u0026#39;uuid_tt_dd=10_220300310-1611402514139-727015; dc_session_id=10_1611402514139.129755; dc_sid=37a633fe075b2698beeae6fb9c306fb4\u0026#39; } response = requests.get(url, headers=headers) response.raise_for_status() data = json.loads(response.text) if (data[\u0026#39;code\u0026#39;] == 200 and data[\u0026#39;message\u0026#39;] == \u0026#39;success\u0026#39;): return list(map(lambda x:x[\u0026#39;articleDetailUrl\u0026#39;], data[\u0026#39;data\u0026#39;])) else: return [] 我们都知道，在通常情况下，Selenium 每次运行时都会打开一个浏览器， 可这个浏览器呢，相对于我们平时使用的浏览器来说是“独立”的，因为细心的朋友一定会发现，虽然我们在 Chrome 或者 Firefox 中早已登录过了某个网站，可此时此刻，当 Selenium 启动浏览器窗口的时候，我们发现这个网站依然是需要登录的。为什么要讨论这个问题呢？因为如果我们希望对 CSDN 实现“一键三连”，登录这一步是必不可少的步骤。那么，有没有一种办法，可以让 Selenium 共享我们本地浏览器中的 Cookie 信息呢？因为只要有了 Cookie，我们就可以专注于实现“一键三连”这部分。相信大家都看过上面的代码啦，答案当然是有的，我们为其指定一个配置文件的路径即可：\n# 设置 Firefox 配置文件 # 默认路径：C:\\\\Users\\\\\u0026lt;User\u0026gt;\\\\AppData\\\\Roaming\\\\Mozilla\\\\Firefox\\\\Profiles\\\\XXXX.default # 参考链接：https://support.mozilla.org/zh-CN/kb/用户配置文件 profile_dir = \u0026#39;C:\\\\Users\\\\YuanPei\\\\AppData\\\\Roaming\\\\Mozilla\\\\Firefox\\\\Profiles\\\\xypbnthd.default-release\u0026#39; firefoxProfile = webdriver.FirefoxProfile(profile_dir) fireFoxOptions = webdriver.FirefoxOptions() webdriver.Firefox(firefox_options=fireFoxOptions, firefox_profile=firefoxProfile) # 设置 Chrome 配置文件 # 默认路径：C:\\\\Users\\\\\u0026lt;User\u0026gt;\\\\AppData\\Local\\Google\\Chrome\\User Data profile_dir = \u0026#39;C:\\\\Users\\\\YuanPei\\\\AppData\\Local\\Google\\Chrome\\User Data\u0026#39; chromeOptions = webdriver.ChromeOptions() chromeOptions.add_argument(\u0026#39;user-data-dir=\u0026#39; + os.path.abspath(profile_dir)) webdriver.Chrome(chrome_options=chromeOptions) 这样，Selenium 启动的就不再是一个“裸”的浏览器，我们平时使用的各种配置、插件等等都会被原封不动地加载到 Selenium 中，这其中同样了我们的 Cookie，所以，当大家看到我的代码的时候，会发现这里没有做任何登录相关的事情，这其实是在用“时间”换取技术实现的“简单”，因为要额外加载大量的信息，所以，Selenium 启动的时候会变得缓慢起来，经过博主自己测试，Firefox 启动大概需要 1 分钟左右，熬过这 1 分钟接下来就是坦途啦！\n其实，除此以外，关于登录这个问题，我们还有一种方案是对 Cookie 进行持久化。简而言之，就是利用 Selenium 的get_cookies() 和 add_cookie() 这一组 API，第一次打开某个网站的时候，首先人为地或者模拟登录，此时可以获得 Cookie 并对其进行序列化，而访问那些需要登陆的资源时，则可以对 Cookie 进行反序列化并将其加载到 Selenium 环境中，基本的代码示例如下：\n# 保存Cokie到本地文件 cookies = driver.get_cookies() with open(\u0026#34;cookies.txt\u0026#34;, \u0026#34;w\u0026#34;) as fp: json.dump(cookies, fp) # 从本地文件加载Cookie with open(\u0026#34;cookies.txt\u0026#34;, \u0026#34;r\u0026#34;) as fp: cookies = json.load(fp) for cookie in cookies: driver.add_cookie(cookie) 下面来做一个简单的演示， CSDN 有一个类似微博热搜的 博客榜单。这里，我们会从中筛选前 5 的博客链接来进行“一键三连”操作。与此同时，博主选取了一部分这些年轻人们喜欢用的评论，就在刚刚，我在这篇博客 《第十二届蓝桥杯模拟赛 Python 组（第一期）》 下面再次发现 Kimol 君 的身影，年轻人你不讲武德啊！我就想起了《开讲啦》里面惹恼易中天教授的那位学生，一个人的文章写得好，大家愿意去读去看，这自然是好事，可正因为梦鸽和大白这些小姐姐们都不在了，这个社区的内容质量完全由点赞、评论、收藏数这些因素在左右着，作为一名博客作者，我更希望别人能真的在读完我的文章后，或者能找出我考虑不周的地方，或者可以就某一个问题深入讨论一番，我发现社区里都喜欢动辄加别人 QQ 或者微信，可如果这种毫无意义地灌水的评论，这一切又有什么意义呢？\n# 如果你经常收到这些评论，千万不要“飘” # 你觉得这些话都是真心的吗？ comments = [ \u0026#39;代码之路任重道远，愿跟博主努力习之。\u0026#39;, \u0026#39;学起来，头秃的那种~\u0026#39;, \u0026#39;写的太好了，很实用\u0026#39;, \u0026#39;好文！希望博主以后多多分享哈！\u0026#39;, \u0026#39;哇，好棒啊，崇拜的小眼神，欢迎回赞，回评哦~~~\u0026#39;, \u0026#39;收藏从未停止，学习从未开始。\u0026#39;, \u0026#39;大佬，看了您的文章，内心的仰慕之情油然而生，赶紧给大佬点个赞！\u0026#39;, \u0026#39;太赞了！666666\u0026#39; ] proxy = Proxy(\u0026#39;C:\\\\Users\\\\YuanPei\\\\AppData\\\\Roaming\\\\Mozilla\\\\Firefox\\\\Profiles\\\\xypbnthd.default-release\u0026#39;) # 热搜前5名的文章 urls = proxy.hotRank(\u0026#39;python\u0026#39;)[:5] # 批量留言，刷存在感 proxy.comment(urls, comments) # 一键三连 proxy.iloveyuou(urls) 当然啦，像我这里提供的关于点赞(vote)、收藏(collect)、关注(follow)等等方法，同样是可以使用的，这里就不再一一例举啦！本身都是基于 jQuery 来操作 DOM，理解上应该没有太大难度，虽然我不大喜欢用 jQuery 写业务代码，可对于爬虫这种事情，自然是越简单越好，因为我不想再去学一门操作 DOM 的语言：XPath， 而关于 Selenium 驱动的安装、配置等细节，可以参考博主的这篇文章：\n作为技术宅的我，是这样追鬼滅の刃的\n博主最近新开了一个付费专栏：Python 数据挖掘系列，主要介绍关于爬虫、PyECharts、结巴分词、Pandas、Matplotlib、SnowNLP、OpenCV 等数据挖掘相关内容，如果大家喜欢或者感兴趣，欢迎订阅。好了，以上就是这篇博客的全部内容啦，欢迎大家在评论区，就你对于这篇博客的想法或者意见进行讨论，再次谢谢大家！如果 Kimol 君 恰好读至此处，最好能一键三连，我权当作为你打广告的广告费啦，哈哈！\n","date":"2021-01-19T22:35:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3148958651/","slug":"3148958651","tags":["爬虫","自动化","Python","Selenium"],"title":"基于 Python 和 Selenium 实现 CSDN 一键三连自动化"},{"categories":["数据分析"],"content":"最近博主在优化一个爬虫程序，它是博主在 2017 年左右刚接触 Python 时写下的一个程序。时过境迁，当 Python 2.X 终于寿终正寝成为过去，当博主终于一只脚迈进 30 岁的大门，一切都来得猝不及防，像一阵龙卷风裹挟着回忆呼啸而去。和大多数学习 Python 的人一样，博主学习 Python 是从写爬虫开始的，而这个爬虫程序刚好是那种抓取“宅男女神”的程序，下载图片无疑是整个流程里最关键的环节，所以，整个优化的核心，无外乎提升程序的稳定性、提高抓取速度。所以，接下来，我会带大家走近 Python 中的多线程编程，涉及到的概念主要有线程(池)、进程(池)、异步I/O、协程、GIL等，而理解这些概念，对我们而言是非常重要的，因为它将会告诉你选择什么方案更好一点。想让你的爬虫更高效、更快吗？在这里就能找到你的答案。\n楔子 现在，假设我们有一组图片的地址(URL)，我们希望通过requests来实现图片的下载，为此我们定义了Spider类。在这个类中，我们提供了getImage()方法来完成下载这个动作。我们可以非常容易地写出一个“单线程”的版本，但这显然这不是我们今天这篇博客的目的。此时，我们来考虑一个问题，怎么样实现一个“多线程”的版本？\nclass Spider: def __init__(self, urls): self.session = requests.session() self.session.headers[\u0026#39;User-Agent\u0026#39;] = fake_useragent.UserAgent().random self.session.headers[\u0026#34;Referer\u0026#34;] = \u0026#34;https://www.nvshens.org\u0026#34; self.urls = urls # 下载图片 def getImage(self, url, fileName, retries=5): try: print(f\u0026#39;{threading.currentThread().name} -\u0026gt; {url}\u0026#39;) response = self.session.get(url, allow_redirects=False, timeout=10, proxies=None ) response.raise_for_status() data = response.content imgFile = open(fileName, \u0026#39;wb\u0026#39;) imgFile.write(data) imgFile.close() return True except : while retries \u0026gt; 0: retries -= 1 if self.getImage(url, fileName, retries): break else: continue 线程与线程池 既然提到了线程，我们会非常自然地想到 Thread 和 ThreadPool ，而这几乎是所有编程语言里都有的通用型概念。可是，Python 中的多线程其实是一种“假”的多线程，这又从何说起呢？答案是全局解释器锁(GIL)，原来在设计 Python 解释器的时候，为了保证同时只有一个线程在运行，引入了这样一个锁，你可以类比游戏开发时主循环的概念来辅助理解。那为什么又说 Python 中的多线程是一种“假”的多线程呢？这是因为它没法发挥出多核的优势，每个线程在执行前都要先获得 GIL ，这就导致一个问题，即使你有多个核心，线程永远只能用到其中一个核，因为多线程在 Python 中只能交替执行。以一言蔽之， Python 中 I/O 密集型任务相比 CPU 密集型任务更能充分发挥多线程的好处。所以，像爬虫这种和网络打交道的事物，是非常适合使用多线程来提高效率的。在这里，我们我们要介绍的是 Thread 和 ThreadPool 以及 ThreadPoolExecutor。\nThread 首先，我们需要了解的是，Python 中的 Thread ，实际上先后有thread和threading两种模块，它们的关系有一点像 .NET 里的Thread和Task，考虑到thread的使用频率非常低，这里我们更推荐大家使用threading，它提供了更高级的、完全的线程管理。例如，我们现在面临的这个“多线程”下载的问题，使用threading的话可以这样解决：\n# 使用Thread下载 def downloadByThread(self): threads = [] for index in range(0, len(self.urls)): thread = threading.Thread( target=self.getImage, args=(self.urls[index], f\u0026#39;{str(index)}.jpg\u0026#39;,) ) threads.append(thread) for thread in threads: thread.setDaemon(True) thread.start() 可以注意到，当我们需要构造一个线程时，只需要指定target和args两个参数，其中，前者是指线程执行的方法，后者是指传递给线程所执行的方法的参数。当我们需要启动线程时，只需要调用线程的start()方法，而通过setDaemon()方法则可以设置一个线程为守护线程。关于守护线程，这里简单说明一下，一旦一个线程被设置为守护线程，那么，只要线程执行的方法中存在等待时间譬如time.sleep(1)，此时等待时间下面的代码都不会再执行。如果线程中执行的方法是一个耗时的操作，此时，我们还可以通过join()方法来阻塞主线程，以确保主线程再子线程执行完后再结束。除了这种函数式的使用方法以外，我们还可以通过继承Thread类并重写其run()方法的方式，对于这一点可以参考官方文档中的线程对象。\n使用Thread下载\rThreadPool 对于线程，我们都知道它是作为一种系统资源而存在的，所以，和这个世界上的大多数资源一样，无法供我们肆意地挥霍和浪费。在 .NET Core 中对象池(Object Pool)的使用 这篇博客中，我曾经大家介绍过“对象池”这种设计，和这篇博客中所提到的原理一样，线程池相对于普通线程而言多了一种可复用的可能性，这意味着我们可以用有限的线程来下载可能无限多的图片资源。在 Python 中我们使用 threadpool 模块来实现线程池的功能，需要注意的是这是一个第三方的模块。下面，我们来一起看看具体的使用方法：\n# 使用ThreadPool下载 def downloadByThreadPool(self, poolSize=3): count = len(self.urls) # 构造线程参数 args = [] for index in range(0, count): args.append((None, {\u0026#39;url\u0026#39;: self.urls[index], \u0026#39;fileName\u0026#39;: f\u0026#39;{str(index)}.jpg\u0026#39;})) # 线程池大小 if count \u0026lt; poolSize: poolSize = count # 构造线程池 pool = threadpool.ThreadPool(poolSize) requests = threadpool.makeRequests(self.getImage, args) [pool.putRequest(req) for req in requests] pool.wait() 在这里，我们声明了一个指定大小的线程池，通过一个方法getImage()和一组参数args来构造“请求”，再将这些请求全部放进线程池里，此时，线程池会自动等待这些“请求”执行完毕。这里唯一比较难理解的，可能是如何构造参数args，尤其是当被执行的方法需要传递多个参数的时候。其实这里有两种传参的方式，第一种是按数组来解构，此时我们可以写[(['',''], None), (['',''], None)]；而第二种则是按字典来解构，此时我们可以写[(None, {'url':'', 'fileName':''}), (None, {'url':'', 'fileName':''})。两者的区别主要在None的位置，不知道大家有没有发现规律。这里我们准备了张图片，而线程池最大线程是 3 个，理论上某个线程会被重复使用，实际结果又是如何呢？\n使用ThreadPool下载\rThreadPoolExecutor 对于ThreadPoolExecutor，相信不用我多说什么，你就能知道它是做什么的吧，这就是博主反复提及的命名规范的问题。简而言之，Python 在 concurrent.futures中为我们提供了 ThreadPoolExecutor 和 ProcessPoolExecutor 两个高级接口，它们都继承自同一个抽象类Executor，它可以让我们在线程池或者进程池中异步地执行回调函数，属于官方提供的标准的“线程池”和“进程池”模块，下面，我们来一起看看具体的使用方法：\n# 使用ThreadPoolExecutor下载 def downloadByThreadPoolExecutor(self, poolSize=3): count = len(self.urls) # 构造线程参数 args = [] for index in range(0, count): args.append({\u0026#39;url\u0026#39;: self.urls[index], \u0026#39;fileName\u0026#39;: f\u0026#39;{str(index)}.jpg\u0026#39;}) # 线程池大小 if count \u0026lt; poolSize: poolSize = count # 构造线程池 pool = ThreadPoolExecutor(max_workers=poolSize) tasks = [] for arg in args: task = pool.submit(self.getImage(arg[\u0026#39;url\u0026#39;], arg[\u0026#39;fileName\u0026#39;]), arg) tasks.append(task) wait(tasks, return_when=ALL_COMPLETED) # tasks = pool.map(lambda arg:self.getImage(arg[\u0026#39;url\u0026#39;], arg[\u0026#39;fileName\u0026#39;]), args) 这里需要注意的是，submit()方法和map()方法的区别，前者相当于声明线程后并不立即执行，故而，需要wait()方法来等待所有任务执行结束；而后者则相当于声明线程并理解执行，故而，返回值实际是每一个任务执行的结果的集合，这里就隐隐有一点 .NET 中 Task 的味道啦！同样地，我们给了一个最大线程数：3，它能否得到和threadpool 类似的结果呢？我们拭目以待：\n使用ThreadPoolExecutor下载\r进程与进程池 看到这里，可能有读者朋友会忍不住吐槽，博主你三十岁不到，怎么越来越糊涂了啊，你这博客标题明明写的是多线程，怎么写着写着就写到进程上来了呢？其实，这里是紧接着 GIL 这个话题来讲的。既然 Python 中的多线程更适合 I/O 密集型的任务，那么，是不是说 Python 不适合处理 CPU 密集型的任务呢？答案是否定的，我们这里将多进程理解为并行就会更容易想明白一点。我们都知道操作系统可以同时执行多个任务，而这每一个任务其实就是一个进程，而每个进程内又可以同时执行多个子任务，这每一个子任务其实就是一个线程。这样说，我们或许就能明白，这意味着，如果我们的确需要并行地去处理某些任务，进程(池)或许是个不错的选择。同样地，这里介绍的是，Process、ProcessPool 和 ProcessPoolExecutor。\nProcess 关于进程，我个人感觉比线程要更好理解一点，因为不论是 Windows 下的任务管理器，亦或者是我们经常听到的“杀进程”，它都不算是一个特别陌生或者抽象的概念，而线程这种东西呢，大概是只有程序员会关注，同时爱之弥深、恨之弥切的一种事物。庆幸的是，在 Python 中线程与进程在代码的编写上是非常相似的，这里我们需要用到的是multiprocessing模块，下面，我们来一起看看 Python 中的进程的的使用方法，你会发现只需要改一下threading.Thread()这部分：\n# 使用Process下载 def downloadByProcess(self): process = [] for index in range(0, len(self.urls)): proc = multiprocessing.Process( target=self.getImage, args=(self.urls[index], f\u0026#39;{str(index)}.jpg\u0026#39;,) ) process.append(proc) for proc in process: proc.start() 此时，我们可以得到下面的结果，可以发现它都是在主线程上运行：\n使用Process下载\rProcessPool 既然有“线程池”，又怎么能少得了进程池呢？同样地，它位于multiprocessing模块中，通过apply()方法来执行某个任务，下面是一个基本的示例：\n# 使用multiprocessing.Pool()下载 def downloadByProcessPool(self, poolSize=3): count = len(self.urls) # 构造线程参数 args = [] for index in range(0, count): args.append((self.urls[index], f\u0026#39;{str(index)}.jpg\u0026#39;, )) # 线程池大小 if count \u0026lt; poolSize: poolSize = count # 构造线程池 pool = multiprocessing.Pool(poolSize) for arg in args: pool.apply(self.getImage, arg) 有朋友难免会好奇“进程池”和“线程池”有什么不一样，我想，下面这张图会告诉你答案：\n使用multiprocessing.Pool()下载\rProcessPoolExecutor 和 ThreadPoolExecutor 类似，我们还可以使用 ProcessPoolExecutor 来实现“进程池”：\n# 使用ProcessPoolExecutor下载 def downloadByProcessPoolExecutor(self, poolSize=3): count = len(self.urls) # 构造线程参数 args = [] for index in range(0, count): args.append({\u0026#39;url\u0026#39;: self.urls[index], \u0026#39;fileName\u0026#39;: f\u0026#39;{str(index)}.jpg\u0026#39;}) # 线程池大小 if count \u0026lt; poolSize: poolSize = count # 构造线程池 pool = ProcessPoolExecutor(max_workers=poolSize) for arg in args: pool.submit(self.getImage(arg[\u0026#39;url\u0026#39;], arg[\u0026#39;fileName\u0026#39;]), arg) 可以看到，“进程池”中的代码都是在主线程上执行的，这一点和multiprocessing.Pool()完全一致：\n使用ProcessPoolExecutor下载\r协程与异步 I/O 其实，如果单单从 I/O 密集型和 CPU 密集型两种场景而言，这篇博客到这里就差不多应该结束啦！不过呢，博主好奇 Scrapy 这个爬虫框架的实现原理，发现它是基于 Twisted 这样一个异步网络框架，考虑到目前为止，我们通过 requests 来下载图片都是采用同步的方式，除了任务调度上的优化以外，任务本身还存在一定的优化空间，所以，这里就顺带着一起整理出来，这里主要结合 asyncio 和 requests 来对 Python 中关于异步 I/O 、协程等的使用方法进行演示和说明。\nasyncio asyncio 是用来编写 并发 代码的库，使用 async/await 语法，它是构建 I/O 密集型和高层级 结构化 网络代码的最佳选择。它提供了类似并发地执行协程、网络 I/O 和进程间通信(IPC)、事件循环等等的能力，例如，我们可以通过下面的代码来创建和使用协程:\nimport asyncio async def say_after(what, delay): await asyncio.sleep(delay) print(what) async def main(): await say_after(\u0026#39;你好\u0026#39;, 1) await say_after(\u0026#39;Hello\u0026#39;, 2) # 方式1 # Python 3.7 + asyncio.rum(main()) # Python 3.7 - asyncio.get_event_loop().run_until_complete(main()) 参考官方文档，我们还可以使create_task()方法来创建asyncio的并发任务：\n# 方式2 async def main(): # Python 3.7 + task1 = asyncio.create_task(say_after(\u0026#39;你好\u0026#39;, 1)) task2 = asyncio.create_task(say_after(\u0026#39;Hello\u0026#39;, 2)) # Python 3.7 - task1 = asyncio.get_event_loop().(say_after(\u0026#39;你好\u0026#39;, 1)) task2 = asyncio.get_event_loop().(say_after(\u0026#39;Hello\u0026#39;, 2)) await task1 await task2 asyncio.get_event_loop().run_until_complete(main()) 这是因为 Python 中的协程、任务 和 Future 都是可等待对象，故而，凡有 async 处皆可 await ，果然，主流编程语言的最终走向是如此的一致啊，回头想想 .NET 中 Thread 、 ThreadPool 、 Task 的进化历程，是不是有种“天下大势，分久必合”的感觉呢？\nrequests 好了，当我们对异步 I/O、协程有了一个基本的了解以后，我们就可以考虑结合着 requests 来做一点小小的尝试，我们大多数时候写的 requests 相关的代码，基本上都是博主这里getImage()类似的画风，最多再加上流式传输(Stream) 和 iter_content。为了配合异步 I/O 来使用，我们这里需要定义一个异步的方法getImageAsync()，一起来看下面的代码：\nasync def getImageAsync(self, url, fileName, retries=5): try: print(f\u0026#39;{threading.currentThread().name} -\u0026gt; {url}\u0026#39;) headers = { \u0026#39;User-Agent\u0026#39;: fake_useragent.UserAgent().random, \u0026#39;Referer\u0026#39;: \u0026#34;https://www.nvshens.org\u0026#34; } future = asyncio.get_event_loop().run_in_executor( None, functools.partial(requests.get, url, headers=headers) ) response = await future data = response.content imgFile = open(fileName, \u0026#39;wb\u0026#39;) imgFile.write(data) imgFile.close() return True except: while retries \u0026gt; 0: retries -= 1 if await self.getImageAsync(url, fileName, retries): break else: continue 接下来，我们还需要定义downloadAsync()方法，这里我们使用了create_task()方法：\nasync def downloadAsync(self): count = len(self.urls) for index in range(0, count): url = self.urls[index] fileName = f\u0026#39;{str(index)}.jpg\u0026#39; await asyncio.get_event_loop().create_task(self.getImageAsync(url, fileName)) 此时，我们可以在入口函数中这样调用：\nspider = Spider(urls) loop = asyncio.get_event_loop() task = loop.create_task(spider.downloadAsync()) loop.run_until_complete(task) 看看结果：\n![异步I/O + Requests 实现并行下载]](https://i.loli.net/2021/01/16/mhtcT78dswjgERa.png)\n这里，针对本文中提到的各种方法，博主做了一个简单对比：\n项目 时间 Thread 0:00:01.789790 ThreadPool 0:00:00.134065 ThreadPoolExecutor 0:00:06.510224 Process 0:00:00.100506 ProcessPool 0:00:11.046871 ProcessPoolExecutor 0:00:02.226153 AsyncIO 0:00:04.096083 本文小结 本文从线程(池)、进程(池)和异步 I/O 三个方面探讨和尝试了多线程编程在 Python 爬虫领域的简单应用。其实，除了以上这些优化的思路以外，我们还可以借助队列(Queue)这类数据结构来改善现有方案的设计，大家可以注意到我给getImage()方法增加了错误重试的机制，这同样是为了增强爬虫程序的健壮性，而关于这个错误重试机制，考虑通过装饰器来进行改良则又是一个新的努力的方向，所以说，没有 deadline 才能让我们不断地自我改善，而有 deadline 只能让我们赶紧做完赶紧清净。好了，以上就是这篇博客的全部内容啦，最后要送给大家一个福利，本文中援引的爬虫程序已开源，地址是：https://github.com/qinyuanpei/zngirls，感兴趣的朋友可以自己去玩一玩，你懂的哦！\n","date":"2021-01-14T20:35:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3247093203/","slug":"3247093203","tags":["Python","多线程","爬虫","技巧"],"title":"使用多线程为你的 Python 爬虫提速的 N 种姿势，你会几种？"},{"categories":["独立博客"],"content":"作为一个经常写博客的人，我有时会在微博上分享博客内容，可不知道从什么时候开始，国内互联网越来越丧失信仰，所有的厂商都在试图打造一个**“只进不出”的信息孤岛，进而达到增强“用户黏度”的目的。以微博为例，微博中的外链永远都会被转化为短地址，并且无法通过微博内置的浏览器进行跳转。即使你通过手动复制链接的方式打开链接，你依然需要至少两个步骤方能见到“庐山真面目”。借鉴/抄袭这一陋习的还有简书，花时间做了一个第三方链接跳转提示页面，唯独不愿意在上面加一个 a 标签，你还是要手动复制黏贴。坦白说，我觉得国内互联网正在丧失着信仰，看起来电商、物流、外卖、打车、支付……此起彼伏逐渐渗透到我们生活的方方面面，成为名副其实的“互联网+”，可在信息泛滥的今天，我们越来越难找到真正有价值的信息……既然外链注定要被屏蔽掉，那我就勉为其难地顺应潮流发“长截图”咯，所以，接下来我会为大家分享实现网页“长截图”**的常见思路，希望对有类似烦恼或者需求的小伙伴们有所帮助。\n通过浏览器实现 要实现网页长截图，显然是和网页打交道，而和网页打交道最多的是谁呢？自然是我们每天都要用的浏览器啦！值得庆幸的是，不管是 Chrome 还是 Firefox ，我们都可以通过它们来是实现这个想法。\nChrome 对于 Chrome 来说，我们只需要“F12”打开开发者工具，并在其中找到“控制台”选项卡，在平时输入 JavaScript 脚本的地方(即 Console 选项卡)输入Ctrl + Shift + P命令，然后你会得到一个类似 VSCode 命令行体验的输入窗口，接下来，输入：Capture full size screenshot并回车。此时，我们就可以得到完整的页面截图。而如果你希望截取网页中的一部分，则可以在选中指定 DOM 元素后采用相同的方式输入命令：Capture node screenshot。此外，更常用的截取浏览器可见范围内的内容，可以使用：Capture screenshot。可能相对于一般可以进行拖拽截图的工具而言，这个方案显得有点笨拙且简陋，可它真的可以完美地实现我们的想法，而且不需要安装任何扩展或者插件。\n使用 Chrome 的截图功能\rFirefox 对于 Firefox 而言，它本身自带截图功能，并且支持拖拽截图，对于我们这些需要长截图的人而言，唯一需要做的就是点击几下数据，确实要比敲命令行要简单一点、友好一点，我个人更喜欢用 Firefox 一点，因为 Chrome 正在从屠龙少年变成恶龙，为了让这个世界上不是只有 Chrome 一种浏览器内核，我决定支持一下 Firefox ，2020 年因为疫情的原因， Mozila 裁员 25%约 250 人，这家几乎靠着理想主义在维护 Gecko 内核的公司，之后可能再无法和 Google 的 Chrome 抗衡，而这个世界只有一种浏览器的时代我们都曾经经历过，它的名字叫做 IE6 ，不禁令人感慨，简直是开放 Web 的罗曼蒂克消亡史。\n使用 Firefox 的截图功能\r通过 Selenium 实现 在我的认知中，有浏览器的地方就有爬虫，而有爬虫的地方就有 Selenium 。原本好端端的 UI 自动化测试框架，怎么就助纣为虐做起爬虫来了呢？其实，主要原因是它提供了一个可以和浏览器交互的环境，从某种意义上来讲，Selenium 、PhantomJS 以及 Playwright 都可以认为是类似的技术，这里我们以 Selenium 为例，而通过 Selenium 实现网页长截图则主要有两种方式：其一，是构造一个足够“大”的浏览器，然后调用save_screenshot()方法进行截图；其二，是通过“拖拽”滚动条来滚动截图，然后再通过PIL进行拼接，下面来看具体的代码实现：\ndef save_screenshot(url, fp_pic): fireFoxOptions = webdriver.FirefoxOptions() fireFoxOptions.set_headless() driver = webdriver.Firefox(firefox_options=fireFoxOptions) driver.get(url) time.sleep(1) # 设置浏览器宽度和高度 width = driver.execute_script( \u0026#34;return document.documentElement.scrollWidth\u0026#34; ) height = driver.execute_script( \u0026#34;return document.documentElement.scrollHeight\u0026#34; ) driver.set_window_size(width, height) time.sleep(1) # 截图 driver.save_screenshot(fp_pic) driver.close() 这里我使用的是 Firefox 的驱动，喜欢 Chrome 的按个人喜好即可，这里我假设你已经掌握了 Python 和 Selenium，如果需要一点辅助知识，可以参考博主的这篇文章：作为技术宅的我，是这样追鬼滅の刃的 。这种方式的“长截图”实现起来非常简单，可是因为需要构造一个非常“大”的浏览器，所以，如果页面适配没有做好的话，可能会出现页面元素变形的问题，其次，这种方式生成的图片体积普遍比较大，所以，从总体上看主要就是这两个缺点。而“滚动截图”实现起来会稍微复杂一点，因为里面会涉及到一小部分计算：\ndef save_screenshot2(url, fp_pic): fireFoxOptions = webdriver.FirefoxOptions() fireFoxOptions.set_headless() driver = webdriver.Firefox(firefox_options=fireFoxOptions) driver.fullscreen_window() # 全屏窗口 driver.get(url) window_height = driver.get_window_size()[\u0026#39;height\u0026#39;] # 窗口高度 page_height = driver.execute_script( \u0026#39;return document.documentElement.scrollHeight\u0026#39; ) # 页面高度 driver.save_screenshot(\u0026#39;temp.png\u0026#39;) if page_height \u0026gt; window_height: n = page_height // window_height # 需要滚动的次数 base_mat = np.atleast_2d(Image.open(\u0026#39;temp.png\u0026#39;)) # 打开截图并转为二维矩阵 for i in range(n): driver.execute_script( f\u0026#39;document.documentElement.scrollTop={window_height * (i+1)};\u0026#39; ) time.sleep(.5) driver.save_screenshot(f\u0026#39;temp_{i}.png\u0026#39;) # 保存截图 mat = np.atleast_2d(Image.open(f\u0026#39;temp_{i}.png\u0026#39;)) # 打开截图并转为二维矩阵 base_mat = np.append(base_mat, mat, axis=0) # 拼接图片的二维矩阵 Image.fromarray(base_mat).save(fp_pic, format=\u0026#39;PNG\u0026#39;) os.remove(f\u0026#39;temp_{i}.png\u0026#39;) os.remove(\u0026#39;temp.png\u0026#39;) driver.quit() 这个方案本身没有太大的问题，可如果你的网页是那种页面滚动时头部固定的设计，譬如类似博主的博客这样的风格，此时这种方案就会有一点问题，每次截取都会包含头部这部分，和我们最后想要实现的效果有一点出入，如果可以计算出头部的高度，截图或者拼接的时候把这个高度考虑进去，就可以彻底解决这个问题，可这样这个问题就从一个通用型问题变成一个局部型问题啦，果然，世上没有完美的解决方案呢……\n通过 JavaScript 实现 有人可能要说，博主你好偏心，为什么 Python 都出来了，作为前端三剑客之一的 JavaScript 还没有出现？嗯，对此我想说——你不用说，我知道不就是**“人生苦短，我用 Python”**吗？人家前端世界里有个叫做 html2canvas 的库，博主你可有耳闻？我笑了笑，我并没有看了看我的劳力士，因为我没有劳力士。好吧，既然这里提到了这个库，那就来说说这个库的实现思路吧，人家不是说了嘛？一切可以实现的东西，最终可以用 JavaScript 来实现，我们来看看具体的代码实现，这里，首先准备一个 HTML 文件：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;script src=\u0026#39;./html2canvas.min.js\u0026#39;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/vue/dist/vue.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body style=\u0026#34;overflow: hidden;\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;app\u0026#34; style=\u0026#34;height: 768px; overflow: hidden;\u0026#34;\u0026gt; 请输入URL: \u0026lt;input type=\u0026#34;text\u0026#34; v-model=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;button v-on:click=\u0026#34;capture\u0026#34;\u0026gt;截取\u0026lt;/button\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;iframe id=\u0026#34;view\u0026#34; v-bind:src=\u0026#39;url\u0026#39; width=\u0026#34;100%\u0026#34; height=\u0026#34;100%\u0026#34; frameborder=\u0026#34;0\u0026#34; ref=\u0026#34;view\u0026#34;\u0026gt; \u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; 非常简单，输入一个地址，然后通过一个 iframe 进行预览，点击按钮进行截图。下面给出 JavaScript 代码：\n\u0026lt;script\u0026gt; var vm = new Vue({ el: \u0026#39;#app\u0026#39;, data: { url: \u0026#39;https://regularly-archive.github.io/2020/Matrix/\u0026#39;, }, methods: { capture: function() { var self = this; var iframe = self.$refs.view.contentWindow; var iframeBody = iframe.document.getElementsByTagName(\u0026#39;body\u0026#39;)[0] html2canvas(iframeBody).then(canvas =\u0026gt; { document.body.appendChild(canvas); //canvas转图片 let canvasImg = canvas.toDataURL(\u0026#34;image/png\u0026#34;); //模拟下载 var a = document.createElement(\u0026#39;a\u0026#39;) a.href = canvasImg; a.download = self.url; let event = document.createEvent(\u0026#34;MouseEvents\u0026#34;) event.initMouseEvent(\u0026#34;click\u0026#34;, true, false, window, 0, 0, 0, 0, 0, false, false, false, false, 0, null) a.dispatchEvent(event) }); } } }); \u0026lt;/script\u0026gt; 效果如下，你可以点击 这里 访问在线演示 DEMO：\n使用html2canvas实现的长截图\r这里使用 iframe 可能会引入跨域的问题，大家可以参考我的这篇文章：聊聊前端跨域的爱恨情仇 ，而 html2canvas 本身就提供了关于跨域问题的解决方案，大家可以参考这里：http://html2canvas.hertzen.com/configuration。\n通过第三方工具实现 我知道程序员都喜欢自己去折腾，如果是前无古人、后无来者的东西，我建议去折腾，因为梦想还是要有的，万一实现了呢？而我们这个圈子里同样有一句经典的话，叫做“不要重复制造轮子”，所以，博主这里找到了几个轮子供大家参考，不喜欢在冬天动手写代码的人，可以收藏下这几个工具，这个冬天实在是太冷了，冷到什么程度呢?大概听见笑话都不大愿意笑，用罗翔老师的话说这叫做搞笑未遂。\nwkhtml2image wkhtml 系列，一个命令行工具，可以将本地 HTML 文件或者远程 URL 指向的网页转化为图片，该系列产品中还有 wkhtml2pdf，顾名思义，网页转 PDF，实际使用过程中基本没什么问题，输出的图片 1:1 还原网页，唯一的缺点是偶尔会丢失样式，尤其是页面中引入了第三方的 JavaScript 或者 CSS 的时候，整体上远程 URL 比本地 HTML 要稳定一点，推荐系数：4 星。\n长截图03.png\rPickFrom PickFrom，一个在线的网页转图片的服务，填写 URL 然后点击按钮即可，提供免费预览一部分图片的功能，完整图片的查看、下载均需要支付一定费用，服务质量还可以，但不适合我们这种被迫“白嫖”的穷人家的孩子，土豪们随意，推荐系数：4 星\nPickFrom\rTiomg Tiomg，接下来是博主要重点推荐，它和 PickFrom 提供着相同的服务，唯一不同的是，它是完全免费的，我现在主要用这个来工具来生成“长截图”，不错，我背叛了上面我写的那些代码，为什么要重复造轮子呢？有时候我想不明白，为什么国内公司都喜欢那种“大而全”的软件，恨不得要拥有竞争对手所有的特性，可明明大家都“卷”成这样了，为什么不试试差异化的路线呢？可能，是因为低端竞争太多吧！推荐系数：5 星\nTiomg\r冬天实在是没有动力去写有技术含量的东西啊！关于“长截图”这个话题，差不多是从一周前开始关注、做实验的，所以，请允许在下偶尔写这样一篇“水文”吧！关于“视频是不能 P 的系列”，因为 Dlib 安装起来实在讨厌，而 OpenCV 提供的 68 特征点算法目前只支持 C++ ，研究起来难免要花一点时间，好了，这篇博客暂时先写到这里吧，博主要先去冬眠啦，再见!\n","date":"2021-01-09T20:37:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3406626380/","slug":"3406626380","tags":["Python","Selenium","长截图","工具"],"title":"实现网页长截图的常见思路总结"},{"categories":["编程语言"],"content":"这段时间在维护一个“遗产项目”，体验可以说是相当地难受，因为它的数据持久化层完全由 ADO.NET 纯手工打造，所以，你可以在项目中看到无所不在的 DataTable，不论是读操作还是写操作。这个 DataTable 让我这个习惯了 Entity Framework 的人感到非常别扭，我并不排斥写手写 SQL 语句，我只是拥有某种自觉并且清醒地知道，自己写的 SQL 语句未必就比 ORM 生成的 SQL 语句要好。可至少应该是像 Dapper 这种程度的封装啊，因为关系型数据库天生就和面向对象编程存在隔离，所以，频繁地使用 DataTable 无疑意味着你要写很多的转换的代码，当我看到DbConnection、DbCommand、DbDataReader、DbDataAdapter这些熟悉的“底层”的时候，我意识到我可以结合着 Dapper 的实现，从中梳理出一点改善的思路，所以，这篇博客想聊一聊ADO.NET、Dapper和Dynamic这三者间交叉的部分，希望能给大家带来新的启发。\n重温 ADO.NET 相信大家都知道，我这里提到的DbConnection、DbCommand、DbDataReader、DbDataAdapte以及DataTable、DataSet，实际上就是 ADO.NET 中核心的组成部分，譬如DbConnection负责管理数据库连接，DbCommand负责 SQL 语句的执行，DbDataReader和DbDataAdapter负责数据库结果集的读取。需要注意的是，这些类型都是抽象类，而各个数据库的具体实现，则是由对应的厂商来完成，即我们称之为“驱动”的部分，它们都遵循同一套接口规范，而DataTable和DataSet则是“装”数据库结果集的容器。关于 ADO.NET 的设计理念，可以从下图中得到更清晰的答案：\nADO.NET架构\r在这种理念的指引，使用 ADO.NET 访问数据库通常会是下面的画风。博主相信，大家在各种各样的DbHelper或者DbUtils中都见过类似的代码片段，在更复杂的场景中，我们会使用DbParameter来辅助DbCommand，而这就是所谓的SQL 参数化查询。\nvar fileName = Path.Combine(Directory.GetCurrentDirectory(), \u0026#34;Chinook.db\u0026#34;); using (var connection = new SQLiteConnection($\u0026#34;Data Source={fileName}\u0026#34;)) { if (connection.State != ConnectionState.Open) connection.Open(); using (var command = connection.CreateCommand()) { command.CommandText = \u0026#34;SELECT AlbumId, Title, ArtistId FROM [Album]\u0026#34;; command.CommandType = CommandType.Text; //套路1：使用DbDataReader读取数据 using (var reader = command.ExecuteReader()) { while (reader.Read()) { //各种眼花缭乱的写法:) Console.WriteLine($\u0026#34;AlbumId={reader.GetValue(0)}\u0026#34;); Console.WriteLine($\u0026#34;Title={reader.GetFieldValue\u0026lt;string\u0026gt;(\u0026#34;Title\u0026#34;)}\u0026#34;); Console.WriteLine($\u0026#34;ArtistId={reader.GetInt32(\u0026#34;ArtistId\u0026#34;)}\u0026#34;); } } //套路2：使用DbDataAdapter读取数据 using (var adapter = new SQLiteDataAdapter(command)) { var dataTable = new DataTable(); adapter.Fill(dataTable); } } } 这里经常会引发的讨论是，DbDataReader和DbDataAdapter的区别以及各自的使用场景是什么？简单来说，前者是按需读取/只读，数据库连接会一直保持；而后者是一次读取，数据全部加载到内存，数据库连接用完就会关掉。从资源释放的角度，听起来后者更友好一点，可显然结果集越大占用的内存就会越多。而如果从易用性上来考虑，后者可以直接填充数据到DataSet或者DataTable，前者则需要费一点周折，你看这段代码是不是有点秀操作的意思：\n//各种眼花缭乱的写法:) Console.WriteLine($\u0026#34;AlbumId={reader.GetValue(0)}\u0026#34;); Console.WriteLine($\u0026#34;Title={reader.GetFieldValue\u0026lt;string\u0026gt;(\u0026#34;Title\u0026#34;)}\u0026#34;); Console.WriteLine($\u0026#34;ArtistId={reader.GetInt32(\u0026#34;ArtistId\u0026#34;)}\u0026#34;); 在这个“遗产项目”中，DbDataReader和DbDataAdapter都有所涉猎，后者在结果集不大的情况下还是可以的，唯一的遗憾就是DataTable和LINQ的违和感实在太强烈了，虽然可以勉强使用AsEnumerable()拯救一下，而前者就有一点魔幻了，你能看到各种GetValue(1)、GetValue(2)这样的写法，这简直就是成心不想让后面维护的人好过，因为加字段的时候要小心翼翼地，确保字段顺序不会被修改。明明这个世界上有Dapper、SqlSugar、SmartSql这样优秀的 ORM 存在，为什么就要如此执著地写这种代码呢？是觉得 MyBatis 在 XML 里写 SQL 语句很时尚吗？\n所以，我开始尝试改进这些代码，我希望它可以像 Dapper 一样，提供Query\u0026lt;T\u0026gt;()和Execute()两个方法足矣！如果要把结果集映射到一个具体的类型上，大家都能想到使用反射，我更想实现的是 Dapper 里的DapperRow，它可以通过“·”或者字典的形式来访问字段，现在的问题来了，你能实现类似 Dapper 里 DapperRow 的效果吗？因为想偷懒的时候，dynamic 不比 DataRow 更省事儿吗？那玩意儿光转换类型就要烦死人了，更不用说要映射到某个 DTO 啦！\n实现 DynamicRow 通过阅读 Dapper 的源代码，我们知道，Dapper 中用DapperTable和DapperRow替换掉了 DataTable 和 DataRow，可见这两个玩意儿有多不好用，果然，英雄所见略同啊，哈哈哈！其实，这背后的一切的功臣是IDynamicMetaObjectProvider，通过这个接口我们就能实现类似的功能，我们熟悉的ExpendoObject就是最好的例子：\ndynamic person = new ExpandoObject(); person.FirstName = \u0026#34;Sherlock\u0026#34;; person.LastName = \u0026#34;Holmes\u0026#34;; //等价形式 (person as IDctionary\u0026lt;string, object\u0026gt;)[\u0026#34;FirstName\u0026#34;] = \u0026#34;Sherlock\u0026#34;; (person as IDctionary\u0026lt;string, object\u0026gt;)[\u0026#34;LastName\u0026#34;] = \u0026#34;Holmes\u0026#34;; 这里，我们用一种简单的方式，让 DynamicRow 继承者 DynamicObject，下面一起来看具体的代码：\npublic class DynamicRow : DynamicObject { private readonly IDataRecord _record; public DynamicRow(IDataRecord record) { _record = record; } public override bool TryGetMember(GetMemberBinder binder, out object result) { var index = _record.GetOrdinal(binder.Name); result = index \u0026gt; 0 ? _record[binder.Name] : null; return index \u0026gt; 0; } //支持像字典一样使用 public object this[string field] =\u0026gt; _record.GetOrdinal(field) \u0026gt; 0 ? _record[field] : null; } 对于DynamicObject这个类型而言，里面最重要的两个方法其实是TryGetMember()和TrySetMember()，因为这决定了这个动态对象的读和写两个操作。因为我们这里不需要反向地去操作数据库，所以，我们只需要关注TryGetMember()即可，一旦实现这个方法，我们就可以使用类似foo.bar这种形式访问字段，而提供一个索引器，则是为了提供类似foo[\u0026quot;bar\u0026quot;]的访问方式，这一点同样是为了像 Dapper 看齐，无非是 Dapper 的 DynamicRow 本来就是一个字典！\n现在，我们来着手实现一个简化版的 Dapper，给IDbConnection这个接口扩展出Query\u0026lt;T\u0026gt;()和Execute()两个方法，我们注意到Query\u0026lt;T\u0026gt;()需要用到DbDataReader或者DbDataAdapter其一，对于DbDataAdapter而言，它的实现完全由具体的子类决定，所以，对于IDbConnection接口而言，它完全不知道对应的子类是什么，此时，我们只能通过判断IDbConnection的类型来返回对应的 DbDataAdapter。读过我之前博客的朋友，应该对 Dapper 里的数据库类型的字典有印象，不好意思，这里历史要再次上演啦！\npublic static IEnumerable\u0026lt;dynamic\u0026gt; Query(this IDbConnection connection, string sql, object param = null, IDbTransaction trans = null) { var reader = connection.CreateDataReader(sql); while (reader.Read()) yield return new DynamicRow(reader as IDataRecord); } public static IEnumerable\u0026lt;T\u0026gt; Query\u0026lt;T\u0026gt;(this IDbConnection connection, string sql, object param = null, IDbTransaction trans = null) where T : class, new() { var reader = connection.CreateDataReader(sql); while (reader.Read()) yield return (reader as IDataRecord).Cast\u0026lt;T\u0026gt;(); } 这里的CreateDataReader()和Cast()都是博主自定义的扩展方法：\nprivate static IDataReader CreateDataReader(this IDbConnection connection, string sql) { var command = connection.CreateCommand(); command.CommandText = sql; command.CommandType = CommandType.Text; return command.ExecuteReader(); } private static T Cast\u0026lt;T\u0026gt;(this IDataRecord record) where T:class, new() { var instance = new T(); foreach(var property in typeof(T).GetProperties()) { var index = record.GetOrdinal(property.Name); if (index \u0026lt; 0) continue; var propertyType = property.PropertyType; if (propertyType.IsGenericType \u0026amp;\u0026amp; propertyType.GetGenericTypeDefinition() == typeof(Nullable\u0026lt;\u0026gt;)) propertyType = Nullable.GetUnderlyingType(propertyType); property.SetValue(instance, Convert.ChangeType(record[property.Name], propertyType)); } return instance; } 而Execute()方法则要简单的多，因为从IDbConnection到IDbCommand的这条线，可以直接通过CreateCommand()来实现：\npublic static int Execute(this IDbConnection connection, string sql, object param = null, IDbTransaction trans = null) { var command = connection.CreateCommand(); command.CommandText = sql; command.CommandType = CommandType.Text; return command.ExecuteNonQuery(); } 实现参数化查询 大家可以注意到，我这里的参数 param 完全没有用上，这是因为IDbCommand的Paraneters属性显然是一个抽象类的集合。所以，从IDbConnection的角度来看这个问题的时候，它又不知道这个参数要如何来给了，而且像 Dapper 里的参数，涉及到集合类型会存在IN和NOT IN以及批量操作的问题，比普通的字符串替换还要稍微复杂一点。如果我们只考虑最简单的情况，它还是可以尝试一番的：\nprivate static void SetDbParameter(this IDbCommand command, object param = null) { if (param == null) return; if (param is IDictionary\u0026lt;string, object\u0026gt;) { //使用字典作为参数 foreach (var arg in param as IDictionary\u0026lt;string, object\u0026gt;) { var newParam = command.CreateParameter(); newParam.ParameterName = $\u0026#34;@{arg.Key}\u0026#34;; newParam.Value = arg.Value; command.Parameters.Add(newParam); } } else { //使用匿名对象作为参数 foreach (var property in param.GetType().GetProperties()) { var propVal = property.GetValue(param); if (propVal == null) continue; var newParam = command.CreateParameter(); newParam.ParameterName = $\u0026#34;@{property.Name}\u0026#34;; newParam.Value = propVal; command.Parameters.Add(newParam); } } } 相应地，为了能在Query\u0026lt;T\u0026gt;()和Execute()两个方法中使用参数，我们需要修改相关的方法：\npublic static int Execute(this IDbConnection connection, string sql, object param = null, IDbTransaction trans = null) { var command = connection.CreateCommand(); command.CommandText = sql; command.CommandType = CommandType.Text; command.SetDbParameter(param); return command.ExecuteNonQuery(); } private static IDataReader CreateDataReader(this IDbConnection connection, string sql, object param = null) { var command = connection.CreateCommand(); command.CommandText = sql; command.CommandType = CommandType.Text; command.SetDbParameter(param); return command.ExecuteReader(); } 现在，唯一的问题就剩下DbType和@啦，前者在不同的数据库中可能对应不同的类型，后者则要面临 Oracle 这朵奇葩的兼容性问题，相关内容可以参考在这篇博客：Dapper.Contrib 在 Oracle 环境下引发 ORA-00928 异常问题的解决。到这一步，我们基本上可以实现类似 Dapper 的效果。当然，我并不是为了重复制造轮子，只是像从 Dapper 这样一个结果反推出相关的技术细节，从而可以串联起整个 ASO.NET 甚至是 Entity Framework 的知识体系，工作中解决类似的问题非常简单，直接通过 NuGet 安装 Dapper 即可，可如果你想深入了解某一个事物，最好的方法就是亲自去探寻其中的原理。现在基础设施越来越完善了，可有时候我们再找不回编程的那种快乐，大概是我们内心深处放弃了什么……\n考虑到，从微软的角度，它鼓励我们为每一家数据库去实现数据库驱动，所以，它定义了很多的抽象类。而从 ORM 的角度来考虑，它要抹平不同数据库的差异，Dapper 的做法是给IDbConnection写扩展方法，而针对每个数据库的“方言”，实际上不管什么 ORM 都要去做这部分“脏活儿”，以前是分给数据库厂商去做，现在是交给 ORM 设计者去做，我觉得 ADO.NET 里似乎缺少了一部分东西，它需要提供一个 IDbAdapterProvider 的接口，返回 IDbAdapter 接口，这样就可以不用关心它是被如何创建出来的。你看，同样是设计接口，可微软和 ServiceStack 俨然是两种不同的思路，这其中的差异，足可窥见一斑矣！实际上，Entity Framework 就是在以 ADO.NET 为基础发展而来的，在这个过程中，还是由厂商来实现对应的 Provider。此时此刻，你悟到了我所说的“温故而知新”了嘛？\n本文小结 本文实则由针对 DataSet/DataTable 的吐槽而引出，在这个过程中，我们重新温习了 ADO.NET 中DbConnection、DbCommand、DbDataReader、DbDataAdapter这些关键的组成部分，而为了解决 DataTable 在使用上的种种不变，我们想到了借鉴 Dapper 中的 DapperRow 来实现“动态查询”，由此引出了.NET 中实现 dynamic 最重要的一个接口：IDynamicMetaObjectProvide，这使得我们可以在查询数据库的时候返回一个 dynamic 的集合。而为了更接近 Dapper 一点，我们基于扩展方法的形式为IDbConnection编写了Query\u0026lt;T\u0026gt;()和Execute()方法，在数据库读写层面上彻底终结了 DataSet/DataTable 的生命。最后，我们实现了一个简化版本的参数化查询，同样是借鉴 Dapper 的思路。这说明一件什么事情呢？当你在一个看似合理、结局固定的现状中无法摆脱的时候，“平躺”虽然能让你获得一丝喘息的机会，但与此同时，你永远失去了跳出这个层级去看待事物的机会，就像我以前吐槽同事天天用StringBuider拼接字符串一样，一味地吐槽是没有什么用的，重要的是你会选择怎么做，所以，后来我向大家推荐了Linquid，2021 年已经来了，希望你不只是增长了年龄和皱纹，晚安！\n","date":"2020-12-30T12:49:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2621074915/","slug":"2621074915","tags":["ADO.NET","Dapper","Dynamic","技巧","温故知新"],"title":"温故而知新，由 ADO.NET 与 Dapper 所联想到的"},{"categories":["编程语言"],"content":"恍惚间，2020 年已接近尾声，回首过去这一年，无论是疫情、失业还是“996”，均以某种特殊的方式铭刻着这一年的记忆。也许，是这个冬天的西安雾霾更少一点。所以，有时透过中午的一抹冬阳，居然意外地觉得春天的脚步渐渐近了，甚至连圣诞节这种“洋节日”都感到亲切而且期待，我想，这大概是我丧了一段时间的缘故吧！可不管怎样，人们对未来的生活时常有一种“迷之自信”，果然生还还是要继续下去的呀！趁着最近的时间比较充裕，我决定开启一个信息的系列：视频是不能 P 的。这是互联网上流传的一个老梗了，正所谓“视频是不能 P 的，所以是真的”。其实，在如今这个亦真亦假的世界里，哪里还有什么东西是不能 PS 的呢？借助人工智能“改头换面”越来越轻而易举，而这背后关于隐私和伦理的一连串问题随之而来，你越来越难以确认屏幕对面的那个是不是真实的人类。所以，这个系列会以 OpenCV 作为起点，去探索那些好玩、有趣的视频/图像处理思路，通过技术来证明视频是可以被 PS 的。而作为这个系列的第一篇，我们将从一个最简单的地方开始，它就是人脸检测。\n第一个入门示例 学习 OpenCV 最好的方式，就是从官方的示例开始，我个人非常推荐的两个教程是 OpenCV: Cascade Classifier 和 Python OpenCV Tutorial，其次是 浅墨大神 的【OpenCV】入门教程，不同的是， 浅墨大神 的教程主要是使用 C++，对于像博主这样的“不学无术”的人，这简直无异于从入门到放弃，所以，建议大家结合自己的实际情况，选择适合自己的“难度”。好了，思绪拉回我们这里，在 OpenCV 中实现人脸检测，主要分为以下三个步骤，即，首先，定义联级分类器CascadeClassifier并载入指定的模型文件；其次，对待检测目标进行灰度化和直方图均衡化处理；最后，对灰度图调用detectMultiScale()方法进行检测。下面是一个简化过的入门示例，使用世界上最省心的 Python 语言进行编写：\nimport cv2 # 步骤1: 定义联级分类器CascadeClassifier并载入指定的模型文件 faceCascade = cv2.CascadeClassifier(\u0026#39;./haarcascades/haarcascade_frontalface_alt2.xml\u0026#39;) # 步骤2: 对待检测目标进行灰度化和直方图均衡化处理 target = cv2.imread(\u0026#39;target.jpg\u0026#39;) target_gray = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY) target_gray = cv2.equalizeHist(target_gray) # 步骤3: 人脸检测 faces = faceCascade.detectMultiScale(target_gray) for (x,y,w,h) in faces: cv2.rectangle(target, (x, y), (x + w, y + h), (0, 255, 0), 2) # 步骤4: 展示结果 cv2.imshow(\u0026#39;Face Detection\u0026#39;, target) cv2.waitKey(0) cv2.destroyAllWindows() 正常情况下，你会得到下面的结果，这里选取的素材是经典日剧《半泽直树》：\nOpenCV人脸检测效果展示\r怎么样？是不是被 OpenCV 的强大给震惊到了？下面我们针对每个步骤做更详细的说明：\n第 1 行引入 OpenCV，需要我们安装 OpenCV 的Python 版本。 第 4 行实例化级联分类器 CascadeClassifier，关于这个级联分类器，它是 OpenCV 下做目标检测的模块，内置Haar、HOG和LBP三类特性算法，而所谓“级联”，则是指它通过多个强分类器串联实现最终分类的思路。在初始化级联分类器的时候，需要载入指定的模型文件，这些模型文件是官方提前训练好的，可以从Github上进行下载，不同的模型文件对应不同的功能，这里使用的haarcascade_frontalface_alt2.xml主要针对面部检测，而像haarcascade_eye_tree_eyeglasses.xml则可以对眼睛进行检测。除此之外，我们还通过训练获得自己的模型文件，当然，这一切都是后话啦！ 第 7~9 行，我们载入了一张图片素材，并对其进行了灰度化和直方图均衡化处理。这里需要关注的三个函数是：imread、cvtColor 和 equalizeHist，它们的作用分别是读取图片、转换颜色和直方图均衡化处理。其中，对人脸检测而言，灰度图是必要的条件，而直方图均衡化则是可选的一个过程。 第 12~14 行，通过 detectMultiScale 方法我们就可以对待检测目标进行检测，关于它的参数，常用的有 scaleFactor、minNeighbors、minSize、maxSize，它可以对人脸检测做进一步的细节上控制，对于我们而言，我们更关心检测的结果，这里我们将检测到的人脸区域以矩形的方式绘制出来。 第 17~19 行，主要是为了方面大家观察结果，实际使用中可能会输出为文件或者实时渲染，这里需要关注的重点函数是：imshow，顾名思义，它可以让图片展示在窗口中，类似我们这个示例中的效果。 柴犬界的“网红” 曾经，有“好事者”分析过微信和 QQ 的年度表情，表情包文化流行的背后，实际上表达方式多样化的一种体现，例如：“笑哭”这一符号，固然有哭笑不得的含义在里面，可又何尝不是二十多岁人生总是边哭边笑的真实写照呢？而“捂脸”这一符号在我看来更多的是一种无可奈何，甚至有一种自我嘲讽的意味在里面。至于“呲牙”，朴实无华的微笑背后，大抵是看惯了庭前花开花落，可以“不以物喜，不以己悲”地笑对人生吧！其实，在这许许多多地表情里，我最喜欢的是微博里的“Doge”，这个眉清目秀的“狗头”能表达出各种丰富的含义，相比之下，微信里的“Doge”就有一点拙劣的模仿的意味了，俗话说**“狗头保命”**，在一个互联网信仰缺失的时代，用这样一种表情作为人类的保护色，又是不是一种反讽呢？而大家都知道，这个“Doge”表情，实际上是源于一个叫做Homestar Runner的网上动画系列，其原型则来源于一只名为Kabosu的柴犬，由于它融合了萌宠和故意搞笑两大特点，因此在网络上迅速走红，并由此衍生出一系列二次创作。\n微信年度表情\rQQ年度表情\r现在，让我们唤醒身体里的中二灵魂，通过 OpenCV 让这个柴犬界的网红出现在我们面前。这里的思路是，在检测到人脸后，在人脸区域绘制一个“狗头”表情，为此，我们需要定义一个copyTo()函数，它可以将一张小图(MaskImage)绘制到大图(SrcImage)的指定位置，我们一起来看它的具体实现：\ndef copyTo(srcImage, maskImage, x, y, w, h): # 按照区域大小对maskImage进行缩放 img_h, img_w, _ = maskImage.shape img_scale = h / img_h new_w = int(img_w * img_scale) new_h = int(img_h * img_scale) img_resize = cv2.resize(maskImage ,(new_w ,new_h)) # “粘贴”小图到大图的指定位置 if (srcImage.shape[2] != maskImage.shape[2]): y1, y2 = y, y + img_resize.shape[0] x1, x2 = x, x + img_resize.shape[1] alpha_s = img_resize[:, :, 3] / 255.0 alpha_l = 1.0 - alpha_s for c in range(0, 3): srcImage[y1:y2, x1:x2, c] = ( alpha_s * img_resize[:, :, c] + alpha_l * srcImage[y1:y2, x1:x2, c] ) else: srcImage[y:y + img_resize.shape[0], x:x + img_resize.shape[1]] = img_resize return srcImage 在这里，我们首先要关注这样一件事情，即 OpenCV 默认使用的是由 R、G、B 组成的三通道，可对于像 PNG 这种格式的图片，它会含有一个 Alpha 通道。这样，当我们尝试把一张含 Alpha 通道的小图，“粘贴”到只有 R、G、B 三个通道的大图上时，就会出现通道数对不上的问题，所以，这个函数实际上对这种情况做了特殊处理。其次，每一个 OpenCV 中的图片，即 Mat，其 shape 属性是一个由三个元素组成的元组，依次为图片高度、图片宽度和图片通道数。“黏贴”的过程实际上是修改对应位置处的像素信息。好了，现在，我们来修改一下第一版的代码：\n# 步骤1: 定义联级分类器CascadeClassifier并载入指定的模型文件 faceCascade = cv2.CascadeClassifier(\u0026#39;./haarcascades/haarcascade_frontalface_alt2.xml\u0026#39;) # cv2.IMREAD_UNCHANGED表示保留Alpha通道信息 doge = cv2.imread(\u0026#39;doge-4.png\u0026#39;, cv2.IMREAD_UNCHANGED) # 步骤2: 对待检测目标进行灰度化和直方图均衡化处理 target = cv2.imread(\u0026#39;target.jpg\u0026#39;) target_gray = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY) target_gray = cv2.equalizeHist(target_gray) # 步骤3: 人脸检测 faces = faceCascade.detectMultiScale(target_gray) for (x,y,w,h) in faces: target = copyTo(target, doge, x, y, w, h) # 粘贴“狗头”表情至每一个面部区域 # 步骤4: 展示结果 cv2.imshow(\u0026#39;Face Detection\u0026#39;, target) cv2.waitKey(0) cv2.destroyAllWindows() 此时，我们就可以得到下面的结果：\n全员Doge!\r其实，我本人更喜欢这张，一张充满精神污染意味的图片：\n来自神烦狗的精神污染\r视频级 PS 入门 OK，相信到这里为止，各位读者朋友，都已经顺着博主的思路实现了图片级别的“PS”，既然我们这个系列叫做视频是不能 P 的，那么大家要问了，视频到底能不能 P 呢？答案显然是可以，不然博主写这个系列就不是“人脸检测”而是“人肉打脸”啦！下面，我们来继续对今天的这个例子做一点升级。考虑在 OpenCV 中，VideoCapture可以通过摄像头捕捉视频画面，所以，我们只需要把这个“狗头”绘制到每一帧画面上，就可以实现视频级别的 PS 啦！\n# 步骤1: 定义联级分类器CascadeClassifier并载入指定的模型文件 faceCascade = cv2.CascadeClassifier(\u0026#39;./haarcascades/haarcascade_frontalface_alt2.xml\u0026#39;) doge = cv2.imread(\u0026#39;doge-4.png\u0026#39;, cv2.IMREAD_UNCHANGED) cap = cv2.VideoCapture(0) #笔记本自带摄像头 while (True): ret, frame = cap.read() if (ret == False): break # 步骤2: 对待检测目标进行灰度化和直方图均衡化处理 # 读取视频中每一帧 target = frame target_gray = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY) target_gray = cv2.equalizeHist(target_gray) # 步骤3: 人脸检测 faces = faceCascade.detectMultiScale(target_gray) for (x,y,w,h) in faces: target = copyTo(target, doge, x, y, w, h) # 步骤4: 展示结果 cv2.imshow(\u0026#39;Face Detection\u0026#39;, target) # 按Q退出 if cv2.waitKey(1) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break cap.release() cv2.destroyAllWindows() 一起来看看实现的效果吧！可能当你看完这篇博客的时候，你会觉得我写这玩意儿到底有什么用？不好意思，这玩意儿还真有用！它解决了像博主这样腼腆、不敢在公开场合抛头露面的人的困惑。暴走大事件里“王尼玛”一直戴着头罩，所以，很多人都好奇他本人到底长什么样子，如果当时能考虑这个思路的话，是不是可以不用一直戴着头罩。同样地，还有在浪客剑心真人版里饰演志志雄真实的藤原龙也，全身上下缠满绷带的造型其实对演员来说是非常不友好的，如果当时能考虑这个思路的话，是不是演员可以不用受那么大的罪。如果说这些都有些遥远的话，那么，至少在采访后期希望保护受访者隐私的场景下，这个思路是完全可行的，就像大家看到的它可以完全的遮挡住我的脸，而类似的打马赛克等等技术，本质上还是对图像进行处理，甚至美颜相机里的各种特效，底层都离不开 OpenCV 里的这些算法。怎么样？现在有没有觉得博主其实是一个非常有趣的人，哈哈!\n视频级别的“PS”\r本文小结 这篇博客主要分享了 OpenCV 在人脸检测方面的简单应用，OpenCV 中的 CascadeClassifier 整合Haar、HOG和LBP三类特性算法，通过预置的模型文件可以实现不同程度的目标检测功能，而在人脸检测的基础上，我们可以通过训练来实现简单的人脸识别，正如今年爆发的新冠疫情让人脸识别出现新的挑战一样，虽然人脸识别的场景正在变得越来越复杂，可作为一个世界上最流行的计算机视觉库，OpenCV 中的各种模块、算法还是一如既往的经典。结合 imread()、resize()、cvtColor()等等的方法，我们可以将“狗头”表情贴到图片或者视频中的人脸区域，而这个思路可以为人脸遮挡相关的场景做一点探索。\n在一个流行美颜的时代，人们对于别人甚至自己的期望在无限拔高，像博主本人一直不怎么喜欢拍照，有时候我们埋怨别人没有把我们拍得好看一点，可那究竟是你眼中的自己还是别人眼中的自己呢？正如相亲的时候，人们总喜欢把最好的、美化过的一面展示给别人，因为只有这样才能让别人对你产生兴趣，可往往现实的落差会让这种来得快的兴趣消失得更快。所以，我想说，虽然在技术面前万物似乎皆可“PS”，可对于我们自己而言，你是否了解真正的自己呢？关于我博客的写作风格，我一直不确定是要用偏严谨还是偏活泼的方式来表达，因为眼看着被后浪们一点点超越，这实在是种难以言说的感觉，欢迎大家在评论区留下你对博客内容或者观点的想法，祝大家周末愉快，一个人一样要活得浪漫！\n","date":"2020-12-25T22:49:47Z","image":"https://i.loli.net/2020/12/26/8QaJlR5XIxLftCo.png","permalink":"https://qinyuanpei.github.io/posts/2997581895/","slug":"2997581895","tags":["OpenCV","Python","图像处理","人脸检测"],"title":"视频是不能 P 的系列：OpenCV 人脸检测"},{"categories":["编程语言"],"content":"\r有人说，“男人至死都是少年”，而这句听起来有一点中二的话，其实是出自一部同样有一点中二的动漫——银魂。我个人的理解是，知世故而不世故。也许，年轻时那些天马行空的想法，就像堂吉诃德大战风车一样荒诞，可依然愿意去怀着这样的梦想去生活。正如罗曼罗兰所言，“世上只有一种英雄主义，就是在认清生活真相之后依然热爱生活”。所以，继《浪客剑心》之后，我再次被一部叫做《鬼灭之刃》的动漫吸引，毕竟男人的快乐往往就是这么朴实无华且枯燥。一个快三十岁的人，如果还能被一部热血少年番吸引，大概可以说明，他身体里的中二少年连同中二少年魂还活着。最早的印象来自朋友圈里的一位二次元“少年”，他和自己儿子站一起，有种浑然天成的协调感，整个人是非常年轻的感觉。所以，大概，男人至死都是少年。\n漫画的抓取 鬼滅の刃的漫画早已更完，令我不舍昼夜去追的，实际上是动画版的鬼滅の刃。虽然 B 站上提供中配版本，可一周更新两集的节奏，还是让我追得有一点焦灼(PS：我没有大会员呢)，甚至熬着夜提前“刷”了无限列车(PS：见文章末尾小程序码)。其实，鬼滅の刃前期并没有特别吸引人的地方，直到那田蜘蛛山那一话开始渐入佳境，鬼杀队和鬼两个阵营所构成的人物群像开始一点一点的展开。它的表达方式有点接近刺客信条，即反派都是在死亡一刹那间有了自我表达的机会，而玩家/观众都可以了解反派的过去。由于鬼是由人转变而来，所以，在热血和厮杀之外，同样有了一点关乎人性的思考。作为一名“自封”的技术宅，我必须要在追番的时候做点什么，从哪里开始好呢？既然漫画版早已更新完毕，我们要不先抓取漫画下来提前过过瘾？\nOK，这里博主找了一个动漫网站，它上面有完整的鬼滅の刃漫画。我意识到从网上抓取漫画的行为是不对的，可这家网站提供的漫画明显是通过扫描获得的，因为正常的漫画都是通过购买杂志的方式获得的。所以，如果经济条件允许的情况下，还是希望大家可以支持正版，这里博主主要还是为了研究技术(逃，无意对这些资源做二次加工或者以任何方式盈利，所以，请大家不要向博主索取任何资源，我对自己的定位永远是一名软件工程师，谁让我无法成为尤小右这样的“美妆”博主呢？这一点希望大家可以理解哈！\n鬼滅の刃作品页面\r鬼滅の刃章节页面\r简单分析下动漫网站结构，可以发现，它主要有两种界面，即作品页面和章节页面。作品页面里面会显示所有的章节，而每个章节里会显示所有的图片。所以，我们的思路是，首先，通过作品页面获取所有章节的链接。其次，针对每一个章节，获取总页数后逐页下载图片即可。注意到这个网站有部分内容是通过 JavaScript 动态生成的，所以，requests针对这种情况会有点力不从心。幸好，我们还有Selenium这个神器可以使用，我们一起来看这部分内容如何实现：\nimport requests from bs4 import BeautifulSoup import fake_useragent import json import urllib from selenium import webdriver from selenium.webdriver.support.ui import Select from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC import os, time import threading import threadpool class DemonSlayer: def __init__(self, baseUrl): self.baseUrl = baseUrl self.session = requests.session() self.headers = { \u0026#39;User-Agent\u0026#39;: fake_useragent.UserAgent(verify_ssl=False).random } # 使用无头浏览器 fireFoxOptions = webdriver.FirefoxOptions() fireFoxOptions.set_headless() self.driver = webdriver.Firefox(firefox_options=fireFoxOptions) # 获取所有章节信息 def getAllChapters(self, opusUrl): chapters = {} response = self.session.get(opusUrl, headers=self.headers) response.encoding = \u0026#39;utf-8\u0026#39; response.raise_for_status() soup = BeautifulSoup(response.content) soup = soup.find(name=\u0026#39;div\u0026#39;, attrs={\u0026#39;class\u0026#39;,\u0026#39;cy_plist\u0026#39;}) if (soup != None): for li in soup.ul.children: chapters[li.a.text] = self.baseUrl + li.a[\u0026#39;href\u0026#39;] return chapters # 获取指定章节信息 def getChapter(self, url): page = 1 maxPage = self.getPageOfChapter(url) images = [] while page \u0026lt;= maxPage: reqUrl = url + \u0026#39;?p={page}\u0026#39;.format(page=page) self.driver.get(reqUrl) wait = WebDriverWait(self.driver, 10) # 等待漫画图片加载完成 wait.until(EC.presence_of_element_located((By.ID, \u0026#34;qTcms_pic\u0026#34;))) html = self.driver.page_source imgSrc = BeautifulSoup(html).find(\u0026#39;img\u0026#39;)[\u0026#39;src\u0026#39;] # 从网页中提取的图片地址需要做一次解码 realSrc = urllib.parse.unquote(imgSrc.split(\u0026#39;?\u0026#39;)[1].split(\u0026#39;\u0026amp;\u0026#39;)[0].split(\u0026#39;=\u0026#39;)[1]) images.append(realSrc) page += 1 return images # 获取指定章节页数 def getPageOfChapter(self, url): self.driver.get(url) wait = WebDriverWait(self.driver, 10) # 等待页数加载完成 wait.until(EC.presence_of_element_located((By.ID, \u0026#34;k_pageSelect\u0026#34;))) html = self.driver.page_source opts = list(BeautifulSoup(html).find(\u0026#39;select\u0026#39;).children) return int(opts[-1][\u0026#39;value\u0026#39;]) # 下载漫画图片 def getImage(self, chapterPath, index, url): imagePath = os.path.join(chapterPath, str(index) + \u0026#39;.jpg\u0026#39;) with open(imagePath, \u0026#39;wb\u0026#39;) as fp: response = self.session.get(url) response.raise_for_status() fp.write(response.content) # 公共入口 def run(self, url): chapters = self.getAllChapters(url) for chapter, url in chapters.items(): images = spider.getChapter(url) # 为每一个章节建立文件夹 chapterPath = \u0026#39;./download/{chapter}/\u0026#39;.format(chapter=chapter) if (not os.path.exists(chapterPath)): os.mkdir(chapterPath) # 使用多线程下载图片 args = [] for index, url in enumerate(images): args.append((None,{\u0026#39;chapterPath\u0026#39;:chapterPath, \u0026#39;url\u0026#39;:url, \u0026#39;index\u0026#39;:index})) pool = threadpool.ThreadPool(max(10, len(images))) requests = threadpool.makeRequests(self.getImage, args) [pool.putRequest(req) for req in requests] pool.wait() if __name__ == \u0026#39;__main__\u0026#39;: spider = DemonSlayer(\u0026#39;http://www.7edm.com/\u0026#39;) spider.run(\u0026#39;http://www.7edm.com/rexue/guimiezhiren/\u0026#39;) 在这里，需要注意的 Firefox 驱动，即GeckoDriver需要提前安装好。同时，确保 Firefox 主程序和驱动程序所在的目录均已配置到环境变量Path中。在有的资料中提到，GeckoDriver需要和 Firefox 主程序在同一个目录底下，不过经过博主的验证，需要放在 Python 主程序的根目录底下。这个驱动程序可以从这里下载：https://github.com/mozilla/geckodriver/releases/。当然，如果你更喜欢使用 Chrome，只需要安装 Chrome 的驱动即可，完全遵从个人意愿即可。在运行这个脚本后，我们就可以获得完整的鬼滅の刃漫画：\n鬼灭之刃204话\r在 Kindle 上“追”番 好了，到现在为止，我们实现了本文的第一个小目标，即漫画的抓取。那么，在抓取到漫画以后，我们可以做什么呢？可能会有读者朋友忍不住吐槽，“废话，漫画除了看还能干什么”。话虽如此，可这样一张张图片显然不能让我们方便地看漫画啊！早就听说在 Kindle 上可以看漫画，可一直没有真正尝试过，可能是因为我还不够中二吧！所以，接下来，我们来考虑将这些图片制作成电子书。Kindle 本身支持像 mobi、docx/doc、pdf 等等这样的格式，这里我们选择一种最简单的方式，利用PyMuPDF这个库来生成 PDF 文件，一起来看具体的代码实现：\ndef createPdf(self, chapterPath): doc = fitz.open() pdfPath = \u0026#39;./ebook/\u0026#39; + os.path.basename(chapterPath) + \u0026#39;.pdf\u0026#39; for image in sorted(os.listdir(chapterPath)): filePath = os.path.join(chapterPath, image) # 为每一张图片创建一个单独的PDF imgdoc = fitz.open(filePath) pdfbytes = imgdoc.convertToPDF() imgpdf = fitz.open(\u0026#34;pdf\u0026#34;, pdfbytes) # 插入当前页 doc.insertPDF(imgpdf) # 将当前页插 # 保存PDF doc.save(pdfPath) 经常使用 Kindle 的朋友，一定对它的邮箱传书功能非常熟悉，这意味着我们此基础上，将生成的 PDF 文件直接推送到 Kindle 上，而在 Python 中发送邮件则是非常简单的，这一点我们不再赘述，那到底能不能实现我们这个想法呢？显然可以，我们一起来看看效果：\n由PyMuPDF生成的“漫画书”\r我们知道 Kindle 对 PDF 的支持是有点差的，主要体现在它不支持重排，在我们这个例子中，我们可以发现图片是横屏显示的，或许是因为我的 Kindle 屏幕太小，或许是因为我没有一个 Pad，或许是因为图片尺寸无法适配，总之，它给人的体验是有点不舒服。博主注意到，Kindle 官方提供了一个漫画书制作软件Kindle Comic Creator，它可以生成 mobi 格式的电子书。作为对比，我们来看一下它在 Kindle 上的显示效果：\n由Kindle Comic Creator生成的“漫画书”\r果然还是 Kindle 自家的格式更好一点，有同样需求的小伙伴们，不妨试一试这个软件！现在，我们终于可以在 Kindle 上“刷”漫画了，快乐肥宅水，安排！\n让漫画“动”起来 一旦看过动画以后，再去看漫画，总觉得没那味儿，果然，“没声音再好的戏都出不来”，除了想念片头 OP 以外，更想念声优们绘声绘色的配音。有人说，博主你可以戴上耳机，放着片头 OP，刷着漫画啊！哎，你可真是小机灵鬼！可作为一名技术宅，我们当然要追求手工耿的无用之美，不就是想看有声音有画面的视频吗？安排！所以，下面我们来通过这些图片生成视频，这里主要用到opencv-python和MoviePy两个库，前者可以通过OpenCV合成视频，而后者则可以对视频进行剪辑，例如加入片头、片尾、背景音乐等等。\ndef createVideo(self, chapterPath): # OpenCV创建视频 videoFps = 40 videoSize = (655, 948) videoPath = \u0026#39;./video/\u0026#39; + os.path.basename(chapterPath) + \u0026#39;.avi\u0026#39; videoWriter = cv2.VideoWriter(videoPath, cv2.VideoWriter_fourcc(\u0026#39;I\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;0\u0026#39;), videoFps, videoSize ) for image in sorted(os.listdir(chapterPath)): # 读取每张图片，并重复写40帧 image_path = chapterPath + \u0026#39;/\u0026#39; + image bg = cv2.imread(\u0026#39;bg.jpg\u0026#39;) max_h, max_w, _ = bg.shape img = cv2.imdecode(np.fromfile(image_path ,dtype=np.uint8),-1) img_h, img_w, _ = img.shape scale = max_h / img_h new_w = int(img_w * scale) new_h = int(img_h * scale) img = cv2.resize(img ,(new_w ,new_h)) offsetX = int((max_w - new_w) / 2) offsetY = int((max_h - new_h) / 2) bg[offsetY:offsetY + img.shape[0], offsetX:offsetX + img.shape[1]] = img for i in range(0, videoFps): videoWriter.write(bg) videoWriter.release() # 为当前视频加入片头/尾 videoClip = VideoFileClip(videoPath) titleClip = VideoFileClip(\u0026#39;./title.mp4\u0026#39;) finalClip = concatenate_videoclips([ titleClip.subclip(0, 12), videoClip, titleClip.subclip(70, 72), titleClip.subclip(85, 89), ]) # 为当前视频增加BGM audioClip = AudioFileClip(\u0026#39;BGM.mp3\u0026#39;).subclip(0, finalClip.duration) finalClip.audio = audioClip finalClip.write_videofile(\u0026#39;output.avi\u0026#39;, codec=\u0026#39;libvpx\u0026#39;) 在使用 OpenCV 合成视频这一步，和大多数第一次接触 OpenCV 的人一样，博主在这里遇到了很多的问题，例如，最典型的问题有：OpenCV 合成的视频无法打开、OpenCV 无法使用中文路径、OpenCV 合成的视频长度较短等等。所以，在这里我简单做一下说明，首先，检查下视频画面的宽/高是否与图片的宽/高一致。其次，选择合适的编码器，在使用 OpenCV 保存视频的时候，需要综合考虑 FOURCC 和视频格式两个因素，前提是当前系统已经安装了对应的编码器，在 Windows 下推荐大家使用 DIVX。最后，关于中文路径，大部分的解决方案都是引入 NumPy，代码片段如下：\n# 读取 cv_img = cv2.imdecode(np.fromfile(file_path, dtype=np.uint8), -1) # 写入 cv2.imencode(\u0026#39;.jpg\u0026#39;, cv_img)[1].tofile(file_path) 经过博客的验证，这个方案的确可以解决，不过同样有缺点，imdecode()方法读取的是 RGB，而 OpenCV 需要的则是 BGR，需要做一次转换，而转换则意味着会损失图片信息。所以，如果在意生成的视频质量的话，最好还是放在英文的路径下面。\n好了，现在来看看效果，当片头 OP 响起的时候，有没有觉得，那种熟悉感觉的又回来了(PS：此处可与飞驰人生梦幻联动)，虽然没有配音，还是有一点视频的样子的。我经常在 B 站看到那种，黑色背景 + 白色文字的视频，虽然没有特别复杂的转场动画，可搭配上小爱同学或者 Siri 同学的声音后，居然感觉还不错。知乎上同样提供了文章转视频的功能，所以，我在想这是不是可以作为一个思路，作为一个努力寻找流量方向的技术博客，我好累啊(逃……\n这个思路，其实还可以用来制作表情包，作为一部拥有大量“名场面”的动漫作品，它为博主带来了不少的“燃点”和“笑点”，下面例举一二供大家欣赏：\n世界名画\r安塞腰鼓\r对于静态表情包，通常只需要用到PIL库就可以完成，下面是富冈义勇迫害时间(逃：\nimage = Image.open(\u0026#39;鬼灭之刃-09.jpg\u0026#39;) width, height = image.size textFont = ImageFont.truetype(\u0026#39;Deng.ttf\u0026#39;, 18) imageDraw = ImageDraw.Draw(image) textPos = (width * 0.34, height * 0.8) text = \u0026#39;我感觉我没有被讨厌\u0026#39; imageDraw.text(textPos, text, fill=\u0026#39;#fff\u0026#39;, font=textFont, align=\u0026#39;left\u0026#39;) image.save(\u0026#39;富岗义勇表情包.jpg\u0026#39;,\u0026#39;jpeg\u0026#39;) 如此，我们就可以得到下面的名场面：\n我感觉我没有被讨厌\r冰柱表示不想和你说话\r而对于动态表情包，我们可以考虑使用imageio和MoviePy，它们都可以从图片或者视频来生成 GIF，一起来看下面的例子：\ndef createGif(self, framesPath, gifPath, duration=0.5): # 读取图片 frames = [] for image_name in sorted(os.listdir(framesPath)): image_path = os.path.join(framesPath, image_name) frames.append(imageio.imread(image_path)) # 保存GIF imageio.mimsave(gifPath, frames, \u0026#39;GIF\u0026#39;, duration=duration) # 读取一组图片并生成GIF handler = ImageHandler() handler.createGif(\u0026#39;./frames\u0026#39;, \u0026#39;登峰造极.gif\u0026#39;) 这里唯一需要注意的就是，imageio依赖于Pillow:\n集中一点，登峰造极\r如果使用MoviePy来生成 GIF，则可以通过VideoFileClip或者ImageSequenceClip来分别从视频和图片创建 GIF。同样，这里有一个简单的例子：\n# 从一组图片并生成GIF和视频 fps = 0.25 sequence = ImageSequenceClip(\u0026#39;./frames\u0026#39;, fps) sequence.write_gif(\u0026#39;登峰造极.gif\u0026#39;) sequence.write_videofile(\u0026#39;登峰造极.avi\u0026#39;, codec=\u0026#39;libvpx\u0026#39;) # 从视频中截取指定片段生成GIF videoClip = VideoFileClip(\u0026#39;input.mp4\u0026#39;) videoClip = videoClip.subclip(33, 45) videoClip.write_gif(\u0026#39;霹雳一闪.gif\u0026#39;) 在这里，我选取的是，我个人非常喜欢的角色——我妻善逸的名场面“霹雳一闪”：\n霹雳一闪\r至此，我们“绞尽脑汁”、“千方百计”、“搜肠刮肚”地想到了各种“刷”鬼滅の刃的方法，总算为这份中二信仰充了值，大家还有什么好的点子吗？欢迎大家在评论区留言、点赞、收藏、一键三连，我仿佛嗅到了 Bilibili 的味道(逃……\n本文小结 坦白来讲，这篇博客的确有蹭热度的嫌疑，不过以鬼灭之刃在日本妇孺皆知的火热程度，足以令人们在这颇感失落的疫情背景下为之一振，甚至于连日本首相菅义伟在国会质询时，都引用了“全集中呼吸”这一经典台词。“纵有疾风起，人生不言弃”，热血少年漫告诉我们，在面对一个又一个的挫折的时候，不要沉溺于昨天的回忆，去勇敢地接受现实，直面惨淡的人生，这又何尝不是我们面对生活的态度呢？不管是旷日持久的疫情，还是似曾相识的失业，人生的道路上还有数不尽的“魑魅魍魉”呢！希望你活得像炭治郎一样乐观善良，像我妻善逸一样勇敢坚毅，像伊之助一样“猪”突猛进。如果这篇博客里所介绍的“追”动漫的方式，能让大家感到快乐和有趣的话，欢迎大家在评论区留言、点赞、收藏、一键三连。以上，这就是我，作为一个技术宅，在“追”动漫方面所做出的努力。少年，想加入鬼杀队吗？\n","date":"2020-12-15T22:49:47Z","image":"/posts/3602353334/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/3602353334/","slug":"3602353334","tags":["鬼滅の刃","Kindle","动漫","OpenCV","Python"],"title":"作为技术宅的我，是这样追鬼滅の刃的"},{"categories":["数据分析"],"content":"\r此时此刻，2020 年的最后一个月，不管过去这一年给我们留下了怎样的记忆，时间终究自顾自地往前走，留给我们的怀念已时日无多。如果要说 2020 年的年度日剧，我想《半泽直树》实至名归，这部在时隔七年后上映的续集，豆瓣评分高达 9.4 分，一度超越 2013 年第一部的 9.3 分，是当之无愧的现象级电视剧，期间甚至因为疫情原因而推迟播出，这不能不感谢为此付出辛勤努力的演职人员们。身为一个“打工人”，主角半泽直树那种百折不挠、恩怨分明的性格，难免会引起你我这种“社畜”们的共鸣，即使做不到“以牙还牙，加倍奉还”，至少可以活得像一个活生生的人。电视剧或许大家都看过了，那么，电视剧相对于原著小说有哪些改动呢？今天，就让我们使用 Python 来抽取半泽直树原著小说中的人物关系吧！\n准备工作 在开始今天的博客内容前，我们有一点准备工作要完成。考虑到小说人物关系抽取，属于自然语言处理(NLP)领域的内容，所以，除了准备好 Python 环境以外，我们需要提前准备相关的中文语料，在这里主要有：半泽直树原著小说、 半泽直树人名词典、半泽直树别名词典、中文分词停用词表。除此之外，我们需要安装结巴分词、PyECharts两个第三方库(注，可以通过 pip 直接安装)，以及用于展示人物关系的软件Gephi(注，这个软件依赖 Java 环境)。所以，你基本可以想到，我们会使用结巴分词对小说文本进行分词处理，而半泽直树人名列表则作为用户词典供结巴分词使用，经过一系列处理后，我们最终通过Gephi和PyECharts对结果进行可视化，通过分析人物间的关系，结合我们对电视剧剧情的掌握情况，我们就可以对本文所采用方法的效果进行评估，也许你认为两个人毫无联系，可最终他们以某种特殊的形式建立了联系，这就是我们要做这件事情的意义所在。本项目已托管在 Github上，供大家自由查阅。\n原理说明 这篇博客主要参考了 Python 基于共现提取《釜山行》人物关系 这个课程，该项目已在 Github 上开源，可以参考：https://github.com/Forec/text-cooccurrence。这篇文章中提到了一种称之为“共现网络”的方法，它本质上是一种基于统计的信息提取方法。其基本原理是，当我们在阅读书籍或者观看影视作品时，在同一时间段内同时出现的人物，通常都会存在某种联系。所以，如果我们将小说中的每个人物都看作一个节点，将人物间的关系都看作一条连线，最终我们将会得到一个图(指数据结构中的Graph)。因为Gephi和PyECharts以及NetworkX都提供了针对Graph的可视化功能，因此，我们可以使用这种方法，对《半泽直树》原著小说中的人物关系进行抽取。当然，这种方法本身会存在一点局限性，这些我们会放在总结思考这部分来进行说明，而我们之所以需要准备人名词典，主要还是为了排除单纯的分词产生的干扰词汇的影响；准备别名词典，则是考虑到同一个人物，在不同的语境下会有不同的称谓。\n过程实现 这里，我们定义一个RelationExtractor类来实现小说人物关系的抽取。其中，extract()方法用于抽取制定小说文本中的人物关系，exportGephi()方法用于输出 Gephi 格式的节点和边信息， exportECharts()方法则可以使用ECharts对人物关系进行渲染和输出：\nimport os, sys import jieba, codecs, math import jieba.posseg as pseg from pyecharts import options as opts from pyecharts.charts import Graph class RelationExtractor: def __init__(self, fpStopWords, fpNameDicts, fpAliasNames): # 人名词典 self.name_dicts = [line.strip().split(\u0026#39; \u0026#39;)[0] for line in open(fpNameDicts,\u0026#39;rt\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;).readlines()] # 停止词表 self.stop_words = [line.strip() for line in open(fpStopWords,\u0026#39;rt\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;).readlines()] # 别名词典 self.alias_names = dict([(line.split(\u0026#39;,\u0026#39;)[0].strip(), line.split(\u0026#39;,\u0026#39;)[1].strip()) for line in open(fpAliasNames,\u0026#39;rt\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;).readlines()]) # 加载词典 jieba.load_userdict(fpNameDicts) # 提取指定小说文本中的人物关系 def extract(self, fpText): # 人物关系 relationships = {} # 人名频次 name_frequency = {} # 每个段落中的人名 name_in_paragraph = [] # 读取小说文本，统计人名出现的频次，以及每个段落中出现的人名 with codecs.open(fpText, \u0026#34;r\u0026#34;, \u0026#34;utf8\u0026#34;) as f: for line in f.readlines(): poss = pseg.cut(line) name_in_paragraph.append([]) for w in poss: if w.flag != \u0026#34;nr\u0026#34; or len(w.word) \u0026lt; 2: continue if (w.word in self.stop_words): continue if (not w.word in self.name_dicts and w.word != \u0026#39;半泽\u0026#39;): continue # 规范化人物姓名，例：半泽-\u0026gt;半泽直树，大和田-\u0026gt;大和田晓 word = w.word if (self.alias_names.get(word)): word = self.alias_names.get(word) name_in_paragraph[-1].append(word) if name_frequency.get(word) is None: name_frequency[word] = 0 relationships[word] = {} name_frequency[word] += 1 # 基于共现组织人物关系 for paragraph in name_in_paragraph: for name1 in paragraph: for name2 in paragraph: if name1 == name2: continue if relationships[name1].get(name2) is None: relationships[name1][name2] = 1 else: relationships[name1][name2] += 1 # 返回节点和边 return name_frequency, relationships # 输出Gephi格式的节点和边信息 def exportGephi(self, nodes, relationships): # 输出节点 with codecs.open(\u0026#34;./output/node.txt\u0026#34;, \u0026#34;w\u0026#34;, \u0026#34;gbk\u0026#34;) as f: f.write(\u0026#34;Id Label Weight\\r\\n\u0026#34;) for name, freq in nodes.items(): f.write(name + \u0026#34; \u0026#34; + name + \u0026#34; \u0026#34; + str(freq) + \u0026#34;\\r\\n\u0026#34;) # 输出边 with codecs.open(\u0026#34;./output/edge.txt\u0026#34;, \u0026#34;w\u0026#34;, \u0026#34;gbk\u0026#34;) as f: f.write(\u0026#34;Source Target Weight\\r\\n\u0026#34;) for name, edges in relationships.items(): for v, w in edges.items(): if w \u0026gt; 0: f.write(name + \u0026#34; \u0026#34; + v + \u0026#34; \u0026#34; + str(w) + \u0026#34;\\r\\n\u0026#34;) # 使用ECharts对人物关系进行渲染 def exportECharts(self, nodes, relationships): # 总频次，用于数据的归一化 total = sum(list(map(lambda x:x[1], nodes.items()))) # 输出节点 nodes_data = [] for name, freq in nodes.items(): nodes_data.append(opts.GraphNode( name = name, symbol_size = round(freq / total * 100, 2), value = freq, )), # 输出边 links_data = [] for name, edges in relationships.items(): for v, w in edges.items(): if w \u0026gt; 0: links_data.append(opts.GraphLink( source = v, target = w, value = w )) # 绘制Graph c = ( Graph() .add( \u0026#34;\u0026#34;, nodes_data, links_data, gravity = 0.2, repulsion = 8000, is_draggable = True, symbol = \u0026#39;circle\u0026#39;, linestyle_opts = opts.LineStyleOpts( curve = 0.3, width = 0.5, opacity = 0.7 ), edge_label = opts.LabelOpts( is_show = False, position = \u0026#34;middle\u0026#34;, formatter = \u0026#34;{b}-\u0026gt;{c}\u0026#34; ), ) .set_global_opts( title_opts = opts.TitleOpts(title=\u0026#34;半泽直树原著小说人物关系抽取\u0026#34;) ) .render(\u0026#34;./docs/半泽直树原著小说人物关系抽取.html\u0026#34;) ) 你可以注意到，在input目录中，博主已经准备好了中文语料。因此，我们可以通过下面的代码来完成任务关系抽取：\nextractor = RelationExtractor(\u0026#39;./input/停用词词典.txt\u0026#39;, \u0026#39;./input/人名词典.txt\u0026#39;, \u0026#39;./input/别名词典.txt\u0026#39; ) nodes, relationships = extractor.extract(\u0026#39;./input/半泽直树.txt\u0026#39;) extractor.exportGephi(nodes, relationships) extractor.exportECharts(nodes, relationships) 此时，我们可以分别在output目录和docs目录获得Gephi和ECharts相关的渲染结果。\n结果展示 这里，通过 Gephi 软件导入生成的节点和边信息，这两个信息默认情况下在output目录下。如果你熟悉这个软件的使用的话，你可以得到下面的结果：\n使用Gephi渲染的小说人物关系图\r作为对比，博主这里同时提供了使用 ECharts 渲染的小说人物关系图：\n使用ECharts渲染的小说人物关系图\r或者，可以直接访问博主托管在 Github Pages 上的 在线版本 。关于 Gephi 软件的使用，请参考： Gephi 网络图极简教程。关于 PyECharts 的使用，请参考： PyECharts。\n总结思考 通过生成的人物关系图，可以发现下列规律：\n大多数人物间的关系是正确的，譬如东田-\u0026gt;浅野匡-\u0026gt;半泽这条线，对应的是第一季西大阪钢铁 5 亿贷款的事件，而箕部-\u0026gt;白井-\u0026gt;半泽这条线，显然对应的第二季议员利用“炼金术”敛财的事件。 我们发现渡真利拥有仅次于半泽的“连线”数量，这符合他在剧中掌握大量信息来源、职场上八面玲珑的形象设定。相比之下，同样作为半泽好友的近藤和苅田，则没有这样强大的光环。 关于大和田，我们都知道他在第二季属于编剧强行“加戏”，一定程度上是在顶替内藤部长的作用，大和田实际上并未参与第二季的剧情，这一点从图中人物节点的联系和大小可以看出。 日本人似乎更喜欢使用姓氏，由于妻子要跟随丈夫的姓氏，剧中很多女性角色譬如半泽花、浅野利惠等似乎都不太好提取出来，除非是像白井、谷川、藤泽这些重要的剧情人物。 考虑到，小说中同一个人的称呼通常会有很多种，与之相关联的领域被称为“中文指代消解问题”，使用姓氏作为关键字会造成“女性角色”的缺失，而这种基于“共现”的理论，无法解决 A 在 B 交谈的过程中提到 C 的问题，此时，C 和 A、C 和 B 可能并没有直接的联系，譬如图中的垣内，理论上与西大阪钢铁 5 亿贷款事件并无直接联系，因为剧情中参与融资的主要是新人中西，更不用说老员工角田居然“孤零零”的一个人，而主流的命名实体识别的理论基本都针对三元组，所以，在这里要心疼下角田这位老人。\n在目前的人物关系抽取案例中，这种情况称为“无效的人名实体共现句”，所以，更好的做法是，采用文本分类模型，结合依存句法去识别实体间的关系，比如同事关系、朋友关系或者亲属关系等等，它有一个非常专业的名词，称为命名实体识别(NER)，而这会让我们的这张图变得更加丰富。在这个方向上，我个人推荐使用哈工大的语言技术平台(LTP)作为进一步改进，因为它可以更好地识别人名。好了，以上就是这篇博客的全部内容啦，欢迎大家在博客下面留言，喜欢我的博客话，请一键三连，点赞收藏，谢谢大家！\n","date":"2020-12-08T22:49:47Z","image":"/posts/1427872047/4TV9CAoE6lksGMg.png","permalink":"https://qinyuanpei.github.io/posts/1427872047/","slug":"1427872047","tags":["Python","NLP","半泽直树","Gephi","ECharts"],"title":"使用 Python 抽取《半泽直树》原著小说人物关系"},{"categories":["数据分析"],"content":"在上一篇博客中，我和大家分享了整个 11 月份找工作的心路历程，而在找工作的过程中，博主发现西安大小周、单休这种变相“996”的公司越来越多，感慨整个行业越来越“内卷”的同时，不免会对未来的人生有一点迷茫，因为深圳已经开始试运行“996”了，如果有一天“996”被合法化并成为一种常态，那么，我们又该如何去面对“人会一天天衰老，总有一天肝不动”的客观规律呢？我注意到 Boss 直聘移动端会展示某个公司的作息时间，所以，我有了抓取西安市职位和公司信息并对其进行数据分析的想法，我想知道，这到底是我一个人的感受呢？还是整个世界的确是这样子的呢？带着这样的想法，博主有了今天这篇博客。所以，在今天这篇博客里，博主会从Boss 直聘、智联招聘以及前程无忧上抓取职位和公司信息，并使用 MongoDB 对数据进行持久化，最终通过pyecharts对结果进行可视化展示。虽然不大确定 2021 年会不会变得更好，可生活最迷人的地方就在于它的不确定性，正如数据分析唯一可以做的，就是帮助我们从变化的事物中挖掘出不变的规律一样。\n爬虫编写 其实，这种类似的数据分析，博主此前做过挺多的啦，譬如 基于 Python 实现的微信好友数据分析 以及 基于新浪微博的男女性择偶观数据分析(下) 这两篇博客。总体上来说，大部分学习 Python 的朋友都是从编写爬虫开始的，而在博主看来，数据分析是最终的目的，编写爬虫则是达到这一目的的手段。而从始至终，“爬”与“反爬”的较量从未停止过，Requests、BeautifulSoup、Selenium、Phantom 等等的技术层出不穷。考虑到现在编写爬虫存在风险，所以，我不会在博客里透露过多的“爬虫”细节，换言之，我不想成为一个教别人写爬虫的人，因为这篇博客的标签是数据分析，关于爬虫的部分，我点到为止，不再过多地去探讨它的实现，希望大家理解。而之所以要从这三个招聘网站上抓取，主要还是为了增加样本的多样性，因为 Boss 直聘上西安市的职位居然只有 3 页，这实在是太让人费解了！\nBoss 直聘 通过抓包，我们可以分析出 Boss 直聘的地址：https://www.zhipin.com/job_detail/?query={query}\u0026amp;city={cityCode}\u0026amp;industry=\u0026amp;position=\u0026amp;page={page}。其中，query为待查询关键词，cityCode为待查询城市代码，page为待查询的页数。可以注意到，industry和position两个参数没有维护，它们分别表示待查询的行业和待查询的职称。因为我们面向的是更一般的“打工人”，所以，这些都可以进行简化。对于cityCode这个参数，我们可以通过下面的接口获得：https://www.zhipin.com/wapi/zpCommon/data/city.json。这里，简单定义一个方法extractCity()来提取城市代码：\ndef extractCity(self, cityName=None): if (os.path.exists(\u0026#39;bossCity.json\u0026#39;) and cityName != None): with open(\u0026#39;bossCity.json\u0026#39;, \u0026#39;rt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as fp: cityList = json.load(fp) for city in cityList: if (city[\u0026#39;name\u0026#39;] == cityName): return city[\u0026#39;code\u0026#39;] else: response = requests.get(self.cityUrl) response.raise_for_status() json_data = response.json(); if (json_data[\u0026#39;code\u0026#39;] == 0 and json_data[\u0026#39;zpData\u0026#39;] != None): cityList = [] for level in json_data[\u0026#39;zpData\u0026#39;][\u0026#39;cityList\u0026#39;]: cityList.extend(self.unfoldLevel(level)) with open(\u0026#39;bossCity.json\u0026#39;, \u0026#39;wt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as fp: json.dump(cityList, fp) if (cityName != None): for city in cityList: if (city[\u0026#39;name\u0026#39;] == cityName): return city[\u0026#39;code\u0026#39;] else: return json_data[\u0026#39;zpData\u0026#39;][\u0026#39;locationCity\u0026#39;][\u0026#39;code\u0026#39;] 接下来，我们可以编写searchJobs()方法来实现职位的检索：\ndef searchJobs(self, cityName, query, page=1): cityCode = self.extractCity(cityName) if (cityCode != None): searchUrl = \u0026#39;https://www.zhipin.com/job_detail/?query={query}\u0026amp;city={cityCode}\u0026amp;industry=\u0026amp;position=\u0026amp;page={page}\u0026#39;.format(cityCode=cityCode, query=query, page=str(page)) html = self.makeRequest(searchUrl) soup = BeautifulSoup(html) details = soup.find_all(name=\u0026#39;div\u0026#39;,attrs={\u0026#39;class\u0026#39;,\u0026#39;job-primary\u0026#39;}) jobItems = [] companyItems = [] for detail in details: jobItem = self.extractJob(detail) if (jobItem == None): continue else: jobItems.append(jobItem) companyItem = self.extractCompany(detail) if (companyItem == None): continue else: jobItem[\u0026#39;company\u0026#39;] = companyItem[\u0026#39;title\u0026#39;] jobItem[\u0026#39;industry\u0026#39;] = companyItem[\u0026#39;industry\u0026#39;] companyItems.append(companyItem) return (jobItems,companyItems) 这里我们会用到requests、fake_useragent以及BeautifulSoup，如果你经常编写爬虫的话，对它们应当不会感到陌生。唯一需要注意的有两点：**第一，Boss 直聘会封杀爬虫的 IP，所以，可以考虑从互联网上抓取免费的代理 IP 作为代理池，每次发起请求时随机选取一个 IP 作为代理 IP，这样可以有效地减少被封杀的可能。第二，Boss 直聘的 Cookie 最多只能使用 4 次，超过 4 次后就需要重新获取 Cookie。**目前，我没有找到好的解决方案，有兴趣的朋友可以参考 2019 年末逆向复习系列之 Boss 直聘 Cookie 加密字段__zp_stoken__逆向分析 这篇博客做一点逆向方面的研究，或者考虑使用PyExecJS载入前端 JavaScript 脚本来生成 Cookie，因为逆向并不是我这篇博客的重点。在解决了这两个问题后，我们就可以提取出每一页的岗位和公司信息，而这些都可以通过BeautifulSoup解决，这里不再赘述，关于 Boss 直聘部分的源代码，请参考：https://github.com/qinyuanpei/job-analyse/blob/master/Spider/bossSpider.py。\n智联招聘 智联招聘相对于 Boss 直聘要简单一点，通过抓包分析，我们可以找到这样一个地址：https://fe-api.zhaopin.com/c/i/sou?at={at}\u0026amp;_v={v}\u0026amp;x-zp-page-request-id={x-zp-page-request-id}\u0026amp;x-zp-client-id={v}\u0026amp;MmEwMD={MmEwMD}。通过这个接口可以直接获得 JSON 格式的数据，可想要构造这几个参数出来，实在是有一点困难，因为它遇到和 Boss 直聘一样的问题，基本都需要一定的逆向功底，而如果尝试去解析 DOM，你会发现它的前端使用了 Vue.js，换句话说，这个网站是由前端完成渲染的，这意味着，如果我直接访问https://sou.zhaopin.com/?jl=854这个地址，是无法拿到可以解析的 DOM 结构的，这就多少会有一点尴尬。所以，实际上博主最后没有实现智联招聘的爬虫，因为在这上面投入太多的精力，实在有一点得不偿失。这里简单说一下思路，基本上我们需要以POST方式调用这个接口，然后在 Body 中写入下面的结构：\n{\u0026#34;pageSize\u0026#34;:\u0026#34;30\u0026#34;,\u0026#34;cityId\u0026#34;:854,\u0026#34;workExperience\u0026#34;:\u0026#34;-1\u0026#34;,\u0026#34;companyType\u0026#34;:\u0026#34;-1\u0026#34;,\u0026#34;employmentType\u0026#34;:\u0026#34;-1\u0026#34;,\u0026#34;jobWelfareTag\u0026#34;:\u0026#34;-1\u0026#34;,\u0026#34;kt\u0026#34;:\u0026#34;3\u0026#34;,\u0026#34;at\u0026#34;:\u0026#34;20673d42d62d48c38add329318fb9e2c\u0026#34;,\u0026#34;rt\u0026#34;:\u0026#34;84a950e77e054854b4d2f9d90826d063\u0026#34;,\u0026#34;_v\u0026#34;:\u0026#34;0.97312845\u0026#34;,\u0026#34;userCode\u0026#34;:662040894,\u0026#34;eventScenario\u0026#34;:\u0026#34;pcSearchedSouIndex\u0026#34;,\u0026#34;cvNumber\u0026#34;:\u0026#34;JM620408945R90500002000\u0026#34;} 这里依然要解决 Cookie 的问题，它这个 Cookie 简直不能更恶心，因为参数实在是太多了：\n这个Cookie相当变态\r那么，我放弃了，感兴趣的朋友可以顺着这个思路继续探索，加油！\n前程无忧 相对于 Boss 直聘和智联招聘，前程无忧要更简单一点，这种简单是从心智体验上来讲。经过分析，它的地址为：https://search.51job.com/list/200200,000000,0000,00,9,99,+,2,{page}.html?lang=c\u0026amp;postchannel=0000\u0026amp;workyear=99\u0026amp;cotype=99\u0026amp;degreefrom=99\u0026amp;jobterm=99\u0026amp;companysize=99\u0026amp;ord_field=0\u0026amp;dibiaoid=0\u0026amp;line=\u0026amp;welfare=。它的简单体现在，可以直接通过修改page这个参数来达到抓取某一页数据的目的，它本身没有特别强大的反爬机制，所以，事实上，它是整个数据分析主要的数据来源，在这个地址里可能有一点大家看不懂的东西，没关系，博主一样看不懂，我们只需要知道它表示西安就可以了，如果想抓取某个城市的职位信息，可以直接在前程无忧上搜索，地址栏会告诉你这一切是如何变化的。需要说明的是，前程无忧的职位信息是存储在window.__SEARCH_RESULT__这个变量里的，所以，我们通过这个正则直接去匹配它即可，不需要再去解析 DOM，这再次体现出了它的简单：\ndef searchJobs(self, cityName, query, page=1): cityCode = cityName if (cityCode != None): searchUrl = \u0026#39;https://search.51job.com/list/200200,000000,0000,00,9,99,+,2,{page}.html?lang=c\u0026amp;postchannel=0000\u0026amp;workyear=99\u0026amp;cotype=99\u0026amp;degreefrom=99\u0026amp;jobterm=99\u0026amp;companysize=99\u0026amp;ord_field=0\u0026amp;dibiaoid=0\u0026amp;line=\u0026amp;welfare=\u0026#39;.format(page=str(page)) html = self.makeRequest(searchUrl) data = re.findall(\u0026#39;window.__SEARCH_RESULT__ =(.+)}\u0026lt;/script\u0026gt;\u0026#39;, str(html))[0] + \u0026#34;}\u0026#34; details = json.loads(data)[\u0026#39;engine_search_result\u0026#39;] jobItems = [] companyItems = [] for detail in details: jobItem = self.extractJob(detail) if (jobItem == None): continue else: jobItems.append(jobItem) companyItem = self.extractCompany(detail) if (companyItem == None): continue else: companyItems.append(companyItem) return (jobItems,companyItems) 因为这里真正起作用的实际上只有page这个参数，所以，我们只需要循环每一页就可以了，博主就是通过这个方法抓取了大量的职位信息。同样地，我们通过extractJob()和extractCompany()两个方法来组装职位和公司的信息，最终通过元组的形式返回，由调用者自己决定要如何去处理这些数据。虽然，我们选择了 MongoDB 这样的数据库，它不像关系型数据库那样重视 Schema，可为了我们最终分析数据的时候方便一点，还是建议使用一致的数据结构。关于前程无忧部分的源代码，请参考：https://github.com/qinyuanpei/job-analyse/blob/master/Spider/job51Spider.py。\n数据分析 在开始今天的数据分析前，首先向大家展示下爬虫抓取到的数据。截止到写这篇的博客的时间，博主一共收集了 20000 个左右的职位/公司信息，如下图所示：\n职位信息展示\r公司信息展示\r接下来，我们从数据库中读取这些数据以开始下面的分析：\nstore = Store.mongoStore.MongoStore(\u0026#39;default\u0026#39;) jobs = list(store.find(\u0026#39;job\u0026#39;,{})) companies = list(store.find(\u0026#39;company\u0026#39;,{})) 行业结构分析 俗话说，“男怕入错行，女怕嫁错郎”。我们今天的社会是一个非常“苛刻”的社会，它要求每一个人在“合适”的年龄做“该做”的事情，可要达到这样一个“标准”则是非常不容易的。在综艺节目《令人心动的 Offer》里，“大龄”、“裸辞”、“背水一战”的丁辉，受到了来自红圈律所的“精英”们的区别对待，仿佛一个人的人生不能有一丁点的差错。或许人生的“试错”成本真的非常高，高到人们在 30 岁左右的时候纷纷遭遇中年危机。所以，我们实在有必要去了解一个行业，它目前的求职现状到底是什么样的，这里以西安市为例：\ndef analyse_industry(): industries = list(map(lambda x:x[\u0026#39;industry\u0026#39;],companies)) counter = Counter(industries) counter = sorted(counter.items(),key = lambda x:x[1],reverse = True)[0:15] counter = dict(counter) c = ( Pie() .add(\u0026#34;\u0026#34;,[list(z) for z in zip(counter.keys(), counter.values())],label_opts=opts.LabelOpts(is_show=True, position=\u0026#34;center\u0026#34;),) .set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;西安市求职招聘行业结构分析(Top15)\u0026#34;,pos_left=325), legend_opts=opts.LegendOpts(type_=\u0026#34;scroll\u0026#34;, pos_left=\u0026#34;left\u0026#34;, orient=\u0026#34;vertical\u0026#34;), ) .set_series_opts(label_opts=opts.LabelOpts(formatter=\u0026#34;{b}: {c}\u0026#34;)) .render(\u0026#34;./Reports/西安市求职招聘行业结构分析(Top15).html\u0026#34;) ) 下面是整个西安市求职招聘排名前 15 位的行业结构：\n西安市求职招聘行业结构分析(Top15)\r可以注意到，其中占据份额较大的行业主要有：房地产、建筑/建材/工程、计算机软件、电子技术/半导体/集成电路、教育/培训/院校等。\n学历结构分析 作为一个“西漂”，博主对西安最深的一个印象就是，西安有着非常丰富的高校资源，正因为如此，博主一度认为西安遍历都是研究生。因为在过去的四年里，的确接触过不少研究生学历的同事，相比之下，博主这样一个普通 211、非科班的本科生，着实显得有点相形见绌。我在之前的博客里有提到去中兴面试的经历，这个经历让我第一次意识到，学历和非科班的出身，终究有一天会成为你进入国企或者大厂的门槛，所以，博主在考虑要不要去读一个在职的研究生。这种认识到底是不是幸存者偏差呢，我们来看看数据分析的结果：\ndef analyse_education(): eduInfos = list(map(lambda x:x[\u0026#39;eduInfo\u0026#39;], jobs)) counter = Counter(eduInfos) counter = sorted(counter.items(),key = lambda x:x[1],reverse = True) counter = dict(counter) c = ( Pie() .add(\u0026#34;\u0026#34;,[list(z) for z in zip(counter.keys(), counter.values())]) .set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;西安市求职招聘学历结构分析\u0026#34;,pos_left=325), legend_opts=opts.LegendOpts(type_=\u0026#34;scroll\u0026#34;, pos_left=\u0026#34;left\u0026#34;, orient=\u0026#34;vertical\u0026#34;), ) .set_series_opts(label_opts=opts.LabelOpts(formatter=\u0026#34;{b}: {c}\u0026#34;)) .render(\u0026#34;./Reports/西安市求职招聘学历结构分析.html\u0026#34;) ) 我承认我在胡说八道，因为结果非常的 Amazing 啊，非常的毕导啊：\n西安市求职招聘学历结构分析\r我没有想到研究生以上的比例这么低，可能是因为我身边这些同事的层次都比较高吧，哈哈！可当学历逐渐成为一种门槛，即使你比本科生多上三年学，最后一样要在这世界上颠沛流离的时候，是不是会和博主有一样的疑问，为什么 IT 行业会变成劳动密集型产业？是因为门槛低让这个行业劣币驱逐良币呢，还是拥有高学历的人才一样要去拧螺丝？\n薪资待遇分析 有时候，我会忍不住想，是不是在任何一个城市里，人们工资增长永远都赶不上房价增长？如果真的是这样，我们为什么又要从三线小城市出来呢？可能是觉得大城市有更好的机会吧，可转眼到了 2020 年，上大学时一心想从事这个行业的我，当时无论如何都想不到若干年后要面对“35 岁”这个问题。当“996”作为一种“福报”的声音越来越强烈，曾经我们认为的那“一点点”机会，真的就是只剩下“一点点”。人有时候就是在靠着那点“不切实际”过日子，譬如固执的认为收入会越来越高，可其实任何工作都是有天花板的存在的，以大多数普通人的努力程度，一辈子连天花板都可能触碰不到，真实的薪资水平到底是什么样的呢？年薪 30 万果真如此寻常等闲？我们一起来看：\ndef analyse_salary(): salaries = list(map(lambda x:x[\u0026#39;avgSalary\u0026#39;], list(filter(lambda x:x[\u0026#39;avgSalary\u0026#39;] != 0, jobs)))) counter = Counter(salaries) counter = sorted(counter.items(),key = lambda x:x[1],reverse = True) records = {\u0026#39;3000元以下\u0026#39;:0, \u0026#39;3000元-5000元\u0026#39;:0, \u0026#39;5000元-8000元\u0026#39;:0, \u0026#39;8000元-12000元\u0026#39;:0, \u0026#39;12000元-15000元\u0026#39;:0, \u0026#39;15000元以上\u0026#39;:0} for (k,v) in counter: if (k \u0026lt; 3000): records[\u0026#39;3000元以下\u0026#39;] += v if (k \u0026gt;= 3000 and k \u0026lt; 5000): records[\u0026#39;3000元-5000元\u0026#39;] += v if (k \u0026gt;= 5000 and k \u0026lt; 8000): records[\u0026#39;5000元-8000元\u0026#39;] += v if (k \u0026gt;= 8000 and k \u0026lt; 12000): records[\u0026#39;8000元-12000元\u0026#39;] += v if (k \u0026gt;= 12000 and k \u0026lt; 15000): records[\u0026#39;12000元-15000元\u0026#39;] += v if (k \u0026gt;= 15000): records[\u0026#39;15000元以上\u0026#39;] += v counter = dict(records) c = ( Pie() .add(\u0026#34;\u0026#34;,[list(z) for z in zip(counter.keys(), counter.values())]) .set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;西安市求职招聘平均工资分析\u0026#34;,pos_left=325), legend_opts=opts.LegendOpts(type_=\u0026#34;scroll\u0026#34;, pos_left=\u0026#34;left\u0026#34;, orient=\u0026#34;vertical\u0026#34;), ) .set_series_opts(label_opts=opts.LabelOpts(formatter=\u0026#34;{b}: {c}\u0026#34;)) .render(\u0026#34;./Reports/西安市求职招聘平均工资分析.html\u0026#34;) ) 由此，我们得到了整个西安市的收入分布情况。显然，**5000~8000*这个收入区间才是大多数普通人的真实写照：\n西安市求职招聘平均工资分析\r我们继续分析，哪些行业的平均工资更高一点，因为这样你会找到同龄人的参考对象：\ndef analyse_industry_salary(): filtered = list(filter(lambda x:x[\u0026#39;avgSalary\u0026#39;] != 0, jobs)) salaries = {} for job in filtered: if (job[\u0026#39;industry\u0026#39;] == \u0026#39;\u0026#39;): continue if salaries.get(job[\u0026#39;industry\u0026#39;]) == None: salaries[job[\u0026#39;industry\u0026#39;]] = [job[\u0026#39;avgSalary\u0026#39;]] else: salaries[job[\u0026#39;industry\u0026#39;]].append(job[\u0026#39;avgSalary\u0026#39;]) counter = {} for (industry, data) in salaries.items(): counter[industry] = int(sum(data) / len(data)) counter = sorted(counter.items(),key = lambda x:x[1],reverse = True)[0:15] counter = dict(counter) c = ( Bar() .add_xaxis(list(counter.keys())) .add_yaxis(\u0026#34;平均工资\u0026#34;, list(counter.values())) .set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;西安市求职招聘行业工资分析(Top15)\u0026#34;, pos_left=325), legend_opts=opts.LegendOpts(type_=\u0026#34;scroll\u0026#34;, pos_left=\u0026#34;left\u0026#34;, orient=\u0026#34;vertical\u0026#34;), ) .render(\u0026#34;./Reports/西安市求职招聘行业工资分析(Top15).html\u0026#34;) ) 类似地，我们这里取排名前 15 位的行业进行分析：\n西安市求职招聘行业工资分析\r可以注意到，工资收入靠前的行业主要集中在：互联网/移动互联网/计算机软件/电子商务、信托/拍卖/典当/担保、 智能硬件、法律、学术/科研、保险、房地产、金融/投资/证券、美容/保健等行业。可惜，从一名 IT 行业从业者的角度来看，西安实际上并没有真正的互联网公司。这个世界常常如此，每个月挣 15K 的人感慨自己买不起房，可还有那么多收入在 8K 以下的人群，还能再说什么呢？\n学历与薪资关系分析 通常大家都认为，学历越高，薪资就会越高，那么，这个是否符合实际情况呢，我们一起来看一下：\ndef analyse_eduInfo_salary(industry=None): filtered = list(filter(lambda x:x[\u0026#39;avgSalary\u0026#39;] != 0, jobs)) if (industry != None): filtered = list(filter(lambda x:x[\u0026#39;industry\u0026#39;] == industry, filtered)) salaries = {} for job in filtered: if (job[\u0026#39;eduInfo\u0026#39;] == \u0026#39;\u0026#39;): continue eduInfo = job[\u0026#39;eduInfo\u0026#39;] if (eduInfo in [\u0026#39;学历不限\u0026#39;,\u0026#39;不限\u0026#39;]): eduInfo = \u0026#39;学历不限\u0026#39; if salaries.get(eduInfo) == None: salaries[eduInfo] = [job[\u0026#39;avgSalary\u0026#39;]] else: salaries[eduInfo].append(job[\u0026#39;avgSalary\u0026#39;]) counter = {} for (eduInfo, data) in salaries.items(): counter[eduInfo] = int(sum(data) / len(data)) c = ( Bar() .add_xaxis(list(counter.keys())) .add_yaxis(\u0026#34;平均工资\u0026#34;, list(counter.values())) .set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;西安市求职招聘学历与薪资关系分析\u0026#34;, pos_left=325), legend_opts=opts.LegendOpts(type_=\u0026#34;scroll\u0026#34;, pos_left=\u0026#34;left\u0026#34;, orient=\u0026#34;vertical\u0026#34;), ) .render(\u0026#34;./Reports/西安市求职招聘学历与薪资关系分析.html\u0026#34;) ) 下面给出可视化以后的结果：\n西安市求职招聘学历与薪资关系分析\r可以发现，整体上学历和薪资是呈正比的，甚至“不限学历”比“高中”的工资还要高一点。可如果有那么多“不限学历”的工作，为什么今年还有那么多人找不到工作呢？我想，这就是我们常说的选择，我们之所以付出努力，无非是想比别人多一点选择，可正如纳什均衡理论所言，如果我们大家都去选择相同的东西，最后的结果可能是大家都得不到这样东西。可话又说回来，明明都知道那个行业热门，如果不做这个选择，反而才是最奇怪的吧……\n经验与薪资关系分析 如果说学历与薪资呈正比，那么经验与薪资则不一定满足这样的关系，因为经验其实是一个不准确的“度量”单位。以 IT 行业为例，在一家公司里，老员工的薪资被新员工的薪资“倒挂”是经常发生的事情。所以，人们似乎达成了某种共识，即期待公司主动涨薪是非常困难的，你唯一能做的就是在面试时争取更多的薪资。这就要说到经验这个话题，IT 行业技术日新月异的特点，实在很难让经验变成一个“褒义词”，因为经验在积累的同时同样在“过期”，更不用说那些一直在“重复”的人了，所以，我觉得掌握通用型的知识譬如算法、数据结构等会更重要。\ndef analyse_exps_salary(industry=None): filtered = list(filter(lambda x:x[\u0026#39;avgSalary\u0026#39;] != 0, jobs)) if (industry != None): filtered = list(filter(lambda x:x[\u0026#39;industry\u0026#39;] == industry, filtered)) salaries = {} for job in filtered: if (job[\u0026#39;exps\u0026#39;] == \u0026#39;\u0026#39;): continue exps = job[\u0026#39;exps\u0026#39;] exps = exps.replace(\u0026#39;经验\u0026#39;,\u0026#39;\u0026#39;) if (exps in [\u0026#39;1年\u0026#39;,\u0026#39;1年以内\u0026#39;,\u0026#39;2年\u0026#39;,\u0026#39;1-3年\u0026#39;]): exps = \u0026#39;1-3年\u0026#39; if (exps in [\u0026#39;不限\u0026#39;,\u0026#39;经验不限\u0026#39;]): exps = \u0026#39;经验不限\u0026#39; if (exps in [\u0026#39;3到4年\u0026#39;,\u0026#39;3到5年\u0026#39;,\u0026#39;3-4年\u0026#39;,\u0026#39;3-5年\u0026#39;]): exps = \u0026#39;3-5年\u0026#39; if (exps in [\u0026#39;8到9年\u0026#39;,\u0026#39;5到10年\u0026#39;,\u0026#39;5到7年\u0026#39;,\u0026#39;8-9年\u0026#39;,\u0026#39;5-10年\u0026#39;,\u0026#39;5-7年\u0026#39;]): exps = \u0026#39;5-10年\u0026#39; if salaries.get(exps) == None: salaries[exps] = [job[\u0026#39;avgSalary\u0026#39;]] else: salaries[exps].append(job[\u0026#39;avgSalary\u0026#39;]) counter = {} for (industry, data) in salaries.items(): counter[industry] = int(sum(data) / len(data)) c = ( Bar() .add_xaxis(list(counter.keys())) .add_yaxis(\u0026#34;平均工资\u0026#34;, list(counter.values())) .set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;西安市求职招聘工作经验与薪资关系分析\u0026#34;, pos_left=325), legend_opts=opts.LegendOpts(type_=\u0026#34;scroll\u0026#34;, pos_left=\u0026#34;left\u0026#34;, orient=\u0026#34;vertical\u0026#34;), ) .render(\u0026#34;./Reports/西安市求职招聘工作经验与薪资关系分析.html\u0026#34;) ) 一切都会向着我们期待的方向发展吗？我们拭目以待：\n西安市求职招聘工作经验与薪资关系分析\r可以发现，整体上，经验越丰富，薪资待遇会越高。前提是你真的收获了经验，而不是在岁月的蹉跎里单单收获了皱纹和沧桑。这是我们每个人都应该去反思的一个问题，如果一切的经验都有过时的那一天，至少你真的拥有过它们，就像爱情这种东西一样。\n招聘热词分析 在招聘网站上，一般都会以标签的方式，对职位要求、公司福利等进行描述，譬如五险一金、弹性打卡等等，通过这些标签，我们就能对职位以及公司有个基本印象。所以，我们可以通过分析这些标签，来展示在求职过程中求职者和招聘方各自关注哪些因素。下面，我们将以词云的形式来展示这些标签：\n# 提取岗位关键字 job_tags = [] for item in map(lambda x:x[\u0026#39;tags\u0026#39;], jobs): if (item != None): job_tags.extend(item) # 提取公司关键字 company_tags = [] for item in map(lambda x:x[\u0026#39;tags\u0026#39;], companies): if (item != None): company_tags.extend(item) def analyse_extract_tags(words,title): words = list(filter(lambda x:x!=\u0026#39;\u0026#39;, words)) data = Counter(words) c= ( WordCloud() .add(series_name=\u0026#34;热门词汇\u0026#34;, data_pair=data.items(), word_size_range=[6, 66]) .set_global_opts( title_opts=opts.TitleOpts( title=title, title_textstyle_opts=opts.TextStyleOpts(font_size=23) ), tooltip_opts=opts.TooltipOpts(is_show=True), ) .render(\u0026#39;.\\Reports\\{title}.html\u0026#39;.format(title=title)) ) analyse_extract_tags( words=job_tags, title=\u0026#39;西安市求职者热词分析\u0026#39; ) analyse_extract_tags( words=company_tags, title=\u0026#39;西安市招聘者热词分析\u0026#39; ) 西安市求职者热词分析\r西安市招聘者热词分析\r可以注意到，五险一金、年终奖金、专业培训、绩效奖金、节日福利、带薪年假是大家普遍关注的点。\n本文小结 本文主要抓取了 Boss 直聘、智联招聘、前程无忧三个招聘网站的职位信息和公司信息，并在此基础上对西安市的求职招聘进行了数据分析，主要从行业结构、学历结构、薪资待遇、学历与薪资关系、经验与薪资关系、招聘热词等方面入手，经分析，针对西安市的求职招聘的求职招聘，我们可以得出下面的结论：(1)西安市排名相对靠前的行业主要有：房地产、建筑/建材/工程、计算机软件、电子技术/半导体/集成电路、教育/培训/院校等；(2)西安市招聘的职位中大专和本科学历约占总职位的 75%左右，硕士以及博士学历相对较低；(3)西安市的平均薪资中，5000~8000这个收入区间是大多数普通人的真实写照，工资收入靠前的行业主要集中在：互联网/移动互联网/计算机软件/电子商务、信托/拍卖/典当/担保、 智能硬件、法律、学术/科研、保险、房地产、金融/投资/证券、美容/保健等行业；(4)拥有高学历的人更有可能拥有高薪资；(5)整体上，经验越丰富，薪资待遇会越高。前提是你真的收获了经验，而不是在岁月的蹉跎里单单收获了皱纹和沧桑；(6)在整个求职招聘中，无论是求职者还是招聘者，普遍看重的因素有：五险一金、年终奖金、专业培训、绩效奖金、节日福利、带薪年假等。虽然一开始的目的是想知道西安有多少“996”的公司，不过在后续的实现过程中，发现从 Boss 直聘上抓取不到这些信息，所以，最终呈现出的结果就变成了现在这个样子，考虑到篇幅，关于公司规模、公司类型的分析，没有在这里写出来，如果大家感兴趣，可以参考：https://github.com/qinyuanpei/job-analyse/tree/master。以上就是这篇博客的全部内容啦，谢谢大家！\n","date":"2020-12-05T12:49:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2147036181/","slug":"2147036181","tags":["Python","求职","可视化","西安"],"title":"厉害了！打工人用 Python 分析西安市职位信息"},{"categories":["生活感悟"],"content":"其实，这段故事说出来，多少有一点难为情，因为我实在没有想到，这一切会变得这样艰难。\n10 月份从上一家公司离职的时候，当时，我手上有两个 Offer，一家是做旅游类产品的创业公司，一家则是声名狼藉的中软国际。因为刚来西安时，面对人生地不熟的新环境，曾经在这里有过一段时间的工作经历，所以，我从本能上排斥再回到那种地方。而创业公司本身的不稳定性，一度让我感到纠结，而最终的结果是，我放弃了这两份 Offer。此时，对于一个工作刚满 5 年的人来说，一个月 13K 或 14K 的薪水，我感到相当的知足，可惜人生常常没有最优解，选择与妥协才是常态。\n此后，我面试了奥博杰天(Objectiva)，在一轮面试后被评定为中级以后，我便再没有接到参加复试的通知，后来，HR 告诉我他们只招高级以上的开发人员。关于这家公司，一件非常有趣的事情是，我的一位朋友在获得这家公司的工作后，因为适应不了终日远程办公的痛苦，最终还是选择从这里离开。大概这是为了印证围城里的那句话，“城外的人想进去，城里的人想出来”。虽然“面试造火箭，入职拧螺丝”有时是求职者的常态，可偏偏你连去拧螺丝的机会都没有。\n后来找到一家做背调业务的公司，甚至都准备好要在这里开始新的旅程，可当我看到毫无架构可言的项目时，终于还是倒吸了一口冷气。虽然此前和面试官交流了很多相对前沿的内容，可真正入职以后，还是被安排去做维护遗留项目的工作，更让我感到沮丧的是，到第三天的时候，一位老员工突然告诉我，他要转岗去做前端，需要交接一部分工作给我。对于这一件事情，我之前的朋友都安慰我，这些问题你都知道该怎么去解决，因为企业招聘你过来就是希望你去解决这些问题。可从一个普通与昂工的角度来看，一个组织实在很难从底层去做出什么改变，何况这家公司采取了和上家公司一样的策略，寄希望于空降一个某个大公司的架构师。\n现在回想起来，在试用期期间离开，或许有一点冲动使然，可听到后来接替我职位的新人，同样在呆了一周后离开了。所以，此时，你问我对这份工作感到后悔嘛？坦白说，我并不知道该怎么去说，而更戏剧性的反转，则是后来一家公司联系到我，问我有没有意向去做这些背调公司的外包，你要知道，在西安几乎没有互联网公司，所以，当绕了一大圈后，再回到原点的时候，我不禁哑然失笑。再后来，经过内推拿到了西安中兴的面试资格，我感觉不管是笔试还是面试，我的完成度都还可以，尤其是在第一轮面试中，从面试官那里收获了很多的东西。而在复试的过程中，领导们对我非科班出身的挑剔，多少让我想起令人心动的 Offer 中，因为非法本 + 年龄大被嫌弃的丁辉，和这些人相比，我们这些普通人根本称不上努力，可至少尊重那些一直在默默努力着的人啊。\n这样辗转了一周，终于还是没能等到中兴的电话，我想应该是彻底凉了吧，果然人生没有那么多逆袭的可能，年少时虚度的光阴，终究会在未来某一天让你感到后悔。就这样，一直等一直面，陆陆续续地接到像软通动力这种外包性质的 Offer，在面试中更是见识了大大小小的各种公司，我发现整个行业都在疯狂的内卷，996 在道德上的批判声还没有褪去，人们又开始钻劳动法的空子，搞大小周、周一/三/四强制加班，这样做的不单单有华为、中兴这样的大厂，同样还有这些小公司。\n今年因为疫情的原因，医疗行业应该赚得盆满钵满，可好多公司还是在疯狂地抢占市场，类似的还有物流行业，可在效益还不错的情况下，我只看到了越来越变本加厉的压榨，我看日剧《下町火箭》的时候就在想，虽然佃制作所这样一家小公司一样会加班，可人家的是火箭级别的阀门和加速器啊，人家愿意花时间去钻研工艺技术，而我们只能通过比别人早交付来赢得客户的青睐，那这是否说明，我们和竞争对手间的服务差异化其实并不大呢？\n现在，只要是个互联网公司，都能蹭一蹭双十一的热度，可在这场疫情背后，实体经济有多么的不景气，今天人们找工作就有多绝望，毕竟连房地产行业都表现出颓势，互联网行业不可能一直这样”热“下去，终有一天，一切都会回复到冷静。那么，对于没有“互联网”的西安 IT 圈子，程序员未来的退路又在哪里呢？我身边有很多 30 多岁的中年男人，那种说不出来的沉重感，时常让我对未来感到迷茫，他们都曾劝我回到三线小城市、考个公务员了却残生。也许我那个时候不大懂，而此时此刻终于感受到这种无奈，业内 35 岁的内卷化越来越严重，甚至我没想到，有一天大小周和周一/三/四强制加班会变成一种常态化。\n这次面试中兴的经历，让我意识到，虽然进入 IT 行业的门槛非常低，可同样不幸的是，大厂/国企对相关岗位的门槛在不断加高，研究生起步基本就是标配啦，我甚至怀疑，我没有通过中兴的面试，是否和我非科班的出身以及没有考过的四级有关，这的确是一个现实问题，不管你想走技术路线，还是想走管理路线，一段大厂的经历能为你增加不少闪光点，可如果你拼进全力依然被这些门槛挡住，你是否会对未来感到迷茫呢？因为西安的 IT 圈子就这么大，你下一次换工作依然会面对这些公司，而人生又有多少个换工作的机会呢？\n所以，我一直在想要不要继续留在西安，在这边固然比老家多“一点”可能性，可就真的只是一点点而已，现在再次充满变数的时候，这一点点的优势就变得不再明显。四年前，为了离当时女朋友近一点而来西安，如今四年过去了，渐渐地很多问题再次浮现出来，也许，那个时候的她就想到了未来的各种可能吧！有天晚上，我找一位很久之前认识的朋友，向他询问关于 35 岁这道坎的想法。当多年未曾听到的声音再次传入耳朵，突然感到一阵亲切，他非常平静地对我说，“我今年已经 36 岁了，这道坎对我来说已经是过去了”。的确，我们总是对未来充满各种各样的担忧，可想到两年前，我一样是怀着忐忑的心情去了上家公司。\n我和朋友在讨论这些问题的时候，都觉得未来去做个 IT 讲师是个好归宿，虽然再反过头去割新一茬年轻人的韭菜，有一点屠龙少年终成恶龙的意味，可我觉得，人生还是早一点考虑备选答案比较好。我们之所以敢去背负 30 年的房贷，是因为我们愿意去相信“未来越来越好”，可事实上是你的身体每一天都在衰老，虽然现在退休年龄延迟到 60 岁了，可想到还在幸苦忙碌着的父母，还是会觉得羞愧难当，古人说 30 而立，可我还没有立起来，父母已经在老去了。说了这么多，工作我还是会去努力地找，但应该不会不给自己留一点时间，因为除了找对象以外，我给自己订了几个学习计划，比如：软考的中/高级证书、英语练习(考个英语证书)、强化公开场合的演讲能力、Google PDE 考试。\n一个 30+的朋友和我说，他想自己干点啥，因为他觉得他再找工作就没人要了，也许生活就是永远这么充满变化吧，就像不变的只有变化本身一样，提前焦虑未来没有什么用，现在的规划将来不一样会按部就班，我只能说，多去想想自己有什么，如果你只是比别人能加班，这实在算不上什么过人的长处，因为随时有年轻人可以替换你下来。人活着啊，不能光长年龄和皱纹，想想两度背水一战的丁辉，我们这点努力能叫做拼尽全力吗？这就是我，一个“西漂”四年的外地打工人的一点想法，如果你有更好的解决“内卷”的思路，我会非常感谢你告诉我这些。\n","date":"2020-11-18T12:49:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1809438689/","slug":"1809438689","tags":["西漂","程序员","求职","西安"],"title":"一个西漂打工人的求职心路"},{"categories":["编程语言"],"content":"前几天，有位朋友问我，你平时都是怎么去排查一个程序的性能问题的啊。不要误会，这位朋友不是我啦，因为我真的有这样一位叫做 Toby 的朋友。说到性能问题，可能大家立马会想到类似并发数、吞吐量、响应时间、QPS、TPS等等这些指标，这些指标的确可以反映出一个系统性能的好坏。可随着我们的系统结构变得越来越复杂，要找到这样一个性能的“损耗点”，同样会变得越来越困难。在不同的人的眼中，对于性能好坏的评判标准是不一样的，譬如在前端眼中，页面打开速度的快慢代表着性能的好坏；而在后端眼中，并发数、吞吐量和响应时间代表着性能的好坏；而在 DBA 眼中，一条 SQL 语句的执行效率代表着性能的好坏。更不用说，现实世界中的程序要在硬件、网络的世界里来回穿梭了，所以，从 80%的功能堆积到 100%，是件非常容易的事情；而从 80%的性能优化到 85%，则不是件非常轻松的事情。想清楚这一点非常简单，因为我们的系统从来都不是简单的1 + 1 = 2。此时，我们需要一个性能分析工具，而今天给大家分享的是 JetBrains 出品的 dotTrace 。\n快速开始(Quick Start) 安装软件的过程此处不表，这里建议大家同时安装 dotTrace 和 dotMemery。因为这都是 JetBrains 全家桶中的软件，安装的时候选一下就可以了，可谓是举手之劳。安装好以后的界面是这样的，可以注意到，它可以对进程中的 .NET 应用、本机的 .NET 应用以及远程的 .NET 应用进行检测，因为这里写一个 .NET Core 应用来作为演示，所以，我们选择 Profile Local App：\ndotTrace主界面\r在这里，我们准备了一个简单的控制台程序：\npublic class Program { static void Main(string[] args) { CPUHack(); MemeryHack(); } public static void MemeryHack() { Console.ReadLine(); var bytes = GC.GetTotalAllocatedBytes(); Console.WriteLine($\u0026#34;AllocatedBytes: { bytes } bytes\u0026#34;); var list = new List\u0026lt;byte[]\u0026gt;(); try { while (true) { list.Add(new byte[85000]); } } catch (OutOfMemoryException) { Console.WriteLine(nameof(OutOfMemoryException)); Console.WriteLine(list.Count); bytes = GC.GetTotalAllocatedBytes(); Console.WriteLine($\u0026#34;AllocatedBytes: { bytes } bytes\u0026#34;); } Console.ReadLine(); } public static void CPUHack() { Parallel.For(0, Environment.ProcessorCount, new ParallelOptions() { MaxDegreeOfParallelism = Environment.ProcessorCount }, i =\u0026gt; { }); } } 其中，CPUHack()方法来自：打爆你的 CPU; MemeryHack()方法来自：通过代码实现 OutOfMemory。顾名思义，我们将利用这两个方法来分别测试 dotTrace 和 dotMemery。\ndotTrace 目前支持以下平台：.NET、.NET Core、WPF、UWP(Universal Windows Platform)、ASP.NET、Windows 服务、WCF、Mono 和 Unity。可以注意到它有四种监测方式，即 Sampling、Tracing、Line by Line 以及 Timeline。按照界面上的描述，Sampling 适用于大多数场景下调用时间的精确测量、Tracing 适用于算法复杂度分析场景下调用次数的精确测量、Line by Line 适用于更高级别的使用场景，Timeline 适用于含多线程在内的数据处理的精确测量。所以，我们这里选择好一个可执行文件，然后选择 Sampling，再点击 “Run”：\n对进行程序进行采样、生成快照\r此时，我们会看到对应程序的的工具栏，我们可以点击 “Get Snapshot and Wait” 进行采样，每次采样会生成一个快照，默认情况下会自动打开生成的快照。我们还可以点击 “Start” 重新进行采样，直至采集到满意的样本为止，而在完成采样后，则可以点击 “Kill” 结束采样。下面来看看生成的快照：\ndotTrace性能快照\r通过这两图，我们可以非常清晰的看到，最耗时的正是我们这里的CPUHack()方法，并且这里一共有四个线程，这是因为博主的计算机使用的是一款 4 核的 i3 处理器，并且在dotTrace中可以直接看到相关的代码片段，当然，这一切的前提是你没有对应用程序做过混淆处理，这样，我们就完成了一个简单的性能分析。类似地，我们启动dotMemery。此时，可以得到下面的结果：\ndotMemery内存分析\r这里，我们通过\u0026lt;YourApp\u0026gt;.runtimeconfig.json文件，设定了 GC 堆的最大值是 1M，而每次向列表中添加超过 85K 的 byte 数组时，当前对象会被分配到大对象堆上。通过这张图我们可以很清楚的看到，整个曲线中蓝色区域的 LOH 占了绝对的比例，换言之，几乎所有的内存都是分配到大对象堆(LOH)上的。此外，有些小对象从 0 代升到了 1 代，在这个例子中，由于可分配的内存不足，最终引发了OutOfMemoryException。而这和我们看到的结果是相符合的：\n{ \u0026#34;runtimeOptions\u0026#34;: { \u0026#34;tfm\u0026#34;: \u0026#34;netcoreapp3.1\u0026#34;, \u0026#34;framework\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Microsoft.NETCore.App\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;3.1.0\u0026#34; }, \u0026#34;configProperties\u0026#34;: { \u0026#34;System.GC.HeapHardLimit\u0026#34;: 1048576 } } } 从 Dump 文件进行分析 到此为止，关于 dotTrace 和 dotMemery 的使用就基本上讲解完啦！可能这时候有些朋友会产生疑问，如果性能问题发生在生产环境怎么办啊。不错，这里我们调试的都是本地的程序，生产环境是没有机会让你这样去搞的。此时，我们可以借助内存转储文件(Dump)文件，它是进程的内存镜像，可以把程序的执行状态通过调试器保存在 Dump 文件中，试想一下，如果程序在前一秒崩溃了，而你在这一瞬间获得了当时程序的状态信息，相当于拿到了“故障”遗留在现场的“罪证”。在 Windows 系统中创建 Dump 文件是非常简单的，通过任务管理器-\u0026gt;创建转储文件即可完成，我们继续使用上面提到的例子：\n创建Dump文件\r其实，拿到 Dump 文件以后，分析它的工具非常多，比如常见的 WinDBG、DebugDiag 等等，这里我们可以直接使用 dotMemery ，因为它本身就支持 Dump 文件的导入，相比前面两种在使用上要更加友好一点。此时，导入这个 Dump 文件，我们就可以获得下面的结果：\n大对象堆分布情况\r一、二代GC分布情况\r这和我们前面分析出的结论是一致的，即，几乎所有的内存都是分配到大对象堆(LOH)上的。除此以外，针对.NET Core，官方提供了 dotnet-dump和dotnet-gcdump两个命令行工具，可以通过下面的命令安装：\ndotnet tool install -g dotnet-dump dotnet tool install -g dotnet-gcdump 这两个命令同样可以对内存进行分析，关于更多的.NET Core 的诊断教程，请参考：https://docs.microsoft.com/zh-cn/dotnet/core/diagnostics/event-counter-perf，这些细节都是针对.NET Core 的，可能不具有普适性，感兴趣的朋友可以自行前去了解。和大多数JetBrains的应用一样，这些程序都有 Visual Studio 的扩展程序，可以直接集成到 Visual Studio 中，这个同样看个人喜好，不再详细讲解。\n本文小结 结合一个简单的示例程序，本文简单地介绍了来自 JetBrains 的两款软件 dotTrace 和 dotMemery 的基本使用，以及如何通过内存转储文件(Dump)对生产环境中的内存进行诊断。在以往的关于程序性能优化的经历中，我个人还使用过 ANTS-Performance-Profiler 这个软件，但体验上感觉还是 dotTrace 和 dotMemery 稍微好用一点，而对于更一般的代码角度的性能分析，我推荐一个轻量级的项目MiniProfiler，性能优化不能靠猜，可是从初中就开始学的“控制变量法”未尝不是一个不错的思路。刷 LeetCode 的这段时间，一个最大的感悟就是，程序的性能，真的是一点一点的优化出来的，就拿最简单的排序来说，你真的要在上面提交很多次，才能渐渐地明白为什么说有些排序算法是“不稳定”的。也许，现在硬件水平越来越好，我们不必像前辈们一样“锱铢必较”，可这一切其实很都公平，你写代码的时候有多浪费，你玩游戏的时候就有多心疼，这里要特别表扬育碧对叛变这一作的优化。好了，这就是这篇博客的内容啦，谢谢大家，晚安！\n参考链接 https://www.jetbrains.com/zh-cn/profiler/ https://www.jetbrains.com/zh-cn/dotmemory https://docs.microsoft.com/zh-cn/dotnet/core/diagnostics/debug-linux-dumps https://docs.microsoft.com/zh-cn/dotnet/core/diagnostics/debug-memory-leak https://docs.microsoft.com/zh-cn/dotnet/core/diagnostics/debug-highcpu?tabs=windows ","date":"2020-11-01T12:19:02Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3672690776/","slug":"3672690776","tags":["dotTrace","JetBrain","性能","调优"],"title":"使用 dotTrace 对 .NET 应用进行性能分析与优化"},{"categories":["编程语言"],"content":"没错，我又借着“面试题”的名头来搞事情了，今天要说的是 HashSet ，而这确实是一个实际面试中遇到的问题。当时的场景大概是这样的，面试官在了解了你的知识广度以后，决心来考察一番你的基本功底，抛出了一个看起来平平无奇的问题：说一说你平时工作中都用到了哪些数据结构。你心想，这还不简单，Array、ArrayList、List、Dictionary、HashSet、Stack、Queue\u0026hellip;等等各种集合类简直如数家珍，甚至你还能说出这些数据结构间的优劣以及各自使用的场景。可没想到，面试官话锋一转，直接来一句，“你能说说 HashSet 去重的原理吗”，好家伙，你这简直不按套路出牌啊\u0026hellip;本着每次面试都有一点收获的初心，于是就有了今天这篇博客，不同的是，顺着这个思路继续深挖下去，博主又发现了几个平时关注不到的技术盲点，所以，博主称之为：一道 HashSet 面试题引发的蝴蝶效应。\nHashSet 源代码解读 OK，首先，我们来回答第一个问题，即：HashSet 去重的原理是什么？。为此，博主翻阅了 HashSet 的 源代码。首先，我们会注意到 HashSet 的构造函数，它需要一个类型为IEqualityComparer\u0026lt;T\u0026gt;的参数。从这个命名上我们就可以知道，这是一个用于相等性比较的接口，我们初步推测，HashSet 去重应该和这个接口有关：\npublic HashSet() : this(EqualityComparer\u0026lt;T\u0026gt;.Default) { } public HashSet(int capacity) : this(capacity, EqualityComparer\u0026lt;T\u0026gt;.Default) { } public HashSet(IEqualityComparer\u0026lt;T\u0026gt; comparer) { } public HashSet(IEnumerable\u0026lt;T\u0026gt; collection) : this(collection, EqualityComparer\u0026lt;T\u0026gt;.Default) { } public HashSet(IEnumerable\u0026lt;T\u0026gt; collection, IEqualityComparer\u0026lt;T\u0026gt; comparer) : this(comparer) { } 我们都知道 HashSet 可以去重，比如，我们向 HashSet 添加多个相同的元素，实际上 HashSet 中最终只会有一个元素。所以，我们自然而然地想到，看看 HashSet 中的 Add() 方法呗，或许能从这里看出一点端倪。HashSet 中一共有两个 Add() 方法，它们内部都调用了 AddIfNotPresent() 方法：\nvoid ICollection\u0026lt;T\u0026gt;.Add(T item) { AddIfNotPresent(item); } public bool Add(T item) { return AddIfNotPresent(item); } 继续循着蛛丝马迹一路 F12 ，我们来看看这个方法的具体实现：\nprivate bool AddIfNotPresent(T value) { if (m_buckets == null) { Initialize(0); } int hashCode = InternalGetHashCode(value); int bucket = hashCode % m_buckets.Length; #if FEATURE_RANDOMIZED_STRING_HASHING \u0026amp;\u0026amp; !FEATURE_NETCORE int collisionCount = 0; #endif for (int i = m_buckets[hashCode % m_buckets.Length] - 1; i \u0026gt;= 0; i = m_slots[i].next) { if (m_slots[i].hashCode == hashCode \u0026amp;\u0026amp; m_comparer.Equals(m_slots[i].value, value)) { return false; } #if FEATURE_RANDOMIZED_STRING_HASHING \u0026amp;\u0026amp; !FEATURE_NETCORE collisionCount++; #endif } int index; if (m_freeList \u0026gt;= 0) { index = m_freeList; m_freeList = m_slots[index].next; } else { if (m_lastIndex == m_slots.Length) { IncreaseCapacity(); // this will change during resize bucket = hashCode % m_buckets.Length; } index = m_lastIndex; m_lastIndex++; } m_slots[index].hashCode = hashCode; m_slots[index].value = value; m_slots[index].next = m_buckets[bucket] - 1; m_buckets[bucket] = index + 1; m_count++; m_version++; #if FEATURE_RANDOMIZED_STRING_HASHING \u0026amp;\u0026amp; !FEATURE_NETCORE if (collisionCount \u0026gt; HashHelpers.HashCollisionThreshold \u0026amp;\u0026amp; HashHelpers.IsWellKnownEqualityComparer(m_comparer)) { m_comparer = (IEqualityComparer\u0026lt;T\u0026gt;) HashHelpers.GetRandomizedEqualityComparer(m_comparer); SetCapacity(m_buckets.Length, true); } #endif // FEATURE_RANDOMIZED_STRING_HASHING return true; } 可以注意到，在这段代码中，首先，会通过 InternalGetHashCode() 方法计算一个 HashCode。其中，Lower31BitMask 是一个常量 0x7FFFFFFF ：\nprivate int InternalGetHashCode(T item) { if (item == null) { return 0; } return m_comparer.GetHashCode(item) \u0026amp; Lower31BitMask; } 接下来，在 HashSet 内部使用了Slot 这个结构来存储元素，该结构设计上类似于链表，每一个 Slot 中都记录对应元素的值、HashCode 以及下一个元素的索引。所以，只需要对它做一次遍历，如果对应元素的 HashCode 和 值 都相等，则认为该元素在 HashSet中已经存在了。此时，AddIfNotPresent() 方法会返回 false。这就是 HashSet 去重的原理啦。在比较元素的值是否相等的时候，我们前面提到的 IEqualityComparer\u0026lt;T\u0026gt; 终于登场，它提供的 Equals() 方法恰好可以比较两个元素是否相等：\ninternal struct Slot { internal int hashCode; // Lower 31 bits of hash code, -1 if unused internal int next; // Index of next entry, -1 if last internal T value; } public interface IEqualityComparer\u0026lt;in T\u0026gt; { bool Equals(T x, T y); int GetHashCode(T obj); } 再接下来，如果对应元素的 HashCode 或 值 都不相等，则认为该元素在 HashSet 中不存在。此时，需要考虑 HashSet 的容量是否足以放得下这个新元素。在容量不满足的情况下，就需要对 HashSet 进行扩容。值得一提的是，这里是使用质数进行扩容的：\nprivate void IncreaseCapacity() { Debug.Assert(m_buckets != null, \u0026#34;IncreaseCapacity called on a set with no elements\u0026#34;); int newSize = HashHelpers.ExpandPrime(m_count); if (newSize \u0026lt;= m_count) { throw new ArgumentException(SR.GetString(SR.Arg_HSCapacityOverflow)); } // Able to increase capacity; copy elements to larger array and rehash SetCapacity(newSize, false); } public static int ExpandPrime(int oldSize) { int newSize = 2 * oldSize; // Allow the hashtables to grow to maximum possible size (~2G elements) before encoutering capacity overflow. // Note that this check works even when _items.Length overflowed thanks to the (uint) cast if ((uint)newSize \u0026gt; MaxPrimeArrayLength \u0026amp;\u0026amp; MaxPrimeArrayLength \u0026gt; oldSize) { Contract.Assert( MaxPrimeArrayLength == GetPrime(MaxPrimeArrayLength), \u0026#34;Invalid MaxPrimeArrayLength\u0026#34;); return MaxPrimeArrayLength; } return GetPrime(newSize); } IEqualityComparer接口 OK，现在我们知道了，HashSet 之所以可以去重，一个重要的原因是 IEqualityComparer\u0026lt;T\u0026gt; 。而回到这个接口本身呢，它只有 Equals() 和 GetHashCode()，这其实非常符合我们的认知，因为这两个方法在对象相等的场景中十分常见，有一个准则是：如果重写了 Equals() 方法，那么，应该同时去重写 GetHashCode() 方法，即，两者在表达相等这个含义时应该具有一致性。这里可能会有一点疑问，那就是，我们平时使用 HashSet 的时候，完全不需要指定 IEqualityComparer\u0026lt;T\u0026gt; ，它一样可以正常工作啊？没错，这是因为微软提供了一个默认的实现：EqualityComparer\u0026lt;T\u0026gt;.Default。我们同样来看看它的实现：\nprivate static EqualityComparer\u0026lt;T\u0026gt; CreateComparer() { Contract.Ensures(Contract.Result\u0026lt;EqualityComparer\u0026lt;T\u0026gt;\u0026gt;() != null); RuntimeType t = (RuntimeType)typeof(T); // Specialize type byte for performance reasons if (t == typeof(byte)) { return (EqualityComparer\u0026lt;T\u0026gt;)(object)(new ByteEqualityComparer()); } // If T implements IEquatable\u0026lt;T\u0026gt; return a GenericEqualityComparer\u0026lt;T\u0026gt; if (typeof(IEquatable\u0026lt;T\u0026gt;).IsAssignableFrom(t)) { return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(GenericEqualityComparer\u0026lt;int\u0026gt;), t); } // If T is a Nullable\u0026lt;U\u0026gt; where U implements IEquatable\u0026lt;U\u0026gt; return a NullableEqualityComparer\u0026lt;U\u0026gt; if (t.IsGenericType \u0026amp;\u0026amp; t.GetGenericTypeDefinition() == typeof(Nullable\u0026lt;\u0026gt;)) { RuntimeType u = (RuntimeType)t.GetGenericArguments()[0]; if (typeof(IEquatable\u0026lt;\u0026gt;).MakeGenericType(u).IsAssignableFrom(u)) { return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(NullableEqualityComparer\u0026lt;int\u0026gt;), u); } } // See the METHOD__JIT_HELPERS__UNSAFE_ENUM_CAST and METHOD__JIT_HELPERS__UNSAFE_ENUM_CAST_LONG cases in getILIntrinsicImplementation if (t.IsEnum) { TypeCode underlyingTypeCode = Type.GetTypeCode(Enum.GetUnderlyingType(t)); // Depending on the enum type, we need to special case the comparers so that we avoid boxing // Note: We have different comparers for Short and SByte because for those types we need to make sure we call GetHashCode on the actual underlying type as the // implementation of GetHashCode is more complex than for the other types. switch (underlyingTypeCode) { case TypeCode.Int16: // short return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(ShortEnumEqualityComparer\u0026lt;short\u0026gt;), t); case TypeCode.SByte: return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(SByteEnumEqualityComparer\u0026lt;sbyte\u0026gt;), t); case TypeCode.Int32: case TypeCode.UInt32: case TypeCode.Byte: case TypeCode.UInt16: //ushort return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(EnumEqualityComparer\u0026lt;int\u0026gt;), t); case TypeCode.Int64: case TypeCode.UInt64: return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(LongEnumEqualityComparer\u0026lt;long\u0026gt;), t); } } // Otherwise return an ObjectEqualityComparer\u0026lt;T\u0026gt; return new ObjectEqualityComparer\u0026lt;T\u0026gt;(); } 在这里，EqualityComparer\u0026lt;T\u0026gt; 类是一个抽象类，实现了 IEqualityComparer\u0026lt;T\u0026gt; 接口。简单来说，对于简单类型如整型、字节型等，微软实现了相应的 IEqualityComparer\u0026lt;T\u0026gt; 接口；而对于复杂的类型，微软提供了 ObjectEqualityComparer\u0026lt;T\u0026gt; 这一实现：\ninternal class ObjectEqualityComparer\u0026lt;T\u0026gt;: EqualityComparer\u0026lt;T\u0026gt; { [Pure] public override bool Equals(T x, T y) { if (x != null) { if (y != null) return x.Equals(y); return false; } if (y != null) return false; return true; } [Pure] public override int GetHashCode(T obj) { if (obj == null) return 0; return obj.GetHashCode(); } } 所以，现在又回到我们刚刚聊起的话题，为什么说一个类型的 Equals() 和 GetHashCode() 方法非常重要呢？因为如果我们不能正确地实现这两个方法，微软实现的这个 ObjectEqualityComparer\u0026lt;T\u0026gt; 就会出现问题，导致 HashSet 在判断元素是否存在时出现问题，所以，这是一系列的连锁反应。有人可能会问，博主你说的这个好夸张耶，像我就从来没有重写过这两个方法。OK，现在来回答我的一个问题，如果你定义了一个类型 Foo ，并尝试用它作为一个字典中的 Key ，那么，你觉得这个字典应该怎么判断这个 Key 是否存在呢？我觉得这是一个好问题，因为它引发了我们在 .NET 知识体系中的蝴蝶效应。\n排序与去重是亲家 排序与去重，在我看来是亲家关系，因为两者都需要“比较”。所以，下面我想从 .NET 中选取一部分接口来阐述我的观点，以及当我们有了 LINQ 以后，是否就应该抛弃它们。可能这些接口大家平时都用不到多少，但我还是想花点时间来梳理这些知识盲点，因为我发现，与其为整个行业 35 岁的的职业生涯而焦虑，倒不如重新捡起这个行业的初心，好好地学一学数据结构、算法和数学。整个行业的火热，容易让每一个人都陷入一种“我非常厉害”的错觉，我写博客的时候，在心里想了这样一句话：战士上战场，整天就知道 CRUD，连 HashSet 都不知道，早晚是个死。用王布斯的口吻说出来，会不会有一种紧迫感呢？\nIEquatable接口 IEquatable\u0026lt;T\u0026gt; 接口在微软官方文档中的定义是，定义值类型或类实现的通用方法，以创建用于确定实例相等性的类型特定方法。我承认，这不是一个特别好的定义，不过，我们可以换个角度来审视这个接口存在的意义。虽然 Object 这个基类提供了 Equals() 方法，但是这个方法只能接受一个 object 类型的参数， 所以，它本身会面临类型安全性缺失和装箱两个问题。为了解决这个问题，就必须要定义一个新的 Equals() 方法，确保它可以接收和当前类型一致的参数，所以就需要这样一个接口，你可以理解为它是 Equals() 方法的泛型版本，而众所周知 C# 是一门不支持多继承的语言，所以，这里只能以接口的形式提供出来。这里说一下我的结论，IEquatable\u0026lt;T\u0026gt; 接口对值类型更有用一点，相反，对引用类型就没有那么有用，因为它没有考虑到协变的问题，对引用类型的继承相对无力。下面是一个简单的例子：\n//定义类型Foo，实现IEquatable\u0026lt;Foo\u0026gt;接口 public class Foo : IEquatable\u0026lt;Foo\u0026gt; { public decimal Value { get; set; } public decimal Weight { get; set; } public override bool Equals(object other) { return Equals(other as Foo); } public bool Equals(Foo other) { if (other == null) return false; return (this.Value == other.Value \u0026amp;\u0026amp; this.Weight == other.Weight); } } //平平无奇的代码 var foo1 = new Foo() { Value = 10, Weight = 1.0M }; var foo2 = new Foo() { Value = 10, Weight = 1.0M }; Assert.AreEqual(true, foo1.Equals(foo2)); ICompareable/ICompareable接口 ICompareable 和 ICompareable\u0026lt;T\u0026gt;是是同一个接口的非泛型与泛型版本，都需要实现 CompareTo() 方法。可能大家会觉得这几个接口都差不多啊，实际上，大家细心观察就能发现它们的区别，“相等”这一类的接口的返回值是布尔型，关注的是两个对象是否相等；而“比较”这一类的接口的返回值是整数型，关注的是哪个大哪个小。我们继续以 Foo 这个类型为例，分别实现IComparerable 和 IComparerable\u0026lt;T\u0026gt;两个接口：\n//继续实现IComparable, IComparable\u0026lt;Foo\u0026gt;接口 public class Foo : IEquatable\u0026lt;Foo\u0026gt;, IComparable, IComparable\u0026lt;Foo\u0026gt; { public decimal Value { get; set; } public decimal Weight { get; set; } public override bool Equals(object other) { return Equals(other as Foo); } public bool Equals(Foo other) { if (other == null) return false; return (this.Value == other.Value \u0026amp;\u0026amp; this.Weight == other.Weight); } public int CompareTo(object obj) { var other = obj as Foo; return CompareTo(other); } public int CompareTo([AllowNull] Foo other) { if (other == null) return 1; return (int)((Value * Weight) - (other.Value * other.Weight)); } } //平平无奇的代码 var foo1 = new Foo() { Value = 10, Weight = 1.0M }; var foo2 = new Foo() { Value = 20, Weight = 1.0M }; Assert.IsTrue(foo2.CompareTo(foo1) \u0026gt; 0); IComparer接口 对于排序来说，理论上有ICompareable 和 ICompareable\u0026lt;T\u0026gt;这两个接口就可以了，为什么还要再定义一组接口呢？其实，我们结合生活中的场景就能想明白，不管是判断两个对象是否相等，还是对两个对象进行排序，这些条件都属于“变量”。ICompareable 和 ICompareable\u0026lt;T\u0026gt;这两个接口设计上的确没什么问题，但这都是一锤子买卖，一旦实现了对应的接口，就意味着如何比较两个对象的逻辑是确定好了的。可生活常识告诉我们，同一组信息不同的人考虑的维度是不一样的，譬如学生的成绩，可以按照某一个科目的成绩来排序，还可以按照各个科目的总成绩甚至是平均分来排序。对于上面的类型 Foo，我们不妨考虑按照 Value 和 Weight 分别进行排序，此时可以这样写：\n//按Value排序 public class FooValueComparer : IComparer\u0026lt;Foo\u0026gt; { public int Compare([AllowNull] Foo x, [AllowNull] Foo y) { if (x == null \u0026amp;\u0026amp; y == null) return 0; if (x != null \u0026amp;\u0026amp; y == null) return 1; if (x == null \u0026amp;\u0026amp; y != null) return -1; return (int)(x.Value - y.Value); } } //按Weight排序 public class FooWeightComparer : IComparer\u0026lt;Foo\u0026gt; { public int Compare([AllowNull] Foo x, [AllowNull] Foo y) { if (x == null \u0026amp;\u0026amp; y == null) return 0; if (x != null \u0026amp;\u0026amp; y == null) return 1; if (x == null \u0026amp;\u0026amp; y != null) return -1; return (int)(x.Weight - y.Weight); } } //平平无奇的代码 var list= new List\u0026lt;Foo\u0026gt;{ new Foo() { Value = 10, Weight = 2.0M }, new Foo() { Value = 10, Weight = 1.0M } }; //使用默认的排序器 list.Sort(); //按Value进行排序 list.Sort(new FooValueComparer()); list.OrderBy(x =\u0026gt; x.Value); //按Weight进行排序 list.Sort(new FooWeightComparer()); list.OrderBy(x =\u0026gt; x.Weight); 在这里有一个点是，在不指定排序器的时候，微软帮我们提供了一个默认的排序器。而这个默认排序器会遵循这样的策略。如果类型 T 实现了 IComparable\u0026lt;T\u0026gt; 接口，则返回 GenericComparer\u0026lt;int\u0026gt; 实例；如果类型 T 实现是一个可空类型 Nullable\u0026lt;U\u0026gt; 并且类型 U 实现了 IComparable\u0026lt;T\u0026gt; 接口，则返回 NullableComparer\u0026lt;int\u0026gt; 实例；否则返回 ObjectComparer\u0026lt;T\u0026gt; 实例。\nprivate static Comparer\u0026lt;T\u0026gt; CreateComparer() { RuntimeType t = (RuntimeType)typeof(T); // If T implements IComparable\u0026lt;T\u0026gt; return a GenericComparer\u0026lt;T\u0026gt; #if FEATURE_LEGACYNETCF // Pre-Apollo Windows Phone call the overload that sorts the keys, not values this achieves the same result if (CompatibilitySwitches.IsAppEarlierThanWindowsPhone8) { if (t.ImplementInterface(typeof(IComparable\u0026lt;T\u0026gt;))) { return (Comparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(GenericComparer\u0026lt;int\u0026gt;), t); } } else #endif if (typeof(IComparable\u0026lt;T\u0026gt;).IsAssignableFrom(t)) { return (Comparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(GenericComparer\u0026lt;int\u0026gt;), t); } // If T is a Nullable\u0026lt;U\u0026gt; where U implements IComparable\u0026lt;U\u0026gt; return a NullableComparer\u0026lt;U\u0026gt; if (t.IsGenericType \u0026amp;\u0026amp; t.GetGenericTypeDefinition() == typeof(Nullable\u0026lt;\u0026gt;)) { RuntimeType u = (RuntimeType)t.GetGenericArguments()[0]; if (typeof(IComparable\u0026lt;\u0026gt;).MakeGenericType(u).IsAssignableFrom(u)) { return (Comparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(NullableComparer\u0026lt;int\u0026gt;), u); } } // Otherwise return an ObjectComparer\u0026lt;T\u0026gt; return new ObjectComparer\u0026lt;T\u0026gt;(); } 更有意思的是，GenericComparer\u0026lt;T\u0026gt; 就是利用 IComparable\u0026lt;T\u0026gt; 的 CompareTo() 方法来说实现的：\ninternal class GenericComparer\u0026lt;T\u0026gt; : Comparer\u0026lt;T\u0026gt; where T: IComparable\u0026lt;T\u0026gt; { public override int Compare(T x, T y) { if (x != null) { if (y != null) return x.CompareTo(y); return 1; } if (y != null) return -1; return 0; } // Equals method for the comparer itself. public override bool Equals(Object obj){ GenericComparer\u0026lt;T\u0026gt; comparer = obj as GenericComparer\u0026lt;T\u0026gt;; return comparer != null; } public override int GetHashCode() { return this.GetType().Name.GetHashCode(); } } 在我们有了 LINQ 以后，通过 OrderBy 和 OrderByDescending 就可以进行排序，如果这个排序字段是一个简单类型，比如字符型、整型、日期型，这些简单类型微软都已经实现了相应的“排序”逻辑，而如果这个排序字段是一个复杂类型，比如一个自定义的类或者结构，此时，为了让这些方法能够“适应”这些复杂类型，最好的还是去实现 IComparer\u0026lt;T\u0026gt; 或者 ICompareable\u0026lt;T\u0026gt; 接口，然后传递给这两个排序方法。类似地，还有 Distinct 这个方法，它接收一个 IEqualityComparer\u0026lt;T\u0026gt; 类型的参数，所以，当你对一个列表进行去重(Distinct)操作时，千万不要想当然地人为它会按照你的期望去去重，如果结果不符合你的期望，很大原因是你没有给它提供一个合适的IEqualityComparer\u0026lt;T\u0026gt; 。所以，你看，我们绕了一大圈，从 HashSet 说到 IEqualityComparer\u0026lt;T\u0026gt;，又从排序说到去重，最终又回到了起点，这是多么有趣的一件事情。而去重(Distinct)这件事情，其实涉及到Dictionary 和 HashSet 两个数据结构，通过结构来推演性质，又通过性质来扫清盲点，这可能是这段时间刷 LeetCode 最大的一个收获吧！\n本文小结 面试中偶然遇到的 HashSet 问题，让我发现自己的知识体系中存在着盲点。通过解读 HashSet 源代码，我们认识到 HashSet 可以去重的一个重要原因是IEqualityComparer\u0026lt;T\u0026gt; 接口，它决定了两个对象的实例在什么情况下可以被判定为相等。而这个接口，不单单在 HashSet 出现，在 Dictionary 中同样会出现，甚至在我们最熟悉不过的去重(Distinct)中还会出现，所以，通过 HashSet 这一个点上的疑问，我搞清楚了很多相关联的内容，这不是蝴蝶效应又是什么呢？而与去重(Distinct)相关联的则是排序，在此基础上，对 IEquatable\u0026lt;T\u0026gt; 接口、ICompareable/ICompareable\u0026lt;T\u0026gt; 接口、IComparer\u0026lt;T\u0026gt; 接口等知识盲点进行梳理。总而言之，排序需要关注的是 ICompareable/ICompareable\u0026lt;T\u0026gt; 接口、IComparer\u0026lt;T\u0026gt; 接口，去重需要关注的是 IEqualityComparer\u0026lt;T\u0026gt; 接口。好了，今天的这只蝴蝶就飞到这里，欢迎大家在博客中留言，谢谢大家！\n","date":"2020-10-20T12:19:02Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3411909634/","slug":"3411909634","tags":["HashSet","编程","面试","源码"],"title":"一道 HashSet 面试题引发的蝴蝶效应"},{"categories":["生活感悟"],"content":"\r当导演张策宣布，不再为朱一旦系列担任编剧和配音时，我终于意识到，“十佳员工”不再是一个梗，而是一个活生生的人。也许，身为老板的“朱一旦”，永远都没有读懂这些黑色幽默背后的含义。而显然，站在普通人对立面的资本家们，终究不会因此而洗心革面，代表劳苦大众向这个时代发声。不管是后浪还是非浪，资本家们不会选择和钱过不去，所以，即使有像鲁迅一般针砭时弊的张策，可在一个“屁股决定脑袋”的世界里，“十佳员工”突然就变成一个不再好笑的词汇，因为，这个人可以是你，可以是我，可以是我们中的任何一个。在新冠疫情肆虐的时候，『一块劳力士的回家之路』让我们感受到了现实的魔幻，可此时此刻，我们终于知道，“艺术来源于生活，而往往高于生活”，果然，如有雷同，是不胜荣幸的了。可能是因为我此刻在经历着同样的事情，所以，难免感同身受地想到 C 座 802 里这群真实存在着的人们。\n我有一位为公司奉献 11 年青春的同事，可当他离开这家公司时，并没有我想象的中那样充满不舍，大概“鸟尽弓藏”、“大地茫茫真干净”这些句子，从古至今就是这样子的吧！马老师说，“996 是一种福报”，而此前的一位马老师则说，“资本家生来就是剥削劳动者的一切剩余价值”。历史像个任人打扮的小姑娘，你方唱罢我登场，文过饰非，到底谁又讲得清对错？有人说，一个人开始成熟，就是从学习这几千年来的厚黑学、阴谋论开始，的确啊，连封神都开始变成一场阴谋，不同的是，这次的因果都落在原始天尊身上，每一个人都渴望像姜子牙一样，断天梯、破枷锁，似乎一定要执著于什么东西，这样的人生会显得更真实一点。可每个人自以为最完美的安排，终究无法让每一个人信服啊，正如“朱一旦”们喜欢“非洲安排”，马小策与张策，说到底不过是一种代号而已，当这种“安排”无法调和的时候，人和神仙一样，都会暴走，都会变身，唯一不同的是，人是要吃饭的，而神仙们早已学会辟谷。\n所以，在“救一个人还是救苍生”这个问题上，其实谁都没有错，我特别喜欢李诞在『奇葩说』中表达的一个观点，“以自私却不伤害别人的方式活着，才能维持世界的运转。而正是那些为了宏图伟业不计后果的牺牲‘小猫’的人，频频地让我们的世界陷入「大火」”。朱一旦不想再做“任人摆布”的老板，张策不想再做“默默无名”的幕后英雄，马老师早已看破这一切，“钱没给到位”，“心委屈了”，身在其位时榨干身体的“996”，人走茶凉时送瘟神般“高效”，一冷一热，果然是“环球同此凉热”呢。在全球变暖的趋势下，如果我们以自私却不伤害别人的方式活着，虽然活得有一点清冷、没有人情味，但这样是不是会更安心一点，骨子里与生俱来就带着“竞争”的基因的我们，是不是一定要学会“狠”、学会“不择手段”、学会“伤害”。我二十多岁的时候，想努力去照顾好一个人，而等到我快要三十岁的时候，我终于能勉强照顾好自己，这简直是一种幸运。\n这种感慨在某个场景下会更加明显，譬如一个人去看电影的时候，虽然我很喜欢和邻座的小朋友说话，可对方父母一句友善的“叔叔”，终于还是让两个人产生了距离。譬如找工作面试的时候，发觉三十上下的“哥哥姐姐”们，都开始面对“总监”级别职位时的恍惚感。也许，我们这一代人真的已经老了吧，而那个人早已离开你很久很久，我无意去对立资本家与劳动者间积怨久矣的矛盾，更无意去揣测封神台下蛰伏已久的阴谋。回想以前，乐无异在『古剑奇谭 2』中说出一句，“众生虽苦，还请诸恶莫作”，当时大概只是觉得这句话酷到不行，倘若议论公平，C 座 802 诸如三濑子、马小玲、马小浩等等角色，每一个都带着无数的梗，没有他们就没有整个朱一旦宇宙，当人们为张策惋惜的时候，是不是就选择性地遗忘了他们呢？朱一旦不会成为劳苦大众的代言人，而且任何人都不会，因为一切的流量到最后都是生意。\n我在 B 站关注过一位阿婆主，起初，他在厂里打工，下班后的“入味儿”是他主要的拍摄内容。后来，因为疫情的原因，他开始学别人拍做菜的视频。再后来，发现他变成了一位外卖小哥。世人皆苦，家家有本难念的经，可我们除了祝福以外，又能做一点什么呢？成人世界里，利益、立场、观点……，该有的一切都有，唯独没有对错，希望一个组织有一致的步伐、一致的声音，可偏偏人是一根会思考的芦苇，我知道，当一个人在某一种身份下，他必须要去推动一种文化形成，可如果这些声音连他自己都不信，这种文化的底蕴应该不会特别丰富，很容易成为政治博弈的牺牲品。我从前天真地以为，在互联网这样一个相对开放的环境里，不会存在政治这种产物。而出于对这种东西的逃避，我没有选择成为三线小城市里的一个公务员，实际上我尝试过，结果证明我真的不适合。可后来我发现我错了，只要有人的地方就会存在政治，无论是公司还是社区，每天都有人宣扬这样或者那样的“文化”，这个时候，我希望我们每一个人都去用心甄别这些概念，因为作为人的自觉，他只会说对自己有利的话，正如择偶标准是最毫无标准可言的标准一样，王垠说编程世界里充满宗派，就是最好的证明。\n所以，我不大愿意去统一什么东西，充满多样性、充满个性的世界，才是一个正常的世界，以结果论的观点而言，只要能送大家都目的地，是飞机还是高铁还是火车，真的重要吗？如果非要去统一什么，我希望是“语言”或者“领域语言”，因为，我们的沟通，因为存在太多的翻译而逐渐失真、甚至被曲解，我们一般把这样的沟通称之为扯皮，就像土味情话虽然美妙动听，但它携带了大量无用的信息。所以，即使冒着成为“钢铁直男”的危险，我依然想成为一个表达清晰的人。有人说，姜子牙就不能和原始天尊好好商量一下吗？非要自断天梯逼得鸿钧老祖出手吗？人类啊，归根到底，只愿意相信自己相信的，只愿意看见自己看见的，这种意念在成年后往往更加强烈，有多少遗憾就是得不到有效沟通造成的呢？九尾狐自觉被原始天尊欺骗、过河拆桥，而原始天尊认为“非我族类，其心必异”，都是选择性地相信了自己愿意去相信的东西。有人说，姜子牙有强迫症，为什么会任由师尊披头散发？因为不是每个人都能像约翰·纳什那样，在最亲密的人面前直抒胸臆，人类就是这么奇怪，和陌生人玩什么真心话大冒险，在亲人面前反而含蓄、羞怯起来，可能是因为某种特殊的磁力限制了声道发声吧，科学与玄学往往就是这么切换自如。\n思绪就像一个无底黑洞，姜子牙与朱一旦，两个八竿子不十竿子都打不着的人，就这么神奇地在我脑海里，完成了一次对话。如果思维存在奇点，将会坍陷于何处，苏格拉有没有底不重要，马老师们谁说得对同样不重要，甚至你看我这满纸荒唐言依然不重要，它仅仅表明我此时此刻在思考，我是一个活生生的人，所谓“我思故我在”，无非给枯燥的人生多一点无用的点缀罢了，你说朱一旦都不枯燥了，我们却还停留在这里，你说，还有比这个更枯燥的事情吗？申公豹形神俱灭，从头开始修行，居然连基因都发生了突变，大概，在这世间，没有什么可以永恒。\n","date":"2020-10-18T12:19:02Z","image":"/posts/1085014581/P2623589679.jpg","permalink":"https://qinyuanpei.github.io/posts/1085014581/","slug":"1085014581","tags":["感悟","电影","生活","随笔"],"title":"当姜子牙遇见朱一旦"},{"categories":["数据存储"],"content":"最近在面试的时候，遇到了一个关于 .NET Core 配置热更新的问题，顾名思义，就是在应用程序的配置发生变化时，如何在不重启应用的情况下使用当前配置。从 .NET Framework 一路走来，对于 Web.Config 以及 App.Config 这两个配置文件，我们应该是非常熟悉了，通常情况下， IIS 会检测这两个配置文件的变化，并自动完成配置的加载，可以说它天然支持热更新，可当我们的视野伸向分布式环境的时候，这种配置方式就变得繁琐起来，因为你需要修改一个又一个配置文件，更不用说这些配置文件可能都是放在容器内部。而有经验的朋友，可能会想到，利用 Redis 的发布-订阅来实现配置的下发，这的确是一个非常好的思路。总而言之，我们希望应用可以随时感知配置的变化，所以，在今天这篇博客里，我们来一起聊聊 .NET Core 中配置热更新相关的话题，这里特指全新的选项模式(Options)。\nOptions 三剑客 在 .NET Core 中，选项模式(Options)使用类来对一组配置信息进行强类型访问，因为按照接口分隔原则(ISP)和关注点分离这两个工程原则，应用的不同部件的配置应该是各自独立的，这意味着每一个用于访问配置信息的类，应该是只依赖它所需要的配置信息的。举一个简单的例子，虽然 Redis 和 MySQL 都属于数据持久化层的设施，但是两者属于不同类型的部件，它们拥有属于各自的配置信息，而这两套配置信息应该是相互独立的，即 MySQL 不会因为 Redis 的配置存在问题而停止工作。此时，选项模式(Options)推荐使用两个不同的类来访问各自的配置。我们从下面这个例子开始：\n{ \u0026#34;Learning\u0026#34;: { \u0026#34;Years\u0026#34;: 5, \u0026#34;Topic\u0026#34;: [ \u0026#34;Hotfix\u0026#34;, \u0026#34;.NET Core\u0026#34;, \u0026#34;Options\u0026#34; ], \u0026#34;Skill\u0026#34;: [ { \u0026#34;Lang\u0026#34;: \u0026#34;C#\u0026#34;, \u0026#34;Score\u0026#34;: 3.9 }, { \u0026#34;Lang\u0026#34;: \u0026#34;Python\u0026#34;, \u0026#34;Score\u0026#34;: 2.6 }, { \u0026#34;Lang\u0026#34;: \u0026#34;JavaScript\u0026#34;, \u0026#34;Score\u0026#34;: 2.8 } ] } } 此时，如果希望访问Learning节点下的信息，我们有很多种实现方式：\n//方式1 var learningSection = Configuration.GetSection(\u0026#34;Learning\u0026#34;); var careerYears = learningSection.GetValue\u0026lt;decimal\u0026gt;(\u0026#34;Years\u0026#34;); var topicHotfix = learningSection.GetValue\u0026lt;string\u0026gt;(\u0026#34;Topic:0\u0026#34;); //方式2 var careerYears = Configuration[\u0026#34;Learning:Years\u0026#34;]; var topicHotfix = Configuration[\u0026#34;Learning:Topic:0\u0026#34;); 而更好的方式是，定义一个类来访问这组配置信息：\n[Serializable] public class LearningOptions { public decimal Years { get; set; } public List\u0026lt;string\u0026gt; Topic { get; set; } public List\u0026lt;SkillItem\u0026gt; Skill { get; set; } } [Serializable] public class SkillItem { public string Lang { get; set; } public decimal? Score { get; set; } } 同样地，茴香的茴字有几种写法，你可知道?\n//写法1：手动绑定 var leaningOptions = new LearningOptions(); Configuration.GetSection(\u0026#34;Learning\u0026#34;).Bind(leaningOptions); //写法2：自动绑定 leaningOptions = Configuration.GetSection(\u0026#34;Learning\u0026#34;).Get\u0026lt;LearningOptions\u0026gt;(); //写法3：自动绑定 + 依赖注入 services.Configure\u0026lt;LearningOptions\u0026gt;(Configuration.GetSection(\u0026#34;Learning\u0026#34;)); //写法4：配置的二次加工 services.PostConfigure\u0026lt;LearningOptions\u0026gt;(options =\u0026gt; options.Years += 1); //写法5：委托绑定 services.Configure\u0026lt;AppInfoOptions\u0026gt;(options =\u0026gt; { options.AppName = \u0026#34;ASP.NET Core\u0026#34;; options.AppVersion = \u0026#34;1.2.1\u0026#34;; }); 我们知道，在 .NET Core 里依赖注入被提升到了一等公民的位置，可谓是无处不在。当我们在 IoC 容器中注入LearningOptions以后，就可以在服务层或者控制器层直接使用它们，此时，我们就会遇到传说中的 Options 三剑客，即IOptions\u0026lt;TOptions\u0026gt;、IOptionsSnapshot\u0026lt;TOptions\u0026gt;和IOptionsMonitor\u0026lt;TOptions\u0026gt;。关于它们三个的区别，官方文档里给出了详细的说明：\nIOptions：生命周期为 Singleton，在应用启动时完成初始化。应用启动后，对配置的修改是非响应式的。 IOptionsSnapshot：生命周期为 Scoped，每次请求时会重新计算选项。应用启动后，对配置的修改是响应式的。 IOptionsMonitor：生命周期为 Singleton，可以随时检索当前配置项。应用启动后，对配置的修改是响应式的。 是不是听起来有一点还有一点绕？长话短说就是，如果希望修改完配置立即生效，那么，更推荐使用IOptionsSnapshot\u0026lt;TOptions\u0026gt;和IOptionsMonitor\u0026lt;TOptions\u0026gt;，前者是在下一次请求时生效，后者则是访问CurrentValue的时候生效。而对于像3.14或者0.618这种运行时期间不会修改的“常量”，更推荐使用IOptions\u0026lt;TOptions\u0026gt;。下面是关于它们的一个例子：\n[ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class WeatherForecastController : ControllerBase { private readonly ILogger\u0026lt;WeatherForecastController\u0026gt; _logger; private readonly IOptions\u0026lt;LearningOptions\u0026gt; _learningOptions; private readonly IOptionsSnapshot\u0026lt;LearningOptions\u0026gt; _learningOptionsSnapshot; private readonly IOptionsMonitor\u0026lt;LearningOptions\u0026gt; _learningOptionsMonitor; private readonly IConfiguration _configuration; public WeatherForecastController(ILogger\u0026lt;WeatherForecastController\u0026gt; logger, IOptions\u0026lt;LearningOptions\u0026gt; learningOptions, IOptionsSnapshot\u0026lt;LearningOptions\u0026gt; learningOptionsSnapshot, IOptionsMonitor\u0026lt;LearningOptions\u0026gt; learningOptionsMonitor, IConfiguration configuration ) { _logger = logger; _learningOptions = learningOptions; _learningOptionsSnapshot = learningOptionsSnapshot; _learningOptionsMonitor = learningOptionsMonitor; _configuration = configuration; _learningOptionsMonitor.OnChange((options, value) =\u0026gt; { _logger.LogInformation($\u0026#34;OnChnage =\u0026gt; {JsonConvert.SerializeObject(options)}\u0026#34;); }); } [HttpGet(\u0026#34;{action}\u0026#34;)] public ActionResult GetOptions() { var builder = new StringBuilder(); builder.AppendLine(\u0026#34;learningOptions:\u0026#34;); builder.AppendLine(JsonConvert.SerializeObject(_learningOptions.Value)); builder.AppendLine(\u0026#34;learningOptionsSnapshot:\u0026#34;); builder.AppendLine(JsonConvert.SerializeObject(_learningOptionsSnapshot.Value)); builder.AppendLine(\u0026#34;learningOptionsMonitor:\u0026#34;); builder.AppendLine(JsonConvert.SerializeObject(_learningOptionsMonitor.CurrentValue)); return Content(builder.ToString()); } } 现在我们修改一下配置文件，因为我们为_learningOptionsMonitor注册了回调函数，可以在控制台看到对应的日志：\n监听配置文件变化\r此时，我们通过 Postman 调用接口，我们会得到下面的结果：\nlearningOptions的值并未更新\r可以注意到，此时，learningOptions 中的值依然是更新前的值，这就是它们三者的区别，清楚了吗？\n除了这些以外，选项模式(Options)中还有一个需要注意的地方，是所谓的命名选项(IConfigureNamedOptions)，主要用在多个 Section 绑定统一属性时。譬如现在的应用程序都流行深色主题，实际上深色主题和浅色主题具有相同的结构，比如前景色和背景色，两者唯一的区别是这些颜色配置不一样。考虑下面的配置信息：\n{ \u0026#34;Themes\u0026#34;: { \u0026#34;Dark\u0026#34;: { \u0026#34;Foreground\u0026#34;: \u0026#34;#fff\u0026#34;, \u0026#34;Background\u0026#34;: \u0026#34;#000\u0026#34; }, \u0026#34;White\u0026#34;: { \u0026#34;Foreground\u0026#34;: \u0026#34;#000\u0026#34;, \u0026#34;Background\u0026#34;: \u0026#34;#fff\u0026#34; } } } 此时，我们该如何定义这个主题选项呢？\npublic class ThemeOptions { public string Foreground { get; set; } public string Background { get; set; } } 接下来，我们通过命名的方式来注入两个不同的主题：\nservices.Configure\u0026lt;ThemeOptions\u0026gt;(\u0026#34;DarkTheme\u0026#34;, Configuration.GetSection(\u0026#34;Themes:Dark\u0026#34;)); services.Configure\u0026lt;ThemeOptions\u0026gt;(\u0026#34;WhiteTheme\u0026#34;, Configuration.GetSection(\u0026#34;Themes:White\u0026#34;)); 在任何你希望使用它们的地方，注入IOptionsSnapshot\u0026lt;ThemeOptions\u0026gt;和IOptionsMonitor\u0026lt;ThemeOptions\u0026gt;即可，这两个类型都提供了一个Get()方法，传入前面定义好的主题就可以获取到对应的主题了。细心的朋友，应该会发现一件事情，这里三剑客只提到了后面两个，IOptions\u0026lt;ThemeOptions\u0026gt;直接被无视了。请记住下面这段话：命名的选项只能通过 IOptionsSnapshot和 IOptionsMonitor来访问。所有选项都是命名实例。 IConfigureOptions 实例将被视为面向 Options.DefaultName 实例，即 string.Empty。 IConfigureNamedOptions 还可实现 IConfigureOptions。 IOptionsFactory 的默认实现具有适当地使用每个实例的逻辑。 null 命名选项用于面向所有命名实例，而不是某一特定命名实例。 ConfigureAll 和 PostConfigureAll 使用此约定。\nIChnageToken 现在，让我们回到本文的主题，博主你不是要说配置热更新这个话题吗？截至到目前为止，我们修改配置文件的时候，ASP.NET Core 应用明明就会更新配置啊，所以，博主你到底想说什么？其实，博主想说的是，的确我们的目的已经达到了，但我们不能永远停留在“知其然”的水平，如果不试图去了解内在的机制，当我们去尝试实现一个自定义配置源的时候，就会遇到一些你没有办法想明白的事情。所以，接下来要讲的IChnageToken这个接口可以说是非常重要。\n首先，我们把目光聚焦到CreateDefaultBuilder这个方法，它通常在入口文件Program.cs中被调用，主要作用是构造一个 IWebHostBuilder 实例并返回，下面是这个方法的内部实现，博主这里对其进行了精简：\npublic static IWebHostBuilder CreateDefaultBuilder(string[] args) { //以下简化后的代码片段 builder.ConfigureAppConfiguration((hostingContext, config) =\u0026gt; { var env = hostingContext.HostingEnvironment; config.AddJsonFile(\u0026#34;appsettings.json\u0026#34;, optional: true, reloadOnChange: true) .AddJsonFile($\u0026#34;appsettings.{env.EnvironmentName}.json\u0026#34;, optional: true, reloadOnChange: true); if (env.IsDevelopment()) { var appAssembly = Assembly.Load(new AssemblyName(env.ApplicationName)); if (appAssembly != null) { config.AddUserSecrets(appAssembly, optional: true); } } config.AddEnvironmentVariables(); if (args != null) { config.AddCommandLine(args); } }) } 可以注意到，通过ConfigureAppConfiguration()方法，框架主要做了下面的工作：\n从appsettings.json和appsettings.${env.EnvironmentName}.json两个配置文件中加载配置 从机密管理器中加载配载 从环境变量中加载配置 从命令行参数中加载配置 实际上，.NET Core 可以从配置文件、环境变量、Azure Key Vault、Azure 应用程序配置、命令行参数、已安装或已创建的自定义提供程序、目录文件、内存中的 .NET 对象等各种各样的来源中加载配置，这里的appsettings.json使用的是JsonConfigurationProvider类，位于Microsoft.Extensions.Configuration.Json这个命名空间，可以注意到，它继承自FileConfigurationProvider类，并重写了Load()方法，通过这些关系，我们最终可以找到这样一段代码：\npublic FileConfigurationProvider(FileConfigurationSource source) { if (source == null) { throw new ArgumentNullException(nameof(source)); } Source = source; if (Source.ReloadOnChange \u0026amp;\u0026amp; Source.FileProvider != null) { _changeTokenRegistration = ChangeToken.OnChange( () =\u0026gt; Source.FileProvider.Watch(Source.Path), () =\u0026gt; { Thread.Sleep(Source.ReloadDelay); Load(reload: true); }); } } 所以，真相就是,所有基于文件的配置提供者，都依赖于FileConfigurationSource，而通过FileConfigurationSource暴露出来的FileProvider都具备监视文件变化的能力，更本质上的代码其实应该是下面这样：\n//ChangeToken + IFileProvider 实现对文件的监听 var filePath = @\u0026#34;C:\\Users\\admin\\Downloads\\孔乙己.txt\u0026#34;; var directory = System.IO.Path.GetDirectoryName(filePath); var fileProvider = new PhysicalFileProvider(directory); ChangeToken.OnChange( () =\u0026gt; fileProvider.Watch(\u0026#34;孔乙己.txt\u0026#34;), () =\u0026gt; { _logger.LogInformation(\u0026#34;孔乙己，你一定又偷人家书了吧！\u0026#34;); } ); 所以，真相只有一个，真正帮助我们实现配置热更新的，其实是IChangeToken这个接口，我们只需要把这样一个实例传入到ChangeToken.OnChange()方法中，就可以在特定的时机触发这个回调函数，而显然，对于大多数的IConfigurationProvider接口而言，这个回调函数其实就是Load()方法，关于微软提供的ChangeToken静态类的实现，大家如果有兴趣去了解的话，可以参考这里：https://github.com/dotnet/extensions/blob/release/3.1/src/Primitives/src/ChangeToken.cs。话说回来，我们说IOptionsSnapshot\u0026lt;T\u0026gt;和IOptionsMonitor\u0026lt;T\u0026gt;是响应式的，当配置发生改变的时候，它们对应的值会跟着改变，从某种意义上来说，是因为IChangeToken提供了这样一个可以监听变化的的能力，试想一下，我们只需要给每一个IConfigurationProvider对应的IChangeToken注册相同的回调函数，那么，当某一个IConfigurationProvider需要重新加载的时候，我们就可以针对这个IConfigurationProvider里对应的键值对进行处理。事实上，微软官方在实现IConfigurationRoot的时候，的确就是这样做的：\npublic class ConfigurationRoot : IConfigurationRoot { private ConfigurationReloadToken _changeToken = new ConfigurationReloadToken(); private IList\u0026lt;IConfigurationProvider\u0026gt; _providers; public ConfigurationRoot(IList\u0026lt;IConfigurationProvider\u0026gt; providers) { _providers = providers; foreach (var provider in providers) { provider.Load(); ChangeToken.OnChange(() =\u0026gt; provider.GetReloadToken(), this.RaiseChanged); } } public IChangeToken GetReloadToken() =\u0026gt; return _changeToken; private void RaiseChanged() { Interlocked.Exchange\u0026lt;ConfigurationReloadToken\u0026gt;(ref _changeToken, new ConfigurationReloadToken()).OnReload(); } public void Reload() { foreach (var provider in _providers) { provider.Load(); } this.RaiseChanged(); } } 自定义配置源 好了，现在你可以说你了解 .NET Core 的配置热更新这个话题了，因为截至到此时此刻，我们不仅仅达到了一开始的目的，而且深刻地理解了它背后蕴含的原理。这样，我们就可以向着下一个目标：自定义配置源努力了。前面提到过，.NET Core 里面支持各种各样的配置源，实际中可能会遇到更多的配置源，比如不同的数据库、YAML 格式以及 Apollo、Consul、Nacos 这些配置中心等等，所以，了解如何去写一个自定义的配置源还是非常有必要的。我们在一开始的时候提到了 Redis 的发布-订阅，那么，下面我们就来基于发布-订阅实现一个简单的配置中心，当我们需要修改配置时，只需要通过可视化的 Redis 工具进行修改，然后再给指定的客户端发一条消息即可。\n实现自定义配置源，需要实现IConfigurationSource和IConfigurationProvider两个接口，前者实现起来非常简单，因为只要返回我们定义的RedisConfigurationProvider实例即可：\npublic class RedisConfigurationSource : IConfigurationSource { private readonly RedisConfigurationOptions _options; public RedisConfigurationSource(RedisConfigurationOptions options) { _options = options; } public IConfigurationProvider Build(IConfigurationBuilder builder) { return new RedisConfigurationProvider(_options); } } 接下来是RedisConfigurationProvider类的实现：\npublic class RedisConfigurationProvider : ConfigurationProvider { private CSRedisClient _redisClient; private readonly RedisConfigurationOptions _options; public RedisConfigurationProvider(RedisConfigurationOptions options ) { _options = options; _redisClient = new CSRedisClient(_options.ConnectionString); if (options.AutoReload) { //利用Redis的发布-订阅重新加载配置 _redisClient.Subscribe((_options.HashCacheChannel, msg =\u0026gt; Load())); } } public override void Load() { Data = _redisClient.HGetAll\u0026lt;string\u0026gt;(_options.HashCacheKey) ?? new Dictionary\u0026lt;string, string\u0026gt;(); } } 为了用起来更得心应手，扩展方法是少不了的：\npublic static class RedisConfigurationExtensions { public static IConfigurationBuilder AddRedisConfiguration(this IConfigurationBuilder builder, RedisConfigurationOptions options) { return builder.Add(new RedisConfigurationSource(options)); } } 现在，我们改一下入口类Program.cs，因为在这个阶段依赖注入是无法使用的，所以，看起来有一点难受，从命名就可以看出来，内部使用了Hash这种结构，理论上每个客户端应该使用不同的 Key 来进行缓存，应该使用不同的 Channel 来接收配置更新的通知：\npublic static IHostBuilder CreateHostBuilder(string[] args) =\u0026gt; Host.CreateDefaultBuilder(args) .ConfigureAppConfiguration(configurationBuilder =\u0026gt; { configurationBuilder.AddRedisConfiguration(new Models.RedisConfigurationOptions() { AutoReload = true, ConnectionString = \u0026#34;127.0.0.1:6379\u0026#34;, HashCacheKey = \u0026#34;aspnet:config\u0026#34;, HashCacheChannel = \u0026#34;aspnet:config:change\u0026#34; }); }) .ConfigureWebHostDefaults(webBuilder =\u0026gt; { webBuilder.UseStartup\u0026lt;Startup\u0026gt;(); }); 假设现在 Redis 里存储着下图所示的信息：\nRedis中的存储结构\r相应地，我们可以在Startup中进行绑定：\nservices.Configure\u0026lt;AppInfoOptions\u0026gt;(Configuration.GetSection(\u0026#34;App\u0026#34;)); 调一下接口看看？完全一致！Yes！\nRedis与客户端的配置一致\r本文小结 回想起这个面试中“邂逅”的问题，针对对这块内容，其实当时并没有和面试官进行太深的交流，提到了分布式配置、配置中心以及像缓存的雪崩、击穿等等常见的问题，我隐约记得配置文件appsettings.json配置的部分有热更新的配置项，但我并没有对选项模式(Options)里的三剑客做过深入的挖掘，所以，这篇博客，一方面是系统地了解了一下选项模式(Options)的使用，而另一方面是由配置热更新这个话题引申出来的一系列细节，在没有理解IChangeToken的时候，实现一个自定义的配置源是有一点困难的，在这篇博客的最后，我们基于 Redis 的发布-订阅实现了一个简单的配置中心，不得不说，Redis 里用:来分割 Key 的方式，实在是太棒了，因为它可以完美地和 .NET Core 里的配置系统整合起来，这一点只能用赏心悦目来形容，好了，国庆节以后的第一篇博客就是这样了，谢谢大家！\n","date":"2020-10-11T12:19:02Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/835719605/","slug":"835719605","tags":[".NET Core","配置中心","配置"],"title":"基于选项模式实现.NET Core 的配置热更新"},{"categories":["数据存储"],"content":"话说最近这两周里，被迫官宣996的生活实在是无趣，在两周时间里安排三周的工作量，倘若用丞相的口吻来说，那便是: 我从未见过有如此厚颜无耻之人。无法为工作的紧急程度排出优先级，这便是身为肉食者们的鄙。古人云：肉食者鄙，未能远谋，诚不欺我也。一味地追求快速迭代，“屎”山越滚越高没有人在乎；一味地追求功能叠加，技术债务越来越多没有人在乎。所以，本着“多一事不如少一事”的原则，直接通过 Dapper 写 SQL 语句一样没有问题，因为被压榨完以后的时间只能写这个。在今天的这篇博客里，我想和大家分享的是，Dapper.Contrib在操作 Oracle 数据库时引发 ORA-00928: 缺失 SELECT 关键字 这一错误背后的根本原因，以及 Dapper 作为一个轻量级 ORM 在设计上做出的取舍。\n问题回顾 在使用 Dapper.Contrib 操作 Oracle 数据库的时候，通过 Insert() 方法来插入一个实体对象，此时，会引发 ORA-00928: 缺失 SELECT 关键字 这种典型的 Oracle 数据库错误，对于经常使用 Dapper 的博主而言，对于 @ 还是 : 这种无聊的语法还是有一点经验的，在尝试手写 SQL 语句后，发现使用 Dapper 提供的 Execute() 扩展方法一点问题都没有，初步判定应该是 Dapper.Contrib 这个扩展库的问题，在翻阅 Dapper 的源代码以后，终于找到了问题的根源所在，所以，下面请跟随博主的目光，来一起解读解读 Dapper.Contrib 这个扩展库，相信你看完以后就会明白，为什么这里会被 Oracle 数据库摆上一道，以及为什么它至今都不考虑合并 Oracle 数据库相关的 PR。\n原因分析 众所周知，Dapper 的核心其实就是一个 SqlMapper ，它提供的 Query() 和 Execute() 接口本身都是附加在 IDbConnection 接口上的扩展方法，所以，最基础的 Dapper 用法其实是伴随着 SQL 语句和以匿名对象为主的参数化查询，这可以说是 Dapper 的核心，而 Dapper.Contrib 在这个基础上提供了 Get()、Insert()、Delete() 和 Update() 等等常见的 CRUD 方法，这些方法都针对的是单主键的表，让 Dapper有了一点 ORM 的感觉，可惜的是 Dapper.Contrib 的实现是不完整的，主要是指下面两个方面，即：第一，官方未能提供 Oracle 版本的 ISqlAdapter。第二，兼容不同数据库自增 ID 的实现，让官方在处理 Id 的参数化查询时束手束脚，对 ISqlAdapter 的设计并不全面。\nOracle 版本的 ISqlAdapter 首先，第一个结论，Dapper.Contrib 没有实现 Oracle 版本的 ISqlAdapter 。关于这个接口，我们可以在 SqlMapperExtensions 这个类中找到定义，而 Dapper.Contrib 内部实际上是维护了一个字典 AdapterDictionary ，在 SqlMapperExtensions.cs 文件的第 62 行 ~ 第 73 行，我们可以注意到，其内部提供了 6 种 ISqlAdapter 的实现，且默认为 SqlServerAdapter ：\nprivate static readonly ISqlAdapter DefaultAdapter = new SqlServerAdapter(); private static readonly Dictionary\u0026lt;string, ISqlAdapter\u0026gt; AdapterDictionary = new Dictionary\u0026lt;string, ISqlAdapter\u0026gt;(6) { [\u0026#34;sqlconnection\u0026#34;] = new SqlServerAdapter(), [\u0026#34;sqlceconnection\u0026#34;] = new SqlCeServerAdapter(), [\u0026#34;npgsqlconnection\u0026#34;] = new PostgresAdapter(), [\u0026#34;sqliteconnection\u0026#34;] = new SQLiteAdapter(), [\u0026#34;mysqlconnection\u0026#34;] = new MySqlAdapter(), [\u0026#34;fbconnection\u0026#34;] = new FbAdapter() }; 一个自然而然的问题是，这个 ISqlAdapter 接口是做什么的呢？为什么说 Dapper.Contrib 没有实现 Oracle 版本的 ISqlAdapter 呢？如果我们看一下 ISqlAdapter 的定义，就可以了解到其作用是告诉 Dapper ，应该怎么样处理数据库里的自增 ID、怎么样表示 Column = Value 这样的结构，以及怎么样处理列名：\npublic partial interface ISqlAdapter { int Insert(IDbConnection connection, IDbTransaction transaction, int? commandTimeout, string tableName, string columnList, string parameterList, IEnumerable\u0026lt;PropertyInfo\u0026gt; keyProperties, object entityToInsert ); void AppendColumnName(StringBuilder sb, string columnName); void AppendColumnNameEqualsValue(StringBuilder sb, string columnName); } 这里以 MySqlAdapter 的实现为例：\npublic partial class MySqlAdapter : ISqlAdapter { public int Insert(IDbConnection connection, IDbTransaction transaction, int? commandTimeout, string tableName, string columnList, string parameterList, IEnumerable\u0026lt;PropertyInfo\u0026gt; keyProperties, object entityToInsert ) { var cmd = $\u0026#34;insert into {tableName} ({columnList}) values ({parameterList})\u0026#34;; connection.Execute(cmd, entityToInsert, transaction, commandTimeout); var r = connection.Query(\u0026#34;Select LAST_INSERT_ID() id\u0026#34;, transaction: transaction, commandTimeout: commandTimeout); var id = r.First().id; if (id == null) return 0; var propertyInfos = keyProperties as PropertyInfo[] ?? keyProperties.ToArray(); if (propertyInfos.Length == 0) return Convert.ToInt32(id); var idp = propertyInfos[0]; idp.SetValue(entityToInsert, Convert.ChangeType(id, idp.PropertyType), null); return Convert.ToInt32(id); } public void AppendColumnName(StringBuilder sb, string columnName) { sb.AppendFormat(\u0026#34;`{0}`\u0026#34;, columnName); ｝ public void AppendColumnNameEqualsValue(StringBuilder sb, string columnName) { sb.AppendFormat(\u0026#34;`{0}` = @{1}\u0026#34;, columnName, columnName); } } 相信看到这里的时候，大家会和我一样感到失望，因为 Dapper 的底层依然是在拼 SQL ，尤其是看到 AppendColumnNameEqualsValue() 这个方法的时候，会有一种恍然大明白的感觉，因为 @ 这个符号对于 Dapper 的参数化查询而言，实在是熟悉得不能再熟悉了。我们都知道为 Dapper 写 SQL 语句的时候，要对 Oracle 区别对待，因为这个奇葩非要用 : 这个奇怪的符号。回到我们一开始的问题，为啥 Dapper.Contrib 在 Oracle 环境下会提示 ORA-XXXXX 这种鬼都看不明白的错误，因为它在处理 SQL 的语句的时候依然使用的是 @ 这个符号。这又是为什么呢？因为当指定的 IDbConnection 在 AdapterDictionary 中不存在的时候，它会使用默认的 SqlServerAdapter ，显然，全世界只有 Oracle 这个奇葩会用 : 这个奇怪的符号。我们不是在调用 Insert() 方法的时候提示这个错误吗？那么 Dapper.Contrib 是怎么实现 Insert() 方法的呢？这个部分实现主要在第 352 行 ~ 第 360 行：\nvar adapter = GetFormatter(connection); for (var i = 0; i \u0026lt; allPropertiesExceptKeyAndComputed.Count; i++) { var property = allPropertiesExceptKeyAndComputed[i]; adapter.AppendColumnName(sbColumnList, property.Name); //fix for issue #336 if (i \u0026lt; allPropertiesExceptKeyAndComputed.Count - 1) sbColumnList.Append(\u0026#34;, \u0026#34;); } 显然，这部分是按照属性名去组织 columnList 和 parameterList 的过程，对于 Oracle ，永远是充满吐槽的，比如不加双引号则强制大写的设定，这意味着如果你的表名或者字段名是区分大小写的话，在 Oracle 这里都要加上双引号，这对 Dapper.Contrib 有什么影响呢？原本我们只需要给实体添加[Table]标签即可，而现在你不得不考虑带上反斜杠转义，甚至当你需要为 DBeaver 下载一个 JDBC 的驱动的时候，甲骨文这家公司居然要强制你去注册，对于一个习惯像·.NET Core、GCC、Python、Lua 和 Node 这样开箱即用的人来说，这就像强迫你注册一大堆真实信息，然后发现 API 接口完全无法匹配你的需求一样痛苦。关于 GetFormatter() 方法，它和我们猜想的完全一致：\nprivate static ISqlAdapter GetFormatter(IDbConnection connection) { var name = GetDatabaseType?.Invoke(connection).ToLower() ?? connection.GetType().Name.ToLower(); return AdapterDictionary.TryGetValue(name, out var adapter) ? adapter : DefaultAdapter; } 好了，在明白了以上种种因果关系以后，我们现在来考虑如何解决 Oracle 的问题。按照人类最直观的思维，既然它没有实现 Oracle 版本的 ISqlAdapter ，我自己实现一个不就好啦：\npublic class OracleSqlAdapter : ISqlAdapter { public void AppendColumnName(StringBuilder sb, string columnName) { sb.AppendFormat(\u0026#34;{0}\u0026#34;, columnName); } public void AppendColumnNameEqualsValue(StringBuilder sb, string columnName) { sb.AppendFormat(\u0026#34;{0} = :{1}\u0026#34;, columnName, columnName); } public int Insert(IDbConnection connection, IDbTransaction transaction, int? commandTimeout, string tableName, string columnList, string parameterList, IEnumerable\u0026lt;PropertyInfo\u0026gt; keyProperties, object entityToInsert) { var sql = $\u0026#34;insert into {tableName} ({columnList}) values ({parameterList})\u0026#34;; return connection.Execute(sql, entityToInsert, transaction, commandTimeout); } public Task\u0026lt;int\u0026gt; InsertAsync(IDbConnection connection, IDbTransaction transaction, int? commandTimeout, string tableName, string columnList, string parameterList, IEnumerable\u0026lt;PropertyInfo\u0026gt; keyProperties, object entityToInsert) { var sql = $\u0026#34;insert into {tableName} ({columnList}) values ({parameterList})\u0026#34;; return connection.ExecuteAsync(sql, entityToInsert, transaction, commandTimeout); } } 坦白说， Dapper.Contrib 这种纯静态类的设计，完全就不给别人留扩展的口子，为此，扩展方法 + 反射搞一个突破口：\npublic static class SqlAdapterrExtensions { public static void UseSqlAdapter\u0026lt;TSqlAdapter\u0026gt;(this IDbConnection connection, TSqlAdapter sqlAdapter) where TSqlAdapter : ISqlAdapter, new() { var adapters = (Dictionary\u0026lt;string, ISqlAdapter\u0026gt;) typeof(SqlMapperExtensions) .GetField(\u0026#34;AdapterDictionary\u0026#34;, BindingFlags.NonPublic | BindingFlags.Instance | BindingFlags.Static) ?.GetValue(null); var connectionType = connection.GetType().Name.ToLower(); if (adapters != null \u0026amp;\u0026amp; !adapters.ContainsKey(connectionType)) adapters?.Add(connectionType, sqlAdapter); } } 这样，我们不但可以满足眼下，还可以着眼未来，虽然未来有时候挺遥远，但梦想还是要有的，开闭原则，我做到了！改进后，我们这样处理即可：\nconnection = new OracleConnection(ConnectionStrings.Default); connection.UseSqlAdapter(new OracleSqlAdapter()); 此时，我们发现，我们解决了 Insert() 的问题，但随之而来的，Get()、Delete()、Update() 这一系列和主键相关的方法，都因为 Dapper.Contrib 中的主键设计而出现了问题，而这就是我们接下来要讲的主键 Id 参数化问题。\n主键 Id 参数化问题 当我谈起这个问题的时候，我对于 Dapper.Contrib 中支持自增 ID 的坚持是怀疑的，因为在分布式盛行的今天，有大量的分布式 ID 生成方案供我们选择，比如基于 Redis 的号段策略，基于雪花算法的 ID 生成等等。大家会注意到我实现的 OracleSqlAdapter 在实现 Insert() 方法的时候简化了大量代码，这是因为我真的不知道，怎么从 Oracle 中获取一个新生成的 ID，尤其是这个 ID 居然还要依赖一个我听都没有听说过的“序列”，而之所以要在 ISqlAdapter 中实现 Insert() 方法，最根本的原因就是，各个数据库对于自增 ID 的实现是不一样的，比如 MySQL 中使用的是 SCOPE_IDENTITY()，而 MSSQL 中使用的则是 SCOPE_IDENTITY() ，就因为这一点点差异，我们就必须要去折腾一遍，可以说， Dapper.Contrib 不支持 Oracle 的一个重要原因，就是在 Oracle 下实现自增 ID 太麻烦了。\n既然大家都不用自增 ID 了，为什么还要在一个通用的 ORM 里折腾这个呢？说实话，我真担心有一天自增 ID 会溢出，谁让每个数据库里的上限都不一样呢？另一方面，既然 Id 在每个数据库的实现都不一样，那么，作为 Id 本身应该考虑放到 ISqlAdapter 接口中由使用者来实现啊，可偏偏 ISqlAdapter 里只定义了一个 Insert() 方法，所以，就算我们实现了 OracleSqlAdapter ，一样无法解决 Insert() 方法以外的其它方法在 Oracle 下面的问题，正因为如此，默认的 @ 符号在 Oracle 环境下下没有被完全替换掉，这就需要修改 Dapper.Contrib 的底层代码，这真的是一个不好的设计，因为使用者完全没有办法通过重写来覆盖某些默认行为，我们一起来看看，需要修改哪些地方：\npublic static T Get\u0026lt;T\u0026gt;(this IDbConnection connection, dynamic id, IDbTransaction transaction = null, int? commandTimeout = null) where T : class { var type = typeof(T); if (!GetQueries.TryGetValue(type.TypeHandle, out string sql)) { var key = GetSingleKey\u0026lt;T\u0026gt;(nameof(Get)); var name = GetTableName(type); //第一个坏事儿的地方，为什么不用AppendColumnName()方法? sql = $\u0026#34;select * from {name} where {key.Name} = @id\u0026#34;; GetQueries[type.TypeHandle] = sql; } var dynParams = new DynamicParameters(); //第二个坏事的地方，什么不用AppendColumnName()方法?? dynParams.Add(\u0026#34;@id\u0026#34;, id); //以下代码已省略 } public static long Insert\u0026lt;T\u0026gt;(this IDbConnection connection, T entityToInsert, IDbTransaction transaction = null, int? commandTimeout = null) where T : class { //以上代码已省略 var sbParameterList = new StringBuilder(null); for (var i = 0; i \u0026lt; allPropertiesExceptKeyAndComputed.Count; i++) { //第三个坏事的地方，什么不用AppendColumnName()方法??? var property = allPropertiesExceptKeyAndComputed[i]; sbParameterList.AppendFormat(\u0026#34;@{0}\u0026#34;, property.Name); if (i \u0026lt; allPropertiesExceptKeyAndComputed.Count - 1) sbParameterList.Append(\u0026#34;, \u0026#34;); } //以下代码已省略 } 其实，仔细阅读 Update() 和 Delete() 两个方法的实现，就会发现它们都非常完美地避开了这一点，就是不知道为什么只有两个方法采用了不同地方式去拼接 SQL ，当然，这里我们会意识到有个列名的问题，尤其是在需要区分大小写的情况下，为此，我们可能需要去定义一个 ColumnAttribute，还能说什么呢？请和我大声地吐槽：**垃圾 Oracle ！**你看，就为了这一点点差异，我们不得不去额外写一点代码，所以，喊了很多年的去 IOE，我表示举双手赞成。\n事实上，社区里已经有类似的PR，可因为改动的范围比较大，官方至今都没有考虑过将其合并到主干分支上，所以，这个问题一直没有解决，这是一个悲伤的故事。\n相关思考 在阅读 Dapper 源码的同时，我查阅了一个和 Dapper.Contrib 类似的项目：DapperExtension，我发现这个项目目前处在“荒废”的状态，因为它遇到了相同的问题，即 SQL 这门看起来统一实则相当不统一的语言，因为每一个数据库厂商几乎都在给标准“添砖加瓦“，就以自增 ID 为例，MySQL、MSSQL、Oracle 居然是三种不同的实现方式，尤其是 Oracle 这个奇葩，居然还需要定义一个序列来解决这个问题，这个奇葩给数据库加注释都那么另类，这带来的问题是什么？Dapper.Contrib 无力去实现 Oracle 的自增 ID 而放弃了 Oracle ，所以，即使社区里提交了 PR，因为实现方式有点脏，官方一直没有合并到主干上去。\n再回过头来看 Dapper.Contrib 支持自增 ID 的举动，总会觉得有点不合时宜，因为不同数据库自增 ID 的上限不一样不说，现在都普遍在分布式的环境中，数据库的自增 ID 其实是非常鸡肋的功能，而实际应用中常常会用 Redis 、雪花算法等来实现分布式 ID，所以，当你回顾历史发展的趋势的时候，就会感慨有标准化的东西该多好，并不是说这个世界不需要多样性，显然这是一个标准约束性不强的领域，看起来大家都实现了 SQL，无一例外地都夹藏了私货，对于商业行为而言，这无可厚非；可对于这个世界而言，这无疑增加了工作量。\n有时候，当一个行业没有什么标准的时候，到底是突破勇气去率先制定标准，还是放弃自我去迎合各种不成文的规则，对于企业而言，是战略上的一种选择；而对于个人而言，其实是人生的一种选择。当彼时青春年少的人们，竞相以标新立异为荣的时候，如果想到有一天，终究要活成千篇一律的人生，为了生活而选择跪着的时候，内心又会有什么不一样的举动呢？\n本文小结 本文分析了 Dapper.Contrib 这个扩展库，在搭配 Oracle 数据库使用时遇到 ORA-XXXXX 系列错误的原因及相应地处理方法，这个问题的表象是 Dapper.Contrib 没有实现 OracleSqlAdapter ，而更深层的原因，实际上是 Dapper.Contrib 选择支持自增 ID 而带来的 SQL 标准差异化问题。因为不同的数据库在实现自增 ID 时的机制不同，Oracle 甚至需要引入序列这个概念，这种差异化，增加了 Dapper 各个扩展库维护的工作量，这是官方一直不愿意实现 OracleSqlAdapter 的原因，其次， Dapper.Contrib 底层设计不合理，除了 Insert() 方法以外，其它依赖主键的方法都没有提供扩展接口，导致使用者只能通过修改底层代码的方式解决问题，这严重违反开闭原则。好了，这是一篇利用 996 闲暇(可能是指做梦)写的一篇博客，如果文章中有什么不周到的地方，欢迎大家在博客下面给我留言，谢谢，晚安！\n","date":"2020-09-05T14:28:20Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3086300103/","slug":"3086300103","tags":["Dapper","ORM","数据库","源码"],"title":"Dapper.Contrib 在 Oracle 环境下引发 ORA-00928 异常问题的解决"},{"categories":["编程语言"],"content":"在此前的博客中，博主参考 eShopOnContainers 实现了一个基于 RabbitMQ 的事件总线(EventBus)。在这个项目中，它提供了一个持久化连接的类DefaultRabbitMQPersistentConnection，主要解决了 RabbitMQ 在连接断开后自动重连的问题，可实际上我们都知道，RabbitMQ 提供的连接数是有一个上限的，如果频繁地使用短连接的方式，即通过ConnectionFactory的CreateConnection()方法来创建一个连接，从本质上讲，一个Connection对象就是一个 TCP 连接，而Channel则是每个Connection对象下有限的虚拟连接，注意“有限”这个限定词，这意味着Channel和Connection一样，都不能毫无节制的创建下去。此时，官方推荐的做法有两种：(1)：一个Connection对应多个Channel同时保证每个Channel线程独占；(2)：创建一个Connection池同时定期清除无效连接。这里的第二种做法，显然就是我们今天要说的对象池(Object Pool)啦，我们将从这里拉开这篇博客的帷幕。\n什么是对象池 首先，我们来回答第一个问题，什么是对象池？简单来说，它就是一种为对象提供可复用性能力的软件设计思路。俗话说**“有借有还，再借不难”**，而对象池就是通过“借”和“还”这样两个动作来保证对象可以被重复使用，进而节省频繁创建对象的性能开销。对象池在游戏设计中使用的更普遍一点，因为游戏中大量存在着像子弹、怪物等等这类可复用的对象，你在玩第一人称射击游戏(FPS)时，总是有源源不断的子弹或者丧尸出现，可事实上这不过是数字世界的循环再生，因为玩家的电脑内存始终都有一个上限。而在数据库的世界里，则存在着一个被称为“连接池”的东西，每当出现数据库无法连接的情况时，经验丰富的开发人员往往会先检查“连接池”是否满了，这其实就是对象池模式在特定领域的具体实现啦，所以，对象池本质上就是负责一组对象创建和销毁的容器，下面是一个基本的对象池示意图：\n对象池示意图\r可以注意到， 对象池最大的优势就是可以自主地管理“池子”内的每个对象，决定它们是需要被回收还是可以重复使用。我们都知道，创建一个新的对象，需要消耗一定的系统资源，而一旦这些对象可以重复地使用，就能有效地节省系统资源的开销，这对于我们提高系统性能会非常有帮助。也许，现在计算机的硬件水平越来越好，可我们还是要重新拾起这个领域的基础知识，即数据结构、算法、数学和英语。如果你完全理解了对象池模式，你应该可以非常轻松地给出你的实现：\npublic class ObjectPool\u0026lt;T\u0026gt; : IObjectPool\u0026lt;T\u0026gt; { private Func\u0026lt;T\u0026gt; _instanceFactory; private ConcurrentBag\u0026lt;T\u0026gt; _instanceItems; public ObjectPool(Func\u0026lt;T\u0026gt; instanceFactory) { _instanceFactory = instanceFactory ?? throw new ArgumentNullException(nameof(instanceFactory)); _instanceItems = new ConcurrentBag\u0026lt;T\u0026gt;(); } public T Get() { T item; if (_instanceItems.TryTake(out item)) return item; return _instanceFactory(); } public void Return(T item) { _instanceItems.Add(item); } } 注：以上代码片段来自微软的一篇文档：How to: Create an Object Pool by Using a ConcurrentBag。实际上，除了ConcurrentBag\u0026lt;T\u0026gt;，我们可以选择的数据结构还可以是Stack\u0026lt;T\u0026gt;、Queue\u0026lt;T\u0026gt;以及BlockingCollection\u0026lt;T\u0026gt;，此中差别，大家可以自己去体会。\n.NET Core 中的对象池 在.NET Core 中，微软已经为我们提供了对象池的实现，即Microsoft.Extensions.ObjectPool。它主要提供了三个核心的组件，分别是ObjectPool、ObjectPoolProvider和IPooledObjectPolicy，关于这三者间的关系，我绘制了下面的 UML 图来作为说明：\nObjectPool核心组件及其关系\r可以注意到，ObjectPool\u0026lt;T\u0026gt;是一个抽象类，它对外提供了 Get()和 Return()两个方法，所谓的“有借有还”，这一点没什么可说的。接下来，ObjectPoolProvider同样是一个抽象类，它的职责就是创建ObjectPool\u0026lt;T\u0026gt;，所以，它提供了两个Create\u0026lt;T\u0026gt;()方法，两者的区别是，无参数版本本质上使用的是DefaultPooledObjectPolicy\u0026lt;T\u0026gt;。顾名思义，它同DefaultObjectPool\u0026lt;T\u0026gt;、DefaultObjectPoolProvider一样，都是微软提供的默认实现，其中IPooledObjectPolicy\u0026lt;T\u0026gt;可以为不同的对象池定义不同的策略，来决定对象如何“借”、是否可以“还”。默认的对象池DefaultObjectPool\u0026lt;T\u0026gt;内部使用ObjectWrapper[]这个数组来管理对象，数组的大小等于 maximumRetained - 1，因为它单独指定了首项，默认情况下，这个 maximumRetained 等于Environment.ProcessorCount * 2，这里主要用到了Interlocked.CompareExchange()方法：\npublic override T Get() { var item = _firstItem; if (item == null || Interlocked.CompareExchange(ref _firstItem, null, item) != item) { var items = _items; for (var i = 0; i \u0026lt; items.Length; i++) { item = items[i].Element; if (item != null \u0026amp;\u0026amp; Interlocked.CompareExchange(ref items[i].Element, null, item) == item) { return item; } } item = Create(); } return item; } // Non-inline to improve its code quality as uncommon path [MethodImpl(MethodImplOptions.NoInlining)] private T Create() =\u0026gt; _fastPolicy?.Create() ?? _policy.Create(); public override void Return(T obj) { if (_isDefaultPolicy || (_fastPolicy?.Return(obj) ?? _policy.Return(obj))) { if (_firstItem != null || Interlocked.CompareExchange(ref _firstItem, obj, null) != null) { var items = _items; for (var i = 0; i \u0026lt; items.Length \u0026amp;\u0026amp; Interlocked.CompareExchange(ref items[i].Element, obj, null) != null; ++i) { } } } } 这里主要用到Interlocked.CompareExchange()这个方法，对于Get()方法而言，它将items[i].Element和null进行交换，相当于将指定元素设为 null 并返回原始值；而对于Return()方法而言，如果将items[i].Element和obj交换后的值不为 null，则表示指定元素已经“归还”，因为这个方法只有在第一个参数和第三个参数相等时才会发生交换。好了，了解了.NET Core 中对象池的实现以后，我们来一起看看具体的使用：\nvar service = new ServiceCollection(); //使用DefaultObjectPoolProvider service.AddSingleton\u0026lt;ObjectPoolProvider, DefaultObjectPoolProvider\u0026gt;(); //使用默认策略 service.AddSingleton\u0026lt;ObjectPool\u0026lt;Foo\u0026gt;\u0026gt;(serviceProvider =\u0026gt; { var objectPoolProvider = serviceProvider.GetRequiredService\u0026lt;ObjectPoolProvider\u0026gt;(); return objectPoolProvider.Create\u0026lt;Foo\u0026gt;(); }); //使用自定义策略 service.AddSingleton\u0026lt;ObjectPool\u0026lt;Foo\u0026gt;\u0026gt;(serviceProvider =\u0026gt; { var objectPoolProvider = serviceProvider.GetRequiredService\u0026lt;ObjectPoolProvider\u0026gt;(); return objectPoolProvider.Create(new FooObjectPoolPolicy()); }); var serviceProvider = _service.BuildServiceProvider(); var objectPool = _serviceProvider.GetService\u0026lt;ObjectPool\u0026lt;Foo\u0026gt;\u0026gt;(); //有借有还，两次是同一个对象 var item1 = objectPool.Get(); objectPool.Return(item1); var item2 = objectPool.Get(); Assert.AreEqual(item1, item2);//true //有借无还，两次是不同的对象 var item3 = objectPool.Get(); var item4 = objectPool.Get(); Assert.AreEqual(item3, item4);//false 其中，Foo和FooObjectPoolPolicy是两个非常典型的“工具类”，类似我们所说的“工具人”：\npublic class Foo { public string Id { get; set; } public DateTime? CreatedAt { get; set; } public string CreatedBy { get; set; } } public class FooObjectPoolPolicy : IPooledObjectPolicy\u0026lt;Foo\u0026gt; { public Foo Create() { return new Foo() { Id = Guid.NewGuid().ToString(\u0026#34;N\u0026#34;), CreatedAt = DateTime.Now, CreatedBy = \u0026#34;Ezio\u0026#34; }; } public bool Return(Foo obj) { return true; } } 当你需要控制对象池内的对象如何被创建的时候，你可以考虑实现自定义的IPooledObjectPolicy\u0026lt;T\u0026gt;，否则，DefaultPooledObjectPolicy\u0026lt;T\u0026gt;这个默认实现完全可以满足你的使用，而这就是.NET Core 中对象池的所有用法，一个实现起来并不复杂但是在某些场景下非常有用的软件设计模式。\n回到起点 好了，回到我们一开始的问题，即：如何解决 RabbitMQ 在多次重连后提示连接数不足的问题。由于 Channel 对象本质上是 Connection 对象上的 TCP 连接的软连接，所以，每当创建一个新的 Channel 的时候，实际上会独占一个 TCP 连接。考虑到在使用 RabbitMQ 的时候，发布消息/消费消息每次都是创建一个 Channel，在高并发场景下可能会导致 TCP 连接数被用完，进而出现无法连接或者响应过慢等一系列问题。既然 TCP 连接数是有限的，为什么不考虑复用这些 TCP 连接呢？从这个角度上来看，数据库连接池承担了相同的角色，增加连接数说到底是一种“治标不治本”的做法。在具体实现上，可以考虑 Connection“池”和 Channel“池”，我们我们像官方推荐的做法一样，一个 Connection 对应多个 Channel，实际上只需要实现 Channel“池”。除非在多个 Connection 对应多个 Channel 的情况下，我们需要考虑同时实现 Connection“池”和 Channel“池”。坦白说，我这里一直没能找到实现 Connection“池”的相关资料，高冷的 Catcher 大神只是让我去认真读官方文档，搞清楚 Connection 和 Channel 的关系。而这个 Channel“池”的实现，结合这篇博客里的内容，实现起来是非常简单的：\npublic class ChannelObjectPoolPolicy : IPooledObjectPolicy\u0026lt;IModel\u0026gt; { private readonly IConnectionFactory _connectionFactory; public ChannelObjectPoolPolicy(IConnectionFactory connectionFactory) { _connectionFactory = connectionFactory; } public IModel Create() { var connection = _connectionFactory.CreateConnection(); return connection.CreateModel(); } public bool Return(IModel obj) { if (!obj.IsOpen) { obj?.Dispose(); return false; } return true; } } 第一步是实现IPooledObjectPolicy\u0026lt;IModel\u0026gt;，注意到，这里通过构造函数注入了ConnectionFactory，所以，除了常规的注入项以外，这里还需要注入ConnectionFactory:\nservices.AddSingleton\u0026lt;IConnectionFactory, ConnectionFactory\u0026gt;(sp =\u0026gt; new ConnectionFactory() { HostName = \u0026#34;localhost\u0026#34;, UserName = \u0026#34;guest\u0026#34;, Password = \u0026#34;guest\u0026#34; }); services.AddSingleton\u0026lt;ObjectPoolProvider, DefaultObjectPoolProvider\u0026gt;(); services.AddSingleton\u0026lt;ObjectPool\u0026lt;IModel\u0026gt;\u0026gt;(serviceProvider =\u0026gt; { var objectPoolProvider = serviceProvider.GetRequiredService\u0026lt;ObjectPoolProvider\u0026gt;(); var connectionFactory = serviceProvider.GetRequiredService\u0026lt;ConnectionFactory\u0026gt;(); return objectPoolProvider.Create(new ChannelObjectPoolPolicy(connectionFactory)); }); 然后，我们只需要在 EventBus 里注入ObjectPool\u0026lt;IModel\u0026gt;即可，此时，我们调用 Channel 的画风是下面这样子的：\nvar channel = _channelPool.Get(); try { //在这里做点什么吧 } finally { //好借好还，再借不难 _channelPool.Return(channel); } 关于 Connection“池”的实现，我认为我的想法还不太成熟，暂时列入未来的思考计划中，所以，这篇博客就先写到这里。\n本文小结 对象池(ObjectPool)是一种通过复用对象来减少资源开销进而实现提高系统性能的软件设计模式，其核心是控制容器内对象的生命周期来规避系统的主动回收，从对象池中(ObjectPool)“借”出的对象必须要及时“归还”，否则会造成对象池(ObjectPool)中没有可用资源。实现对象池可以考虑ConcurrentBag\u0026lt;T\u0026gt;、Stack\u0026lt;T\u0026gt;、Queue\u0026lt;T\u0026gt;以及BlockingCollection\u0026lt;T\u0026gt;等多种数据结构，而微软在.NET Core 中已经为我们实现了一个简单的对象池，大多数情况下，我们只需要定义自己的IPooledObjectPolicy\u0026lt;T\u0026gt;去决定对象应该怎么样“借”、怎么样“还”。因为此前实现基于 RabbitMQ 的 EventBus 的时候，我们是每次创建一个 Channel，即官方所谓的“短连接”的方式，因为 Channel 本质上是 Connection 在 TCP 连接上的一个虚拟连接，所以，每次创建 Channel 都会占用一个 TCP 连接，当我们系统中的 TCP 连接被用完的时候，就会出现无法连接、连接过慢的问题，为了解决这个问题，我们最终引入了对象池，实际上这里是实现了一个 Channel“池”，关于是否应该实现 Connection“池”，这一点我还没有想好，总而言之，游戏世界里可以复用的 GameObject、各种数据库里的连接池，都是对象池模式在各自领域中的具体实现，这就是这篇博客的内容啦，欢迎大家在评论中留言，谢谢大家！\n","date":"2020-08-15T16:37:23Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2414960312/","slug":"2414960312","tags":["对象池","设计模式",".NET Core","技巧"],"title":".NET Core 中对象池(Object Pool)的使用"},{"categories":["数据存储"],"content":"终于到这个系列的最后一篇，在前两篇博客中，我们分别了介绍了Binlog的概念和事件总线(EventBus)的实现，在完成前面这将近好几千字的铺垫以后，我们终于可以进入正题，即通过 EventBus 发布 Binlog，再通过编写对应的 EventHandler 来订阅这些 Binlog，这样就实现了我们“最初的梦想”。坦白说，这个过程实在有一点漫长，庆幸的是，它终于还是来了。\nBinlog 读取与解析 首先，我们通过 Python-Mysql-Replication 这个项目来读取 Binlog，直接通过pip install mysql-replication安装即可。接下来，我们编写一个简单的脚本文件，这再次印证那句名言——人生苦短，我用 Python：\ndef readBinLog(): stream = BinLogStreamReader( # 填写IP、账号、密码即可 connection_settings = { \u0026#39;host\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;port\u0026#39;: 3306, \u0026#39;user\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;passwd\u0026#39;: \u0026#39;\u0026#39; }, # 每台服务器唯一 server_id = 3, # 主库Binlog读写完毕时是否阻塞连接 blocking = True, # 筛选指定的表 only_tables = [\u0026#39;order_info\u0026#39;, \u0026#39;log_info\u0026#39;], # 筛选指定的事件 only_events = [DeleteRowsEvent, WriteRowsEvent, UpdateRowsEvent]) for binlogevent in stream: for row in binlogevent.rows: event = { \u0026#34;schema\u0026#34;: binlogevent.schema, \u0026#34;table\u0026#34;: binlogevent.table, \u0026#34;log_pos\u0026#34;: binlogevent.packet.log_pos } if isinstance(binlogevent, DeleteRowsEvent): event[\u0026#34;action\u0026#34;] = \u0026#34;delete\u0026#34; event[\u0026#34;origin\u0026#34;] = dict(row[\u0026#34;values\u0026#34;].items()) event[\u0026#34;current\u0026#34;] = None event = dict(event.items()) elif isinstance(binlogevent, UpdateRowsEvent): event[\u0026#34;action\u0026#34;] = \u0026#34;update\u0026#34; event[\u0026#34;origin\u0026#34;] = dict(row[\u0026#34;before_values\u0026#34;].items()) event[\u0026#34;current\u0026#34;] = dict(row[\u0026#34;after_values\u0026#34;].items()) event = dict(event.items()) elif isinstance(binlogevent, WriteRowsEvent): event[\u0026#34;action\u0026#34;] = \u0026#34;insert\u0026#34; event[\u0026#34;origin\u0026#34;] = None event[\u0026#34;current\u0026#34;] = dict(row[\u0026#34;values\u0026#34;].items()) event = dict(event.items()) stream.close() 发布 Binlog 在读取到 Binlog 以后，我们需要将其发布到 EventBus 里，为此，在.NET Core 这边提供一个 Web API 接口，只需要注入IEventBus然后调用Publish()即可：\n// Post: /\u0026lt;controller\u0026gt;/Publish [HttpPost] [Route (\u0026#34;PublishBinLog\u0026#34;)] public Task PublishBinLog (BinLogEventModel\u0026lt;dynamic\u0026gt; eventModel) { if (eventModel.action == \u0026#34;insert\u0026#34; \u0026amp;\u0026amp; eventModel.table.StartsWith (\u0026#34;log_\u0026#34;)) _eventBus.Publish (eventModel.MapTo\u0026lt;WriteLogEvent\u0026gt; ()); if (eventModel.action == \u0026#34;insert\u0026#34; \u0026amp;\u0026amp; eventModel.table == \u0026#34;order_info\u0026#34;) _eventBus.Publish (eventModel.MapTo\u0026lt;OrderInfoCreateEvent\u0026gt; ()); return Task.CompletedTask; } 相应地，我们需要在脚本中添加调用 Web API 的逻辑代码，使用我们最熟悉的requests库即可：\ndef sendBinLog(event): url = \u0026#34;https://localhost:44348/EventBus/PublishBinLog\u0026#34; headers = { \u0026#39;Content-Type\u0026#39;: \u0026#34;application/json\u0026#34;, } try: payload = json.dumps(event,cls=ComplexEncoder) response = session.request(\u0026#34;POST\u0026#34;, url, data=payload, headers=headers, verify=False) except Exception: pass 在这里，在处理 Binlog 的序列化的问题时，我们可能会遇到默认的 JSON 序列化器无法对 event 进行序列化的问题，此时，我们可以编写一个自定义的序列化器，下面是博主目前在使用的序列化器：\nclass ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, datetime): return obj.strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) elif isinstance(obj, date): return obj.strftime(\u0026#39;%Y-%m-%d\u0026#39;) elif isinstance(obj, decimal.Decimal): return str(obj) elif isinstance(obj, bytes): return obj.decode(\u0026#39;utf-8\u0026#39;) else: return json.JSONEncoder.default(self, obj) 订阅 Binlog 现在，为了订阅这些 Binlog，我们来编写对应的 EventHandler，这里我们定义两个 EventHandler，一个用于打印日志编号、日志内容、日志级别等信息，一个用于统计不同级别的日志的数目。代码实现如下：\n//打印日志的EventHandler public class WriteLogEventHandler : IEventHandler\u0026lt;WriteLogEvent\u0026gt; { private ILogger\u0026lt;WriteLogEventHandler\u0026gt; _logger; public WriteLogEventHandler (ILogger\u0026lt;WriteLogEventHandler\u0026gt; logger) { _logger = logger; } public Task Handle (WriteLogEvent @event) { _logger.LogInformation ($\u0026#34;日志编号：{@event.TRANSACTION_ID}，日志级别：{@event.LOG_LEVEL}，主机：{@event.HOST_NAME}，IP：{@event.HOST_IP}，内容：{@event.CONTENT}\u0026#34;); return Task.CompletedTask; } } //分析日志的EventHandler public class AnalyseLogEventHandler : IEventHandler\u0026lt;WriteLogEvent\u0026gt; { private readonly ILogger\u0026lt;AnalyseLogEventHandler\u0026gt; _logger; private readonly IDistributedCache _cache; public AnalyseLogEventHandler (ILogger\u0026lt;AnalyseLogEventHandler\u0026gt; logger, IDistributedCache cache) { _logger = logger; _cache = cache; } public Task Handle (WriteLogEvent @event) { var cacheCount = _cache.GetString (@event.LOG_LEVEL); if (string.IsNullOrEmpty (cacheCount)) cacheCount = \u0026#34;1\u0026#34;; else cacheCount = (int.Parse (cacheCount) + 1).ToString (); _cache.SetString (@event.LOG_LEVEL, cacheCount);; return Task.CompletedTask; } } 注意，这里需要在Startup中注入EventHandler、EventBus以及各种必要的依赖项，你可以手动注册，或者参考下面的代码，实现扫描注册：\nservices.AddSingleton\u0026lt;IRabbitMQPersistentConnection, DefaultRabbitMQPersistentConnection\u0026gt; (); services.AddSingleton\u0026lt;IEventBusSubscriptionManager, EventBusSubscriptionManager\u0026gt; (sp =\u0026gt; new EventBusSubscriptionManager ()); services.AddSingleton\u0026lt;IConnectionFactory, ConnectionFactory\u0026gt; (sp =\u0026gt; new ConnectionFactory () { HostName = \u0026#34;localhost\u0026#34;, UserName = \u0026#34;guest\u0026#34;, Password = \u0026#34;guest\u0026#34; }); services.AddSingleton\u0026lt;ObjectPoolProvider, DefaultObjectPoolProvider\u0026gt; (); services.AddControllers ().AddNewtonsoftJson (); services.AddDistributedMemoryCache (options =\u0026gt; { options.ExpirationScanFrequency = TimeSpan.FromMinutes (5); options.SizeLimit = 10; }); //自动注册 services.AddEventBus(); //手动注册 services.AddSingleton\u0026lt;IEventBus, RabbitMQEventBus\u0026gt; (sp =\u0026gt; { var eventBus = new RabbitMQEventBus (sp.GetRequiredService\u0026lt;IRabbitMQPersistentConnection\u0026gt; (), sp.GetRequiredService\u0026lt;IEventBusSubscriptionManager\u0026gt; (), sp.GetRequiredService\u0026lt;ILogger\u0026lt;RabbitMQEventBus\u0026gt;\u0026gt; (), sp, \u0026#34;eventbus-exchange\u0026#34;, \u0026#34;eventbus-queue\u0026#34;); eventBus.Subscribe\u0026lt;WriteLogEvent, WriteLogEventHandler\u0026gt;(): eventBus.Subscribe\u0026lt;WriteLogEvent, AnalyseLogEventHandler\u0026gt;(); return eventBus; }); services.AddTransient\u0026lt;WriteLogEventHandler\u0026gt;(); services.AddTransient\u0026lt;AnalyseLogEventHandler\u0026gt;(); 一起来看看效果，简直太完美了，我就是不想写中间表啊，这样多好！！！ Python 读取 Binlog 演示\r.NET Core 消费 Binlog演示\rRabbitMQ Dashboard 演示\r本文小结 通过三篇博客的篇幅，我们实现了“利用 MySQL 的 Binlog 实现数据同步与订阅”的想法。在这个过程中，我们了解了 Binlog 的相关概念，参考微软的 eShopOnContainers 项目实现了一个基于 RabbitMQ 的 EventBus，而这一切都在这篇博客中完成了最终的“拼合”，通过 Python-Mysql-Replication 实现了 Binlog 解析，而 EventBus 则作为整个事件系统的“上帝”对所有事件处理器(EventHandler)进行统一调度，最终我们不需要关心这些事件是如何被发布到 EventBus 中的，只需要知道它对应哪一个 Event 并为它编写对应的 EventHandler 即可，除了这篇博客中提到的 Binlog 以外，实际上它还可以作为系统内的“领域事件”来实现业务上的事件驱动，譬如OrderInfoCreateEvent这个事件可以表示一个订单被创建，而关心订单状态的人则可以通过 EventHandler 来实现订阅，实现类似发短信、发邮件、发微信等等的功能，或者可以让第三方的 Web API 来消费事件中携带的信息。同理，第三方的数据在流入系统时，可以先发布到消息队列中，再通过对应的 EventHandler 来进行异步处理，极大地改善系统接口的吞吐性能，而如果在这中间抽象出来一个数据交换层出来，那么就能收获更多不一样的东西，就在写这篇博客的时候，我在 Github 上的代码被收入了微软的\u0026quot;北极冰川火种计划\u0026quot;，虽然数字世界远比现实世界宽广得多，可能为这个世界减少一点“无用”的数据或者代码，应该一样可以算作是环保行为吧！\n","date":"2020-07-31T12:01:14Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3424138425/","slug":"3424138425","tags":["Binlog","RabbitMQ","MySQL"],"title":"利用 MySQL 的 Binlog 实现数据同步与订阅(下)：EventBus 篇"},{"categories":["数据存储"],"content":"紧接上一篇博客中的思路，这次我们来说说事件总线(EventBus)，回首向来，关于这个话题，我们可能会联想到发布-订阅模式、观察者模式、IObservable与 IObserver、消息队列等等一系列的概念。所以，当我们尝试着去解释这个概念的时候，它到底是什么呢？是一种设计模式？是一组 API 接口？还是一种新的技术？显而易见，发布-订阅模式和观察者模式都是设计模式，而 IObservable与 IObserver、消息队列则是具体的实现方式，就像你可以用委托或者事件去实现一个观察者模式，而 Redis 里同样内置了发布-订阅模型，换言之，这是抽象与具体的区别，消息队列可以用来实现 EventBus，而 EventBus 主要的用途则是系统间的解耦，说到解耦，你可能会对观察者模式和发布-订阅模式这两种模式感到困惑，因为它们实在是太像了，一个最本质的区别在于发布者(主题)是否与订阅者(观察者)存在强依赖关系，而发布-订阅引入了类似主题/Topic/Channel 的中介者，显然从解耦的角度要更彻底一些，所以，我们今天就来一起实现一个事件总线(EventBus)。\nEventBus 整体设计 通过前面的探讨，我们可以知道，EventBus 其实是针对事件的发布-订阅模式的实现，所以，在设计 EventBus 的时候，我们可以结合发布-定阅模式来作为对照，而一个典型的发布-订阅模式至少需要三个角色，即发布者、订阅者和消息，所以，一般在设计 EventBus 的时候，基本都会从这三个方面入手，提供发布消息、订阅消息、退订消息的接口。由于 EventBus 本身并不负责消费消息，所以，还需要借助IEventHandler\u0026lt;T\u0026gt;来编写对应的事件处理器，这是 EventBus 可以实现业务解耦的重要原因。而为了维护事件和事件处理器的关系，通常需要借助 IoC 容器来注册这些 EventHandler，提供类似Castle或者Autofac从程序集中批量注册的机制，下面是博主借鉴 eShopOnContainers 设计的 EventBus，首先是 IEventBus 接口，其定义如下：\npublic interface IEventBus { void Publish\u0026lt;TEvent\u0026gt; (TEvent @event) where TEvent : EventBase; void Subscribe\u0026lt;T, TH\u0026gt; () where T : EventBase where TH : IEventHandler\u0026lt;T\u0026gt;; void Unsubscribe\u0026lt;T, TH\u0026gt; () where TH : IEventHandler\u0026lt;T\u0026gt; where T : EventBase; } 注意到，这里对事件(EventBase)和事件处理器(EventHandler)均有一定约束，这是为了整个 EventBus 的实现，在某些 EventBus 的实现中，可能会支持非泛型的EventHandler，以及Func这样的委托类型，这里不考虑这种情形，因为从 Binlog 中获取的数据，基本上都是格式固定的 JSON。关于这部分，下面给出对应的定义：\npublic interface IEventHandlerBase { } public interface IEventHandler\u0026lt;TEvent\u0026gt; : IEventHandlerBase where TEvent : EventBase { Task Handle (TEvent @ebent); } public class EventBase { public Guid EventId { get; set; } = Guid.NewGuid (); public DateTime CreatedAt { get; set; } = DateTime.UtcNow; } 而为了维护事件(EventBase)和事件处理器(EventHandler)间的订阅关系，博主这里定义了IEventBusSubscriptionManager接口，相信以你对发布-订阅模式的理解，你可以非常容易地想到，这里应该会用到一个字典来存储每一个事件以及该事件对应的事件处理器的类型信息。你猜对了，事实上大多数的EventBus都是这样实现的，尤其当你在实现一个基于内存或者进程内通信的EventBus的时候, 到这一步其实已经完成了大多数的功能。理论上你还应该定义一个IEventStore接口，显而易见，这是针对事件的持久化接口，不过当我们选择RabbitMQ的时候，它无形中就自动帮我们实现了这个接口。\npublic interface IEventBusSubscriptionManager { EventHandler\u0026lt;EventBusSubscriptionEventArgs\u0026gt; OnSubscribe { get; set; } EventHandler\u0026lt;EventBusSubscriptionEventArgs\u0026gt; OnUnsubscribe { get; set; } void Subscribe\u0026lt;T, TH\u0026gt; () where T : EventBase where TH : IEventHandler\u0026lt;T\u0026gt;; void Unsubscribe\u0026lt;T, TH\u0026gt; () where T : EventBase where TH : IEventHandler\u0026lt;T\u0026gt;; bool IsEventSubscribed\u0026lt;T\u0026gt; () where T : EventBase; bool IsEventSubscribed (string eventName); Type GetEventTypeByName (string eventName); void Clear (); IEnumerable\u0026lt;Type\u0026gt; GetHandlersForEvent\u0026lt;T\u0026gt; () where T : EventBase; IEnumerable\u0026lt;Type\u0026gt; GetHandlersForEvent (string eventName); string GetEventKey\u0026lt;T\u0026gt; () where T : EventBase;; } IEventBusSubscriptionManager接口主要提供了维护事件(Event)和事件处理器(EventHnadler)两者关系的一系列方法，我个人认为理解起来相对容易一点，实际上看 eShopOnContainers 的时候，我每次都是从其中的一个微服务开始研究的，因为这样你才能发现其中的神秘之处，不得不说，平时看那种流水账代码看惯了，看到这样清晰、优雅的代码，内心还是觉得幸福啊，对技术的热爱再度被燃起。\n基于 RabbitMQ 的实现 Publish 好了，下面我们来看如何基于RabbitMQ实现上面定义的IEventBus接口，首当其冲的是Publish()方法的实现：\npublic void Publish\u0026lt;TEvent\u0026gt; (TEvent @event) where TEvent : EventBase { if (!_persistentConnection.IsConnected) _persistentConnection.TryConnect (); using (var channel = _persistentConnection.CreateModel ()) { channel.ExchangeDeclare (_exchangeName, \u0026#34;direct\u0026#34;, true, false, null); var eventName = @event.GetType ().FullName; var message = JsonConvert.SerializeObject (@event); var body = Encoding.UTF8.GetBytes (message); var properties = channel.CreateBasicProperties (); properties.DeliveryMode = 2; channel.BasicPublish (exchange: _exchangeName, routingKey: eventName, mandatory: true, basicProperties: properties, body: body); _logger.LogDebug ($\u0026#34;Publish message with RabbmitMQ BasicPublish: {message}\u0026#34;); } } 这里首先介绍下RabbitMQ中的两个概念，即Connection和Channel。其中，Connection是操作RabbitMQ的基础，就像我们操作数据库的时候，需要首先建立数据库连接一样。那么，Channel又是什么东西呢？它是真正去操作RabbitMQ的东西。继续以数据库作为例子，那么Channel可以理解为ADO.NET中的Command，即，那个真正负责执行 SQL 语句的家伙。一个典型的使用RabbitMQ的过程，大概是下面这个样子：\nvar connectionFactory = new ConnectionFactory() { HostName = \u0026#34;Your IP\u0026#34;, UserName = \u0026#34;You User\u0026#34;, Password = \u0026#34;Your Pass\u0026#34; }; var connection = connectionFactory.CreateConnection(); var channel = connection..CreateModel(); 回到我们的 EventBus 中，因为RabbitMQ的链接可能会在一段时间后自动关闭，所以，在微软的 eShopOnContainers 项目，它设计了一个支持自动重连的链接持久化类，我们这里同样有这个机制，当发现链接断开的时候，自动尝试重连，而接下来就由我们熟悉的Channel登场啦！这个时候，我们发现又出现了一个新面孔——交换器(Exchange)，好吧，这又要引出 RabbitMQ 中消息投递的原理，即 RabbitMQ 中消息并非由发布者直接发送给消费者，而是需要经过交换器这个中介者，虽然你可以直接去读写队列，但是实际应用中通常都不会这么做。其实，在某种意义上，我们的 EventBus 一样承担着中介者的角色，我们只需要关注怎么发布消息，这个消息将由哪一个订阅者来消费完全不需要我来关心，一个典型的消息投递过程如下图所示：\nRabbitMQ消息投递示意图\r在这里，我们对消息进行序列化以后，按照事件的类型信息生成routingKey，并指定交换器的类型为direct，这是一个 RabbitMQ 中自带的发布-订阅实现，因为交换器会根据routingKey投递消息到对应的队列中，关于 RabbitMQ 中四种交换器的说明，可以在下一节找到答案。注意到在声明交换器的时候，第二个参数被设为 true，这是在 RabbitMQ 需要对这个交换器进行持久化；而第三个参数被设为 false，这是在告诉 RabbitMQ 这个交换器内的消息不允许自动删除；DeliveryMode 设为 2 则表示消息需要持久化到磁盘上，这样即使 RabbitMQ 发生意外宕机，依然可以从磁盘上恢复消息。最终，我们调用BasicPublish()将消息投递到指定的交换机中，这样就完成了事件的发布功能。\nSubscribe/Unsubscribe 接下来，我们来看Subscribe()和Unsubscribe()两个方法的实现过程。这里实际上需要实现两部分的功能，一个是管理事件(EventBase)与事件处理器(EventHandler)间的关系，一个是管理消费者、消费者队列与交换器间的关系。因为考虑到后续可能需要实现类似MediatR的进程内通信的功能，所以，我们考虑将这两部分剥离开来，这样方便对EventBus进行扩展。为此，我们定义了IEventSubscriptionManager这个接口，它的定义我们在前面已经见过，最终我们会在EventBus里引用这个中间层，这样可以让EventBus显得更加清爽一点，一起来看它的具体实现：\npublic void Subscribe\u0026lt;T, TH\u0026gt; () where T : EventBase where TH : IEventHandler\u0026lt;T\u0026gt; { var eventName = GetEventKey\u0026lt;T\u0026gt; (); if (_eventHandlers.ContainsKey (eventName) \u0026amp;\u0026amp; !_eventHandlers[eventName].Any (x =\u0026gt; x == typeof (TH))) { _eventHandlers[eventName].Add (typeof (TH)); } else { _eventHandlers[eventName] = new List\u0026lt;Type\u0026gt; () { typeof (TH) }; _eventTypes.Add (typeof (T)); } if (OnSubscribe != null) OnSubscribe (this, new EventBusSubscriptionEventArgs () { EvenType = typeof (T), HandlerType = typeof (TH) }); } public void Unsubscribe\u0026lt;T, TH\u0026gt; () where T : EventBase where TH : IEventHandler\u0026lt;T\u0026gt; { var eventName = GetEventKey\u0026lt;T\u0026gt; (); if (_eventHandlers.ContainsKey (eventName) \u0026amp;\u0026amp; _eventHandlers[eventName].Any (x =\u0026gt; x == typeof (TH))) { _eventHandlers[eventName].Remove (typeof (TH)); } if (_eventHandlers.ContainsKey (eventName) \u0026amp;\u0026amp; !_eventHandlers[eventName].Any ()) { _eventHandlers.Remove (eventName); _eventTypes.RemoveAll (x =\u0026gt; x.FullName == eventName); } if (OnUnsubscribe != null \u0026amp;\u0026amp; !GetHandlersForEvent\u0026lt;T\u0026gt; ().Any ()) OnSubscribe (this, new EventBusSubscriptionEventArgs () { EvenType = typeof (T), HandlerType = typeof (TH) }); } 可以注意到，订阅就是注册 EventHandler 到对应的键的过程，而取消订阅就是从对应的键里移除 EventHandler 的过程。为了确保在订阅或者退订的时候，可以通知到具体的 EventBus 实现者，譬如 RabbitMQ、Kafka 等，我们定义了OnSubscribe和OnUnsubscribe两个委托，实际设计中，我们会在 EventBus 初始化的时候，将这两个委托指向 EventBus 内部订阅和退订的方法。对于订阅，我们需要用到 RabbitMQ 的BasicConsume()方法；而对于取消订阅，我们需要用到 RabbitMQ 的UnbindQueue()方法。下面给出关键部分的代码实现：\n//RabbitMQ中订阅指定的routingKey private void StartBasicConsume (string routingKey) { _logger.LogTrace (\u0026#34;Starting RabbitMQ BasicConsume...\u0026#34;); if (!_persistentConnection.IsConnected) _persistentConnection.TryConnect (); var queueName = GetQueueName (routingKey); var channel = _persistentConnection.CreateModel (); channel.ExchangeDeclare (_exchangeName, \u0026#34;direct\u0026#34;, true, false, null); channel.QueueDeclare (queueName, true, false, false, null); channel.QueueBind (queueName, _exchangeName, routingKey, null); var consumer = new EventingBasicConsumer (channel); consumer.Received += async (s, e) =\u0026gt; { var routingKey = e.RoutingKey; var message = Encoding.UTF8.GetString (e.Body.ToArray ()); var tasks = ProcessEvent (routingKey, message); await Task.WhenAll (tasks); channel.BasicAck (e.DeliveryTag, false); }; channel.BasicConsume (queue: $\u0026#34;Q:{routingKey}\u0026#34;, autoAck : false, consumer : consumer); } //调用EventHandler处理事件 private IEnumerable\u0026lt;Task\u0026gt; ProcessEvent (string eventName, string message) { if (_subscriptionManager.IsEventSubscribed (eventName)) { //基于Polly构建超时合重试策略 var policy = BuildProcessEventPolicy (); using (var serviceScope = _serviceProvider.CreateScope ()) { foreach (var handlerType in _subscriptionManager.GetHandlersForEvent (eventName)) { var handler = serviceScope.ServiceProvider.GetRequiredService (handlerType); if (handler == null) continue; var eventType = _subscriptionManager.GetEventTypeByName (eventName); var integrationEvent = JsonConvert.DeserializeObject (message, eventType); var concreteType = typeof(IEventHandler\u0026lt;\u0026gt;).MakeGenericType (eventType); _logger.LogInformation ($\u0026#34;Process event \\\u0026#34;{eventName}\\\u0026#34; with \\\u0026#34;{handler.GetType().Name}\\\u0026#34;...\u0026#34;); yield return (Task)policy.Execute(() =\u0026gt; concreteType.GetMethod (\u0026#34;Handle\u0026#34;).Invoke (handler, new object[] { integrationEvent })); } } } } //RabbitMQ中退订某个事件 private void UnbindQueue (string routingKey) { if (!_persistentConnection.IsConnected) _persistentConnection.TryConnect (); var channel = _persistentConnection.CreateModel (); var queueName = GetQueueName (routingKey); channel.QueueUnbind (queueName, _exchangeName, routingKey, null); } 其中，ProcessEvent()方法是 EventBus 通过一个或多个 EventHandler 处理业务的核心方法。当从 RabbitMQ 中接收到消息时，首先检查当前事件是否已注册。如果已注册，则获取当前事件对应的 EventHandler 集合，然后通过 IoC 容器逐个地取得对应实例，因为在定义 EventHandler 的时候，我们让Handle()方法返回了一个 Task，所以，我们可以顺利成章地使用Task.WhenAll()，而当所有的 EventHandler 都处理完成的时候，我们就可以认为这条消息被处理完了，此时，我们可以手动进行 ACK，这样这条消息就会从队列中移除，至此，我们已经实现了一个完整的 EventBus。\nRabbitMQ 进阶与释疑 我在写这篇博客的时候，周围有很多人都劝我不要用 RabbitMQ，而主要的理由则是 RabbitMQ 的吞吐量不如 Kafka。我怀疑我们有时候会严重地高估自己，“面试造火箭，入职拧螺丝”，这种事情难道还少吗？与其一张嘴就是高并发、高可用，不如诚实一点结合实际来选择，我相信 RabbitMQ 里遇到的问题，可能有一些同样会在 Kafka 里遇到，因为这个世界上就没有最完美的解决方案，对于我写这篇博客的初心而言，我是为了把 Binlog 发布到 RabbitMQ 上，方便第三方来订阅这些数据的“变化”，所以，可靠性是不是要比吞吐量更重要一点呢？好了，下面，我们来看一些“杞人忧天”式的 RabbitMQ 的进阶话题，就是当你熟悉了 RabbitMQ 的 API 以后，需要去着重考虑的东西。\nRabbitMQ 丢消息怎么办 第一个问题是最普遍的一个问题，按照“生产者 -\u0026gt; 交换器 -\u0026gt; 队列 -\u0026gt; 消费者”的模式，一旦发生丢消息的情况，无非有三种情况：生产者丢消息、消息队列丢消息、消费者丢消息。下面我们逐个进行分析：\n1、对于生产者丢消息，RabbitMQ提供的transaction和confirm机制可以保证生产者不丢消息，transaction机制类似数据库的事务，只有当消息发送成功，事物才会被提交，否则事务被被回滚。因为每次发消息都必须开启事物，所以transaction机制会导致 RabbitMQ 吞吐量降低，一般建议使用confirm机制，即消息被正确投递则发送 ACK 给生产者，否则发送 NACK 给生产者。\n2、对于消息队列丢消息，解决方案我们在前面有提到过，主要有两点，第一，声明队列的时候设置 durable 为 true，这表示这是一个支持持久化的队列。第二，发送消息的时候，设置 DeliveryMode 为 2，这表示消息支持持久化的磁盘，如果有一天 RabbitMQ 遭遇不幸，消息会被持久化到磁盘上，所以说，习惯性保存是个好习惯啊……\n3、对于消费者丢消息，解决方案是手动 ACK，因为只有队列收到 ACK 时，它才会从队列中删除这条消息，否则，这条消息会重新回到队列中，只要它能重新回到队列、重新处理，它怎么会丢呢？你说对吧？\nRabbitMQ 重试与超时 先说结论，关于重试与超时这个话题，我们有两种实现思路，一种是像博主这样，采用 Polly 定义超时+重试的组合策略，然后将这个策略附加到每一个 Handle()方法上，通过程序来实现重试与超时。而第二种思路，则是利用消息/队列的 TTL 实现超时，利用死信实现重试，消息 TTL 和队列 TTL 的不同在于，一个队列超时则队列内的消息会被全部清空，而一个消息超时则可以在清空前决定是否要清空。\n重试与超时最大的问题其实在于幂等性，因为在以往的实践中，当我们的消费者变成一个第三方的 API 接口的时候，我们很难知道，一个消息到底需要处理多久，我一直不明白，为什么宝洁这样的公司，它一个 API 接口居然能等将近 30 分钟，而更加令人难以忍受的，是大量只能调用一次的接口，这类接口既无法保证能 100%调用成功，同样无法保证，第二次调和第一次调效果完全一样，所以，关于重试与超时这部分，其实应该结合实际业务去设计，因为每个人的诉求可能都不一样。\nRabbitMQ 的四种模式 在实现 EventBus 的过程中，博主用到了direct类型的交换器，并说这是 RabbitMQ 内置的发布-订阅实现，实际上，这里应该有direct、fanout、topic和head四种类型的交换器，下面我们来逐个地进行说明。\n1、fanout相当于广播，所有绑定了该交换器的队列都会收到消息。如下图所示：\nRabbitMQ-fanout模式\r2、direct相当于发布订阅，只有绑定了该交换器且 routingKey 完全匹配的队列会收到消息。如下图所示：\nRabbitMQ-driect模式\r3、topic相当于direct + 模糊匹配，所有绑定了该交换器的队列，且 routingKey 符合给定的模式，就会收到消息。如下图所示：\nRabbitMQ-topic模式\r4、header相当于给每条消息定义了一个“头”，只有当头中的一个键值对(Any)或者全部键值对(All)匹配的时候，才会收到消息，这种实际应用中非常少，如下图所示：\nRabbitMQ-header模式\rRaabitMQ 的死信机制 RabbitMQ 中的死信(Dead Letter)机制，我认为是一个非常有意思的东西，因为从实用性的角度来讲，它可以帮助我们实现“延时队列\u0026quot;，虽然在更多的场景下，我们希望消息能被立即处理，因为这样看起来更像一个“实时”的行为。可在实际应用过程中，我们难免会遇到这样一种情况，一条消息经过手动 ACK 以后从队列中移除，结果消费者端问你能不能再消费一次这条消息，所以，Kafka 里就提供了两种策略，即最多一次和至少一次，最多一次保证的是消息不会被重复消费，而至少一次保证的是消息 100%被成功消费。所以，简单来说，在为 RabbitMQ 配置了死信的情况下，可以让部分消息有机会重新进入队列、重新被消费。那么，什么情况下会产生死信呢？主要有下面三种情况：\n消息被否定确认，使用channel.basicNack或channel.basicReject，并且此时requeue属性被设置为 false。 消息在队列的存活时间超过设置的 TTL 时间。 消息队列的消息数量已经超过最大队列长度。 接下来，为了配合死信机制，我们必须要声明死信队列，建议为每一个需要配置死信的事件单独定义一个死信队列，声明方法如下：\n//声明死信交换器 channel.ExchangeDeclare(\u0026#34;exchange.with.dlx\u0026#34;, \u0026#34;direct\u0026#34;, true, false); //声明死信队列 var args = new Dictionary\u0026lt;string, object\u0026gt;(); //该队列中所有消息都进入死信交换器 args.Add(\u0026#34;x-dead-letter-exchange\u0026#34;, \u0026#34;exchange.with.dlx\u0026#34;); //该队列中指定routingKey的消息进入死信交换器 args.Add(\u0026#34;x-dead-letter-routing-key\u0026#34;, \u0026#34;foo.bar\u0026#34;); channel.QueueDeclare(\u0026#34;queue.with.dlx\u0026#34;, true, false, false, args); 本文小结 本文参考微软的 eShopOnContainers 项目，实现一个基于 RabbitMQ 的事件总线，事件总线是发布-订阅模式的一种延伸，可以在分布式的环境中令消息的发布者、订阅者完美地解耦，是领域驱动设计(DDD)中重要的基础设施之一，对于实现业务上的“事件驱动”非常有帮助。而实现 EventBus 最关键的三个方法，即 Publish()、Subscribe()和 Unsubscribe()，这其中需要了解一部分 RabbitMQ 的知识，所以，在这篇博客中，你可以了解到 RabbitMQ 的四种交换器、死信机制、重试超时机制等等，在此基础上，我们将在下一篇博客中，通过 Python-Mysql-Replication 实现 Binlog 的发布，而一旦我们将 Binlog 发布到消息队列中，本文实现的 EventBus 就可以作为消息的中介者而登场啦，欢迎大家继续关注我的博客，我们下一篇见！\n","date":"2020-07-15T14:39:07Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/580694660/","slug":"580694660","tags":["RabbitMQ","EventBus","事件订阅"],"title":"利用 MySQL 的 Binlog 实现数据同步与订阅(中)：RabbitMQ 篇"},{"categories":["数据存储"],"content":"终于等到了周末，在经历了一周的忙碌后，终于可以利用空闲写篇博客。其实，博主有一点困惑，困惑于这个世界早已“堆积”起人类难以想象的“大”数据，而我们又好像执着于去“造”一个又一个“差不多”的“内容管理系统”，从前我们说互联网的精神是开放和分享，可不知从什么时候起，我们亲手打造了一个又一个的“信息孤岛”。而为了打通这些“关节”，就不得不去造一张巨大无比的蜘蛛网，你说这就是互联网的本质，对此我表示无法反驳。我更关心的是这其中最脆弱的部分，即：一条数据怎么从 A 系统流转到 B 系统。可能你会想到API或者ETL这样的关键词，而我今天想说的关键词则是Binlog。假如你经常需要让数据近乎实时地在两个系统间流转，那么你应该停下来听我——一个不甘心整天写CRUD换取996福报的程序员，讲讲如何通过Binlog实现数据同步和订阅的故事。\n什么是 Binlog 首先，来回答第一个问题，什么是 Binlog？Binlog 即 Binary Log，是 MySQL 中的一种二进制日志文件。它可以记录MySQL内部对数据库的所有修改，故，设计 Binlog 最主要的目的是满足数据库主从复制和增量恢复的需要。对于主从复制，想必大家都耳熟能详呢，因为但凡提及数据库性能优化，大家首先想到的所谓的“读写分离”，而无论是物理层面的一主多从，还是架构层面的CQRS，这背后最大的功臣当属主从复制，而实现主从复制的更底层原因，则要从 Binlog 说起。而对于数据库恢复，身为互联网从业者，对于像“rm -f”和“删库”、“跑路”这些梗，更是喜闻乐见，比如像今年的绿盟删库事件，在数据被删除以后，工程师花了好几天时间去抢救数据，这其中就用到了 Binlog。\n可能大家会好奇，为什么 Binlog 可以做到这些事情。其实，从 Binlog 的三种模式上，我们就可以窥其一二，它们分别是：Statement、Row、Mixed，其中Statement模式记录的是所有数据库操作对应的 SQL 语句，如 INSERT、UPDATE 、DELETE 等 DML 语句，CREATE 、DROP 、ALTER 等 DDL，所以，从理论上讲，只要按顺序执行这些 SQL 语句，就可以实现不同数据库间的数据复制。而Row模式更关心每一行的变更，这种在实际应用中会更普遍一点，因为有时候更关心数据的变化情况，例如一个订单被创建出来，司机通过 App 接收了某个运输任务等。而Mixed模式可以认为是Statement模式和Row模式的混合体，因为Statement模式和Row模式都有各自的不足，前者可能会导致数据不一致，而后者则会占用大量的存储空间。在实际使用中，我们往往会借助各种各样的工具，譬如官方自带的mysqlbinlog、支持 Binlog 解析的StreamSets等等。\n好了，下面我们简单介绍下 Binlog 相关的知识点。在使用 Binlog 前，首先需要确认是否开启了 Binlog，此时，我们可以使用下面的命令：\nSHOW VARIABLES LIKE \u0026#39;LOG_BIN\u0026#39; 如果可以看到下面的结果，则表示 Binlog 功能已开启。 Binlog已开启示意图\r如果 Binlog 没有开启怎么办呢？此时，就需要我们手动来开启，为此我们需要修改 MySQL 的my.conf文件，通常情况下，该文件位于/etc/my.cnf路径，在[mysqld]下写入如下内容：\n# 设置Binlog存储目录 log_bin = /var/lib/mysql/bin-log # 设置Binlog索引存储目录 log_bin_index = /var/lib/mysql/mysql-bin.index # 删除7天前的Binlog expire_logs_days = 7 # 集群内MySQL服务器的ID server_id = 0002 # 设置Binlog日志模式 binlog_format = ROW 除此之外，我们还可以设置下面这些选项：\n# 设置Binlog文件最大的大小 max_binlog_size # 设置当前多少个事务缓存在内存中 binlog_cache_size # 设置当前多少个事务暂存在磁盘上 binlog_cache_disk_use # 设置最大有多少个事务缓存在内存中 max_binlog_cache_size # 设置选取或者忽略的数据库 binlog_do_db/binlog_ingore_db 设置完以后，通过下面的命令重启 MySQL 即可：\nservice mysql restart 或者\nservice mysqld restart 通常，我们可以通过下面的命令来获取 Binlog 的当前状态，请注意，该命令必须要在主库上执行：\nSHOW MASTER STATUS 此时，我们会得到下面的结果： 查看Binlog状态\r这里可以得到三个重要的信息，即从日志文件mysql-bin.000388的特定位置135586062开始，可以获得一组新的日志信息，而这些日志信息都是来自数据库实例b1328d03-0b5c-11ea-8ee8-005056a1616f:1-27768340。有了这三个信息以后，我们就可以去查看对应的 BinLog，此时，我们需要使用到下面的命令：\nSHOW BINLOG EVENTS IN \u0026#39;MYSQL-BIN.000388\u0026#39; FROM 135586062 此时，ROW 模式下的 Binlog 如下图所示： ROW模式下的Binlog\r可以注意到，这些 Binlog 由不同的事件构成。如果你是在 MySQL 终端下输入命令，那么，你还可以使用官方自带的工具mysqlbinlog，博主这里使用的开源的数据库工具DBeaver，如果你经常需要和不同的数据库打交道，而又不想每一种数据库都去安装一个客户端的话，我认为这是一个非常不错的选择。关于 Binlog 的使用我们就先暂时说到这里，因为还有更重要的事情要做。\nBinlog 有什么用 实现数据库审计 你可能觉得我明知故问，你刚刚不是说 Binlog 主要用来做主从复制和增量恢复吗？自然，这是 Binlog 在设计之初的主要用途。可我们都知道，事物有时候并不会想着我们期待的方向发展，譬如原子弹成为战争机器、社交软件成为“约炮神器”、共享单车成为“城市垃圾”等等。还记得博主曾经写过一篇关于数据库审计的[博客](https://blog.yuanpei.me/posts/1289244227/吗？当时，我们是重写了 EF/EF Core 中 DbContext 的 SaveChanges()方法，并借助 ChangeTracker 对获取实体修改前后的值。其实，从现在的角度来看，我们有更好的选择，毫无疑问，Row 模式下的 Binlog 本身就是天然的数据库审计，每一行数据变化前后的情况，我们都可以获得，并且可以区分出它是 Insert ，还是 Update，还是 Delete，所以，Binlog 的第一个用途就是可以用来做数据库审计，因为它发生在数据库层，从某种意义上来讲，消解了 EF 和 Dapper 这种 ORM 间的差异。\n实现事件驱动 其次，我们在实际业务中，常常需要用到\u0026quot;领域事件\u0026ldquo;这个概念，即使项目并没有采用**领域驱动设计(DDD)**的思想，即使项目中并没有采用”事件驱动“的业务模式，可事实就是，总有人关心着数据的产生和变更，而能提供给第三方系统订阅自己感兴趣的事件的能力，无疑要比开发一个又一个大同小异的同步接口要好得多，推(Push)模式在大多数情况下要比拉(Pull)模式要好，为什么呢？因为数据传输的压力更小，更能满足数据实时性的要求。然而，由于没有按照领域模型去设计业务，导致事件代码与业务代码耦合非常严重，基于 Binlog 的事件分发机制显然有更好的普适性。以博主最近处理的业务为例，A 系统中的司机、设备、用户在新建/更新更新时，需要把新数据推送到 B 系统，因为这类纯数据类的\u0026quot;变化\u0026quot;没有实际业务意义，所以，人们不舍得为这些变化去分发事件，而要想分发事件，又不得不去面对强耦合带来的阵痛，所以，Binlog 的第二个用途是可以作为事件源来实现事件驱动。\n业内主流方案 如果你觉得通过第一节的内容，可以非常容易地实现 Binlog 的解析，那么，我觉得你并没有想清楚 Binlog 处理过程中的难点在哪里？首先，每次读取 Binlog，必须要知道对应的日志文件和位置，而如果在新的 Binlog 产生前，没有处理完原来的 Binlog，就必须要记录对应的日志文件和位置，而且经过博主本人测试，Binlog 无法直接给查询语句追加过滤条件，来达到筛选某些数据库、表以及事件的目的，而且日志文件的格式会因为模式的不同而不同，最主要的一点是，直接在主库上读取 Binlog 会给数据库带来访问压力，所以，主流的方案，是让客户端伪装成“从库”，关于一点，我们可以配合下面的图片来理解。 MySQL主从复制原理\r可以注意到，完成主从复制需要一个 Relaylog + 两个线程，即，主库产生的 Binlog，首先由从库的 I/O 线程进行读取，这一步会产生 Relaylog，顾名思义，这是一个处在中间状态的中继日志，而中继日志最终会交由从库的 SQL 线程来处理，所以，这是从库执行 SQL 语句的阶段，整个过程是异步化的操作，所以，不会对主库产生太大的压力。如果我们直接读取主库的 Binlog，实际上是把所有压力都转移到主库，不仅需要负责“读”，还需要复杂“写”。主流的方案，目前比较推荐的是阿里的Canal、Zendesk 的Maxwell、以及来自社区的Python-Mysql-Replication，下面是一个简单的对比，方便大家做技术选型。\nCancal Maxwell Python-Mysql-Rplication 开源方 阿里巴巴 Zendesk 社区 开发语言 Java Java Python 活跃度 活跃 活跃 活跃 高可用 支持 支持 不支持 客户端 Java/Go/PHP/Python/Rust 无 Python 消息落地 Kafka/RocketMQ 等 Kafka/RabbitNQ/Redis 等 自定义 消息格式 自定义 JSON 自定义 文档详略 详细 详细 详细 Boostrap 不支持 支持 不支持 说说我的构想 众所知周，我是一个有一点“懒惰”的人，考虑到前面两种方案都比较重，即使通过 Docker 来安装。对我来说，这是一个验证想法的过程，所以，我选择的搭配是 RabbitMQ + .NET Core + Python 的方案，因为 Kafka 需要 ZooKeeper，而在验证想法的阶段，自然是越简单越好。我正打算参考微软的 eShopOnContainers 的项目， 实现一个消息总线(EventBus)，恰好这个项目中使用了 RabbitMQ，而且从某种意义上来说，RabbitMQ 更接近传统意义上的消息队列，它提供的重试、确认、死信等这些机制都比较完善，可以让我把精力集中在快速实现上，毕竟你看到这些博客，都是我挤出时间来完成的。选择 Python 就更直接了，因为安装、运行都非常容易，或许 Kafka 的吞吐性能更好，但我觉得掌握核心思想才是最重要的吧！\n总而言之，在这里，我选择了自己最熟悉的技术栈。整体思路是，首先，.NET Core + RabbitMQ 实现一个消息总线，并对外提供发布事件的 API 接口。其次，利用 Python-Mysql-Replication 实现一个读取 Binlog 的后台程序，这些 Binlog 最终会以 JSON 的形式发布到 RabbitMQ 上。最后，实现针对特定事件的 IEventHandler接口，消息总线会自动调用这些 Handler 去处理消息。至此，就实现了针对 Binlog 的订阅和消费。众所周知，消息总线的一大优点就是解耦，我们就可以摆脱以往定时轮询 + 打标记(Flag)的宿命轮回，只需要编写对应的 Handler 即可，其实我觉得这是一种思维上的转变，就是\u0026quot;主动\u0026quot;到\u0026quot;被动\u0026quot;的转变，并不是说我们帮客户做得越多越好，而是我们能让客户意识到它可以做哪些事情。同样的，我绘制了一个简单的流程图来作为说明： 基于RabbitMQ的EventBus实现\r本文小结 其实，重复的工作做久了都会感到厌烦的，所以，真正让你摆脱“体力劳动”的只能是换一种高度来看问题。这几年做 2B 业务下来，最大的体会是企业级软件最难的是，如何在各种种类繁多的软件，譬如 OA 、金蝶、用友、SAP 、ERP 、CRM 等中做好一个“配角”，数据如果无法在这张网络中流通，则永远都是一潭死水，而如果要打通各个系统间的数据，则免不了写一个又一个的同步接口。这篇博客以 MySQL 的 Binlog 为切入点，试图通过 Binlog 来实现特定业务的“事件驱动”。Binlog 是实现主从复制的重要机制，而基于这一机制，业界普遍的做法是利用 MySQL 的交换协议，让客户端\u0026quot;伪装\u0026quot;成一个从库，在比较了 Canal 、Maxwell 以及 Python-Mysql-Replication 后，博主选择了. NET Core + RabbitMQ + Python 的方案，目标是让 Binlog 可以发布到消息总线(EventBus)中供消费者订阅和消费。在下一篇博客中，我们讲介绍基于 RabbitMQ 实现一个消息总线(EventBus)的相关细节，欢迎大家继续关注我的博客，今天这篇博客就先写到这里，大家晚安！\n","date":"2020-07-07T09:23:59Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1333693167/","slug":"1333693167","tags":["MySQL","Binlog","事件订阅"],"title":"利用 MySQL 的 Binlog 实现数据同步与订阅(上)：基础篇"},{"categories":["开发工具"],"content":"突然发觉，古人其实特别有趣，譬如有古语云：『常在河边走，哪有不湿鞋』，实在是富有生活气息的一句俗语，可古人又有言语：『光脚的不怕穿鞋的』，更是朴实无华的一句话。上周下班适逢天降大雨，我撑伞送一位同事到地铁站，结果走到半路人家来一句，“你快点走吧，我穿着凉鞋”，一时竟无语凝噎。常在河边走，固然会有湿鞋的顾虑，可真正的气度绝不是光着脚满地跑，如何做到湿了鞋子而不慌呢？答案是脚上无凉鞋而心中有凉鞋。今天，我将为大家我在使用Git过程中如何“湿鞋”、如何不怕“湿鞋”的一个故事(逃\n蓝屏重启后 Git 居然坏了 中国传统小说喜欢从神话讲起，端的是汪洋恣肆、纵横捭阖。而国外小说则喜欢从一片常青藤叶这种不显眼的事物写起，足可见二者见天地众生视角之不同。而我这个故事，是再普通不过的一次蓝屏。重启后 Visual Studio 提示恢复了未保存的代码，此时，我并未注意到 Git 仓库损坏的情况，就这样，我在一个“游离态”的版本上编写代码，直到我打开 SourceTree 的时候(作者注：我就是那个命令行和 GUI 混合使用的奇葩)，发现左侧本地分支全部消失，在命令行里git status，发现根本没有这个分支，而.git/refs/对应分支指向了一个错误的 Hash，我意识到我的 Git 仓库文件可能损坏了，这意味着我写的新 feature 可能丢失了，此时，Git 中提示的类似的错误信息：\n$ error: refs/remotes/origin/HEAD: invalid sha1 pointer 0000000000000000000000000000000000000000 在此之前，其实博主已经经历过类似的事情，在没有未提交的代码的情况下，其实可以暴力删除. git目录，然后在git init即可，这相当于重新初始化仓库啦，在这种情况下，本地的分支会被删掉，你需要重新建新分支。可是这次不一样啊，在做的是一个即将发版的新 feature，不允许我出这样的选择啊！博主双掌合一，像夏洛克一样冷静思考，缓缓地在命令行下敲出git reflog，这条命令相当于你在 Git 中的监控日志，你对 Git 所做的一切都会成为呈堂证供。此时，你会得到下面的信息——沉默是今晚的康桥……\n$ fatal: You are on a branch yet to be born 这是什么意思呢？意思就是这个分支还是一个“新生儿“的状态，新生儿怎么可能又活动记录呢？所以，使用 Git 的准则之一，只要仓库没有坏，通过git reflog找到对应的 Hash ，git checkout就可以找回代码，哪怕你刚刚手滑删除了一个未提交的分支，这种情况下都可以找回来。But 现在这种状况下，这条路显然是走不通啦。继续双掌合一，像夏洛克一样冷静思考，每个分支里其实是记录着一个 hash ，对应着最后的一次提交，现在是这个 hash 不对，那就要找到正确的 hash 啊。命令行已经非常明确地告诉你，是因为某些 object 丢失或者损坏了，那不妨先用git fsck试试。\n$ git fsck notice: HEAD points to an unborn branch (master) Checking object directories: 100% (256/256), done. Checking objects: 100% (589/589), done. error: refs/remotes/origin/HEAD: invalid sha1 pointer 0000000000000000000000000000000000000000 notice: No default references dangling tag 92d0fe18f9a55177d955edf58048b49db7987d5b dangling commit aa7856977e80d11833e97b4151f400a516316179 dangling commit 16e449da82ec8bb51aed56c0c4c05473442db90a dangling commit 864c345397fcb3bdb902402e17148e19b3f263a8 dangling tag be9471e1263a78fd765d4c72925c0425c90d3d64 此时，我们就会得到这样的信息。我天，这简直太良心了好吧，连哪一个 object 丢了都明明白白地告诉你。既然是提示解包(unpack)的时候失败，不妨先手动解包看看呗，好吧，果然程序是不会欺骗人的。这个时候，我注意到这些里面有一些提交(commit)，我在想这些有没有可能是残留的有效分支，于是使用下面的命令创建临时分支，一番折腾发现这些分支都离我的分支比较远，所以，基本可以排除了。\n//尝试手动解包 $ mv .git/objects/pack/pack-0672bd01813664b80248dbe8330bf52da9c02b9f.pack . $ git unpack-objects -r \u0026lt; pack-0672bd01813664b80248dbe8330bf52da9c02b9f.pack //从某个commit新建临时分支 $ git update-ref refs/heads/recovery-1 aa7856977e80d11833e97b4151f400a516316179 我又不甘心地看了看git fsck命令，发现它居然有一个--lost-found的参数可以用，这样子，我居然就得到一个名为lost-found的文件夹，它里面有一些以 hash 命名的文件，我挑选了一个离我蓝屏时间最近的文件，直接git checkout过去，发现这正是我需要的内容，赶紧git checkout –b存档，这实在是太珍贵了！\n$ git fsck --lost-found error: inflate: data stream error (unknown compression method) error: unable to unpack header of .git/objects/67/781ba4991aee01c0bc0d640ae9ee8b674b2f47 error: 67781ba4991aee01c0bc0d640ae9ee8b674b2f47: object corrupt or missing: .git/objects/67/781ba4991aee01c0bc0d640ae9ee8b674b2f47 error: inflate: data stream error (unknown compression method) error: unable to unpack header of .git/objects/6f/34f2bbde304619622f77f9ca159ed97b6ddafd error: 6f34f2bbde304619622f77f9ca159ed97b6ddafd: object corrupt or missing: .git/objects/6f/34f2bbde304619622f77f9ca159ed97b6ddafd error: inflate: data stream error (unknown compression method) error: unable to unpack header of .git/objects/89/6e969a25c2238ebbb41e895753e82da1cdc7af error: 896e969a25c2238ebbb41e895753e82da1cdc7af: object corrupt or missing: .git/objects/89/6e969a25c2238ebbb41e895753e82da1cdc7af error: inflate: data stream error (unknown compression method) error: unable to unpack header of .git/objects/d8/a180969f6cf8047def4b50c7c920dcd2b6f5cd error: d8a180969f6cf8047def4b50c7c920dcd2b6f5cd: object corrupt or missing: .git/objects/d8/a180969f6cf8047def4b50c7c920dcd2b6f5cd 其实，接触 Git 的这些年里，使用命令行并没有让我觉得 Git 难以接近，相反它让我对 GUI 理解更深一点，就像好多人分不清pull和fetch，因为你不看命令行的输出啊；有好多人每次 SourceTree 一报错就不知道该怎么办 ，其实 Git 给的提示真的相当清晰了；我之前一直不知道什么叫cherry-pick，后来发现这玩意儿就是我们所说的“补丁”。平时这种问题可能就放过去了，可这次“扶大厦于将顷”，让代码失而复得的经历，的确令人难忘，所以，我更想把它写下来，当你都能真正驾驭它了，是用命令行还是用 GUI 就真的不在重要啦！这次的一个例外是索引没有坏，如果索引坏了，可以试试下面的命令：git reset --mixed。我还是坚持一个观点，Git 仓库坏了，能修复尽量去修复，不到万不得已，千万不要去删. git目录。\n各种场景下的 Git 恢复/撤销 在这篇文章刚开始的时候，我问大家，如何做到湿了鞋子而不慌呢？答案是脚上无凉鞋而心中有凉鞋。虽然 Git 本身是一款非常复杂的软件，可我们依然有很多的策略去应对各种“失误”，正如这篇文章 Undoing all kinds of mistakes 所言，Git 深知人类都是不完美的，面对平时使用 Git 过程中的各种失误，我们可以尝试使用下面的思路来解决。\n更改未提交到暂存区 //放弃所有文件的更改 $ git reset --hard //放弃指定文件的更新 $ git checkout -- \u0026lt;path/to/file\u0026gt; 更改已提交到暂存区 //回到最近的一次提交(改变指针) $ git reset --hard HEAD^ //回到某一次提交(改变指针) $ git reset --hard \u0026lt;commitId\u0026gt; //全部放弃=回到最近的一次提交(改变指针) $ git reset --hard 全部放弃 //放弃提交指定文件 $ git reset HEAD \u0026lt;path/to/file\u0026gt; //修改提交信息 $ git commit --amend 更改已推送到远程服务器 //撤销前一次提交(产生新的提交) $ git revert HEAD //撤销前前一次提交(产生新的提交) $ git revert HEAD^ //撤销某一个提交(产生新的提交) $ git revert commit 万能公式 //万能公式 $ git reflog $ git checkout \u0026lt;commitId\u0026gt; //退而求其次 $ git fsck --lost-found 除了 SourceTree，我想安利第二个 Git GUI 工具：Fork，大家感兴趣的话可以安装试用。\n参考链接 Repairing and recovering broken git repositories Git 撤销\u0026amp;回滚操作 Git 撤销合并 How to get the parents of a merge commit in git? ","date":"2020-06-23T17:08:17Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/686567367/","slug":"686567367","tags":["Git","工具","软件"],"title":"记一次从已损坏的 Git 仓库中找回代码的经历"},{"categories":["编程语言"],"content":"在上一篇博客里，我们为.NET Core原生 DI 扩展了基于名称的注入功能。而今天，我们要来聊一聊属性注入。关于属性注入，历来争议不断，支持派认为，构造函数注入会让构造函数变得冗余，其立意点主要在代码的可读性。而反对派则认为，属性注入会让组件间的依赖关系变得模糊，其立意点主要在代码是否利于测试。我认识的一位前辈更是留下一句话：只要构造函数中超过 5 个以上的参数，我就觉得无法忍受。我个人是支持派，因为我写这篇博客的动机，正是一位朋友向我吐槽公司项目，说一个控制器里单单是构造函数里的参数就有十来个。在这其中最大的痛点是，有些在构造函数中注入的类型其实是重复的，譬如ILogger\u0026lt;\u0026gt;、IMapper、IRepository\u0026lt;\u0026gt;以及用户上下文信息等，虽然继承可以让痛苦减轻一点，可随之而来的就是冗长的 base 调用链。博主参与的项目里不乏有大量使用静态类、静态方法的，譬如 LogEx、UserContext 等等，可这种实践显然与依赖注入的思想背道而驰，为吾所不取也，这就是这篇博客产生的背景啦！\n好了，当视角正式切入属性注入的时候，我们不妨先来考虑这样一件事情，即：当我们从容器里 Resolve 一个特定的类型的时候，这个实例到底是怎么被创建出来的呢？这个问题如果给到三年前的我，我会不假思索的说出两个字——反射。的确，这是最简单的一种实现方式，换句话说，首先，容器收集构造函数中的类型信息，并根据这些类型信息 Resolve 对应的实例；其次，这些实例最终会被放到一个object[]里，并作为参数传递给Activator.CreateInstance()方法。这是一个一般意义上的 Ioc 容器的工作机制。那么，相对应地，关于属性注入，我们可以认为容器 Reslove 一个特定类型的时候，这个类型提供了一个空的构造函数(这一点非常重要)，再创建完实例以后，再去 Reslove 这个类型中的字段或者是属性。所以，为了在微软自带的 DI 上实现属性注入，我们就必须实现自己的 ServiceProvider——AutowiredServiceProvider，这个 ServiceProvider 相比默认的 ServiceProvider 多了一部分功能，即反射属性或者字段的过程。一旦想通这一点，我们可以考虑装饰器模式。\npublic class AutowiredServiceProvider : IServiceProvider, ISupportRequiredService { private readonly IServiceProvider _serviceProvider; public AutowiredServiceProvider (IServiceProvider serviceProvider) { _serviceProvider = serviceProvider; } public object GetRequiredService (Type serviceType) { return GetService (serviceType); } public object GetService (Type serviceType) { var instance = _serviceProvider.GetService (serviceType); Autowried (instance); return instance; } private void Autowried (object instance) { if (_serviceProvider == null || instance == null) return; var flags = BindingFlags.Public | BindingFlags.NonPublic; var type = instance as Type ?? instance.GetType (); if (instance is Type) { instance = null; flags |= BindingFlags.Static; } else { flags |= BindingFlags.Instance; } //Feild foreach (var field in type.GetFields (flags)) { var autowriedAttr = field.GetCustomAttribute\u0026lt;AutowiredAttribute\u0026gt; (); if (autowriedAttr != null) { var dependency = GetService (field.FieldType); if (dependency != null) field.SetValue (instance, dependency); } } //Property foreach (var property in type.GetProperties (flags)) { var autowriedAttr = property.GetCustomAttribute\u0026lt;AutowiredAttribute\u0026gt; (); if (autowriedAttr != null) { var dependency = GetService (property.PropertyType); if (dependency != null) property.SetValue (instance, dependency); } } } } 装饰器模式，又被称之为“静态代理\u0026quot;，是面向切面编程(AOP)的实现方式之一，我们在这里为默认的 ServiceProvider 增加了Autowired()方法，它会扫描所有含[Autowired]标签的字段或属性，并尝试从容器中获取对应类型的实例。所以，这又说到了反对属性注入第二个理由，即：使用反射带来的性能问题，尤其是当依赖项间的引用关系异常复杂的时候。当然，所谓“兵来将挡，水来土掩”，反射产生性能损失，可以考虑用 Emit 或者表达书树作来替代反射，不过，微软貌似在.NET Core 中阉割了一部分 Emit 的 API，这些都是 Todo 啦你懂就好，我们继续往下说。接下来，为了替换掉微软默认的 ServiceProvider，我们还必须实现自己的 ServiceProviderFactory，像 Autofac、Unity、Castle 等容器，都是采用类似的做法来支持.NET Core。\npublic class AutowiredServiceProviderFactory : IServiceProviderFactory\u0026lt;IServiceCollection\u0026gt; { public IServiceProvider CreateServiceProvider (IServiceCollection containerBuilder) { var serviceProvider = containerBuilder.BuildServiceProvider (); return new AutowiredServiceProvider (serviceProvider); } IServiceCollection IServiceProviderFactory\u0026lt;IServiceCollection\u0026gt;.CreateBuilder (IServiceCollection services) { if (services == null) return new ServiceCollection (); return services; } } 因为我们是以微软内置的 DI 为基础来进行扩展的，所以，在实现AutowiredServiceProviderFactory的时候，提供的泛型参数依然是IServiceCollection。它需要实现两个方法：CreateBuilder和CreateServiceProvider，在这里我们需要返回我们“装饰”过的 ServiceProvider。接下来，万事俱备，只欠东风，我们需要在项目入口(Program.cs)调用UseServiceProviderFactory()方法，如果你在.NET Core 使用 Autofac，应该会对此感到亲切：\npublic static IHostBuilder CreateHostBuilder(string[] args) =\u0026gt; Host.CreateDefaultBuilder(args) .ConfigureWebHostDefaults(webBuilder =\u0026gt; { webBuilder.UseStartup\u0026lt;Startup\u0026gt;(); }) .UseServiceProviderFactory(new AutowiredServiceProviderFactory()); 至此，我们就完成了对微软默认的 ServiceProvider 的替换。假设我们有两个接口：IFooService和IBarService：\n//IFooService \u0026amp;\u0026amp; FooService public interface IFooService { string Foo (); IBarService Bar { get; set; } } public class FooService : IFooService { [Autowired] public IBarService Bar { get; set; } public string Foo () =\u0026gt; \u0026#34;I am Foo\u0026#34;; } //IBarService \u0026amp;\u0026amp; BarService public interface IBarService { string Bar(); } public class BarService : IBarService { public string Bar () =\u0026gt; \u0026#34;I am Bar\u0026#34;; } 注意到FooService依赖IBarService，而我们只需要给Bar加上[Autowired]标签即可，风格上借鉴了Spring的@Autowired。只要这两个接口被注入到 Ioc 容器中，这个属性就可以自动获得相应的服务实例。一起来看下面的代码：\nservices.AddTransient\u0026lt;IFooService,FooService\u0026gt;(); services.AddTransient\u0026lt;IBarService, BarService\u0026gt;(); var serviceProvider = new AutowiredServiceProvider(services.BuildServiceProvider()); var fooService = serviceProvier.GetRequiredService\u0026lt;IFooService\u0026gt;(); Console.WriteLine($\u0026#34;{fooService.Foo()} , {fooService.Bar.Bar()}\u0026#34;); 回到我们一开始遇到的那个问题，如果我们让IFooService变成 Controller 中的一个属性，是否就能解决构造函数参数冗余的问题了呢？下面是一段简单的代码：\n[ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class WeatherForecastController : ControllerBase { [Autowired] public IFooService Foo { get; set; } [Autowired] public ILogger\u0026lt;WeatherForecastController\u0026gt; Logger { get; set; } [HttpGet] [Route(\u0026#34;Autowired\u0026#34;)] public ActionResult GetAutowriedService() { return Content($\u0026#34;{Foo.Foo()} , {Foo.Bar.Bar()}\u0026#34;); } } 此时，我们会发现Foo属性提示空引用错误，这是为什么呢？这是因为 Controller 并不是通过 IoC 容器来负责创建和销毁的，为了实现属性注入的目的，我们就必须让 IoC 容器来全面接管 Controller 的创建和销毁，此时，我们需要做两件事情，其一，注册 Controller 到 IoC 容器中；其二，实现自定义的IControllerActivator，并替换默认的 ControllerActivator:\nservices.AddControllers(); services.AddControllersWithViews().AddControllersAsServices(); services.Replace(ServiceDescriptor.Transient\u0026lt;IControllerActivator, AutowiredControllerActivator\u0026gt;()); 其中，AutowiredControllerActivator实现如下：\npublic class AutowiredControllerActivator : IControllerActivator { public object Create(ControllerContext context) { if (context == null) throw new ArgumentNullException(nameof(ControllerContext)); var controllerType = context.ActionDescriptor.ControllerTypeInfo.AsType(); var serviceProvider = context.HttpContext.RequestServices; if(!(serviceProvider is AutowiredServiceProvider)) serviceProvider = new AutowiredServiceProvider(context.HttpContext.RequestServices); var controller = serviceProvider.GetRequiredService(controllerType); return controller; } public void Release(ControllerContext context, object controller) { if (context == null) hrow new ArgumentNullException(nameof(ControllerContext)); if (controller == null) throw new ArgumentNullException(nameof(controller)); var disposeable = controller as IDisposable; if (disposeable != null) disposeable.Dispose(); } } } 此时，一切都会像我们期待的那样美好，返回正确的结果。目前，这个方案最大的问题是，在非 Controller 层使用的时候，还是需要构造AutowirdServiceProvider实例。其实，在AutowiredControllerActivator里同样有这个问题，就是你即使实现IServiceProviderFactory接口，依然没有办法替换掉默认的 ServiceProvider 实现，只能说它能解决一部分问题，同时又引入了新的问题，最直观的例子是，你看到一个接口的时候，你并不能找全所有加了[Autowired]标签的依赖项，所以，直接造成了依赖关系模糊、不透明、难以测试等等的一系列问题，我认为，在一个可控的、小范围内使用还是可以的。\n","date":"2020-06-20T13:10:31Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1658310834/","slug":"1658310834","tags":["DI","依赖注入",".NET Core","技巧"],"title":".NET Core 原生 DI 扩展之属性注入实现"},{"categories":["编程语言"],"content":"接触 .NET Core 有一段时间了，最大的感受无外乎无所不在的依赖注入，以及抽象化程度更高的全新框架设计。想起三年前 Peter 大神手写 IoC 容器时的惊艳，此时此刻，也许会有不一样的体会。的确，那个基于字典实现的 IoC 容器相当“简陋”，就像 .NET Core 里的依赖注入，默认(原生)都是采用构造函数注入的方式，可其实从整个依赖注入的理论上而言，属性注入和方法注入的方式，同样是依赖注入的实现方式啊。最近一位朋友找我讨论，.NET Core 里该如何实现 Autowried，这位朋友本身是 Java 出身，一番攀谈了解到原来是指属性注入啊。所以，我打算用两篇博客来聊聊 .NET Core 中的原生 DI 的扩展，而今天这篇，则单讲基于名称的注入的实现。\nAutofac是一个非常不错的 IoC 容器，通常我们会使用它来替换微软内置的 IoC 容器。为什么要这样做呢？其实，微软在其官方文档中早已给出了说明，即微软内置的 IoC 容器实际上是不支持以下特性的： 属性注入、基于名称的注入、子容器、自定义生存期管理、对迟缓初始化的 Func 支持、基于约定的注册。这是我们为什么要替换微软内置的 IoC 容器的原因，除了 Autofac 以外，我们还可以考虑 Unity 、Castle 等容器，对我个人而言，其实最需要的一个功能是“扫描”，即它可以针对程序集中的组件或者服务进行自动注册。这个功能可以让人写起代码更省心一点，果然，人类的本质就是让自己变得更加懒惰呢。好了，话题拉回到本文主题，我们为什么需要基于名称的注入呢？它其实针对的是“同一个接口对应多种不同的实现”这种场景。\nOK ，假设我们现在有一个接口 ISayHello，它对外提供一个方法 SayHello：\npublic interface ISayHello { string SayHello(string receiver); } 相对应地，我们有两个实现类，ChineseSayHello 和 EnglishSayHello：\n//ChineseSayHello public class ChineseSayHello : ISayHello { public string SayHello(string receiver) { return $\u0026#34;你好，{receiver}\u0026#34;; } } //EnglishSayHello public class EnglishSayHello : ISayHello { public string SayHello(string receiver) { return $\u0026#34;Hello，{receiver}\u0026#34;; } } 接下来，一顿操作猛如虎：\nvar services = new ServiceCollection(); services.AddTransient\u0026lt;ISayHello, ChineseSayHello\u0026gt;(); services.AddTransient\u0026lt;ISayHello, EnglishSayHello\u0026gt;(); var serviceProvider = services.BuildServiceProvider(); var sayHello = serviceProvider.GetRequiredService\u0026lt;ISayHello\u0026gt;(); 没想到，尴尬的事情就发生了，大家来猜猜看，这个时候我们获取到的ISayHello到底是哪一个呢？事实上，它会获取到EnglishSayHello这个实现类，为什么呢？因为它后注册的呀！当然，微软的工程师们不可能想不到这个问题，所以，官方推荐的做法是使用IEnumerable\u0026lt;ISayHello\u0026gt;，这样我们就能拿到所有注册的ISayHello，然后自己决定到底要使用一种实现，类似下面这样：\nvar sayHellos = _serviceProvider.GetRequiredService\u0026lt;IEnumerable\u0026lt;ISayHello\u0026gt;\u0026gt;(); var chineseSayHello = sayHellos.FirstOrDefault(x =\u0026gt; x.GetType() == (typeof(ChineseSayHello))); var englishSayHello = sayHellos.FirstOrDefault(x =\u0026gt; x.GetType() == (typeof(EnglishSayHello))); 可这样还是有一点不方便啊，继续改造：\nservices.AddTransient\u0026lt;ChineseSayHello\u0026gt;(); services.AddTransient\u0026lt;EnglishSayHello\u0026gt;(); services.AddTransient(implementationFactory =\u0026gt; { Func\u0026lt;string, ISayHello\u0026gt; sayHelloFactory = lang =\u0026gt; { switch (lang) { case \u0026#34;Chinese\u0026#34;: return implementationFactory.GetService\u0026lt;ChineseSayHello\u0026gt;(); case \u0026#34;English\u0026#34;: return implementationFactory.GetService\u0026lt;EnglishSayHello\u0026gt;(); default: throw new NotImplementedException(); } }; return sayHelloFactory; }); 这样子，这个工厂类看起来就消失了对吧，其实并没有(逃\nvar sayHelloFactory = _serviceProvider.GetRequiredService\u0026lt;Func\u0026lt;string, ISayHello\u0026gt;\u0026gt;(); var chineseSayHello = sayHelloFactory(\u0026#34;Chinese\u0026#34;); var englishSayHello = sayHelloFactory(\u0026#34;English\u0026#34;); 这距离我们的目标有一点接近了哈，唯一的遗憾是这个工厂类对调用方是透明的，可谓是隐藏细节上的失败。有没有更好的方案呢？好了，我不卖关子啦，一起来看下面的实现。\n首先，我们定义一个接口INamedServiceProvider, 顾名思义，就不需要再解释什么了:\npublic interface INamedServiceProvider { TService GetService\u0026lt;TService\u0026gt;(string serviceName); } 接下来，编写实现类NamedServiceProvider:\npublic class NamedServiceProvider : INamedServiceProvider { private readonly IServiceProvider _serviceProvider; private readonly IDictionary\u0026lt;string, Type\u0026gt; _registrations; public NamedServiceProvider(IServiceProvider serviceProvider, IDictionary\u0026lt;string, Type\u0026gt; registrations) { _serviceProvider = serviceProvider; _registrations = registrations; } public TService GetService\u0026lt;TService\u0026gt;(string serviceName) { if(!_registrations.TryGetValue(serviceName, out var implementationType)) throw new ArgumentException($\u0026#34;Service \\\u0026#34;{serviceName}\\\u0026#34; is not registered in container\u0026#34;); return (TService)_serviceProvider.GetService(implementationType); } } 可以注意到，我们这里用一个字典来维护名称和类型间的关系，一切仿佛又回到三年前 Peter 大神手写 IoC 的那个下午。接下来，我们定义一个INamedServiceProviderBuilder, 它可以让我们使用链式语法注册服务：\npublic interface INamedServiceProviderBuilder { INamedServiceProviderBuilder AddNamedService\u0026lt;TService\u0026gt;(string serviceName, ServiceLifetime lifetime) where TService : class; INamedServiceProviderBuilder TryAddNamedService\u0026lt;TService\u0026gt;(string serviceName, ServiceLifetime lifetime) where TService : class; void Build(); } 这里，Add 和 TryAdd 的区别就是后者会对已有的键进行检查，如果键存在则不会继续注册，和微软自带的 DI 中的 Add/TryAdd 对应，我们一起来看它的实现：\npublic class NamedServiceProviderBuilder : INamedServiceProviderBuilder { private readonly IServiceCollection _services; private readonly IDictionary\u0026lt;string, Type\u0026gt; _registrations = new Dictionary\u0026lt;string, Type\u0026gt;(); public NamedServiceProviderBuilder(IServiceCollection services) { _services = services; } public void Build() { _services.AddTransient\u0026lt;INamedServiceProvider\u0026gt;(sp =\u0026gt; new NamedServiceProvider(sp, _registrations)); } public INamedServiceProviderBuilder AddNamedService\u0026lt;TImplementation\u0026gt;(string serviceName, ServiceLifetime lifetime) where TImplementation : class { switch (lifetime) { case ServiceLifetime.Transient: _services.AddTransient\u0026lt;TImplementation\u0026gt;(); break; case ServiceLifetime.Scoped: _services.AddScoped\u0026lt;TImplementation\u0026gt;(); break; case ServiceLifetime.Singleton: _services.AddSingleton\u0026lt;TImplementation\u0026gt;(); break; } _registrations.Add(serviceName, typeof(TImplementation)); return this; } public INamedServiceProviderBuilder TryAddNamedService\u0026lt;TImplementation\u0026gt;(string serviceName, ServiceLifetime lifetime) where TImplementation : class { switch (lifetime) { case ServiceLifetime.Transient: _services.TryAddTransient\u0026lt;TImplementation\u0026gt;(); break; case ServiceLifetime.Scoped: _services.TryAddScoped\u0026lt;TImplementation\u0026gt;(); break; case ServiceLifetime.Singleton: _services.TryAddSingleton\u0026lt;TImplementation\u0026gt;(); break; } _registrations.TryAdd(serviceName, typeof(TImplementation)); return this; } } 相信到这里，大家都明白博主的意图了吧，核心其实是在Build()方法中，因为我们最终需要的是其实是NamedServiceProvider，而在此之前的种种，都属于收集依赖、构建 ServiceProvider 的过程，所以，它被定义为NamedServiceProviderBuilder，我们在这里维护的这个字典，最终会被传入到NamedServiceProvider的构造函数中，这样我们就知道根据名称应该返回哪一个服务了。\n接下来，为了让它和微软自带的 DI 无缝粘合，我们需要编写一点扩展方法：\npublic static class ServiceCollectionExstension { public static TService GetNamedService\u0026lt;TService\u0026gt;(this IServiceProvider serviceProvider, string serviceName) { var namedServiceProvider = serviceProvider.GetRequiredService\u0026lt;INamedServiceProvider\u0026gt;(); if (namedServiceProvider == null) throw new ArgumentException($\u0026#34;Service \\\u0026#34;{nameof(INamedServiceProvider)}\\\u0026#34; is not registered in container\u0026#34;); return namedServiceProvider.GetService\u0026lt;TService\u0026gt;(serviceName); } public static INamedServiceProviderBuilder AsNamedServiceProvider(this IServiceCollection services) { var builder = new NamedServiceProviderBuilder(services); return builder; } } 现在，回到我们一开始的问题，它是如何被解决的呢？\nservices .AsNamedServiceProvider() .AddNamedService\u0026lt;ChineseSayHello\u0026gt;(\u0026#34;Chinese\u0026#34;, ServiceLifetime.Transient) .AddNamedService\u0026lt;EnglishSayHello\u0026gt;(\u0026#34;English\u0026#34;, ServiceLifetime.Transient) .Build(); var serviceProvider = services.BuildServiceProvier(); var chineseSayHello = serviceProvider.GetNamedService\u0026lt;ISayHello\u0026gt;(\u0026#34;Chinese\u0026#34;); var englishSayHello = serviceProvider.GetNamedService\u0026lt;ISayHello\u0026gt;(\u0026#34;English\u0026#34;); 这个时候，对调用方而已，依然是熟悉的 ServiceProvider，它只需要传入一个名称来获取服务即可，由此，我们就实现了基于名称的依赖注入。回顾一下它的实现过程，其实是一个逐步推进的过程，我们使用依赖注入，本来是希望依赖抽象，即针对同一个接口，可以无痛地从一种实现切换到另外一种实现。可我们发现，当这些实现同时被注册到容器里的时候，容器一样会迷惑于到底用哪一种实现，这就让我们开始思考，这种基于字典的 IoC 容器设计方案是否存在缺陷。所以，在.NET Core 里的 DI 设计中还引入了工厂的概念，因为并不是所以的 Resolve都可以通过Activator.Create来实现，更不必说 Autofac 和 Castle 中还有子容器的概念，只能说人生不同的阶段总会有不同的理解吧！好了，这篇博客就先写到这里，欢迎大家给我留言，晚安！\n","date":"2020-06-10T13:08:03Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1734098504/","slug":"1734098504","tags":["DI","依赖注入",".NET Core","技巧"],"title":".NET Core 原生 DI 扩展之基于名称的注入实现"},{"categories":["独立博客"],"content":"有时候，我不禁在想，我们到底处在一个什么样的时代呢？而之所以会有这样的疑问，则是因为我们的习惯在不断地被这个时代向前推进，就像我用了两年多的魅蓝 Note6 屏幕出现了问题，扫视了一圈新手机，居然再找不出一款带实体键的手机，刘海屏、水滴屏、破孔屏、异形屏、曲面屏等等简直令人眼花缭乱，唯独没有一款让我感到熟悉的非全面屏手机。做软件的时候，会不明白那些似是而非的定制需求的差异，可为什么偏偏到了硬件的时候，大家就能被迫适应这些越来越同质化的东西呢？也许有和我一样怀念非全面屏的人，可对于这个时代而言，一切都好像无足轻重，喜欢魅族对产品的设计，喜欢小而美的不妥协，可当大家都越来越相似的时候，也许，是因为我们终于都长大了吧，而怀念则是一种可有可无、甚至有一点多余的东西。在被告知一切向前看的路上，我们能拥有、用留住的东西本就不多，可偏偏我们就在给世间一切东西，努力刻上时间的温度，经历着花繁叶茂，经历着落叶归根。\n写博客，曾经是件很有意思的事情，透过网页去读每条留言背后的人，常常令你产生神交已久的感觉，即便网络如此发达的今天，让一个人失散，无非是动动手指拉黑、删除。陈星汉先生有一款游戏作品叫做《风之旅人》，游戏里的玩家依靠某种微弱的信号相互联系，而一旦失散彼此，将永远迷失在浩瀚无际的沙海里，你说，这是不是有人生本身的意味在里面呢？再后来 140 个字符的微博开始流行，而这些沉迷在博客时代里的人们，或固执地继续在博客这一方天地里挥洒，或搭乘移动互联网的 “高铁” 通往新的彼岸。有人这样比喻朋友圈和微博，说朋友圈装饰别人梦境的月亮，而微博则是装饰自己梦境的镜子。其实呢，在隐私问题基本荡然无存的今天，我们都只是在装饰资本的 “窗户” 吧！\n曾经运营过一段时间的微信公众号，最后发觉还是博客的载体更适合自己，虽然这些年没少为博客投入 “钱财”，在博客时代一去不复返的时间禁锢里，通过博客来盈利的想法堪堪聊以自慰，更不必说后来流行起来的 “在线教育” 和 Vlog。有人说，靠工资是没有办法挣到钱的，挣钱要靠这些 “睡后收入”，可当一件事物风头正盛的时候，彼时的你不足以追逐这一切的时候，这种感觉该如何言明呢？大概就像你在最落魄的时候，遇到一生中最想要保护的那个人一样，这听起来多少有点讽刺，人在不成熟的时候，总是后知后觉，可有一天真成熟了，再难有那时的运气或是豪气。所以呢，继续写下去吧，也许有一天，当你看着从前写的幼稚的文字，或哭或笑皆可入题，这不就是 “嬉笑怒骂，皆成文章” 了吗？\n果然，一不小心又扯远了。虽然说博客平时没什么流量，可像搜索引擎优化(SEO)、前端构建(CI/CD)、PWA 等等这些东西倒是有所钻研，提高博客访问量的方式除了增加搜索引擎里的权重和曝光率以外，其实，还有一种方式就是减少跳出时间。换句话说，访客在你博客里停留的时间越长，这意味着你有更多的内容可以被对方访问到，所以，增加内链是一个不错的思路。最直接的方式，就是在每篇博客结束以后推荐相关的博客供访客继续阅读。之前曾经尝试过像 hexo-recommended-posts 这样的插件，坦白说效果不是特别好，因为有时候加载这些站外的内容，导致博客页面打开的时候异常卡顿，所以，我们今天将采用原生的 JavaScript 来为 Hexo 实现博客推理功能，希望对大家有所启发。\n首先，我们来说说原理，推荐系统一般是需要一部分量化的指标来表征不同内容的相关性的。譬如通过 TF-IDF 来计算文本的相似度，通过公共词袋中的词频构造向量再配合余弦公式来计算，通过 TextRank 这类借鉴 PageRank 思想的方法来计算等等。这里呢，我们不采用这些方法来实现，主要是考虑到 200 篇左右的博客，两两计算相似度特别耗费时间，对于 Hexo 这种静态博客而言，我们还是应该节省生成静态页面的时间，虽然这部分时间都是 Travis CI 去跑的(逃……。我们采用的方案是基于标签和日期的推荐方式，即根据当前文章的标签筛选相同标签的文章，根据当前文章的日期筛选相同日期的文章。有了这两种策略，配合 Hexo 中提供的全局变量，我们可以很容易地编写出下面的代码：\n\u0026lt;% function shuffle(a) { for (let i = a.length; i; i--) { let j = Math.floor(Math.random() * i); [a[i - 1], a[j]] = [a[j], a[i - 1]]; } return a; } function recommended_posts(page, site, limit = 5) { page.tags = page.tags || [] if (page.tags.length == 0) return []; let pageTags = page.tags.map(x=\u0026gt;x.name); let sitePosts = site.posts.toArray().map(x=\u0026gt; { return {tags:x.tags.toArray().map(y=\u0026gt;y.name), title:x.title, permalink:x.permalink, date:x.date} }); let relatedPosts = pageTags.map(x=\u0026gt;sitePosts.filter(y=\u0026gt;y.title != page.title \u0026amp;\u0026amp; (y.tags.indexOf(x) != -1 || y.date.format(\u0026#39;MM/DD\u0026#39;) == page.date.format(\u0026#39;MM/DD\u0026#39;)))).reduce((prev,next)=\u0026gt;{ return prev.concat(next); },[]); return shuffle(Array.from(new Set(relatedPosts))).slice(0, limit); } %\u0026gt; \u0026lt;% var post_list = recommended_posts(page, site, config.recommended_posts.limit) %\u0026gt; \u0026lt;% if(post_list.length \u0026gt; 0 \u0026amp;\u0026amp; config.recommended_posts.enable) { %\u0026gt; \u0026lt;div class=\u0026#34;recommended_posts\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;%= config.recommended_posts.title %\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;% post_list.forEach(function(link) { %\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;\u0026lt;%= link.permalink %\u0026gt;\u0026#34;\u0026gt;\u0026lt;%= link.title %\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;% }) %\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;% } %\u0026gt; 代码非常直白，按照标签和日期两种策略筛选出文章，打乱顺序后从中提取出若干个返回，而剩下的工作，就是将其渲染到页面中。在这里，博主单独定义了一个模板文件，所以，我们在博客的适当位置引入即可，博主是放在博客结束以后的位置：\n\u0026lt;div class=\u0026#34;post-content\u0026#34; id=\u0026#34;post-content\u0026#34; itemprop=\u0026#34;postContent\u0026#34;\u0026gt; \u0026lt;%- post.content %\u0026gt; \u0026lt;%- partial(\u0026#39;post/recommended_posts\u0026#39;) %\u0026gt; \u0026lt;/div\u0026gt; 最终实现的效果如下图所示：\n本文实现的相关文章推荐功能\r当然，当你看到这篇博客的时候，你已经看到博主为你推荐的内容了，是否有兴趣继续读下去呢？如果这样的话，就说明这两个内容是相关的。而基于日期的推荐，即所谓的“去年今日”，它本身的相关性可能并不强，但可以让你产生一种强烈的对比感，原来，这一天我是这样度过的啊。好了，这就是这篇博客的内容啦，晚安～\n","date":"2020-06-08T12:30:54Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/478946932/","slug":"478946932","tags":["Hexo","推荐","插件"],"title":"原生 JavaScript 实现 Hexo 博客推荐功能"},{"categories":["编程语言"],"content":"相信大家都有这样一种感觉，Linq和Lambda是.NET 中一以贯之的存在，从最早的 Linq to Object 到 Linq to SQL，再到 EF/EF Core 甚至如今的.NET Core，我们可以看到Lambda表达式的身影出现地越来越频繁。虽然 Linq to Object 和 Linq to SQL，分别是以IEnumerable\u0026lt;T\u0026gt;和IQueryable \u0026lt;T\u0026gt;为基础来实现的。我个人以为，Lambda呢，其实就是匿名委托的“变种”，而Linq则是对Lambda的进一步封装。在System.Linq.Expressions命名空间下，提供大量关于表达式树的 API，而我们都知道，这些表达式树最终都会被编译为委托。所以，动态创建 Lambda 表达式，实际上就是指从一个字符串生成对应委托的过程，而一旦这个委托被生成，可以直接传递给 Where()方法作为参数，显然，它可以对源数据进行过滤，这正是我们想要的结果。\n事出有因 在今天这篇博客中，我们主要介绍System.Linq.Dynamic.Core这个库，即我所说的 Dynamic Linq。本着“艺术源于生活的态度”，在介绍它的用法之前，不妨随博主一起看看，一个“简单“的查询是如何随着业务演进而变得越来越复杂。从某种意义上来说，正是它让博主想起了 Dynamic Linq。我们为客户编写了一个生成订单的接口，它从一张数据表中“消费”订单数据。最开始，它只需要过滤状态为“未处理”的记录，对应的 CRUD 可以表示为这样：\nvar orderInfos = repository.GetByQuery\u0026lt;tt_wg_order\u0026gt;(x =\u0026gt; x.STATUS == 10); 后来，因为业务方存在重复/错误下单的情况，业务数据有了“软删除”的状态，相应地查询条件再次发生变化，这看起来还行对吧：\nvar orderInfos = repository.GetByQuery\u0026lt;tt_wg_order\u0026gt;(x =\u0026gt; x.STATUS == 10 \u0026amp;\u0026amp; x.Isdelete == 0); 再后来，因为接口处理速度不理想，无法满足客户的使用场景，公司大佬们建议“加机器”，而为了让每台服务器上消费的订单数据不同(据说是为了避免发生并发)，大佬们要求博主开放所有字段作为查询条件，这样，每台服务器上可以配置不同查询条件。自此，又双叒叕改：\nvar repository = container.Resolve\u0026lt;CrudRepositoryBase\u0026gt;(); var searchParameters = new SearchParameters() { PageInfo = new PageInfo() { PageSize = parameters.PAGE_SIZE.Value }}; searchParameters.QueryModel.Items.Add(new ConditionItem { Field = \u0026#34;STATUS\u0026#34;, Method = QueryMethod.Equal, Value = 10 }); searchParameters.QueryModel.Items.Add(new ConditionItem { Field = \u0026#34;Isdelete\u0026#34;, Method = QueryMethod.Equal, Value = 0 }); //此处省略更多的查询条件:) var orderInfos = repository.GetByPage\u0026lt;tt_wg_order\u0026gt;(searchParameters); 可以想象得出，终极终终极的查询会变成下面这张图。这种方式看起来很美好对不对？可谁能想到，就在五一放假前的某一天里，博主还在替某个“刁钻”客户排查一组同样“刁钻”的过滤条件为什么没有生效。显然，我需要有一种更友好的方式，它可以从一个字符串变成一个委托，就像 JavaScript 里\u0026quot;邪恶\u0026quot;的 Eval()函数一样，说它邪恶，是因为它的输入是不可控的，\u0026ldquo;机智\u0026quot;的人类习惯把事件万物都当成 SQL 语句，其实，RESTful 接口里传 SQL、调存储过程难道不可以吗？同样，是因为这种做法太\u0026quot;邪恶\u0026rdquo;。\n![过滤条件在风中凌乱]](https://i.loli.net/2020/05/11/QEDHwA9bZUTInJY.png)\nParseLambda 首先，通过nuget安装：System.Linq.Dynamic.Core。这里主要介绍的是介绍的是其中的 ParseLambda()方法，顾名思义，它可以把一个字符串转换为指定类型的委托，一起来看下面的例子。首先，我们定义一个通用方法 BuildLambda：\nFunc\u0026lt;T, bool\u0026gt; BuildLambda\u0026lt;T\u0026gt;(string exps) { var sourceType = typeof(T); var sourceParameter = Expression.Parameter(sourceType); var lambdaExps = DynamicExpressionParser.ParseLambda( new[] { sourceParameter }, typeof(bool), exps ); return lambdaExps.Compile() as Func\u0026lt;T, bool\u0026gt;; } var students = new List\u0026lt;Student\u0026gt;() { new Student() { Name = \u0026#34;长安书小妆\u0026#34;, Age = 25, Address = \u0026#34;洛阳市洛龙区\u0026#34;, Teacher = new Teacher() { Name = \u0026#34;孔子\u0026#34; } }, new Student() { Name = \u0026#34;飞鸿踏雪\u0026#34;, Age = 28, Address = \u0026#34;宁夏中卫市\u0026#34;, Teacher = new Teacher() { Name = \u0026#34;孔子\u0026#34; } }, }; var exps = \u0026#34;Age\u0026lt;=25 \u0026amp;\u0026amp; Address.Contains(\\\u0026#34;洛阳市\\\u0026#34;) \u0026amp;\u0026amp; Teacher.Name=\\\u0026#34;孟子\\\u0026#34;\u0026#34;; var lambda = BuildLambda\u0026lt;Student\u0026gt;(exps); var results = students.Where(lambda); 注意到，核心的代码其实只有DynamicExpressionParser.ParseLambda()这一句，这充分暴露了博主“调包侠”的本质。按照示例代码中的过滤条件，我们知道给定数据中是没有符合条件的数据的。假如你真的运行了这段代码，你就会得到真正的结果：我说的是对的(逃\nOne More Thing 其实，我们今天所说这一切，从本质上来讲，还是属于表达式树的范畴，因为上面的例子，我们同样可以使用表达式树来编写，无非是这个第三方库帮我们隐藏了这部分细节。对于上面这个例子，如果用表达式树来写，会是什么样子的呢？相信熟悉表达式树的朋友，可以非常容易地写出下面的代码：\n//x var parameter = Expression.Parameter(typeof(tt_wg_order), \u0026#34;x\u0026#34;); //x.STATUS == 10 var condStatus = Expression.Equal(Expression.Property(parameter, \u0026#34;STATUS\u0026#34;), Expression.Constant(10)); //x.Isdelete == 0 var condIsDelete = Expression.Equal(Expression.Property(parameter, \u0026#34;Isdelete\u0026#34;), Expression.Constant(0)); //x.STATUS == 10 \u0026amp;\u0026amp; x.Isdelete == 0 var condAndAlso = Expression.AndAlso(condStatus, condIsDelete); //x =\u0026gt; x.STATUS == 10 \u0026amp;\u0026amp; x.Isdelete == 0 var lambda = Expression.Lambda\u0026lt;Func\u0026lt;tt_wg_order,bool\u0026gt;\u0026gt;(condAndAlso, parameter); 我们可以注意到，一个 Lmabda 表达式，可以抽象为:参数(Parameter)和函数体(Body)两部分，而Body实际上是由一个操作符和一个值组成。譬如这里的第一个条件：x.STATUS == 10。在这里基础上，我们可以定义一个类型：SearchParameters，它将每个条件抽象为字段(Field)、查询方法(QueryMethod)、值(Value)和或分组(OrGroup)。所以，它的处理逻辑就是，将相同 OrGroup 的条件放在一起用 Or 连接，然后再和其它条件放在一起用 And 连接。故而，它可以通过表达式构造出一个 Predict类型的委托，而我们的数据持久层是使用 EF 来实现的，所以，它可以顺利成章地和 IQueryable搭配使用，这就是我们这个 SearchParameters 的实现原理，它唯一让我觉得不好的地方是，字段(Field)不能通过一个 Lambda 表达式去构造，而必须传入一个字符串，这给了使用者写错字段名称的机会(逃：\npublic static class LambdaExpressionBuilder { private static Expression GetExpression (ParameterExpression parameter, Condition condition) { var propertyParam = Expression.Property (parameter, condition.Field); var propertyInfo = propertyParam.Member as PropertyInfo; if (propertyInfo == null) throw new ArgumentException ($\u0026#34;Invalid field \\\u0026#34;{condition.Field}\\\u0026#34;\u0026#34;); var realPropertyType = Nullable.GetUnderlyingType (propertyInfo.PropertyType) ?? propertyInfo.PropertyType; if (condition.Op != Operation.StdIn \u0026amp;\u0026amp; condition.Op != Operation.StdNotIn) condition.Value = Convert.ChangeType (condition.Value, realPropertyType); var constantParam = Expression.Constant (condition.Value); switch (condition.Op) { case Operation.Equals: return Expression.Equal (propertyParam, constantParam); case Operation.NotEquals: return Expression.NotEqual (propertyParam, constantParam); case Operation.Contains: return Expression.Call (propertyParam, \u0026#34;Contains\u0026#34;, null, constantParam);; case Operation.NotContains: return Expression.Not (Expression.Call (propertyParam, \u0026#34;Contains\u0026#34;, null, constantParam)); case Operation.StartsWith: return Expression.Call (propertyParam, \u0026#34;StartsWith\u0026#34;, null, constantParam); case Operation.EndsWith: return Expression.Call (propertyParam, \u0026#34;EndsWith\u0026#34;, null, constantParam); case Operation.GreaterThen: return Expression.GreaterThan (propertyParam, constantParam); case Operation.GreaterThenOrEquals: return Expression.GreaterThanOrEqual (propertyParam, constantParam); case Operation.LessThan: return Expression.LessThan (propertyParam, constantParam); case Operation.LessThanOrEquals: return Expression.LessThanOrEqual (propertyParam, constantParam); case Operation.StdIn: return Expression.Call (typeof (Enumerable), \u0026#34;Contains\u0026#34;, new Type[] { realPropertyType }, new Expression[] { constantParam, propertyParam }); case Operation.StdNotIn: return Expression.Not (Expression.Call (typeof (Enumerable), \u0026#34;Contains\u0026#34;, new Type[] { realPropertyType }, new Expression[] { constantParam, propertyParam })); } return null; } private static Expression GetGroupExpression (ParameterExpression parameter, List\u0026lt;Condition\u0026gt; orConditions) { if (orConditions.Count == 0) return null; var exps = orConditions.Select (c =\u0026gt; GetExpression (parameter, c)).ToList (); return exps.Aggregate\u0026lt;Expression, Expression\u0026gt; (null, (left, right) =\u0026gt; { if (left == null) return right; return Expression.OrElse (left, right); }); } public static Expression\u0026lt;Func\u0026lt;T, bool\u0026gt;\u0026gt; BuildLambda\u0026lt;T\u0026gt; (IEnumerable\u0026lt;Condition\u0026gt; conditions) { if (conditions == null || !conditions.Any ()) return x =\u0026gt; true; var parameter = Expression.Parameter (typeof (T), \u0026#34;x\u0026#34;); //简单条件 var simpleExps = conditions.ToList ().FindAll (c =\u0026gt; string.IsNullOrEmpty (c.OrGroup)) .Select (c =\u0026gt; GetExpression (parameter, c)) .ToList (); //复杂条件 var complexExps = conditions.ToList ().FindAll (c =\u0026gt; !string.IsNullOrEmpty (c.OrGroup)) .GroupBy (x =\u0026gt; x.OrGroup) .Select (g =\u0026gt; GetGroupExpression (parameter, g.ToList ())) .ToList (); var exp = simpleExps.Concat (complexExps).Aggregate\u0026lt;Expression, Expression\u0026gt; (null, (left, right) =\u0026gt; { if (left == null) return right; return Expression.AndAlso (left, right); });; return Expression.Lambda\u0026lt;Func\u0026lt;T, bool\u0026gt;\u0026gt; (exp, parameter); } } 接下来，我们就可以以一种优雅的方式来对编写查询条件：\nvar searchParameters = new SearchParameters(); searchParameters.Query = new QueryModel(); searchParameters.Query.Add(new Condition() { Field = \u0026#34;IntValue\u0026#34;, Op = Operation.LessThan, Value = 30 }); searchParameters.Query.Add(new Condition() { Field = \u0026#34;StringValue\u0026#34;, Op = Operation.Contains, Value = \u0026#34;山\u0026#34;, OrGroup = \u0026#34;StringValue\u0026#34; }); searchParameters.Query.Add(new Condition\u0026lt;Foo\u0026gt;() { Field = x =\u0026gt; x.StringValue, Op = Operation.Contains, Value = \u0026#34;有朋\u0026#34;, OrGroup = \u0026#34;StringValue\u0026#34; }); var lambda = LambdaExpressionBuilder.BuildLambda\u0026lt;Foo\u0026gt;(searchParameters.Query); var where = lambda.Compile(); var result = list.Where(where); 这种实现可以说相当巧妙啦，因为通过有限的条件，我们就可以覆盖到大部分查询的场景，而如果直接去解析一个 Lambda 表达式，难度显然会增加不少。这里是以一个普通的泛型列表作为示例的，而在实际使用中，常常是结合 EntityFramework 这类 ORM 来使用的。相应地，我们只需要为 IQueryable 接口扩展出支持 SearchParameter 作为参数进行查询地扩展方法即可，这分别对应了我们在文章一开头所提到的IEnumerable\u0026lt;T\u0026gt;和IQueryable \u0026lt;T\u0026gt;。\n可如果遇上 Dapper 这样的轻量级 ORM，我们要考虑的问题就变成了怎么通过 Lambda 表达式生成 SQL 语句，所以，通过 Dapper 来扩展功能的时候，最困难的地方，往往在于没法儿像 EF/EF Core 一样去随心所欲地 Where()，像 Dapper.Contrib 则只能先查询出所有结果再去做进一步的过滤，这种在数据量特别大的时候就会出问题。通过 Lambda 生成 SQL，最难的地方是，你压根不知道，人家会写一个什么样的表达式，而这个表达式，又怎么通过 SQL 去表达。那么，退而求其次，我们继续用 SearchParameters 来实现，因为它里面的 QueryMethod 是有限的，下面给出一个简单的实现：\npublic static class SearchParametersExtension { public static (string, Dictionary\u0026lt;string, object\u0026gt;) BuildSqlWhere (this SearchParameters searchParameters) { var conditions = searchParameters.Query; if (conditions == null || !conditions.Any ()) return (string.Empty, null); var sqlExps = new List\u0026lt;string\u0026gt; (); var sqlParam = new Dictionary\u0026lt;string, object\u0026gt; (); //构建简单条件 var simpleConditions = conditions.FindAll (x =\u0026gt; string.IsNullOrEmpty (x.OrGroup)); sqlExps.Add (simpleConditions.BuildSqlWhere (ref sqlParam)); //构建复杂条件 var complexConditions = conditions.FindAll (x =\u0026gt; !string.IsNullOrEmpty (x.OrGroup)); sqlExps.AddRange (complexConditions.GroupBy (x =\u0026gt; x.OrGroup).ToList ().Select (x =\u0026gt; \u0026#34;( \u0026#34; + x.BuildSqlWhere (ref sqlParam, \u0026#34; OR \u0026#34;) + \u0026#34; )\u0026#34;)); var sqlWhwere = sqlExps.Count \u0026gt; 1 ? string.Join (\u0026#34; AND \u0026#34;, sqlExps) : sqlExps[0]; return ($\u0026#34; WHERE {sqlWhwere} \u0026#34;, sqlParam); } public static string BuildSqlWhere (this IEnumerable\u0026lt;Condition\u0026gt; conditions, ref Dictionary\u0026lt;string, object\u0026gt; sqlParams, string keywords = \u0026#34; AND \u0026#34;) { if (conditions == null || !conditions.Any ()) return string.Empty; var sqlParamIndex = 1; var sqlExps = new List\u0026lt;string\u0026gt; (); foreach (var condition in conditions) { var index = sqlParams.Count + sqlParamIndex; switch (condition.Op) { case Operation.Equals: sqlExps.Add ($\u0026#34;{condition.Field} = @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = condition.Value; break; case Operation.NotEquals: sqlExps.Add ($\u0026#34;{condition.Field} \u0026lt;\u0026gt; @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = condition.Value; break; case Operation.Contains: sqlExps.Add ($\u0026#34;{condition.Field} LIKE @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;%{condition.Value}%\u0026#34;; break; case Operation.NotContains: sqlExps.Add ($\u0026#34;{condition.Field} NOT LIKE @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;%{condition.Value}%\u0026#34;; break; case Operation.StartsWith: sqlExps.Add ($\u0026#34;{condition.Field} LIKE @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;%{condition.Value}\u0026#34;; break; case Operation.EndsWith: sqlExps.Add ($\u0026#34;{condition.Field} LIKE @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}%\u0026#34;; break; case Operation.GreaterThen: sqlExps.Add ($\u0026#34;{condition.Field} \u0026gt; @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; case Operation.GreaterThenOrEquals: sqlExps.Add ($\u0026#34;{condition.Field} \u0026gt;= @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; case Operation.LessThan: sqlExps.Add ($\u0026#34;{condition.Field} \u0026lt; @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; case Operation.LessThanOrEquals: sqlExps.Add ($\u0026#34;{condition.Field} \u0026lt;= @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; case Operation.StdIn: sqlExps.Add ($\u0026#34;{condition.Field} IN @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; case Operation.StdNotIn: sqlExps.Add ($\u0026#34;{condition.Field} NOT IN @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; } sqlParamIndex += 1; } return sqlExps.Count \u0026gt; 1 ? string.Join (keywords, sqlExps) : sqlExps[0]; } } 现在，我们可以换一种方式来查 Dapper，果然是因为手写 SQL 没有安全感的缘故啊！\nvar searchParameters = new SearchParameters(); searchParameters.Page = new PageModel() { PageSize = 10, CurrentPage = 1 }; searchParameters.Query = new QueryModel(); searchParameters.Query.Add(new Condition() { Field = \u0026#34;OrgCode\u0026#34;, Op = Operation.Contains, Value = \u0026#34;飞天御剑流\u0026#34;, OrGroup = \u0026#34;OrgCode\u0026#34; }); searchParameters.Query.Add(new Condition() { Field = \u0026#34;OrgCode\u0026#34;, Op = Operation.Equals, Value = \u0026#34;新选组\u0026#34;, OrGroup = \u0026#34;OrgCode\u0026#34; }); searchParameters.Query.Add(new Condition() { Field = \u0026#34;CreatedAt\u0026#34;, Op = Operation.GreaterThenOrEquals, Value = new DateTime(2020, 1, 1)}); _repository.GetByQuery\u0026lt;BusinessUnit\u0026gt;(searchParameters); 对于定义Condition时，Field属性安全感缺失的问题，我们可以这样来解决：\npublic class Condition\u0026lt;T\u0026gt; : Condition public new Expression\u0026lt;Func\u0026lt;T, dynamic\u0026gt;\u0026gt; Field { get; set; } public Operation Op { get; set; } public object Value { get; set; } public string OrGroup { get; set; } } public class QueryModel : List\u0026lt;Condition\u0026gt; { public void Add\u0026lt;T\u0026gt;(Condition\u0026lt;T\u0026gt; condition) where T : class { var filedName = string.Empty; var memberExp = condition.Field.Body as MemberExpression; if (memberExp == null) { var ubody = (UnaryExpression)condition.Field.Body; memberExp = ubody.Operand as MemberExpression; } filedName = memberExp.Member.Name; Add(new Condition() { Field = filedName, Op = condition.Op, Value = condition.Value, OrGroup = condition.OrGroup }); } } 其实，这还是表达式树的内容，在上面的代码片段中，早已出现过它的身影，回想起多年前用这个东西改造 INotifyPropertyChanged 的时候，总觉得一切似曾相识：\nsearchParameters.Query.Add(new Condition\u0026lt;Foo\u0026gt;() { Field = x =\u0026gt; x.StringValue, Op = Operation.Contains, Value = \u0026#34;有朋\u0026#34;, OrGroup = \u0026#34;StringValue\u0026#34; }); 本文小结 和博主的大多数博客一样，这篇博客是一个“醉翁之意不在酒”的博客。听起来在说如何动态创建 Lambda 表达式，实际上讲的还是表达式树，至于原因，则还是博客开篇所提到的“一以贯之”。博主想写这篇博客，是源于实际工作中遇到的“查询”问题，而最后解决的还真就是查询的问题。不管是 Dynamic Linq 中的 ParseLambda()还是表达式树中的 LambdaExpression，本质上都是同一个东西，最终的命运都是 Predict这个委托。SearchParameters 则是对前者的一种简化，通过控制 Lambda 表达式的复杂度来简化问题，相比起直接传一个字符串过来，这种在风险的控制上要更高一点，之所以要传字符串，则是又一个非关技术的无聊的问题了，用 Jira 里的概念说应该叫做设计如此(By Design)。好了，以上就是这篇博客的内容啦，谢谢大家！\n","date":"2020-05-08T12:27:11Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/118272597/","slug":"118272597","tags":["Linq","Lambda","表达式树"],"title":"使用 Dynamic Linq 构建动态 Lambda 表达式"},{"categories":["数据存储"],"content":"相信大家都有过周末被电话“吵醒”的经历，这个时候，客服同事会火急火燎地告诉你，客户反馈生产环境上某某数据“异常”，然后你花费大量时间去排查这些错误数据，发现这是客户使用某一种“骚”操作搞出来的“人祸”。可更多的时候，你不会这么顺利，因为你缺乏有力的证据去支持你的结论。最终，你不情愿地去处理了这些错误数据。你开始反思，为什么没有一种流程去记录客户对数据的变更呢？为什么你总要花时间去和客户解释这些数据产生的原因呢？好了，这就要说到我们今天这篇博客的主题——审计。\n什么是审计 结合本文引言中的描述的场景，当我们需要知道某条数据被什么人修改过的时候，或者是希望在数据变更的时候去通知某个人，亦或者是我们需要追溯一条数据的变更历史的时候，我们需要一种机制去记录数据表中的数据变更，这就是所谓的审计。而实际的业务中，可能会有类似，查询某一个员工一天内审批了多少单据的需求。你不要笑，人类常常如此无聊，就像我们有一个异常复杂的计费逻辑，虽然审计日志里记录了某个费用是怎么计算出来的，可花时间最多的地方，无一例外是需要开发去排查和解释的，对于这一点，我时常感觉疲于应对，这是我这篇文章里想要写审计的一个重要原因。\nEF/EF Core 实体跟踪 EF 和 EF Core 里都提供了实体跟踪的功能，我的领导经常吐槽我，在操作数据库的时候，喜欢显式地调用repository.Update()方法，因为他觉得项目中的实体跟踪是默认打开的。可当你学习了Vue以后，你了解到Vue中是检测不到数组的某些变化的，所以，这个事情我持保留意见，显式调用就显式调用呗，万一哪天人家把实体跟踪给关闭了呢？不过，话说回来，实体跟踪确实可以帮我们做一点工作的，其中，就包括我们今天要说的审计功能。\nEF 和 EF Core 中的实体追踪主要指 DbContext 类的 ChangeTracker，而通过 DetachChanges()方法，则可以获得那些变化了的实体的集合。所以，使用实体追踪来实现审计功能，本质上就是在 SaveChanges()方法调用前后，记录实体中每一个字段的变化情况。为此，我们考虑编写下面的类——AuditDbContextBase，顾名思义，这是一个审计相关的 DbContext 基类，所以，希望实现审计功能的 DbContext 都会继承这个类。这里，我们重写其 SaveChanges()方法，其基本定义如下：\npublic class AuditDbContextBase : DbContext, IAuditStorage { public DbSet\u0026lt;AuditLog\u0026gt; AuditLog { get; set; } public AuditDbContextBase(DbContextOptions options, AuditConfig auditConfig) : base(options) { } public virtual Task BeforeSaveChanges() { } public virtual Task AfterSaveChanges() { } public override async Task\u0026lt;int\u0026gt; SaveChangesAsync(bool acceptAllChangesOnSuccess, CancellationToken cancellationToken = default) { await BeforeSaveChanges(); var result = await base.SaveChangesAsync(acceptAllChangesOnSuccess, cancellationToken); await AfterSaveChanges(); return result; } public void SaveAuditLogs(params AuditLog[] auditLogs) { AuditLog.AddRange(auditLogs); base.SaveChangesAsync(); } } 接下来，就是去实现BeforeSaveChanges()和AfterSaveChanges()两个方法：\n//BeforeSaveChanges public virtual Task BeforeSaveChanges() { ChangeTracker.DetectChanges(); _auditEntries = new List\u0026lt;AuditEntry\u0026gt;(); foreach (var entityEntry in ChangeTracker.Entries()) { if (entityEntry.State == EntityState.Detached || entityEntry.State == EntityState.Unchanged) continue; if (entityEntry.Entity.GetType() == typeof(AuditLog)) continue; if (_auditConfig.EntityFilters.Any(x =\u0026gt; x(entityEntry))) continue; var auditEntry = new AuditEntry(entityEntry, _auditConfig); _auditEntries.Add(auditEntry); } return Task.CompletedTask; } //AfterSaveChanges public virtual Task AfterSaveChanges() { if (_auditEntries == null || !_auditEntries.Any()) return Task.CompletedTask; _auditEntries.ForEach(auditEntry =\u0026gt; auditEntry.UpdateTemporaryProperties()); var auditLogs = _auditEntries.Select(x =\u0026gt; x.AsAuditLog()).ToArray(); if (!_auditConfig.AuditStorages.Any()) _auditConfig.AuditStorages.Add(this); _auditConfig.AuditStorages.ForEach( auditStorage =\u0026gt; auditStorage.SaveAuditLogs(auditLogs) ); return Task.CompletedTask; } 可以注意到，我们会在SaveChanges()方法执行前，通过ChangeTracker.DetectChanges()方法显式地捕获“变化\u0026quot;，这些“变化”会被存储到一个临时的列表中。而在SaveChanges()方法执行后，则会更新那些只有在数据提交后才可以获得的“临时”数据，最典型的例子是自增的 ID，在数据提交前，我们是无法获得真正的 ID 的。这个列表中的内容最终会通过AsAuditLog()方法进行转化。下面是AuditEntry中的部分代码片段：\n//SetValuesCollection private void SetValuesCollection(List\u0026lt;PropertyEntry\u0026gt; properties) { foreach (var property in properties) { var propertyName = property.Metadata.GetColumnName(); if (_auditConfig.PropertyFilters.Any(x =\u0026gt; x(_entityEntry, property))) continue; switch (OperationType) { case OperationType.Created: NewValues[propertyName] = property.CurrentValue; break; case OperationType.Updated: if (_auditConfig.IsIgnoreSameValue \u0026amp;\u0026amp; property.OriginalValue.ToString() == property.CurrentValue.ToString()) continue; OldValues[propertyName] = property.OriginalValue; NewValues[propertyName] = property.CurrentValue; break; case OperationType.Deleted: OldValues[propertyName] = property.OriginalValue; break; } }; } //AsAuditLog public AuditLog AsAuditLog() { return new AuditLog() { Id = Guid.NewGuid().ToString(\u0026#34;N\u0026#34;), TableName = TableName, CreatedBy = string.Empty, CreatedDate = DateTime.Now, NewValues = NewValues.Any() ? JsonConvert.SerializeObject(NewValues) : null, OldValues = OldValues.Any() ? JsonConvert.SerializeObject(OldValues) : null, ExtraData = ExtraData.Any() ? JsonConvert.SerializeObject(ExtraData) : null, OperationType = (int)OperationType }; } 在此基础上，我们可以编写我们实际的 DbContext，这里以 CustomerContext 为例，当我们向其中添加、修改和删除 Customer 的时候，就会触发审计相关的逻辑，默认情况下，审计产生的数据 AuditLog 和 Customer 在同一个数据库上下文中，当然，我们可以通过注入 IAuditStore 来实现更精细的控制，例如，可以将审计日志输入到文本文件，甚至是 Mongodb 这样的非关系型数据库里，因为有依赖注入的存在，这些实现起来会非常的简单！\n//注入AuditLog配置 services.AddAuditLog(config =\u0026gt; config .IgnoreTable\u0026lt;AuditLog\u0026gt;() .IgnoreProperty\u0026lt;AuditLog\u0026gt;(x =\u0026gt; x.CreatedDate) .WithExtraData(\u0026#34;Tags\u0026#34;, \u0026#34;.NET Core\u0026#34;) .WithStorage\u0026lt;FileAuditStorage\u0026gt;() .WithStorage\u0026lt;MongoAuditStorage\u0026gt;() ); //注入DbContext services.AddDbContext\u0026lt;CustomerContext\u0026gt;(options =\u0026gt; options.UseSqlServer(Configuration.GetConnectionString(\u0026#34;DefaultConnection\u0026#34;))); //像平时一样使用EF var entity = _context.Customer.Where(x =\u0026gt; x.Id == customer.Id).FirstOrDefault(); entity.Name = customer.Name; entity.Email = customer.Email; entity.Address = customer.Address; entity.Tel = customer.Tel; _context.Customer.Update(entity); await _context.SaveChangesAsync(); 下面是最终生成的审计日志信息：\n审计日志表展示\rCastle 动态代理 而对于像 Dapper 这种轻量级的 ORM，它本身没有类似 EF/EF Core 的 ChangeTracker 的设计，如果我们在项目中使用 Dapper，并且希望实现审计的相关功能，直观上看就会有一点困难。其实，平时在混合使用 EF/Dapper 的过程中，经常遇到的问题就是，如何确保传统的 ADO.NET 和 EF 在一个数据库事务中，如何确保 Dapper 和 EF 在一个数据库事务中等等。此时，我们就需要一点抽象，首先去实现一个 Dapper 的仓储模式，然后再借助 Castle 这类动态代理库实现对接口的拦截。这里以 Dapper 的扩展库 Dapper.Contrib 为例。首先，我们定义一个仓储接口 IRepository:\npublic interface IRepository { TEntity GetByID\u0026lt;TEntity\u0026gt;(object id) where TEntity : class; TEntity GetByKeys\u0026lt;TEntity\u0026gt;(object keys) where TEntity : class; TEntity QueryFirst\u0026lt;TEntity\u0026gt;(string sql, object param) where TEntity : class; TEntity QuerySingle\u0026lt;TEntity\u0026gt;(string sql, object param) where TEntity : class; [AuditLog(OperationType.Created)] void Insert\u0026lt;TEntity\u0026gt;(params TEntity[] entities) where TEntity : class; [AuditLog(OperationType.Updated)] void Update\u0026lt;TEntity\u0026gt;(params TEntity[] entities) where TEntity : class; [AuditLog(OperationType.Deleted)] void Delete\u0026lt;TEntity\u0026gt;(params TEntity[] entities) where TEntity : class; void Delete\u0026lt;TEntity\u0026gt;(params object[] ids) where TEntity : class; IEnumerable\u0026lt;TEntity\u0026gt; GetByQuery\u0026lt;TEntity\u0026gt;(Expression\u0026lt;Func\u0026lt;TEntity,bool\u0026gt;\u0026gt; exps) where TEntity : class; IEnumerable\u0026lt;TEntity\u0026gt; GetByQuery\u0026lt;TEntity\u0026gt;(string sql, object param) where TEntity : class; IEnumerable\u0026lt;TEntity\u0026gt; GetAll\u0026lt;TEntity\u0026gt;() where TEntity : class; } 接下来，我们就可以在拦截器中实现数据审计功能，因为 Dapper 本身没有 ChangeTracker，所以，我们必须要在先从数据库中查出来 OldValue，所以，实际效率应该并不会特别高，这里权当做为大家扩展思路吧！\npublic class AuditLogInterceptor : IInterceptor { public void Intercept(IInvocation invocation) { var repository = invocation.Proxy as IRepository; var entityType = GetEntityType(invocation); var tableName = GetTableName(entityType); var tableIdProperty = entityType.GetProperty(\u0026#34;Id\u0026#34;); var auditLogAttrs = invocation.Method.GetCustomAttributes(typeof(AuditLogAttribute), false); if (auditLogAttrs == null || auditLogAttrs.Length == 0 || entityType == typeof(AuditLog)) { invocation.Proceed(); return; } var auditLogAttr = (auditLogAttrs as AuditLogAttribute[])[0]; var auditLogs = new List\u0026lt;AuditLog\u0026gt;(); switch (auditLogAttr.OperationType) { case Domain.OperationType.Created: auditLogs = GetAddedAuditLogs(invocation, tableName); break; case Domain.OperationType.Updated: auditLogs = GetUpdatedAuditLogs(invocation, tableName, entityType, tableIdProperty, repository); break; case Domain.OperationType.Deleted: auditLogs = GetDeletedAuditLogs(invocation, tableName); break; } invocation.Proceed(); repository.Insert\u0026lt;AuditLog\u0026gt;(auditLogs.ToArray()); } } 同样地，这里需要需要使用Autofac将其注册到 IoC 容器中：\nbuilder.RegisterType\u0026lt;DapperRepository\u0026gt;().As\u0026lt;IRepository\u0026gt;() .InterceptedBy(typeof(AuditLogInterceptor)) .EnableInterfaceInterceptors(); builder.RegisterType\u0026lt;AuditLogInterceptor\u0026gt;(); 思路延伸：领域事件 最近这段时间，对于数据同步这类“需求”略有感触，譬如某种单据在两个互为上下游的系统里流转，譬如不同系统间实时地对基础资料进行同步等。这类需求可能会通过ETL、DBLink这类“数据库”手段实现，亦有可能是通过互相调用 API 的方式实现，再者无非是通过数据库实现类似消息队列的功能……而我个人，更推崇通过事件来处理，因为它更接近人类思考的本质，希望在适当的时机来“通知”对方，而论询实际上是一种相当低效的沟通方式。一个订单被创建，一条记录被修改，本质上都是一个特定事件，而在业务上对此感兴趣的任何第三方，都可以去订阅这个事件，这就是事件驱动的思想。\n领域事件\r我拜读了几篇关于“领域驱动设计(DDD)”文章，了解到 DDD 中有领域事件和集成事件的概念。最直接的体会就是，DDD 是主张“充血模型”的，它把事件附加到实体上，最大的好处就是，可以让“发送(Dispatch)”事件的代码，集中地放在一个地方。而我们现在的业务代码，基本是高度耦合的，每次去添加一个事件的时候，最担心地就是遗漏了某个地方。按照 DDD 的思想，实现领域事件，最常用的伎俩是重写 DbContext 的 SaveChanges()方法，或者在 EF 中去指定 DbContext 的 Complate 事件。这里同样借助了 ChangeTracker 来实现：\npublic class OrderContext : DbContext { public async Task\u0026lt;bool\u0026gt; SaveChangesAsync(CancellationToken cancellationToken = default(CancellationToken)) { var aggregateRoots = dbContext.ChangeTracker.Entries().ToList(); await _eventDispatcher.DispatchAsync(aggregateRoots,cancellationToken); var result = await base.SaveChangesAsync(); } } 其中，_eventDispatcher作为事件分发器来分发事件，它实现了IEventDispatcher接口。相对应地，事件订阅者需要实现IDomainEventHandler接口。如果是最简单的进程内通信，那么你需要一个容器来管理IDomainEvent和IDomainEventHandler间的关系；而如果是不同微服务间的通信，那么你需要引入RabbitMQ或者kafka这类消息队列中间件。\npublic interface IDomainEvent { } public interface IDomainEventHandler\u0026lt;in TDomainEvent\u0026gt; where TDomainEvent : IDomainEvent { Task HandleAysnc(TDomainEvent @event, CancellationToken cancellationToken = default); } public interface IEventDispatcher { Task DispatchAsync\u0026lt;TDomainEvent\u0026gt;( TDomainEvent @event, CancellationToken cancellationToken = default) where TDomainEvent :IDomainEvent; } 所以，你现在问我怎么样做数据同步好，我一定会说，通过事件来处理。因为这样，每一条数据的新增、更新、删除，都可以事件的形式发布出去，而关心这些数据的下游系统，则只需要订阅这些事件，该干嘛好嘛，何乐而不为呢？搞什么中间表，打什么标记，用数据库一遍遍地实现消息队列有意思吗？同样地，你会意识到，仓储模式，哪怕 ORM 换成 Dapper，我们一样可以去发布这些事件，增量同步自然是要比全量同步优雅而且高效的。最重要的是，程序员再不需要到处找地方埋点了，你看我博客更新频率这么低，不就是因为这些事情浪费了时间吗(逃？因为，全量 + 实时同步就是一个非常愚蠢的决定。\n本文小结 本文分别针对EF Core和Dapper实现了数据库审计的功能。对于前者，主要是通过重写 DbContext 的SaveChanges()方法来实现，而EF及EF Core中的ChangeTracker则提供了一种获取数据库表记录变化前后值的能力。而对于后者，主要是实现了Dapper的仓储模式，在此基础上结合Castle的动态代理功能，对仓储接口进行拦截，以此实现审计日志的记录功能。整体来看，后者对代码的侵入性要更小一点，理论上我们可以实现EF或EF Core的仓储模式，这样两者在实现上会更接近一点，当然，更直接的方案是去拦截SaveChanges()方法，这和我们使用继承的目的是一样的，由于 Dapper 本身没有ChangeTracker，所以，在处理Update()相关的仓储接口时，都需要先查询一次数据库，这一点是这个方案里最大的短板。而顺着这个方案扩展下去，我们同样可以挖掘出一点DDD里领域事件的意味，这就变得很有意思了，不是吗？这篇博客就先写到这里吧……再见\n","date":"2020-04-24T08:20:32Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1289244227/","slug":"1289244227","tags":["EF","Dapper","审计"],"title":"通过 EF/Dapper 扩展实现数据库审计功能"},{"categories":["编程语言"],"content":"博主曾经在「声明式 RESTful 客户端 WebApiClient 在项目中的应用」这篇博客中，介绍过.NET 平台下的“Retrofit”——WebApiClient，它是一种声明式的 RESTful 客户端，通过动态代理来生成 Http 调用过程代码，而调用方只需要定义一个接口，并使用相关“注解”对接口进行修饰即可，类似的实现还有Refit，是一种比 HttpWebRequest、HttpClient 和 RestSharp 更为优雅的接口调用方式。在今天这篇博客中，我想聊聊 WebApiClient 中动态路由的实现与使用。\n一个典型的 WebApiClient 使用流程如下，首先定义一个接口，并使用“注解”对接口进行修饰：\npublic interface ISinoiovApiClient : IHttpApiClient { /// \u0026lt;summary\u0026gt; /// 运单取消接口 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpPost(\u0026#34;/yl/api/waybill/cancel\u0026#34;)] [AuthorizeFilter] [LoggingFilter] [JsonReturn] ITask\u0026lt;BaseApiResult\u0026lt;object\u0026gt;\u0026gt; CancelShipment([JsonContent]BaseShipmentDto shipment); } 接下来，调用就变得非常简单：\nvar config = new HttpApiConfig () { HttpHost = new Uri (baseUrl) }; using (var client = HttpApiClient.Create\u0026lt;ISinoiovApiClient\u0026gt; (config)) { var result = await client.CancelShipment (new BaseShipmentDto () { }); //TODO：TODO的意思就是永远都不做 } 有多简单呢？简单到调用的时候我们只需要给一个 baseUrl 就可以了！然而，如果你真这么想的话，就太天真了！虽然现在是一个遍地都是微服务和容器的时代，可是因为 RESTful 风格本身的约束力并不强，实际使用中难免会出现以下情况：\n//测试环境 http://your-domain.com/test/api/waybill/cancel //正式环境 http://your-domain.com/prod/api/waybill/cancel 是的，你猜对了，实际运作过程中，测试环境和正式环境不单单会使用不同的域名，可能还会使用不同的路由，虽然，理论上两个环境的程序应该完全一样，应该使用相同的路由。这样子就让我们有一点尴尬，因为我们的路由是写在特性(Attribute)里的，这玩意儿的实例化是附着在对应的类上面的，并且在整个运行时期间是不允许修改的。所谓**“兵来将挡水来土掩”**，接下来，我们来考虑如何解决这个问题。\n使用[Uri] 第一种思路是给接口加一个 Url 参数，此时，调整接口方法声明如下：\n/// \u0026lt;summary\u0026gt; /// 运单取消接口 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpPost] [AuthorizeFilter] [LoggingFilter] [JsonReturn] ITask\u0026lt;BaseApiResult\u0026lt;object\u0026gt;\u0026gt; CancelShipment([Uri]string url, [JsonContent]BaseShipmentDto shipment); 这种方式可以解决问题，可我使用 WebApiClient 的原因之一，就是我不喜欢在客户端(调用方)维护这些地址。作为一个 ApiCaller，在微服务架构流行以来，接口越来越多，逐渐呈现出爆炸式增加的趋势。当我作为一个后端工程师的时候，编写接口是件非常惬意的事情。可当我为了\u0026quot;全栈工程师\u0026quot;的虚名，去做一个面无表情的 ApiCaller 的时候，我是不情愿去配置这些 Url 的，有本事你把配置中心搭起来啊！所以，道理我都懂，But，我拒绝！\n使用{foobar} 第二种思路是同样是给接口增加一个片段参数，此时，调整接口方法声明如下:\n/// \u0026lt;summary\u0026gt; /// 运单取消接口 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpPost(\u0026#39;/{prefix}/api/waybill/cancel)] [AuthorizeFilter] [LoggingFilter] [JsonReturn] ITask\u0026lt;BaseApiResult\u0026lt;object\u0026gt;\u0026gt; CancelShipment([JsonContent]BaseShipmentDto shipment, string prefix = \u0026#34;yl\u0026#34;); 这种方式和第一种方式原理一致，无非是需要配置的参数从多个变成一个。我个人更喜欢这种方式，为什么呢？可能我认为专业的 Api 接口会有版本的概念，类似于：\n//版本号路由 /api/v2.0/abc/xyz //查询参数路由 /api/abc/xyz?v=2.0 这样，我们就在无形中解决了一类问题，对于第二种形式，版本号以查询参数的方式出现，我们选择在过滤器中AddUrlQuery()或者使用[PathQuery]来解决。如果让我选择，我一定会选择这种方式，因为它更优雅一点吗？不，因为我懒，写程序的终究目的就是为了不写代码，就好像一个程序试图去杀死它自己的进程。\n使用服务发现 第三种思路，我承认有一点赌的成份，你猜对接客户的接口的时候，会不会提供服务发现这套基础设施给你？可如果在自己的项目里有服务发现，还需要再配置每个服务的 Url 吗？这样想是不是觉得还不错，的确，我们在微服务架构里引入 WebApiClient 这种类 Retrofit 的库，本质上还是为了弱化服务的界限感，如果我调用一个服务和调用本地方法的体验一样，那么，这是什么呢？不用怀疑，这就是 RPC(大雾)。这里，我实现了一个简单的示例：\n//通过Consul获取可用地址 var services = await _consul.Health.Service(\u0026#34;SinoiovApi\u0026#34;, string.Empty, true); var serviceUrls = services.Response.Select(s =\u0026gt; $\u0026#34;{s.Service.Address}:{s.Service.Port}\u0026#34;).ToList(); serviceUrl = serviceUrls[new Random().Next(0, serviceUrls.Count - 1)]; //今天的你我，怎样重复昨天的故事 var config = new HttpApiConfig () { HttpHost = new Uri (serviceUrl) }; using (var client = HttpApiClient.Create\u0026lt;ISinoiovApiClient\u0026gt; (config)) { var result = await client.CancelShipment (new BaseShipmentDto () { }); //TODO：TODO的意思就是永远都不做 } 当然，我说了这有赌的成份，前提是这些服务在 Consul 中提前注册，这一点相信大家都知道啦！WebApiClient 的作者提供了类似扩展:WebApiClient.Extensions.DiscoveryClient，该扩展基于Steeltoe打造，感兴趣的朋友，可以前去了解一下。\n","date":"2020-04-02T10:26:53Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2488769283/","slug":"2488769283","tags":["RESTful","Retrofit","WebApi"],"title":"WebApiClient 中动态路由的实现与使用"},{"categories":["编程语言"],"content":"Hi，各位朋友，大家好！欢迎大家关注我的博客，我的博客地址是: https://blog.yuanpei.me。今天是远程办公以来的第一个周末，虽然公司计划在远程两周后恢复正常办公，可面对着每天都有人离开的疫情，深知这一切都不会那么容易。窗外的阳光透过玻璃照射进屋子，这一切都昭示着春天的脚步渐渐近了。可春天来了，有的人却没有再回来。那些在 2019 年结束时许下的美好期待、豪言壮语，在这样一场灾难面前，终究是如此的无力而苍白。可不管怎么样，生活还是要继续，在这些无法出门的日子里，在这样一个印象深刻的春节长假里，除了做好勤洗手、多通风、戴口罩这些防疫保护措施以外，博主还是希望大家能够抽空学习，通过知识来充实这“枯燥\u0026quot;的生活。所以，从今天开始，我将为大家带来 .NET Core + ELK 搭建可视化日志分析平台 系列文章，希望大家喜欢。\n什么是 ELK 当接触到一个新的事物的时候，我们最好是从它的概念开始入手。那么，什么是 ELK 呢？ELK，是 Elastaicsearch 、 Logstash 和 Kibana 三款软件的简称。其中，Elastaicsearch 是一个开源的全文搜索引擎。如果你没有听说过它，那至少应该听说过 Lucene 这个开源搜索引擎。事实上，Elastaicsearch 是 Lucene 的封装，它提供了 REST API 的操作接口 。而 Logstash 则是一个开源的数据收集引擎，具有实时的管道，它可以动态地将不同的数据源的数据统一起来。最后，Kibana 是一个日志可视化分析的平台，它提供了一系列日志分析的 Web 接口，可以使用它对日志进行高效地搜索、分析和可视化操作。至此，我们可以给 ELK 一个简单的定义：\nELK 是一个集日志收集、搜索、日志聚合和日志分析于一身的完整解决方案。\n下面这张图，展示了 Elastaicsearch 、 Logstash 和 Kibana 三款软件间的协作关系。可以注意到，Logstash 负责从应用服务器收集日志。我们知道，现在的应用程序都是跨端应用，程序可能运行在 PC、移动端、H5、小程序等等各种各样的终端上，而 Logstash 则可以将这些不同的日志信息通过管道转换为统一的数据接口。这些日志将被存储到 Elasticsearch 中。我们提到 Elastaicsearch 是一个开源的全文搜索引擎，故而它在数据查询上相对传统的数据库有着更好的优势，并且 Elasticsearch 可以根据需要搭建单机或者集群。最终，Kibana 从 Elasticsearch 中查询数据并绘制可视化图表，并展示在浏览器中。在最新的 ELK 架构中，新增了FireBeat这个软件，它是它是一个轻量级的日志收集处理工具(Agent)，适合于在各个服务器上搜集日志后传输给 Logstash。\nELK-01.png\r总而言之，ELK 可以让我们以一种更优雅的方式来收集日志，传统的日志收集通常会把日志写到文件或者数据库中。前者，不利于日志的集中管理和查询；后者，则无法应对海量文本检索的需求。所以，使用 ELK 可以为我们带来下面这些便利：分布式日志数据集中式查询和管理；系统监控，譬如对系统硬件和应用各个组件的监控；故障排查；报表功能；日志查询，问题排查，上线检查； 服务器监控、应用监控、错误报警；性能分析、用户行为分析、时间管理等等。\n如何安装 ELK 安装 ELK 的方式，首推以 Docker 方式安装。关于 Docker 的安装、使用请大家查阅官方文档：https://docs.docker.com/。这里我假设大家都已经掌握了 Linux 和 Docker 的使用。首先我们拉取 ELK 镜像：\ndocker pull sebp/elk 接下来，我们利用此镜像来运行一个容器:\ndocker run -p 5601:5601 -p 9200:9200 -p 5044:5044 --name elk sebp/elk 通常情况下，完成这两个步骤以后，我们就完成了 ELK 安装。此时，我们可以在浏览器中输入地址：http//localhost:9200，这是 Elasticsearch 的默认端口。如果浏览器中返回了了类似下面的信息，则表示 ELK 安装成功。这里是博主获得的关于 Elasticseach 的信息：\n{ \u0026#34;name\u0026#34; : \u0026#34;elk\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;elasticsearch\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;GGlJrOvtT2uSfoHioLCWww\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;7.5.2\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;tar\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;8bec50e1e0ad29dad5653712cf3bb580cd1afcdf\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2020-01-15T12:11:52.313576Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;8.3.0\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;6.8.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;6.0.0-beta1\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; } 接下来，我们继续在浏览器中输入地址：http://localhost:5601/app/kibana。显然，这是 Kibana 的默认地址，至此 ELK 的 “庐山真面目” 终于揭晓，首次安装完 ELK，Kibana 的界面应该试类似下面这样：\nELK的庐山真面目\r按照指引，我们可以添加示例数据来感受下ELK全家桶的魅力：\nELK示例 - Global Flight Dashboard\r这样，我们就完成 ELK 环境的快速搭建。下面，按照惯例，我们将实现一个 “Hello World” 级别的实例，即：通过 ELK 来收集一个 ASP .NET Core 应用的日志信息。为了让这个示例尽可能地简单一点，我们选择了直接向Elasticsearch 写入日志的方式，这里选择的日志库是 Serilog 。\nHello ELK 本文所用的例子已发布到Github。首先，我们准备一个 ASP.NET Core 的项目，MVC 或者 Web API 都可以。接下来，在项目中引入三个依赖项：Serilog、Serilog.Extensions.Logging 和 Serilog.Sinks.ElasticSearch。对于前两个，如果大家用过 Log4Net 或者 NLog 应该会感到非常熟悉啦，这一点不在赘述。而第三个，从名字就可以看出来这是冲着 Elasticsearch 来的，因为这是这个系列的第一篇文章，所以，我们直接写 Elasticsearch 即可。Logstash 管道相关的内容，是一个非常复杂的东西，我们会在下一篇文章中单独来讲。接下来，主要是Serilog在 ASP.NET Core 中的配置。首先是 Startup 类，在构造函数中初始化 Serilog ：\npublic Startup(IConfiguration configuration) { Log.Logger = new LoggerConfiguration() .Enrich.FromLogContext() .MinimumLevel.Debug() .WriteTo.Elasticsearch( new ElasticsearchSinkOptions(new Uri(\u0026#34;http://localhost:9200\u0026#34;)) { MinimumLogEventLevel = LogEventLevel.Verbose, AutoRegisterTemplate = true }) .CreateLogger(); Configuration = configuration; } 还记得 http://localhost:9200 这个地址是什么吗？不错，这是 Elasticsearch 的默认地址，所以，这部分代码主要的作用就是告诉 Elasticsearch ，接下来的日志信息都写到 Elasticsearch 中。为了让日志的信息更丰富一点，我们这里设置最小的日志事件级别为 Verbose 。接下来，在 ConfigureServices() 方法中注册 ILogger 实例：\nservices.AddLogging(loggingBuilder =\u0026gt; loggingBuilder.AddSerilog(dispose: true)); 接下来，在业务层增加日志：\nprivate readonly ILogger _logger = Log.Logger; [HttpGet] public double Add(double n1, double n2) { _logger.Information($\u0026#34;Invoke {typeof(CoreCalculatorService).Name}/Add: {n1},{n2}\u0026#34;); return n1 + n2; } 至此，ELK 在 ASP.NET Core 中的集成已经全部结束，这意味着我们所有的日志都会写入到 ELK 中。那么，要到那里去找这些日志信息呢？且听博主娓娓道来。我们在 Kibana 中点击左侧导航栏最底下的设置按钮，然后再点击右侧的 Create index pattern 按钮创建一个索引。什么叫做索引呢？在 Elasticsearch 中索引相当于一张\u0026quot;表\u0026quot;，而这个“表”中的一条行记录则被称为 Document，如图：\n为Kibana创建索引1\r创建索引的时候，会发现列表中列出了目前Elasticsearch中可用的数据。以博主为例，这里的 logstash-2020.02.15 就是本文中的 ASP.NET Core 应用产生的日志信息。在这里，我们可以通过一个模糊匹配来匹配同种类型的数据。通常这里需要我们选择一个过滤字段，我们选择时间戳即可：\n为Kibana创建索引2\r创建完索引，就可以看到目前收集的日志信息了，在此基础上，我们可以做进一步的检索、过滤，来生成各种各样的“查询”。而每一个“查询”实际上就是一个数据源。我们就可以利用这些数据源来完成可视化，这是利用 ELK 进行可视化分析的一般流程：\n在Kibana中查看当前日志信息\r下面是博主自己制作的一个简单的可视化看板，果然很长时间没有再用过 Kibana ，我都快忘记了要怎么做一个折线图。这实在是一篇迟到的博客，我早该在 2019 年的时候就完成这个系列的，这要命的拖延症啊，虽然没有新冠病毒恐怖，可终究不是什么好习惯！\n一个简单的可视化看板\r本文小结 这篇博客是这个系列的第一篇，是一篇珊珊来迟的博客，因为博主早在 2019 年就开始着手学习 ELK。考虑最新公司有使用 ELK 的打算，而因疫情又让博主有充足的时间，所以，博主决定把 ELK 相关的内容花点时间梳理出来。ELK 是一个集日志收集、搜索、日志聚合和日志分析于一身的完整解决方案。博主计划在接下来的篇幅中介绍 Logstash 和 FileeBeat 管道配置、Docker 容器内的日志收集、以及自定义日志组件开发这些话题，希望大家继续关注我的博客。以上就是这篇博客的全部内容啦，晚安！\n","date":"2020-02-15T16:01:13Z","image":"/posts/3687594958/lighthouse-gd3af11c39_1280.jpg","permalink":"https://qinyuanpei.github.io/posts/3687594958/","slug":"3687594958","tags":[".NET Core","ELK","日志","监控"],"title":".NET Core + ELK 搭建可视化日志分析平台(上)"},{"categories":["独立博客"],"content":"最近给博客做了升级，从 3.x 升级到了 4.x，主要是在官网看到了关于静态页面生成效率提升的内容。众所周知，Hexo 在文章数目增加以后会越来越慢。博主大概是从 14 年年底开始使用 Hexo 这个静态博客的，截止到目前一共有 176 篇博客，其中的“慢”可想而知，中间甚至动过使用 Hugo 和 VuePress 的念头，所以，听说有性能方面的提升，还是决定第一时间来试试。整个升级过程挺顺利的，唯一遇到的问题是关于外部链接检测方面的，具体可以参考这里。今天，博主主要想和大家分享下关于如何使用jsDelivr来为博客提供免费、高效的 CDN 服务，希望对大家有所帮助。\njsDelivr是一个免费、快速和可信赖的 CDN 加速服务，官网上声称它每个月可以支撑680亿次的请求。博主是在去年年底的时候，偶然了解到这个服务的存在，这次趁着疫情肆虐的间隙，终于把这个服务集成到了博客中。更重要的是，这个服务在 Github 上是开源的。目前，它提供了针对npm、Github和WordPress的加速服务，只需要一行代码就可以获得加速效果，以常用的jQuery和Bootstrap为例：\n// load jQuery v3.2.1 https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js // load bootstrap v4.4.1 https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.js 这意味着我们只需要发布一个 npm 的包，就可以使用它提供的加速服务。CDN 加速的好处我这里就不再多说了，只要我们的项目中用到了第三方的静态资源，譬如 JavaScript/CSS 等等都应该考虑接入到 CDN 中。有人常常担心 CDN 挂掉或者是私有化部署无法接入外网环境。我想说，我们目光应该长远一点，现在早已不是早年那种单打独斗式的开发模式了，我们不可能把所有资源都放到本地来。随着云计算的概念越发地深入人心，越来越多的基础服务都运行在一台又一台虚拟化的“云服务器”上，这种情况下，搞这种集中化配置的做法，是完全违背分布式的发展趋势的。\n如果说，针对 npm 包的 CDN 加速服务离我们还有点遥远，因为我们大多数情况下都是在使用别人写好的库。那么，接下来，针对 Github 的 CDN 加速服务应该会让我们无比兴奋吧，毕竟 Github Pages 的“慢”大家是可以感受得到的。不然，为什么大家要用 Coding Pages 做国内/国外的双线部署呢？首先，我们在浏览器里输入下面这个地址：https://cdn.jsdelivr.net/gh/qinyuanpei/qinyuanpei.github.io@latest/\njsDelivr提供的CDN加速资源\r此时，可以注意到，jsDelivr可以把我们 Github 上的资源呈现出来，只要我们在 Github 上发布过相应的版本即可。这里的版本，可以理解为一次 Release，对应 Git 中 tag 的概念，虽然 Github 现在引入了包管理器的概念，试图统一像 npm、nuget、pip 等等这样的包管理器。它提供的 CDN 服务有一个基本的格式：\nhttps://cdn.jsdelivr.net/gh/user/repo@version/file\n如果大家感兴趣，可以把这里的 user 和 repo 改成自己的来体验一番。需要注意的是，这里的版本号同样可以换成 Commit ID 或者是分支的名称。我个人建议用 tag，因为它通常携带了版本号信息，语义上要更好一点。那么，顺着这个思路，我们只要把 Hexo 中的资源的相对路径改为 jsDelivr 的 CDN 加速路径就好啦！为了让切换更加自如，这里我们为 Hexo 写一个 Helper，它可以理解为 Hexo 中的辅助代码片段。我们在 \u0026lt;YouTheme\u0026gt;/scripts/ 目录下新建一个plugins.js 文件，这样 Hexo 会在渲染时自动加载这个脚本文件：\nconst source = (path, cache, ext) =\u0026gt; { if (cache) { const minFile = `${path}${ext === \u0026#39;.js\u0026#39; ? \u0026#39;.min\u0026#39; : \u0026#39;\u0026#39;}${ext}`; const jsdelivrCDN = hexo.config.jsdelivr; return jsdelivrCDN.enable ? `//${jsdelivrCDN.baseUrl}/gh/${jsdelivrCDN.gh_user}/${jsdelivrCDN.gh_repo}@latest/${minFile}` : `${minFile}?v=${version}` } else { return path + ext } } hexo.extend.helper.register(\u0026#39;theme_js\u0026#39;, (path, cache) =\u0026gt; source(path, cache, \u0026#39;.js\u0026#39;)) hexo.extend.helper.register(\u0026#39;theme_css\u0026#39;, (path, cache) =\u0026gt; source(path, cache, \u0026#39;.css\u0026#39;)) 接下来，修改布局文件，项目中的 JavaScript 和 CSS 文件，均通过 theme_js() 和 thems_css() 两个函数引入：\n//加载JavaScript \u0026lt;script src=\u0026#34;\u0026lt;%- url_for(theme_js(\u0026#39;assets/scripts/search\u0026#39;, cache)) %\u0026gt;\u0026#34; async\u0026gt;\u0026lt;/script\u0026gt; //加载CSS \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;\u0026lt;%- url_for(theme_css(\u0026#39;/assets/styles/style\u0026#39;, cache)) %\u0026gt;\u0026#34;\u0026gt; 既然是否使用 CDN 加速是可配置的，我们要在 _config.yml 文件中添加相应的配置项：\n# jsdelivr CDN jsdelivr: enable: true gh_user: qinyuanpei gh_repo: qinyuanpei.github.io baseUrl: cdn.jsdelivr.net 除此以外，我们还需要在部署博客的时候，生成一个名为 latest 的 tag。虽然官网上说，在引用 CDN 的时候版本号可以省略，不过经过博主反复尝试，不带版本号并不会指向正确的版本，有些资源文件会报 404，因为这部分资源文件回滚以后发现还是没有。所以，最后博主只好把这个版本号给固定下来了，这样又引入一个新问题，即：每次部署的时候都要先删除远程的 latest。所以，这块儿的Travis CI脚本看起来会有点讨厌，如果大家有更好的方案，欢迎大家在博客中留言：\ngit tag latest git push --force --quiet \u0026#34;https://${CI_TOKEN}@${GH_REF}\u0026#34; master:master --tags 好了，现在重新生成、部署，来看看效果吧： Coding Pages速度\rGithub Pages速度\r感觉效果还不错，Github Pages 比平时要快很多，博主顺便就给 Coding Pages 启用了 CDN 加速。话说，看到这张图的时候总是感慨，如果肺炎疫情地图能像这两张图一样就好啦！面对这场无声的战役，有很多人一直在一线抗击病魔，还有很多人默默无闻地在支援武汉。或许，宅在家里的你我，什么都做不了，可即便如此，还是让我们一起来祈祷疫情快点结束吧，因为春天都要来了呢……好了，这就是这篇博客的全部内容啦，谢谢大家！\n2020/02/13 更新 在此之前，博主提到版本号的问题，即每一次在 CDN 上生成的版本，怎样体现到 Hexo 中引用的资源上面。当时采用了一个取巧的方法，Hexo 中固定版本号为 latest，然后每次都推送这个 tag。这样引发一个问题，每次都先去远程删除这个 tag，显然这不是我期望的解决方案。最终，我采用的方案是，通过 Travis CI 编译部署的时候，首先导出变量 $TRAVIS_BUILD_NUMBER 到一个文本文件中，然后Hexo在生成静态页面的时侯，从这个文本文件中读取该变量的值作为版本号，这样每次编译部署的时候，我们总能获得一个新的 tag，而这个 tag 和 Hexo 中引用的资源版本一致，这样就彻底解决了这个遗留问题。修改后的 plugins.js 文件内容如下：\nvar fs = require(\u0026#39;fs\u0026#39;); var version = \u0026#39;latest\u0026#39; fs.readFile(\u0026#39;./BUILD_NUMBER.txt\u0026#39;, function (error, data) { if (error) { console.log(\u0026#39;load BUILD_NUMBER.txt fails, \u0026#39; + error) } else { version = data.toString().trim(); } }); const source = (path, cache, ext) =\u0026gt; { if (cache) { const minFile = `${path}${ext === \u0026#39;.js\u0026#39; ? \u0026#39;.min\u0026#39; : \u0026#39;\u0026#39;}${ext}`; const jsdelivrCDN = hexo.config.jsdelivr; return jsdelivrCDN.enable ? `//${jsdelivrCDN.baseUrl}/gh/${jsdelivrCDN.gh_user}/${jsdelivrCDN.gh_repo}@${version}/${minFile}` : `${minFile}?v=${version}` } else { return path + ext } } hexo.extend.helper.register(\u0026#39;theme_js\u0026#39;, (path, cache) =\u0026gt; source(path, cache, \u0026#39;.js\u0026#39;)) hexo.extend.helper.register(\u0026#39;theme_css\u0026#39;, (path, cache) =\u0026gt; source(path, cache, \u0026#39;.css\u0026#39;)) 修改后的 .travis.yml 文件可以在 这里 获取。\n","date":"2020-02-05T19:01:00Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1417719502/","slug":"1417719502","tags":["Hexo","CDN","jsDelivr"],"title":"使用 jsDelivr 为 Hexo 博客提供高效免费的CDN加速"},{"categories":["编程语言"],"content":"有时候，版本更新太快并不是一件好事。虽然，两周一个迭代的“敏捷”开发依然被客户嫌弃交付缓慢，可一边是前端领域“求不要再更新了，学不动了”的声音，一边则是.NET Core从1.x到2.x再到3.x的高歌猛进。版本更新太快，带来的是API的频繁变动，无法形成有效的知识沉淀，就像转眼到了2020年，Python 2.x和Windows 7都引来了“寿终正寝”，可能你都还没有认真地学习过这些知识，突然就被告知这些知识要过期了，想想还是觉得挺疯狂啊。最近一直在捣鼓，如何让.NET Core应用跑在Heroku平台上，因为Docker镜像里使用最新的.NET Core 3.1运行时，所以，痛定思痛之余，决定把手头项目升级到3.1。上一次痛苦还是在2.1升级2.2，这还真没过多长时间。所以呢，这篇博客主要梳理下从2.2升级到3.1过程中遇到的问题。\n更新项目文件 调整目标框架为netcoreapp3.1 删除引用项：Microsoft.AspNetCore.App、Microsoft.AspNetCore.Razor.Design 删除AspNetCoreHostingModel，如果项目文件中的值为InProcess(因为ASP.NET Core 3.0 或更高版本项目默认为进程内承载模型） 更新程序入口 CreateWebHostBuilder()方法的返回值类型由IWebHostBuilder调整为IHostBuilder 增加引用项：Microsoft.Extensions.Hosting Kestrel配置变更至ConfigureWebHostDefaults()方法 public static IHostBuilder CreateWebHostBuilder(string[] args) =\u0026gt;\rHost.CreateDefaultBuilder(args)\r.ConfigureWebHostDefaults(webBuilder =\u0026gt;\r{\rwebBuilder.ConfigureKestrel(serverOptions =\u0026gt;\r{\r// Set properties and call methods on options\r})\r.UseStartup\u0026lt;Startup\u0026gt;();\r}); 如果通过 HostBuilder手动创建宿主，则需要在 ConfigureWebHostDefaults()方法中显式调用·UseKestrel()：\npublic static void Main (string[] args) {\rvar host = new HostBuilder ()\r.UseContentRoot (Directory.GetCurrentDirectory ())\r.ConfigureWebHostDefaults (webBuilder =\u0026gt; {\rwebBuilder.UseKestrel (serverOptions =\u0026gt; {\r// Set properties and call methods on options\r})\r.UseIISIntegration ()\r.UseStartup\u0026lt;Startup\u0026gt; ();\r})\r.Build ();\rhost.Run ();\r} 更新Startup Configure()方法第二个参数由``IHostingEnvironment调整为IWebHostEnvironment(需要引用Microsoft.Extensions.Hosting`) 从管道中删除UseMvc()扩展方法，相应地，删除AddMvc()及其链式调用相关方法 AddMvc()等价于AddRazorPages() + AddControllersWithViews() AddControllers()对应WebApi模板，AddControllersWithViews()对应MVC模板， AddRazorPages()对应SPA模板 路由注册由传统路由调整为终结点路由： public void Configure(IApplicationBuilder app, IWebHostEnvironment env)\r{\rapp.UseStaticFiles();\rapp.UseRouting();\rapp.UseCors();\rapp.UseAuthentication();\rapp.UseAuthorization();\rapp.UseEndpoints(endpoints =\u0026gt;\r{\r//SignalR路由 endpoints.MapHub\u0026lt;ChatHub\u0026gt;(\u0026#34;/chat\u0026#34;);\r//RazorPages路由\rendpoints.MapRazorPages()\r//特性路由(WebApi)\rendpoints.MapControllers();\r//控制器路由(MVC)\rendpoints.MapControllerRoute(\u0026#34;default\u0026#34;, \u0026#34;{controller=Home}/{action=Index}/{id?}\u0026#34;);\r});\r} 如果希望继续使用传统路由，则可以使用下列方法任一：\nservices.AddMvc(options =\u0026gt; options.EnableEndpointRouting = false);\rservices.AddControllers(options =\u0026gt; options.EnableEndpointRouting = false);\rservices.AddControllersWithViews(options =\u0026gt; options.EnableEndpointRouting = false);\rservices.AddRazorPages().AddMvcOptions(options =\u0026gt; options.EnableEndpointRouting = false); 序列化/反序列化 从.NET Core 3.0 开始，System.Text.Json默认作为替代Newtonsoft.json的新一代JSON API 直接从.NET Core 3.0 创建的SignalR项目，服务端返回的JSON数据存在大小写的问题，这是由System.Text.Json引起的。解决方案是： services.AddSignalR()\r.AddJsonProtocol(options =\u0026gt; options.PayloadSerializerOptions.PropertyNamingPolicy = null); 同理，对于该方案对于services.AddControllers()一样有效，前提是项目中使用了System.Text.Json。同理，对于SignalR的客户端项目，我们有：\nnew HubConnectionBuilder()\r.WithUrl(\u0026#34;/chatHub\u0026#34;)\r.AddJsonProtocol(options =\u0026gt;\r{ //TODO\r})\r.Build(); SignalR的JavaScript客户端由@aspnet/signalr 调整为为 @microsoft/signalr： const signalR = require(\u0026#34;@microsoft/signalr\u0026#34;);\rlet connection = new signalR.HubConnectionBuilder().withUrl(url).build(); 如果希望继续使用Newtonsoft.json，则需要安装AspNetCore NewtonsoftJson。相应地，需要显式调用AddNewtonsoftJson()扩展方法： services.AddControllers()\r.AddNewtonsoftJson(options =\u0026gt; {\roptions.SerializerSettings.ContractResolver = new CamelCasePropertyNamesContractResolver();\r}); 同样地，AddNewtonsoftJson()支持AddControllers()， AddControllersWithViews()， AddRazorPages()所有方法\n疑难杂症 升级后提示无法加载类型：Microsoft.AspNetCore.Mvc.MvcJsonOptions，解决方案是： 升级Swashbuckle.AspNetCore至最新版本(5.0+)，调整Swagger中间件配置代码： services.AddSwaggerGen(swagger =\u0026gt;\r{\r//这里发生了变化，需要引用：Microsoft.OpenApi.Models\rswagger.SwaggerDoc(\u0026#34;v1\u0026#34;, new OpenApiInfo { Title = \u0026#34;ynamic WebApi\u0026#34;, Version = \u0026#34;v1.0\u0026#34; });\r}); 安装完 .NET Core 3.x，使用dotnet build编译项目提示找不到Microsoft.NET.Sdk.Web。解决方案是： 升级2.2的时候，调整项目文件中的Microsoft.NET.Sdk.Web为Microsoft.NET.Sdk可以解决，而这个方法在3.x以后失效。 此时，可以检查环境变量MSBuildSDKsPath中的SDK版本和实际版本是否一致，尤其是像博主这样从2.0一路升级到3.x的朋友，应该都会遇到这个问题。 参考链接 从 ASP.NET Core 2.2 迁移到3.0 升级 ASP.NET Core 3.0 设置 JSON 返回 PascalCase 格式与 SignalR 问题 Incompatibility with ASP.NET Core 3.0 ","date":"2020-01-22T10:23:08Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3099575458/","slug":"3099575458","tags":[".NET Core","SignalR","迁移"],"title":"从 .NET Core 2.2 升级到 3.1 的踩坑之旅"},{"categories":["生活感悟"],"content":"我以为，时间是这个世界上最残忍的存在。因为，无论如何，你都无法阻止这如齿轮般互相咬合的时光机器，即使这世界上并没有所谓的“永动机”。习惯于沉默的时间之轮，你在或者不在，丝毫不影响它衡量宇宙万物的尺度。也许，是因为我们所拥有的时间太过短暂，所以，当一切都流失殆尽时，我们所能寄托的便只有不那么确定的未来。时间怎么会变得残忍呢？它无喜无悲俯视众生，倒像是一位入定参禅的老僧，有情感的分明是我们这些人类啊。\n孔夫子说：发奋忘食，乐以忘忧，不知老之将至。而有时候，你甚至都没有怎么“发奋”、“快乐”，就不知老之将至了。也许，花了不少时间在工作甚至加班上面，如果这些可以算作“发奋”，老之将至才是符合人类生理趋势的必然。上个周末去看了《叶问 4》的完结篇，突然发现，无论是戏里的叶师傅，还是戏外的甄子丹，居然都出现衰老的迹象。而童年记忆中的黄飞鸿则永远是白鹤亮翅的宗师气象，大概是因为《黄飞鸿》系列不曾像《叶问》系列，在功夫片的体裁外，多了一点传记电影的味道。有人说，这是华语功夫片的一次谢幕，而我更愿意理解为，这是演员同过去的自己的阶段性告别。人总是会老的，从公交车上为老人让座的宣传广播，到父母见一次就白一次的鬓角，再到一天比一天翻得飞快的日历……你，又是如何同过去的自己告别的呢？\nFlag 这种东西，是一种不立没有所谓“仪式感”，而立了又难免让你自愧虚度时光的存在。在过去的一年里，索性一个 Flag 都不立，这样看过来的时候，人生充满了一种荒芜感：微信公众号运营失败，因为缺少那个想让你运营下去的观众；博客写作无功无过，每月 1 至 2 篇文章，作为阶段性的回顾尚可；懒散/拖延症中晚期，此时此刻还有来自 2019 年的 Todo；通过微软小英练习单词和口语，这一点没能坚持下来，更不必说连 50 音图都没学会的日语；没有被消费主义洗脑，量入为出、精简开支(穷得如此清新脱俗)；一个人做饭没怎么坚持下来，单单是准备食材就挺麻烦了，更何况炒菜锅坏了一直没换新的呢；工作快 5 年了，我还是没太大长进，还是喜欢怼人怼空气，沟通能力是挺重要了，可惜精力都被开会、扯皮这种事情消耗得差不多了啊；阅读量还是太少，从公司/图书馆借来的书，一般都能找时间去读，而下载下来放 Kindle 里的，读着读着就被遗忘了，订阅的 RSS 读起来倒没有这种压力，果然“书非借不能读也”。《一代宗师》里说，人活得是一个起伏，而我这一年是没能活成一杯烈酒的。人喜欢用平凡是真自我安慰，可都怕活成最平庸的样子，用天哥的话说，做人没意思啊！\n醒来的时候和往常一样，一样到和平时上班没什么区别，直到我坐上公交车，惊诧于路上行人为何如此稀少时，我突然意识到，原来今天是 2020 年的第一天啊，原来 2019 年就这样失去了啊，原来今天元旦放假啊……习惯其实是件可怕的事情，我妈和我说，是我工作太认真了，确切地说，来到这家新公司后，太多的习惯都被改变了，譬如 Deadline 驱动开发而导致的加班，譬如身为乙方这个弱势群体的被动，譬如周末一样要被同事电话打扰的无力感……互联网在深入到这个世界的各个角落的同时，互联网从业者的生存环境反倒更加举步维艰，资本家们鼓吹 996 是一种福报，某企业用 251 来对待离职的员工，因为加班而过劳死留下孤儿遗孀的软通员工，因为被裁员而无力维持生活选择跳楼的员工……\n詹青云在《奇葩说》里的一段话令我印象深刻，她说，整个社会都在选择性忽视对与错的问题，仅仅是因为这样子做更划算些，一群活生生的人就被当做冰冷的数字一样计算。《红楼梦》里说，“机关算尽太聪明，反误了卿卿性命”，一个大家都互相算计的世界是绝望的，而这种“划算”的想法有一天变成主流则是可怕的。有好几次工作到深夜凌晨，回到家困到直接穿着衣服睡着的我，恍惚中应该会同我的灵魂对话：到底是一件多么惊天动地的事情，需要我连命都不要地熬到这个点。对企业对说，它需要的是“划算”的员工。而对员工来说，生命比一切都重要。即使为社会这部大机器而殚精竭虑甚至牺牲生命，这部如永动机一般的大机器依旧不会停止，我们不需要去追赶整个社会的效率。如果追赶会有什么下场呢？卓别林的《摩登时代》已经告诉过你答案。\n可笑的是，人类能接受同类所指定规则，唯独要抗衡比人类更高层次的自然规律。你、我，这个世界上的每一个人都会死，这是所有人都无法逃脱的自然规律，即使是同为人类的医生一样会死，难道医生都是神灵或者天使吗？《白色巨塔》中的財前医生医术精湛，可当面对身患癌症的自己时，一样回天乏术。医学的发展自始至终都是建立在死亡上的，我们不能在享受医学带来的好处的同时，仅仅因为那个人是你或我的亲人，就去伤害这些医疗工作者，因为他们和我们一样，都是普普通通的人，他们唯一比我们多的就是医术，可医术甚至于这世界上一切人类发明的东西，都不是万能的啊。\n伤害别人，永远无法弥补我们对逝者的愧疚。生命原本就如此脆弱，如果身为医生而没能抢救过来自己的亲人，按照这套“划算”但不“正确”的理论，那么医生是不是应该选择自杀？我说，这个问题根本不需要多想，因为逝者已逝，让更多的人活下来，九泉之下有知的逝者或许会感到欣慰吧……如果你相信人死后灵魂会得到转世，那么，让逝者的生命从下一个新生命中得到延续不好吗？我们这个世界有一种病态的观念，对待客户要毕恭毕敬，对待患者要高风亮节，可如果有一天这些人要对你做出过分的事情，难道你还要一忍再忍吗？\n人有时候会刻意拉大时空的疏离感，就像我第一次看《叶问》还是在同学的 MP4 上，我甚至都没有看过《叶问 1》里“我要打 10 个”的名场面，因为第一次看《叶问》的时候，叶师傅已经在大圆桌上同洪师傅切磋武艺了。可当你回首时，时间已经过去 10 年啦，虽然在这 10 年里，罗师傅的武功一直没什么长进，而叶师傅的对手则一直在变强。翻过年以后，我就 28 岁啦，如果回头看我的 10 年，时间大概一样会变得空泛，因为有的人来来回回地从你生命里来了又去，而有的人甚至从未真正进入过你生命里。当时过境迁，你唯一能留下的就只有自己，我虽庆幸见证过那些花儿的开放，可那些花儿终究不是我的。也许，她们和我一样都渐渐老去了吧，听起来有些矫情对不对？其实，昨天和这些年里的每一天没有什么不同，甚至还要更普通些，因为我又没能控制住情绪发了火，记忆啊，终究带着些美化的滤镜……\n","date":"2020-01-01T08:46:24Z","image":"/posts/888549816/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/888549816/","slug":"888549816","tags":["回顾",2019,"年度"],"title":"不知老之将至"},{"categories":["数据存储"],"content":"在平时的开发工作中，接口对接是一件无可避免的事情。虽然在“前后端分离”的大趋势下，后端的角色逐渐转换为数据接口的提供者，然而在实际的应用场景中，我们面对的往往是各种不同的“数据”，譬如企业应用中普遍使用的企业服务总线(ESB)，这类服务要求服务接入者必须使用 WebService 来作为数据交换格式；再譬如电子数据交换(EDI)这种特定行业中使用的数据交换格式，从可读性上甚至还不如基于 XML 的 WebService……而更为普遍的则可能是需要使用 Word、Excel、CSV 来作为数据交换的媒介。顺着这个思路继续发散下去，进入我们失业的或许还有各种数据库，譬如 MySQL 和 MongoDB；各种大数据平台，譬如 Hadoop 和 Spark；各种消息队列，譬如 RabbitMQ 和 Kafka 等等。\n注意到，这里反复提到的一个概念是数据交换(Data Switching)，它是指在多个数据终端设备间，为任意两个终端设备建立数据通信临时互联通路的过程。自从阿里提出“中台”的概念以来，越来越多的公司开始跟风“中台”概念，并随之衍生出譬如组织中台、数据中台、业务中台、内容中台等等的概念。今天这篇博客，我并不打算故弄玄虚地扯这些概念，我的落脚点是接口级别的数据交换，主要通过 Liquid 这款模板引擎来实现。它对应我在这篇博客开头提到的场景：一个对外提供 RESful 风格 API 的系统，如何快速地和一个 WebService 实现对接。总而言之，希望能对这篇博客对大家有所启发吧！\n关于 Liquid 首先，我们来介绍Liquid，通过它的官方网站，我们应该它是一门模板语言。对于模板语言，我们应该是非常熟悉啦，JavaScript 里的Handlebars和Ejs就是非常著名的模板语言。如大家所见，这个博客就是用 Ejs 模板渲染出来的。而到了三大前端框架并驾齐驱的时代，模版语法依然被保留了下来，比如 Vue 中 {% raw %}{{model.userName}}{% endraw %} 标记常常用来做文本插值。所以，如果要认真追溯起来的话，也许这些框架都或多或少的收到了 Liquid 的影响，因为它的基本语法如下：\n// 使用page实例的title属性插值 {{ page.title}} 假设 page 是一个对象，它的 title 属性值为：Introduction，此时，渲染后的结果即为：Introduction。是不是感觉非常简单呢? 我们继续往下看。除了基本的“插值”语法以外，我们可以用 {% raw %}{% tag %}{% endraw %} 这种结构(Liquid 称之为 Tag)：\n// 声称变量author并赋值 {% sssign author = \u0026#39;猫先森\u0026#39; %} // 条件语句 {% if author == \u0026#39;猫先森\u0026#39; %} 帅哥，你好 {% endif %} // 循环语句 {% for post in posts %} {{post.date}}-{{post.title}} {% endfor %} 这里仅仅展示了一部分Liquid的特性，但对于我们了解一门“语言”已经足够了，因为对于一门编程语言来说，只要学会顺序、条件和循环三种结构足矣。言下之意呢，像常规else、elseif、break和continue，Liquid都是支持的，这样子是不是更有编程语言的感觉了呢？除此之外，它还支持像 tablerow 这样的 Tag，主要用来渲染 HTML 里的表格。\n也许有人想说，这玩意儿有什么用呢？抱歉啊，这玩意儿还真有用。像发送邮件、发送短信这种一般都需要写个字符串模板的，简单的大家可以用 String.Format() 或者 $ 来搞定，可一旦遇上循环的场景，这种基于字符串替换的方式就有点力不从心了。不开玩笑地说，在代码里用 StringBuilder 拼接 HTML 的方式，实在是太傻逼了。如果用 Liquid 写可能就是：\n亲爱的{{ model.UserID }}: 您好！您有以下设备即将超过校验有效期，请及时采取有效行动。 {% for equipment in model.Equipments %} {{ equipment.EquipmentID }} {% endfor %} {{ model.SendBy }} 显然，这个代码比拼接字符串要优雅很多。博主曾经在一个前端页面看到过大量的 HTML 拼接操作，果然是 jQuery 操作 DOM 一时爽，jQuery 操作 DOM 一直爽，可明明前端就有 Handlebars 和 Ejs 这样的模板语言。最近一位同事写前端页面的经历不由得让我感慨，眼睛觉得简单的事情，为什么总是要求手去做呢？直接操作 DOM 带来的弊端就是，业务逻辑永远和 DOM 纠缠在一起，那些没有人敢改的 JavaScript 代码，那些未经模块化全局引入的 JavaScript 代码，虽然马上就要 2020 年了，写下这些句子的时候还是感到魔幻，可能这就是所谓的魔幻现实主义吧。\nOK, 我们把思绪拉回到 Liquid 。除了使用各种 Tag 实现流程控制以外， Liquid 中还提供了过滤器(Filter)的概念，过滤器主要是配合 {% raw %}{{ variable | filter }}{% endraw %} 语法来使用的。比如说，数据层返回了一个负数，而展示层希望展示正数，在不确定这个数值是否被别人使用的情况下，贸然去修改数据层的返回值是件危险的事情。此时，我们可以：\n//对绑定的变量或者值取绝对值 {{ -17 | abs}} // 保留小数位 {{ 183.357 | round: 2 }} // 日期/时间格式 {{ article.created_date | data: %b %d, %Y}} 类似小数点位数、日期/时间格式等问题，均可以在 Liquid 中找到相应的过滤器。需要说明的是， Liquid 使用 Ruby 进行开发的。也许在读到这篇博客前，大家都没有听说过 Liquid ，那么至少听说过 Jekyll 这个著名的静态博客生成器吧。实际上，在我写这篇博客的时候，我刚刚了解到一件事情， Jekyll 就是基于 Liquid 而开发的，想到当初搭建这个博客时被 Ruby 劝退的回忆，我大概想不到有一天会再次接触它吧，不得不说，人生还真是奇妙啊！\n一个简单的想法 好了，关于 Liquid 的介绍我们先了解到这里。写到这里，再回头去看我们一开始的问题，即：怎么把上游的数据(Model)转化为下游的数据(Template)。这里暂且抛开它到底是 XML、JSON 还是 EDI 这种细节性的问题，我想我们大概会有一个简单的想法，如果把需要传输给对方的接口报文做成模板，然后通过Liquid语法完成数据的绑定，那么数据映射这一层的工作就可以减轻不少，毕竟写 A.XXX=B.XXX 这种赋值语句是没什么前途的啦，而 AutoMapper 则需要提前写好 Map 并注册，经过一番权衡，我们来验证一下我们的想法吧！\n这段时间一直在和金蝶 K3Cloud 接口做对接，坦白说我觉得金蝶的接口设计得非常糟糕，从它那个奇葩的 FNumber 字段就能看出来，而且它试图用一个接口做完所有事情的做法恕我不敢苟同，在我看来它违反了单一职责原则。因为要对接的接口数量多、字段多，我首先根据字段对应关系制作了一份 Liquid 模板，并根据业务上的需要，用主表(Main) + 明细表(Details)的方式来定义数据，这意味着我接下来只需要根据业务实现不同的数据源即可：\n基于Liquid的JSON报文模板\r好了，现在我们使用 Liquid 的 .NET 版本 DotLiquid 来负责模板的解析和渲染，这个库可以直接通过 Nuget 安装，可以注意到这个代码非常的简单：\nstring RenderTpl(string filePath, dynamic model) { var content = File.ReadAllText(filePath); var template = Template.Parse(content); var output = template.Render(Hash.FromAnonymousObject(model)); return output; } 实际上渲染后的文本就是对方需要的接口报文了，此时，该怎么样就怎么样处理，只需要把这个报文发送给对方就可以了。唯一需要花时间的就是对字段、写绑定，相比写实体类的方式效率要高更多。这种方式的话，我个人觉得更适合分工合作，如果需要数据加字段，那在数据层(Model)里增加就好了，而像改字段映射关系、字段默认值都可以由别人来完成。我一直相信，开发并不是帮别人做越多事情越好，而是可以提供一种能力让别人去做更多的事情，这就是我们常常听到的“赋能”。继续延伸下去的话，传统的 MVC 其实和Liquid是一个道理，都是根据数据去生成视图，无非是我们这里的\u0026quot;视图\u0026quot;变成了数据报文。\n本文小结 通过日常工作中的接口对接这一典型场景，我们引出了“数据交换”的概念，而最低层级的数据交换实际上是接口报文的交换。为此，我们介绍了 Liquid 模板引擎，它提供的语法可以让我们完成一系列的绑定，顺着这个思路，博主为大家展示了这种想法的可行性。 Liquid 是一个非常成熟的模板引擎，无论是编写邮件、短信的文本模板，还是轻量级的文本表达式实现，都是一个非常不错的选择。即使是做一个 ApiCaller，一定要做一个有头脑的 ApiCaller。好了，以上就是这篇博客的全部内容啦，欢迎大家留言，谢谢大家。\n2020-01-09 更新 在组织 JSON 中的数组结构时，需要在各元素间添加,，同时最后一个元素不需要,，此时，可以使用以下语法：\n\u0026#34;FEntity\u0026#34;: [ {% for Detail in Details %} { \u0026#34;FCOSTID\u0026#34;: { \u0026#34;FNumber\u0026#34;: \u0026#34;{{Detail.FCOSTID}}\u0026#34; }, \u0026#34;FCOSTDEPARTMENTID\u0026#34;: { \u0026#34;FNumber\u0026#34;: \u0026#34;BM000005\u0026#34; }, \u0026#34;FINVOICETYPE\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;FTOTALAMOUNTFOR\u0026#34;: {{Detail.FEE_AMOUNT}}, } {% if forloop.last == false %},{% endif %} {% endfor %} ] ","date":"2019-12-22T09:36:42Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3742212493/","slug":"3742212493","tags":["Liquid","数据交换","模板引擎"],"title":"使用 Liquid 实现简单的数据交换"},{"categories":["编程语言"],"content":"诗人郑愁予曾经在一首诗中写道：我达达的马蹄是个美丽的错误，我不是归人，是个过客。而对我来说，十九岁之前的我，一样是个沉浸在诗歌中的文艺少年。十九岁之后的我，作为一名程序员，更多的是邂逅各种错误。可偏偏人类世界对待错误从来都不宽容，所以，错误本身既不美丽，亦不浪漫。接近中年的我，无论如何，都写不出年轻时令人惊艳的句子，这或许和我们面对错误时的不同心境，有着莫大的关联，而今天这篇博客，同样要从一个历史上的错误说起。\n因拼写而怀疑人生 话说，博主这天做了一个非常“简单”的功能，它允许用户通过富文本编辑器来编写 HTML，而这些 HTML 会被插入到页面的特定位置，譬如用户可以为页脚的备案号添加一个超链接，当用户点击备案号的时候，就可以调转到工信部备案号查询的网站上。这个功能非常简单吧，因为这就是 HTML 中 a 标签的作用。博主快速了引入 UEditor，虽然这个项目百度都不再继续维护了，虽然它直接把跨域问题甩锅给使用者，可我还是完成了这个功能。相信你能感受到我的不情愿吧，显然这不是重点，因为剧情的反转才是……\n结果没高兴多久，测试同事就同我讲，客户提供的地址填进去以后，点击链接浏览器直接返回 4XX，可明明这个地址敲到浏览器里就能打开啊……我脑海中快速地浮现出那道经典的面试题，浏览器里敲完地址按下回车的瞬间到底发生了什么？习惯性怀疑人生后，我发现居然是因为 Referer 的问题，从我们站点调转到客户站点的时候携带了 Referer，虽然有很多种方法可以让浏览器禁止携带 Referer，但我还是被这种历史性的错误搞得怀疑人生。因为人生最难的事情，就是“揣着明白装糊涂”和“揣着糊涂装明白”，所谓“假作真时真亦假”。\n请注意区分Referer和Referrer这两个单词，眼尖的人会发现后者多了一个 r，这有点像什么呢，大概类似于 usr 和 user。我们总是不情愿地相信这是历史的错误，而固执地想要找到一种能自圆其说的理由。诚然，“前人栽树，后人乘凉”，可我实在不肯承认，这是一群卓越而智慧的先驱们，所创造出的某种高效简写。回顾一下，使用 Referer 的场合，基本都是在 HTTP 头部，最常见的场景就是防盗链，Nginx 能用 Referer 判断访问者来源，爬虫就能用 Referer 和 UserAgent 伪造访问者身份。那什么时候用 Referrer 呢？我目前发现是在 a 标签的 rel 属性里，例如下面的例子：\n\u0026lt;a rel=\u0026#34;noreferrer\u0026#34; href=\u0026#34;https://www.w3school.com.cn/tags/att_a_rel.asp\u0026#34;\u0026gt;w3school\u0026lt;/a\u0026gt; 除此之外，rel 属性还支持像 nofollow、friend、licence 这样的属性，详细地大家可以参考这里。相信大家想到博主经历了什么了，没错，我就是按照平时的书写习惯写了 Referer，然后被 Web 标准委员会给疯狂地嘲讽了。那么，为什么表达同一个含义的词会有两种写法？为什么有时候要用 Referer，而有时候要用 Referrer? 这特么到底是怎么一回事儿……带着这些疑问，让我们一起回顾野蛮生长的 Web 标准，为什么要埋这样一个坑在这里。\n后世不忘，前世之锅 故事要追溯到上个世纪 90 年代，当时 HTTP 协议中需要有一个用来表示页面或资源来源的请求头部，Philip Hallam-Baker 将这个请求头部定义为 Referer，并将其写入了RFC1945，这就是著名的 HTTP/1.0 协议。\nHTTP/1.0协议中定义的Referer\r然而这里发生一件有趣的事情，这个单词实际上是被作者给拼错了，即正确的拼写应该是Referrer。因为发现这个错误时为时已晚，大量的服务端和客户端都采用了这个错误的拼写，谁让它被写到了 HTTP 协议里呢？这其中就有像 Nginx 里的ngx_http_referer_module、Django 里的HttpRequest.META.HTTP_REFERER等等。考虑到这个错误波及的范围过大，HTTP 标准制定者奉决心将错就错，于是在接下来的RFC2616，即 HTTP/1.1 中，HTTP 标准制定者追加了针对这个错误的说明:\nHTTP/1.1协议中定义的Referer\r说到这里，大家至少明白了一件事情，这个错误的Referer其实是指Referrer。对于标准写错了这件事情，大家其实都能理解，因为只要是人就免不了会出错。可为什么不能一错到底呢？既然要使用Referer这个错误的拼写，那就一直这样错下去好了，为什么特么又冒出来个Referrer，虽然它的拼写的确是对的，可不统一的写法还是会让人抓狂啊！君不见main和mian傻傻分不清，君不见 C++里false与flase的神奇宏定义。假如没有今天这个事情，我完全不知道还有Referrer的存在啊，可都拼错多少年了，我都把假当作真了，你突然这样搞，我还是会感到手足无措的啊！就像Configuration这个单词，虽然博主英语并不算太好，可至少敢拍着胸脯说这个单词没写错，结果有次我写对了反而让测试给我提了 Bug，因为特么项目里定义的实际上是Configuation。你说，你这样让人崩溃不？\n那么，为什么会有Referrer这个正确的拼写呢？这就要说到Referrer-Policy这个 HTTP 头部。不错，这次你没有看错，标准制定老爷们这次终于写对了。顾名思义，这是一种用来告诉浏览器应该如何发送 Referer 的策略。常见的取值有：no-referrer、no-referrer-when-downgrade、origin、origin-when-cross-origin、same-origin、strict-origin、strict-origin-when-cross-origin、unsafe-url，关于它们的含义及用途，大家可以参考这里。虽然我们经常吐槽 JavaScript 是一门垃圾语言，但是这一次，大家居然都非常齐心地统一了写法，譬如DOM Level 2里定义的 document.referrer、Fetch API中的Request接口的referrer属性等，这一次都写对了。而 Referrer-Policy 除了和 JavaScript 可以集成以外，同样可以和 HTML、CSS 集成。博主一开始遇到的问题，实际上就是和 HTML 集成的一个场景。\n//meta标签里的\u0026#39;referrer\u0026#39; \u0026lt;meta name=\u0026#34;referrer\u0026#34; content=\u0026#34;origin\u0026#34;\u0026gt; //出现在a, area, img, iframe, script, \u0026lt;link\u0026gt;等元素里的\u0026#39;referrer\u0026#39; \u0026lt;a href=\u0026#34;http://example.com\u0026#34; referrerpolicy=\u0026#34;origin\u0026#34;\u0026gt; //出现在a, area, link等标签的rel属性里的\u0026#39;referrer\u0026#39; \u0026lt;a href=\u0026#34;http://example.com\u0026#34; rel=\u0026#34;noreferrer\u0026#34;\u0026gt; 而和 CSS 集成实际上就是 style 标签中的referrerpolicy属性，它默认是 no-referrer-when-downgrade，我们可以在返回一个 CSS 文件的时候设置响应流的Referrer-Policy，或者是设置 style 标签中的referrerpolicy属性，这个就不展开讲啦！\n本文小结 通过这次被标准制定者按在地上摩擦的经历，居然无意中收获了这样一段\u0026quot;迷人\u0026quot;的历史。假如 JavaScript 这里为了兼容历史错误而使用Referer的话，可能博主就不会一边吐槽这个错误，一边又乖乖地滚去读 RFC2616。从这里可以得出一个结论：HTTP 请求中的 Referer 是一个典型的拼写错误，历史悠久，可以预见还会一直错下去，以后 Referer 变成一个专有名词也说不定。所以一般涉及到读取 HTTP 请求头的场景，我们需要用 Referer 这种错误拼写(后端)；除此之外一般都要用 Referrer 这种正确的拼写(前端)。有人说，使用 JavaScript 开发同构应用的体验非常好，恐怕从今天这篇博客以后要打个折扣，因为你刚刚在后端写完referer，转眼就要在前端写referrer，希望像博主这样的伪全栈工程师不会因此而精神分裂。实用主义者能用就行的策略，让这个错误在很多年以后还被人提起，假如这些标准制定者尚在人世的话，不知道会不会在浏览网页的时候，想起第一次起草RFC1945的那个下午。果然，历史还真是迷人啊！\n","date":"2019-12-04T17:22:33Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2015300310/","slug":"2015300310","tags":["HTTP","历史","Referrer"],"title":"Referrer 还是 Referer? 一个迷人的错误"},{"categories":["数据存储"],"content":"请原谅我使用了这样一个“直白”的标题，因为我实在想不到更好的描述方法。或许，是因为临近年底的“996”式冲刺，让许久没有读完一本书的我，第一次感受到输出时的闭塞。是时候为自己的知识体系补充新鲜血液啦，而不是输给那些“无聊”的流程和关系。说这句话的缘由，是想到《Unnatural》中的法医三澄美琴，一个视非正常死亡为敌人的女法医。而对程序员来说，真正的敌人则是难以解决 Bug 和问题。可更多的时间，我们其实是在为流程和关系方面的事情消耗精力。\n我越来越发现，人类所面对的绝大多数问题，都并非是寻求一个最优解，而是在于平衡和牵制。人类总是不可避免地堕入熵增的圈套，伴随着流程产生的除了规范还有复杂度。每当人们试图为这种复杂度找一种友好的说辞的时候，我终于意识到，有的人不愿意去寻找问题的本质，它们需要的就只是一种友好的说辞，仿佛只要有了这种说辞，问题就能自动解决一样。我想，我大概知道这段时间感到焦灼的原因了，因为这样的事情在工作中基本是常态。人类每天面对的事情，无外乎两种：\u0026ldquo;明知不可为而为之\u0026quot;和\u0026quot;什么都想兼顾的美好理想\u0026rdquo;。\n我今天想说的是，一个业务中遇到的单位转换的问题，我们平时在存储货物的重量时，默认都是以千克作为单位来存储的，直到我们对接了一家以大宗商品交易作为主要业务的客户，对方要求我们在界面上统一用吨来展示数据，因为这样更符合客户方的使用习惯。按理说，这是一个非常简单的需求，是不需要用一篇博客来说这件事情的，可我觉得这是个有意思的话题，还是想和大家一起来聊聊相关方案的思路。带着问题，我首先拜访了Cather Wong大佬，大佬微微一笑，表示在视图层上加个字段就可以了嘛。的确，这是最简单的做法，大概是下面这个样子：\nclass OrderInfoQueryDTO { /// \u0026lt;summary\u0026gt; /// 以千克为单位的净重 /// \u0026lt;/summary\u0026gt; public decimal? NET_WEIGHT { get; set; } /// \u0026lt;summary\u0026gt; /// 以吨为单位的净重 /// \u0026lt;/summary\u0026gt; public decimal? NET_WEIGHT_WITH_TON { get { return NET_WEIGHT / 1000; } } } 我不甘心地追问，客户要在原来的字段上显示这个数值啊，这样能行吗？大佬稍作沉思，随即问道：“你们公司的项目就算做不到 DDD，AutoMapper 这种实体间映射转换的东西总有吧！”。我连忙接话道：“这个自然是有的”。其实我心里想的是，总算有点符合我的心理预期啦，这样的方案还像个大佬的样子。按照大佬的提示，使用 AutoMapper 来做单位的转换，应该是下面这样：\nvar config = new MapperConfiguration(cfg =\u0026gt; { cfg.CreateMap\u0026lt;order_info, OrderInfoQueryDTO\u0026gt;() .ForMember(d =\u0026gt; d.NET_WEIGHT, opt =\u0026gt; opt.MapFrom(x =\u0026gt; x.NET_WEIGHT/1000)); }); 这样看起来是比加字段要好一点，可实际项目中，我们往往会把单位作为一种配置持久化到数据库中，以我们公司为例，我们实际上是支持千克和吨两种单位混合使用的，不过在表头汇总的时候，为了统一到一起，所以使用了千克作为单位。这样就引申出一个新问题，假如我在数据库里存了多行明细的重量，当需要在表头展示汇总以后的总重量，那么，这个总重量到底是汇总好存在数据库里，还是展示的时候交由调用方 Sum()呢？\n我个人倾向于第二种，因为它能有效避免表头和明细行数据不一致的问题，当然缺点是给了调用方一定的计算压力。我们项目中采用的第一种方案，我印象非常深刻，在计算件数、重量和体积的时候，必须要等所有明细行都计算完以后，再通过调用 Sum()方法给表头赋值，实际上这个表头字段，完全可以通过只读属性的方式取值啊，更何况我们还使用了外键，表头实体本身就引用了明细表实体，因为有外键的存在，序列化表头实体的时候会出现循环引用，对此，我想说，干得漂亮！\n通过 AutoMapper 中的 ForMember 扩展方法，可以实现我们这里这个功能。可考虑到要在 AutoMapper 里引入权限啊、角色啊这些东西，AutoMapper 作为实体映射的纯粹性就被彻底破坏了。为此，我们考虑使用 AutoMapper 中提供的Value Converters和Type Converters。关于这两者的区别，大家可以参考官方文档中的描述。此时，我们可以通过下面的方式使用这些“转换器”：\nvar config = new MapperConfiguration(cfg =\u0026gt; { cfg.CreateMap\u0026lt;order_info,OrderInfoQueryDTO\u0026gt;() .ForMember(d =\u0026gt; d.NET_WEIGHT, opt =\u0026gt; opt.ConvertUsing\u0026lt;WeightValueConverter,decimal?\u0026gt;()); }); var mapper = config.CreateMapper(); var orderInfo = new order_info() { ORDER_ID = Guid.NewGuid().ToString(\u0026#34;N\u0026#34;), NET_WEIGHT = 1245.78M, CREATED_DATE = DateTime.Now, CREATED_BY = \u0026#34;灵犀一指陆小凤\u0026#34; }; var orderInfoQueryDTO = mapper.Map\u0026lt;order_info,OrderInfoQueryDTO\u0026gt;(orderInfo); 而对于 WeightValueConverter 这个类而言，它实现了 IValueConverter 接口：\npublic class WeightValueConverter : IValueConverter\u0026lt;decimal?, decimal?\u0026gt; { public decimal? Convert (decimal? sourceMember, ResolutionContext context) { //TODO：可以查数据库或者是由规则决定，是否转换以及如何转换 if (!sourceMember.HasValue) return null; return sourceMember.Value / 1000; } } 现在，虽然代码还是这个代码，可至少我们不用在 MapFrom 里写太重的业务逻辑了，而且这个转换器是可以复用的。显然，我们的系统中不会只有订单模块会涉及到重量、体积的转换。此时，我们可以考虑使用 ITypeConverter 接口，遗憾地是，这个接口在实现的时候就必须指定源类型和目标类型，这样离我们设想地全局转换器实际上是有一点差距的。例如，我们有时候希望源类型中 Null 值不会覆盖到目标类型，最常见的情况是，从一个 EditDTO 转化为数据库实体对象并更新数据库。为了解决这个问题，AutoMapper 下面的做法就非常棒：\ncfg.ForAllMaps((a, b) =\u0026gt; b.ForAllMembers(opt =\u0026gt; opt.Condition((src, dest, sourceMember) =\u0026gt; sourceMember != null))); 可对于我们这里这个场景，显然，我们必须要提供一部分类型信息，我们几乎很难给所有的 Map 增加一个通用的类型转换器。我最终还是通过反射解决了这个问题，即在使用 AutoMapper 前，从数据库查出数据后，首先要做的第一件事情就是对数值进行转换：\nvar userSetting = UserContext.GetLoginUser().UserSettng; var formatSetting = userSetting.FormatSetting; //当默认重量单位为KG时不做任何处理 if (formatSetting.DefaultWeightUom == WeightUnit.KG) return; var properties = typeof(TDestination).GetProperties() .Where(p =\u0026gt; p.Name.EndsWith(\u0026#34;WEIGHT\u0026#34;) || p.Name.EndsWith(\u0026#34;Weight\u0026#34;)); if (properties == null || !properties.Any()) return; foreach(var item in destList) { //转化结果为吨 foreach(var property in properties) { var weightValue = property.GetValue(item, null); if(property.PropertyType == typeof(decimal)) { property.SetValue(item, (decimal)weightValue / 1000); } else if(property.PropertyType == typeof(Nullable\u0026lt;decimal\u0026gt;)) { if (weightValue != null) property.SetValue(item, (decimal)weightValue / 1000); } else if(property.PropertyType == typeof(string)) { if (!string.IsNullOrEmpty(weightValue.ToString())) property.SetValue(item, decimal.Parse(weightValue.ToString()) / 1000); } } } } 不得不说，这段代码相当无聊，可无论多么无聊的功能，只要客户觉得好就给积极地去做，对吧！其实，说到底，这是我们在设计数据库表结构时遗留的一个问题。假如我们在存储的时候就存储为吨，问题还不会有什么不一样呢？实际上，它还是会有问题，因为你不得不去设计一个单位转换表，类似下面这样的：\n原始单位 目标单位 进率 Kg T 1/1000 T Kg 1000 g Kg 1/1000 Kg g 1000 我们目前设计的表结构中实际上是有重量单位的，不同的是，我们以千克为单位存储的量，数据库中对应的 WEIGHT_UOM 存储的是 1，而以吨为单位存储的量，数据库中对应的 WEIGHT_UOM 存储的是 1000。所以，理论上真实的重量都应该是数据库中存储的量 X WEIGHT_UOM。这样看起来是没有问题的，可当你结合今天这篇博客的背景来看是，就会发现一个问题，所有的数值在展示的时候都必须要知道，数据库里存储的数值的原始单位是什么，而使用者希望在界面上看到的数值的单位又是什么。\n不单单如此，当用户通过界面查询的时候，一个简单的数字便不等再用简单地使用像大于、小于、等于、不等于这样的查询条件，因为现在每个量都带着单位，你必须明确得知道，用户认为的单位是什么，而数据库里对应的单位又是什么？这样听起来貌似还是统一使用一种单位比较好，正因为如此，博主可以在查询前把吨转化为千克，而在查询后则可以把千克转换为吨。\n人类世界总是存在着这些奇奇怪怪的规则，不同的小数位精度要求，不同的货币金额展示方式，不同的日期格式显示要求，就在我写下这篇博客的时候，产品同事反馈我千克转成吨展示以后，应该至少保留三位小数，否则会让人觉得数字会丢失了精度。我还能说什么呢？联想到最近软通因为加班而猝死的同行，我大概只能说一句：**恭喜你，还请节哀顺变，欢迎来到无法随心所欲的爱与欲望的世界！**作为拖延症中晚期的博主，努力写完每月一篇的博客，抽空读读书、看看电影，这已然是种简单的幸福了呢！好了，这篇博客就先写到这里！\n","date":"2019-11-15T09:43:54Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2318173297/","slug":"2318173297","tags":["单位","设计","数据库"],"title":"关于单位转换相关问题的常见思路"},{"categories":["独立博客"],"content":"Valine是一个基于LeanCloud的评论系统，在很长的一段时间里，一直作为多说、Gitalk、Gitment等等的一个替代品，博主所使用的评论系统实际上就是 Valine，虽然独立博客的整体活跃度无法媲美专业博客，可还是想在这纷扰的世界里有自己的一方天地啊。多说评论的关闭，某种意义上来说，是很多 90 后站长们关于互联网的集体记忆，因为从博主搭建第一个 WordPress 博客的时候，多说就一直作为首选的评论系统而存在。那个时候通过多说就能接入主流的社交媒体，对于一个还不大会编写 Web 应用的博主来说，此刻想来实在是有种时过境迁的感觉。所以，Valine 作为一个相当优秀的评论系统，凭借着简洁大方的界面和开箱即用的优势，在这个时间点进入人们的视野，我们就不难理解，为什么它会成为博客作者们的“新宠”。\nValine 本身是利用 LeanCloud 的数据存储 SDK 来实现评论的读写的，所以，它相对于“多说”这种第三方的服务，在数据安全性上更有保障一点，虽然“多说”在关闭评论服务以前，提供了导出 JSON 格式评论信息的功能。可话说回来，以国内这种“敏感”的网络环境，其实没有一家云服务提供商敢打这样的包票，像阿里云、LeanCloud、七牛云存储这些服务，都曾经出现过宕机或者封杀域名的事情，所以，趁着数据还在自己手上，尽可能地做好备份工作吧！Valine 本身并没有提供评论推送的功能，我还是挺怀念过去“多说”推送评论到邮箱的功能。虽然Valine-Admin这个项目提供了类似的功能，但我感觉使用起来并不顺手，尤其是配置邮箱的时候，国内像 QQ、163 这些都非常麻烦，遇到一两个废弃的手机号，你就会发现短信验证码，是件多么尴尬而繁琐的事情，如同扫码使用的共享电话一般魔幻。\n为了解决这个问题，我想到了 Valine 搭配 Server 酱实现评论推送的方案。首先，Valine 是基于 LeanCloud 而开发的，用户发表评论实际上就是向Comment表插入记录。因此，我们可以利用 LeanCloud 提供的Hooks来捕获写入评论的事件。所谓“Hooks”呢，通俗地说就是数据库里触发器的概念，我们可以在数据写入前后做点“小动作”。而Server 酱则是一个消息推送服务，它提供了一个基于 HTTP 的请求接口，通过这个接口，我们就能实现向微信推送消息，前提是关注“方糖”公众号。关于 Server 酱的原理大家可以进一步去看它的文档，我们这里只需要考虑怎么样把它们结合起来，这就是工程师和科学家的区别所在[doge]。\n运行在Valine云引擎中代码\rLeanCloud 提供了一个称作“云引擎”的环境，它可以提供运行比如 Nodejs、Python 等等的环境，实际上，Valine-Admin这个项目就是用 Nodejs 编写的，你可以理解为，只要你的应用符合它的规范，就能在它的环境里运行，这就是所谓的“FAAS”(函数即软件)和“BAAS”(后端即软件)。所以，说白了我们就是想利用它这个“云引擎”来调用 Server 酱的接口，幸运的是，LeanCloud 提供的 Hooks 目前是支持 Nodejs 的，所以，到这里思路就非常清晰了，我们给Comment这张表加一个AfterSave类型的 Hooks，在保存完以后调用 Server 酱接口推送评论信息即可。创建 Hooks 是在部署-\u0026gt;云引擎选项下，我们来看下面的代码：\nAV.Cloud.afterSave(\u0026#39;Comment\u0026#39;, async function(request) { var http = require(\u0026#34;request\u0026#34;); var obj = request.object; console.log(\u0026#39;收到一条新的评论：\u0026#39; + JSON.stringify(obj)); var title = \u0026#34;收到一条新的评论\u0026#34;; var url = request.object.get(\u0026#39;url\u0026#39;); var nick = obj.get(\u0026#39;nick\u0026#39;); if (nick == \u0026#39;Anonymous\u0026#39;){ nick = \u0026#39;陌生人\u0026#39;; } var comment = obj.get(\u0026#39;comment\u0026#39;); var content = nick + \u0026#34;给你留言：\\n\\n\u0026#34; + comment + \u0026#34;\\n\\n详情请访问：\\n\\n\u0026#34; + url; var options = { method: \u0026#39;GET\u0026#39;, url: \u0026#39;https://sc.ftqq.com/\u0026lt;在这里输入你的token\u0026gt;.send\u0026#39;, qs: { text: title, desp: content }, headers: { } }; http(options, function (error, response, body) { if (error) throw new Error(error); console.log(body); }); }); 这里主要利用了 Nodejs 中的requests模块来发送 HTTP 请求，其中 token 是 Server 酱经过 Github 授权以后获得的，具体可以参考 Server 酱的文档。这里有一点要注意，Comment 表里的记录是无法区分发出人的，因为有时候我们可能忘记填写邮箱或者昵称，所以，目前只要写入记录都会发送消息到手机。这个消息模板是 Server 酱作者提供的，我们无法对它的样式进行自定义，收到消息以后需要点击查看详情。不过，我认为这个方案可以满足我的日常使用，因为博客的评论数量并不多，而 Servet 酱的接口调用次数完全足够。免费的 LeanCloud 实例虽然会强制休眠，只要大部分时间能覆盖到就可以啦，谁让这些东西都是免费的呢，博主表示已经相当知足啦，哈哈！好了，看看消息推送到手机的效果吧！\n博客评论推送到手机上的展示效果\r如果大家想调整消息的格式，参考文章中给出的代码即可，每次调整完可以直接部署到线上，这是我在这个过程中体验到的 Serverless 的魅力，相比我们中华田园式的 996 敏捷开发，这种方式真的能缩短部署的周期。我还是那句话，敏捷开发是大家一起敏捷，不是只有开发苦哈哈地加班加点干活，快速交付的前提是基础设施完善，具备自动化测试、自动化部署的能力，让开发安心地写代码比什么都重要，就像 LeanCloud 里提供的云函数和 Hooks，开发写完代码就能自动部署，这是真正的敏捷、真正的灵活。好了，这篇博客就先写到这里。想试试博主能不能第一时间收到你们的留言？欢迎在博客评论区留下你的足迹，谢谢大家！\n","date":"2019-11-06T18:15:14Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/369095810/","slug":"369095810","tags":["Valine","Server 酱","评论"],"title":"Valine 搭配 Server 酱实现博客评论推送"},{"categories":["数据存储"],"content":"国庆节前有段时间，新浪的“图床”一直不大稳定，因为新浪开启了防盗链，果然免费的永远是最贵的啊。为了不影响使用，我非常粗暴地禁止了浏览器发送 Referer，然后我就发现了一件尴尬的事情，“不蒜子”统计服务无法使用了。这是一件用脚后跟想都能想明白的事情，我禁止了浏览器发送 Referer，而“不蒜子”正好使用 Referer 来识别每个页面，所以，这是一个再明显不过的因为需求变更而引入的 Bug。这个世界最离谱的事情，就是大家都认为程序员是一本“十万个为什么”，每次一出问题就找到程序员这里。其实，程序员是再普通不过的芸芸众生里的一员，人们喜欢听/看到自己愿意去听/看到的事物，而程序员同样喜欢解决自己想去解决的问题。所以，今天的话题是关于如何设计一个 PV/UV 统计系统。OK，Let\u0026rsquo;s Hacking Begin。\nPV/UV 的概念 首先，我们从两个最基本的概念 PV 和 UV 开始说起。我们都知道，互联网产品的核心就是流量，前期通过免费的产品吸引目标客户的目的，在积累了一定用户流量以后，再通过广告等增值服务实现盈利，这可以说是互联网产品的典型商业模式啦。而在这个过程中，为了对一个产品的流量进行科学地分析，就产生了譬如访客数(UV)、浏览量(PV)、访问次数(VV)等等的概念，这些概念通常作为衡量流量多少的指标。除此以外，我们还有类似日活跃用户(DAU)、月活跃用户(MAU)等等这种衡量服务用户粘性的指标，以及平均访问深度、平均访问时间、跳出率等等这种衡量流量质量优劣的指标。如果各位和我一样都写博客的话，对这些概念应该都不会感到陌生，因为我们多多少少会使用到诸如百度站长、站长统计、腾讯统计、Google Analytics这样的统计服务，这些统计服务可以让我们即时掌握博客的访问情况。博主目前使用了腾讯统计来查看整个博客的流量情况，而每一篇博客的访问量则是通过**“不蒜子”**这个第三方服务，这里再次对作者表示感谢。\n使用腾讯统计来查看网站的流量情况\r回到问题本身，PV，即Page View，表示页面浏览量或者点击量，每当一个页面被打开或者被刷新，都会产生一次 PV，只要这个请求从浏览器端发送到了服务器端。聪明的各位肯定会想到，如果我写一个爬虫不停地去请求一个页面，那么这个页面的 PV 不就会一直增长下去吗？理论上的确是这样，所以，我们有第二个指标 UV，来作为进一步的参考，所谓 UV，即Unique Visitor，表示独立访客数。在上面这个问题中，尽管这个页面的 PV 在不断增长，可是因为这些访客的 IP 都是相同的，所以，这个页面只会产生一次 UV，这就是 PV 和 UV 的区别。所以，我们结合这两个指标，可以非常容易得了解到，这个页面实际的访问情况是什么样的。这让我想起数据分析中的一个例子，虽然以统计学为背景的数学计算不会欺骗人类，可如果人类片面地相信某一个方面的分析结果，数据分析一样是带有欺骗性的。就像有人根据《战狼 2》和《前任 3》两部电影的观众购买冷/热饮的情况，得出下面的结论：看动作片的观众更喜欢喝冷饮来清凉紧绷着的神经，而看爱情片的观众更喜欢喝热饮来温暖各自的内心。其实想想就知道这里混淆了因果性和相关性，选择冷饮还是热饮无非是两部电影上映的季节不同而已。\n如何设计一个访问统计系统 OK，了解了 PV 和 UV 的概念后，我们来思考如何去设计一个访问统计系统，这是今天这篇博客的主题内容。我知道，如果问如何设计一个访问系统，大家可能都会不由自主地想到建两张表。的确，这是最简单的做法。可问题是，我们对于 PV 的认识，其实一直都在不断地变化着。比如 PV 的定义是是一个页面被打开或者被刷新时视为一次有效 PV，所以，我们通常的做法是在页面底部嵌入 JavaScript 脚本，这种方式一直工作得非常好。可在引入 AJAX 以后，用户几乎不会主动去刷新页面，那么，在这个过程中用户点击更多或者使用下拉刷新时，是否应该算作一次有效 PV 呢？甚至在 PC 端网页逐渐式微以后，越来越多的工作转移到手机等移动设备上来，越来越多的原生+Web 混合 App 或者是单页面应用(SPA)或者是渐进式应用(PWA)，此时我们又该如何认识 PV 呢？微信公众号里的 PV 甚至更为严格，必须通过微信内置的浏览器访问才能算作一次有效 PV。\n可以发现，我们对 PV 的认识其实一直在不断的变化着，更多的时候，我们想追踪的并非页面被加载(Page Load)的次数，而是页面被浏览(Page View)的次数。这时候，我们可以 Page Visiblity 和 History API 结合的方式。前者在页面的 visibilityState 可见或者由隐藏变为可见时发送一次 Page View，而后者则是在浏览器地址发生变化的时候发送一次 Page View。这听起来非常像单页面应用(SPA)里前端路由的那套玩法，的确，当一个地址中的 pathname 或者 search 部分发生变化时，应该发送一次 Page View 请求，而 hash 部分的变化则应该忽略，因为它表示的是应用内部页面的跳转。对于页面的 visibilityState 由隐藏变为可见，不同的人有不同的看法，因为有时我们像合并多次 Page View，而有时候则想通过 Page View 了解所谓的”回头客“，所以，这里面还可以继续引入 Session 的概念，比如 Google Analytics 默认会在 30 分钟内无交互的情况下结束。所以，这个问题要考虑的东西实际上比想象中的要多。\n现在，我们至少可以前端部分达成共识，即通过在前端页面上埋点的方式收集 PV 和 UV。就像我们设计一个 Page View 的表结构会非常简单，而一旦要开始考虑 Unique Visitor，可能我们就需要收集诸如 IP、省市、UA 等等的信息，这些信息的数量会非常大，而 Page View 的数据规模实际上取决于一个站点下有多少个页面。所以，这些数据在后端要怎么样处理，这是我们接下来要去考虑的问题。直接去写数据库是万不得已的做法，因为如果你处理不好并发的问题，这些统计数据的正确性就会让人产生怀疑，所以，接下来，我们介绍三种不同的方法来处理这类问题，它们分别是：通过 Nginx 的 access_log 实现统计、通过 Redis 的 Hyperlog 实现统计，以及通过 LeanCloud 的 Hook 实现统计。同大家一样，我是第一次考虑这类问题，如果有什么不周到的地方，希望大家可以谅解。\n通过 Nginx 的 access_log 实现统计 我们首先来介绍 Nginx 的 access_log，顾名思义，这是 Nginx 的访问日志，由 ngx_http_log_module 模块提供相应功能。Nginx 会把每一个用户访问网站的日志信息记录到指定文件里，从而帮助网站提供者分析用户的浏览行为。而 PV/UV 则是分析用户的浏览行为的最基础指标，所以，通过 Nginx 的访问日志来统计 UV 和 PV 是再合适不过的啦！在 Nginx 里主要使用log_format和access_log 两条指令来完成相关的配置。这里以博主自己使用的配置为例来说明：\nlog_format main \u0026#39;$remote_addr - $remote_user [$time_iso8601] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log logs/access.log main; 可以注意到，我们在这里首先通过log_format命令定义了一个日志格式，而这个日志格式则被定义为 main，这表示我们我们可以在 Nginx 的配置文件中定义多个日志格式。它其实就是一个日志模板，相信大家在使用 NLog、Log4Net 这类日志库的时候，都接触过 Layout 这个概念，这里就是 Nginx 中访问日志的 Layout。那么，在定义了这样一个日志格式以后，我们该怎么使用这个日志格式呢？这就要说到下面的access_log指令，它的基本用法就是一个路径 + 一个模板，在这里我们使用了定义好的 main 模板，然后指定了日志路径为：\\logs\\localhost.access_log.log。当然啦，大家使用 NLog 和 Log4Net 时，日志对应的 Layout 中都会有“变量”这样的概念，同样地，在 Nginx 中我们有一些常用的“变量”：\nNginx 日志变量 说明 $remote_addr 记录访问网站的客户端地址 $http_x_forward_for 当前端有代理服务器时，设置 Web 节点记录客户端地址的配置 $remote_user 远程客户端用户名称 $time_local 记录带时区的访问时间 $request 记录用户 HTTP 请求起始行信息 $status 记录用户 HTTP 请求状态码 $body_bytes_sents 记录服务端返回给客户端响应 Body 字节数 $http_referer 记录本次请求是从哪一个链接访问过来的 $http_user_agent 记录客户端类型信息，比如 Chrome、微信等等 为什么说这些时最常用的“变量”呢？因为通过这些，我们想要统计 PV 和 UV 的想法就能变成现实，关于更多的 Nginx 日志变量，大家可以从这里来了解：http://nginx.org/en/docs/http/ngx_http_log_module.html。现在，通过 Nginx 托管一个简单的静态页面，然后在浏览器中访问：localhost:9090，此时，我们应该可以在前面设置的日志路径里找到 Nginx 生成的日志文件，它大概长下面这个样子：\nNginx日志长什么样子\rOK，现在有日志文件啦，这 PV/UV 到底从哪里来呢？其实，到这里已经无所谓用什么方法啦，因为你可以用 ELK 全家桶把给它收集了去，或是选一门你喜欢的语言用正则给它匹配出来，这都完全没有问题，无非就是一个工具选择的问题。为了简单起见，我们直接用 Shell 命令：\n#统计指定页面的PV grep / localhost.access.log | wc -l grep /favicon.ico localhost.access.log | wc -l #统计站点PV awk \u0026#39;{print $6}\u0026#39; localhost.access.log | wc -l #$6表示模板中的第6个变量，即Referer #统计访客IP awk \u0026#39;{print $1}\u0026#39; localhost.access.log | sort -r |uniq -c |wc -l #$1表示模板中第一个变量，即客户端IP 至此，我们就达到了基于 Nginx 访问日志实现 PV/UV 统计的目的。我知道有同学要问啦，你不是说要在前端通过埋点的方式来收集访客的信息吗，你这说了半天，完全就是说 Nginx 的事情嘛！的确，我们现在可以统计出自己网站的 PV/UV 了，可如果我们想对外提供一个访问统计的服务，我们又该如何做呢？这里简单分享下博主的思路，因为开发环境一直不是很稳定，所以，一直没有时间动手去实践(逃。 一种PV/UV统计的思路\r通过这张图片，我们可以大致梳理出整个流程，即前端页面中通过 JavaScript 来调用后端提供的 Analysis Service，此时这个请求会携带一个 Referer 信息，而这个 Referer 对应被访问的站点。注意到这个后端服务经过了一层 Nginx 转发，显然 Nginx 可以获得客户端的 IP 地址，这两个结合起来，表示的就是某个 IP 访问了某个站点，即 PV。像百度站长和腾讯统计会在页面中注入一个 token 或者 Id，主要用途就是确保请求的确是从当前站点中发出的，这就是这类访问统计产品统计的原理。也许在计算 PV/UV 的算法上存在差异，然而核心的原理应该没多大差别啦！\n通过 Redis 的 HyperLogLog 实现统计 不知道大家有没有发现，统计 PV 其实蛮简单的，因为它只需要对访问量做更新即可。可统计 UV 就会有点麻烦啦，因为同一个人可以多次访问同一篇文章。有时候我们希望统计一天内的访客数，而有时候我们希望统计一周甚至一个月内的访客数，所以，UV 并不像 PV 那样简单，PV 更多的时候是作为一种“汇总”数据，而 UV 则有“实时”的属性。简而言之，我们需要一张表来记录访客数据，博主在设计这张表的时候，更是引入了地理位置、UserAgent 等等相关的字段设计，因为我们会有了解访客来源、访客设备等等一系列“行为”相关的数据记录。对应到数据库的概念，VisitorRecored 这张表实际上是在不停地写入记录的。那么，面对每一个查看实时访客数的请求，我们真的要每次都要去这张表里统计一遍吗？也许我们会想到使用数据库任务去做定时的汇总，或者是任意形式的定时任务譬如 CORN、Hangfire，在这里，我们有更好的选择——HyperLogLog。\n什么是 HyperlLogLog 呢？我们提到的统计 UV 的这个场景，实际上是一个基数计数(Cardinality Counting)的问题，即统计一个集合中不重复的元素个数，例如集合{1,3,5,7,5,7,8}的基数为 5。所以，HyperLogLog 实际上就是一个在误差允许的范围内，快速统计元素数目的算法。为什么说是误差允许范围内呢？因为它来源于一个经典的概率分布——伯努利分布。高中时候，老师讲到这个知识，我们笑称它为“白努力”，因为有一段时间，排列组合属于我怎么学都学不会东西，可不就是白努力吗？HyperLogLog 是在 LogLog 的基础上优化的一种算法，它主要的改进是采用了桶算法作为每一轮伯努利实验的估计值，同时使用调和平均数代替平均数，进而求出最终的估算值。它可以在不存储整个集合的情况下，使用极小的内存统计出集合元素的个数。\n对应到 Redis 里，主要体现在 PFADD、PFCOUNT、PFMERGE 三个命令上。\nPFADD：将多个值存入指定的 HyperLogLog。 PFCOUNT：获取指定 HyperLogLog 的基数。 PFMERGE：合并多个 HyperLogLog，合并前与合并后的基数一致(取并集)。 博主在写这篇博客的时候，基于 LeanCloud 的访问统计LeanCloud-Counter已经再线上运行了一段时间。下面，我们就以这些数据为例来展示下 HyperLogLog 的用法。为了方便起见，我选择使用 Python 来读写 Redis：\n# 连接Redis r = redis.Redis(host=\u0026#39;localhost\u0026#39;, port=6379, db=0) # 查询访客记录 VisitorRecord = leancloud.Object.extend(\u0026#39;VisitorRecord\u0026#39;) query = VisitorRecord.query query.limit(1000) queryResults = query.find() # 对每个页面使用PFADD for result in queryResults: r.pfadd(result.get(\u0026#39;page_url\u0026#39;),result.get(\u0026#39;visitor_ip\u0026#39;)) # 使用PFCOUNT返回每个页面的基数 pageUrls = list(set(map(lambda x:(x.get(\u0026#39;page_url\u0026#39;),x.get(\u0026#39;page_title\u0026#39;),r.pfcount(x.get(\u0026#39;page_url\u0026#39;))), queryResults))) pageUrls = sorted(pageUrls,key=lambda x:x[2],reverse=True) print(pageUrls[0:10]) 运行完脚本，我们可以统计出访客数目：\n使用HyperLogLog统计访客数目\r通过 LeanCloud 的 Hooks 实现统计 像 Hexo、Jekyll 这类静态博客，本质上是非常依赖 Valine、不蒜子等等的第三方服务，而使用 LeanCloud 作为访问量统计的服务提供商，更是早就在博客圈子里流行了。不过我注意到，这些设计都少都会有一点不足，那就是网上的各种设计都没有实现站点的 PV/UV 统计。当我被迫从”不蒜子“上迁移过来以后，我其实非常想实现一个和”不蒜子“一模一样的统计服务，因为这样子的话，我对博客的修改会非常非常小。所以， 我不得不在现有方案上扩展更多的功能，实现单篇文章的 UV、整个站点的 PV/UV、访客 IP/地理位置、客户端 UA 等的统计功能。\n在这个过程中，我发现 LeanCloud 不支持传统关系型数据库里的 Sum()操作，而我更不想在客户端通过分页去对表记录做 Sum()操作。官方提供了离线分析和云函数，可这两个东西都是商业版里支持的东西。最终我找到了，通过 Hooks 来实现站点 PV/UV 统计的这样一种方法。所谓 Hooks，你可以理解为传统关系型数据库里的触发器，它可以在你更新或者插入某个对象的时候，去做一点额外的工作。所以，单篇文章会根据文章链接+访客 IP 生成一条 UV，而 PV 则是每次打开文章就视为一条 PV。所以，最终的方案是插入访客记录(VisitorRecord)时更新文章的对应的访问次数(VisitorCounter)，而单篇文章的更新则会触发站点 UV/PV 的更新。听起来有点绕人，我们直接来看下面的代码：\n//新建访客记录时，更新对应的UV记录 AV.Cloud.afterSave(\u0026#39;VisitorRecord\u0026#39;, async function(request) { var query = new AV.Query(\u0026#39;VisitorCounter\u0026#39;); var page_url = request.object.get(\u0026#39;page_url\u0026#39;); console.log(\u0026#39;query page_url: \u0026#39; + page_url); query.equalTo(\u0026#39;page_url\u0026#39;, page_url); return query.find().then(function (counters) { if (counters.length \u0026gt; 0){ counters[0].increment(\u0026#39;page_uv\u0026#39;); console.log(\u0026#39;increment UV of page_url: \u0026#39; + page_url + \u0026#34;, \u0026#34; + counters[0].get(\u0026#39;page_pv\u0026#39;)); return counters[0].save() } }); }); //页面PV/UV更新时，更新站点PV/UV AV.Cloud.afterUpdate(\u0026#39;VisitorCounter\u0026#39;, async function(request) { var page_url = request.object.get(\u0026#39;page_url\u0026#39;); if(page_url.indexOf(\u0026#39;//\u0026#39;) == -1){ return; } var site_url = page_url.split(\u0026#39;//\u0026#39;)[1]; site_url = site_url.substring(0, site_url.indexOf(\u0026#39;/\u0026#39;)); console.log(\u0026#39;now to update site PV/UV with: \u0026#39; + site_url); if (request.object.updatedKeys.indexOf(\u0026#39;page_pv\u0026#39;) != -1) { var query = new AV.Query(\u0026#39;VisitorCounter\u0026#39;); query.equalTo(\u0026#39;page_url\u0026#39;,site_url); query.find().then(function(counters){ if(counters.length\u0026gt;0){ counters[0].increment(\u0026#39;page_pv\u0026#39;); console.log(\u0026#39;update site PV of \u0026#39; + site_url + \u0026#34;, \u0026#34; + counters[0].get(\u0026#39;page_pv\u0026#39;)); return counters[0].save(); } }); } else if (request.object.updatedKeys.indexOf(\u0026#39;page_uv\u0026#39;) != -1) { var query = new AV.Query(\u0026#39;VisitorCounter\u0026#39;); query.equalTo(\u0026#39;page_url\u0026#39;,site_url); query.find().then(function(counters){ if(counters.length\u0026gt;0){ counters[0].increment(\u0026#39;page_uv\u0026#39;); console.log(\u0026#39;update site PV of \u0026#39; + site_url + \u0026#34;, \u0026#34; + counters[0].get(\u0026#39;page_uv\u0026#39;)); return counters[0].save(); } }); } }); 实际上这里整个站点的 UV 统计是不严谨的，因为严格地来讲，同一个 IP 访问了同一个站点下的 N 篇文章，它的 UV 严格地来说应该算 1 次，可我们这个方案本身就是向 LeanCloud 妥协的一种做法，就像我这里直接使用了location.href和document.title，它带来的问题就是，一个网站的域名或者链接发生变化的时候，访问统计就会被重置从 0 开始。“不蒜子”本身就有这个问题。所以，博主这个博客从 15 年到现在，总访问量只有 3 万多，就是因为中间更换过两次域名。从我切换到自己写的统计服务以后，我发现每天来读我博客的人居然不少，我实在不忍心写下这种夸自己的句子啊！\n想解决这个问题，并不是没有办法。像博主一开始设计的时候，是打算用每个页面唯一的 Id 来存储的，而这就要通过 HTML5 中的**data-**或者通过 JavaScript 来传参。可当你打算设计一个更通用的东西的时候，这些想法就显得有点多余，我和大部分人一样，喜欢开箱即用的东西，所以，最好它可以像大多数统计服务一样，只需要在页面里加入一行 JavaScript 脚本。所以，最终采用这样的设计是为了最大限度的开箱即用。考虑到“不蒜子”里因为更换域名而导致的访问统计重置的问题，我增加了一个初始化站点 UV/PV 的功能，满足了像博主这样虚荣心爆棚的人的需要。这一刻，我突然觉得，我和产品经理们一样“自信”啊。正如你所看到的这样，博客底部的访问统计已经从“不蒜子”切换到“LeanCloud-Counter”，为此我在博客上增加了LeanCloud的链接，也许下一阶段会加上 Heroku，总之，我已经完成了访问统计的平滑切换。关于这个项目，如果大家感兴趣，可以参考这个地址：LeanCloud-Counter。\n本文小结 这篇文章写下来，最大的感受或许是，有一台 Linux 环境的服务器是多么的重要。起初，是在 Windows10 下面的 WSL 里搭了 Docker 环境，再通过 Docker 镜像搭建 Nginx，因为之前的 Consul、ELK 几乎都是这样运作的，而且一直运行的相当稳定，唯一的缺点大概就是 Docker 太容易吃硬盘，有时候难免搞出个内存不足。Nginx 搭好以后，发现需要经常改配置文件，Docker 环境里改起来相当痛苦。直接在 WSL 里安装 Nginx 的话，因为和 Windows 共享端口，和 IIS 明显搞不到一起。想通过 Docker 挂载本机分区，突然想起来 WSL 里的 Docker 只是一个客户端，真正的主角是跑在 Windows 上的 Docker for Windows。最后被迫装了 Windows 版本的 Nginx，果然还是会和 IIS 冲突，我想说，心好累有木有啊_(:з」∠)_。好了，这篇博客总算写完了！\n","date":"2019-10-22T12:50:49Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3494408209/","slug":"3494408209","tags":["访问量","Nginx","Hyperlog"],"title":"浅析网站 PV/UV 统计系统的原理及其设计"},{"categories":["编程语言"],"content":"插件化应用是个老话题啦，在我们的日常生活中更是屡见不鲜。无论是多年来臃肿不堪的 Eclipse，亦或者是扩展丰富著称的 Chrome，乃至近年来最优秀的编辑器 VSCode，插件都是这其中重要的组成部分。插件的意义在于扩展应用程序的功能，这其实有点像 iPhone 手机和 AppStore 的关系，没有应用程序的手机无非就是一部手机，而拥有了应用程序的手机则可以是 Everything。显然，安装或卸载应用程序并不会影响手机的基本功能，而应用程序离开了手机同样无法单独运行。所以，所谓“插件”，实际上是一种按照一定规范开发的应用程序，它只能运行在特定的软件平台/应用程序且无法运行。这里，最重要的一点是应用程序可以不依赖插件单独运行，这是这类“插件式”应用的基本要求。\n好了，在了解了插件的概念以后，我们来切入今天的正文。博主曾经在《基于 Python 实现 Windows 下壁纸切换功能》这篇文章中编写了一个小程序，它可以配合 Windows 注册表实现从 Unsplash 上抓取壁纸的功能。最近，博主想为这个小程序增加 必应壁纸 和 WallHaven 两个壁纸来源，考虑到大多数的壁纸抓取流程是一样的，博主决定以“插件”的方式完成这次迭代，换句话说，主程序不需要再做任何调整，当我们希望增加新的数据源的时候，只需要写一个.py 脚本即可，这就是今天这篇文章的写作缘由。同样的功能，如果使用 Java 或者 C#这类编译型语言来做，我们可能会想到为插件定义一个 IPlugin 接口，这样每一个插件实际上都是 IPlugin 接口的实现类，自然而然地，我们会想到通过反射来调用接口里的方法，这是编译型语言的做法。而面对 Python 这样的解释型语言，我们同样有解释型语言的做法。\n首先，我们从一个最简单的例子入手。我们知道，Python 中的 import 语法可以用来引入一个模块，这个模块可以是 Python 标准库、第三方库和自定义模块。现在，假设我们有两个模块：foo.py 和 bar.py。\n#foo.py import sys class Chat: def send(self,uid,msg): print(\u0026#39;给{uid}发送消息：{msg}\u0026#39;.format(uid=uid,msg=msg)) def sendAll(self,msg): print(\u0026#39;群发消息：{msg}\u0026#39;.format(msg=msg)) #bar.py import sys class Echo: def say(self): print(\u0026#34;人生苦短，我用Python\u0026#34;) def cry(): print(\u0026#34;男人哭吧哭吧不是罪\u0026#34;) 通常, 为了在当前模块(main.py)中使用这两个模块，我们可以使用以下语句：\nimport foo from bar import * 这是一种简单粗暴的做法，因为它会导入模块中的全部内容。一种更好的做法是按需加载，例如下面的语句：\nfrom foo import Chat 到这里，我们先来思考第一个问题，Python 是怎么样去查找一个模块的呢？这和 Python 中的导入路径有关，通过sys.path我们可以非常容易地找到这些路径，常见的导入路径有当前目录、site-package目录和PYTHONPATH。熟悉 Python 的朋友应该都知道，site-package和PYTHONPATH各自的含义，前者是通过 pip 安装的模块的导入目录，后者是 Python 标准库的导入目录。当前目录这个从何说起呢？事实上，从我们写下from…import…语句的时候，这个机制就已经在工作了，否则 Python 应该是找不到 foo 和 bar 这两个模块的了。这里还有相对导入和绝对导入的问题，一个点(.)和两个点(..)的问题，这些我们在这里暂且按下不表，因为我们会直接修改sys.path(逃\n在 Python 中有一种动态导入模块的方式，我们只需要告诉它模块名称、导入路径就可以了，这就是下面要说的importlib标准库。继续用 foo 和 bar 这两个神奇的单词来举例，假设我们现在不想通过 import 这种偏“静态”的方式导入一个模块，我们应该怎么做呢？一起来看下面代码：\nimport foo from foo import Chat from bar import * import importlib #调用foo模块Chat类方法 foo.Chat().send(\u0026#39;Dear\u0026#39;,\u0026#39;I Miss You\u0026#39;) moduleFoo = importlib.import_module(\u0026#39;.\u0026#39;,\u0026#39;foo\u0026#39;) classChat = getattr(moduleFoo,\u0026#39;Chat\u0026#39;) classChat().send(\u0026#39;Dear\u0026#39;,\u0026#39;I Miss You\u0026#39;) #调用bar模块Echo类方法 Echo().say() moduleBar = importlib.import_module(\u0026#39;.\u0026#39;,\u0026#39;bar\u0026#39;) classEcho = getattr(moduleBar,\u0026#39;Echo\u0026#39;) classEcho().say() #调用bar模块中的cry()方法 cry() methodCry = getattr(moduleBar,\u0026#39;cry\u0026#39;) methodCry() 可以注意到，动态导入可以让我们在运行时期间引入一个模块(.py)，这恰恰是我们需要的功能。为了让大家对比这两种方式上的差异，我给出了静态引入和动态引入的等价代码。其中，getattr()其实可以理解为 Python 中的反射，我们总是可以按照模块-\u0026gt;类-\u0026gt;方法的顺序来逐层查找,即：通过 dir()方法，然后该怎么调用就怎么调用。所以，到这里整个“插件化”的思路就非常清晰了，即：首先，通过配置来为 Python 增加一个导入路径，这个导入路径本质上就是插件目录。其次，插件目录内的每一个脚本文件(.py)就是一个模块，每个模块都有一个相同的方法签名。最终，通过配置来决定要导入哪一个模块，然后调用模块中类的实例方法即可。顺着这个思路，博主为 WallPaper 项目引入了插件机制，核心代码如下：\nif(pluginFile == \u0026#39;\u0026#39; or pluginName == \u0026#39;\u0026#39;): spider = UnsplashSpider() imageFile = spider.getImage(downloadFolder) setWallPaper(imageFile) else: if(not check(pluginFile,addonPath)): print(\u0026#39;插件%s不存在或配置不正确\u0026#39; % pluginName) return module = importlib.import_module(\u0026#39;.\u0026#39;,pluginFile.replace(\u0026#39;.py\u0026#39;,\u0026#39;\u0026#39;)) instance = getattr(module,pluginName) imageFile = instance().getImage(downloadFolder) setWallPaper(imageFile) 接下来，我们可以很容易地扩展出 必应壁纸 和 WallHaven 两个“插件”。按照约定，这两个插件都必须实现 getImage()方法，它接受一个下载目录作为参数，所以，显而易见，我们在这个插件里实现壁纸的下载，然后返回壁纸的路径即可，因为主程序会完成剩余设置壁纸的功能。\n# 必应每日壁纸插件 class BingSpider: def getImage(self, downloadFolder): searchURL = \u0026#39;https://cn.bing.com/HPImageArchive.aspx?format=js\u0026amp;idx=0\u0026amp;n=1\u0026amp;mkt=zh-CN\u0026#39; response = requests.get(searchURL) data = json.loads(response.text) resultId = data[\u0026#39;images\u0026#39;][0][\u0026#39;hsh\u0026#39;] resultURL = \u0026#39;https://cn.bing.com\u0026#39; + data[\u0026#39;images\u0026#39;][0][\u0026#39;url\u0026#39;] print(u\u0026#39;正在为您下载图片:%s...\u0026#39; % resultId) if(not path.exists(downloadFolder)): os.makedirs(downloadFolder) jpgFile = resultId + \u0026#39;.jpg\u0026#39; jpgFile = os.path.join(downloadFolder, jpgFile) response = requests.get(resultURL) with open(jpgFile,\u0026#39;wb\u0026#39;) as file: file.write(response.content) return jpgFile # WallHaven壁纸插件 class WallHavenSpider: def getImage(self,downloadFolder): url = \u0026#39;https://alpha.wallhaven.cc/wallpaper/\u0026#39; response = requests.get(url) print(response.text) soup = BeautifulSoup(response.text,\u0026#39;html.parser\u0026#39;) imgs = soup.find_all(\u0026#39;img\u0026#39;) length = len(imgs) if length \u0026gt; 0: match = random.choice(imgs) rawUrl = match.get(\u0026#39;src\u0026#39;) rawId = rawUrl.split(\u0026#39;/\u0026#39;)[-1] rawUrl = \u0026#39;https://w.wallhaven.cc/full/\u0026#39; + rawId[0:2] + \u0026#39;/wallhaven-\u0026#39; + rawId raw = requests.get(rawUrl) imgFile = os.path.join(downloadFolder, rawId) with open(imgFile,\u0026#39;wb\u0026#39;) as f: f.write(raw.content) return imgFile 好了，现在功能是实现了，我们来继续深入“插件化”这个话题。考虑到 Python 是一门解释型的语言，我们在编写插件的时候，更希望做到“热插拔”，比如修改了某个插件后，希望它可以立刻生效，这个时候我们就需要重新加载模块，此时 importlib 的 reload 就能满足我们的要求，这正是博主一开始就要使用 importlib，而不是 import 语法对应内建方法__import__()的原因。以 C#的开发经历而言，虽然可以直接更换 DLL 实现更新，可更新的过程中 IIS 会被停掉，所以，这种并不能被称之为“热更新”。基于以上两点考虑，博主最终决定使用 watchdog 配合 importlib 来实现“热插拔”，下面是关键代码：\nclass LoggingEventHandler(FileSystemEventHandler): # 当配置文件修改时重新加载模块 # 为节省篇幅已对代码进行精简 def on_modified(self, event): super(LoggingEventHandler, self).on_modified(event) what = \u0026#39;directory\u0026#39; if event.is_directory else \u0026#39;file\u0026#39; confPath = os.path.join(sys.path[0],\u0026#39;config.ini\u0026#39;) if(what ==\u0026#39;file\u0026#39; and event.src_path == confPath): importlib.reload(module) logging.info(\u0026#34;Modified %s: %s\u0026#34;, what, event.src_path) 好了，现在我们就完成了这次“插件化”的迭代，截止到目前为止，博主共完成了 Unsplash 、 Bing 壁纸 、 WallHaven 和 国家地理 四个“源”的接入，这些插件在实现上基本大同小异，本质上来讲它们是一个又一个的爬虫，只要实现了 getImage()这个方法都可以接入进来，这就是我们通常说的“约定大于配置”，关于更多的代码细节，大家可以通过Github来了解。\n简单回顾下这篇博客，核心其实是 importlib 模块的使用，它可以让我们在运行时期间动态导入一个模块，这是实现插件化的重要前提。以此为基础，我们设计了基于 Python 脚本的单文件插件，即从指定的目录加载脚本文件，每个脚本就是一个插件。而作为插件化的一个延伸，我们介绍了 watchdog 模块的简单应用，配合 importlib 模块的 reload()方法，就可以实现所谓的“热更新”。好了，以上就是这篇博客的所有内容了，我们下一篇见！\n","date":"2019-10-11T08:56:27Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1960676615/","slug":"1960676615","tags":["Python","插件化","壁纸"],"title":"使用 Python 开发插件化应用程序"},{"categories":["前端开发"],"content":"在上一篇博客中关于 Vue 表单验证的话题里，我提到了这段时间在做的城市配载功能，这个功能主要着眼于，如何为客户提供一条路线最优、时效最短、装载率最高的路线。事实上，这是目前物流运输行业智能化、专业化的一个趋势，即面向特定行业的局部最优解问题，简单来说，怎么样能在装更多货物的同时满足运费更低的条件，同时要考虑超载等等不可抗性因素，所以，这实际上是一个数学问题。而作为这个功能本身，在地图上加载大量标注更是基础中的基础，所以，今天这篇博客想说说，通过百度地图 API 加载海量标注时，关于性能优化方面的一点点经验。\n问题还原 根据 IP 定位至用户所在城市后，后台一次性查询出近一个月内的订单，然后将其全部在地图上展示出来。当用户点击或者框选标注物时，对应的订单配载到当前运单中。当用户再次点击标注物，则对应的订单从当前运单中删除。以西安市为例，一次性加载 850 个左右的订单，用户操作一段时间后，Chrome 内存占用达 250 多兆，拖拽地图的过程中可以明显地感觉到页面卡顿。因为自始至终，地图上的订单数量不变，即不会移除覆盖物，同时需要在内存中持久化订单相关的信息。所以，在城市配载 1.0 版本的时候，测试同事给我提了一个性能方面的 Bug。可开始提方案并坚持这样做的，难道不是产品吗？为什么要给开发提 Bug 呢？OK，我们来给不靠谱的产品一点点填坑吧，大概想到了下面三种方案，分别是标注物聚合 、Canvas API 和视野内可见。\n密密麻麻的地图\r标注物聚合方案 所谓“标注物聚合”，就是指在一定的地图层级上，地图上的覆盖物主要是以聚合的形式显示的，譬如显示某一个省份里共有多少个订单，而不是把所有订单都展示出来，除非地图放大到一定的层级。这种其实在我们产品上是有应用的，比如运单可视化基本上是全国范围内的车辆位置，这个时候在省一级缩放比例上使用聚合展示就非常有必要。可在城市配载这里就相当尴尬啦，因为据说客户会把地图放大到市区街道这种程度来对订单进行配载，所以，这种标注物聚合方案的效果简直是微乎其微，而且更尴尬的问题在于，官方的 MarkerClusterer 插件支持的是标准的覆盖物，即 Marker 类。而我们的产品为了好看、做更复杂的交互，设计了更复杂的标记物原型，这就迫使我们必须使用自定义覆盖物，而自定义覆盖物通常会用 HTML+CSS 来实现。\n标注聚合器MarkerClusterer\r所以，一个简洁的 Marker 类和复杂的 DOM 结构，会在性能上存在巨大差异，这恰恰是我们加载了 800 多个点就产生性能问题的原因，因为一个“好看”的标注物，居然由 4 个 DOM 节点组成，而这个“好看”的标注物还不知道要怎么样实现 Marker 类里的右键菜单。所以，追求“好看”有问题吗？没有，可华而不实的“好看”，恰恰是性能降低的万恶之源，更不用说，因为覆盖物不会从地图上删除，每次框选都要进行 800 多次的点的检测了，而这些除了开发没有人会在乎，总有人摆出一副“这个需求很简单，怎么实现我不管”的态度……虽然这种方案已经被 Pass 掉了，这里我们还是通过一个简单的示例，来演示下 MarkerClusterer 插件的简单使用吧！以后对于前端类的代码，博主会优先使用 CodePen 进行展示，因为这样子显然比贴代码要生动呀！\nSee the Pen Marker-Clusterer by qinyuanpei (@qinyuanpei)\ron CodePen.\r这里稍微提带说一下这个插件的优化，经博主测试，在标记物数目达到 100000 的时候，拖拽地图的时候可以明显的感觉的卡顿，这一点大家可以直接在 CodePen 中进行测试。产生性能问题的原因主要在以下代码片段：\n/** * 向该聚合添加一个标记。 * @param {Marker} marker 要添加的标记。 * @return 无返回值。 */ Cluster.prototype.addMarker = function(marker){ if(this.isMarkerInCluster(marker)){ return false; }//也可用marker.isInCluster判断,外面判断OK，这里基本不会命中 if (!this._center){ this._center = marker.getPosition(); this.updateGridBounds();// } else { if(this._isAverageCenter){ var l = this._markers.length + 1; var lat = (this._center.lat * (l - 1) + marker.getPosition().lat) / l; var lng = (this._center.lng * (l - 1) + marker.getPosition().lng) / l; this._center = new BMap.Point(lng, lat); this.updateGridBounds(); }//计算新的Center } marker.isInCluster = true; this._markers.push(marker); var len = this._markers.length; if(len \u0026lt; this._minClusterSize ){ this._map.addOverlay(marker); //this.updateClusterMarker(); return true; } else if (len === this._minClusterSize) { for (var i = 0; i \u0026lt; len; i++) { this._markers[i].getMap() \u0026amp;\u0026amp; this._map.removeOverlay(this._markers[i]); } } this._map.addOverlay(this._clusterMarker); this._isReal = true; this.updateClusterMarker(); return true; }; 这段代码主要的问题在于频繁地向地图添加覆盖物，换言之，在这里产生了对 DOM 的频繁修改，具体可参考 _addToClosestCluster 方法。一种比较好的优化是，等所有计算结束后再一次性应用到 DOM。所以，这里我们可以封装一个 render() 方法：\nCluster.prototype.render = function(){ var len = this._markers.length; if (len \u0026lt; this._minClusterSize) { for (var i = 0; i \u0026lt; len; i++) { this._map.addOverlay(this._markers[i]); } } else { this._map.addOverlay(this._clusterMarker); this._isReal = true; this.updateClusterMarker(); } } 关于原理介绍及性能对比方面的内容，大家可以参考这篇文章：百度地图点聚合 MarkerClusterer 性能优化\nCanvas API 方案 OK，接下来介绍第二种方案，其实从 Canvas API 你就可以想到我要说什么了。Canvas API 是 HTML5 中提供的图形绘制接口，类似于我们曾经接触过的 GDI/GDI+、Direct2D、OpenGL 等等。有没有觉得和游戏越来越近啦，哈哈！百度地图 API v3 中提供了基于 Canvas API 的接口，我们可以把这些“好看”的覆盖物绘制到一个层上面去，显然这种方式会比 DOM 更高效，因为博主亲自做了实验，一次性绘制 10 万个点放到地图上，真的是一点都不卡诶，要说缺点的话嘛，嗯，你想嘛，这都是不是 DOM 了，产品经理那些吊炸天的脑洞还怎么搞？比如最基本的点击，可能要用简单的 2D 碰撞来处理啦，然后就是常规的坐标系转换，听起来更像是在做游戏了，对不对？谁让那么多的游戏都是用 HTML5 开发的呢？同样的，这里给出一个简单的示例：\nSee the Pen Marker-Clusterer by qinyuanpei (@qinyuanpei)\ron CodePen.\r这个方案真正尝试去做的时候，发现最难的地方是给 Canvas 里的元素绑定事件，细心的朋友会发现，博主在这里尝试了两种方案。**第一种，通过判断点是否在矩形内来判断是否完成了点击，主要的问题是随着点的数目的增加判断的量级会越来越大。第二种，通过 addHitRegion()增加一个可点击区域，这种的性能明显要比第一种好，唯一的限制在于浏览器的兼容性。**目前，需要在 Chrome 中开启 Experimental Web Platform features 。这个探索的过程是相当不易的，大家可以通过 CodePen 进一步感受一下哈！\n视野内可见方案 相信大家听完前两个方案都相当失望吧，一个方案用不了，一个方案太麻烦，那这个肯定就是最终可行的方案了吧！猜对了，这真的是体现了大道至简，一开始试着从内存里持久化的数据入手，可最终收到效果的反而是这个最不起眼的方案。简单来说，就是把视野内的覆盖物设为 visible，而把视野外的覆盖物设置 hidden。相当朴素的一种思维对吧，百度地图 API 中有一个返回当前视野的接口 GetBounds()，它回返回一个矩形。所以，我们只需要调用百度接口判断覆盖物在不在这个矩形里就可以了，显然，这里又会循环 800 多次，不过产品经理们都不在乎对吧……顺着这个思路，我们可以写出下面的代码，并在拖动地图和缩放地图的时候调用它：\n//监听地图缩放/拖拽事件 map.addEventListener(\u0026#34;moveend\u0026#34;, showOverlaysByView); map.addEventListener(\u0026#34;zoomend\u0026#34;, showOverlaysByView); //根据视野来显示或隐藏覆盖物 function showOverlaysByView() { var bounds = map.getBounds(); for (var i = 0; i \u0026lt; overlays.length; i++) { var overlay = overlays[i]; var point = overlay._point; if (BMapLib.GeoUtils.isPointInRect(point, bounds) || BMapLib.GeoUtils.isPointOnRect(point, bounds)) { overlay.show(); } else { overlay.hide(); } } } 现在，我只能说，效果挺显著，拖动地图的时候不会卡顿了，因为 visible 和 hidden 的切换会引发浏览器重绘，对于这一切我个人表示满意。当然，这一切离好还很遥远，因为，人类的需要是永无止境的啊。\n本文小结 就在我写下这篇博客的时候，产品经理热情洋溢地给我描述了城市配载 2.0 的设想。看了看同类产品的相关设计，我预感这个功能会变成一个以地图为核心的可视化运输系统，这符合国内用户一贯的“大而全”的使用习惯，地图上的交互会更加复杂，需要展示的信息会越来越多，所以，这篇文章里提到的优化，在未来到底有没有用犹未可知。我只能告诉你这样几个原则：尽可能的使用 Marker 类；尽可能的简化 DOM 结构；地图层级变化越大越要考虑使用聚合；视野外的覆盖物该隐藏就隐藏(反正看不到咯……)。一次性加载百万级别数据要求，我从来不觉得合理，因为就算我能加载出来，你能看的过来吗？本身就是伪需求好吧(逃……好了，这就是这篇博客的全部内容啦，以上……\n","date":"2019-09-10T09:44:18Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3131944018/","slug":"3131944018","tags":["地图","标注","配载"],"title":"百度地图加载海量标注性能优化策略"},{"categories":["编程语言"],"content":"本文开篇第一句话，想引用鲁迅先生《祝福》里的一句话，那便是：“我真傻，真的，我单单知道后端整天都是 CRUD，我没想到前端整天都是 Form 表单”。这句话要从哪里说起呢？大概要从最近半个月的“全栈工程师”说起。项目上需要做一个城市配载的功能，顾名思义，就是通过框选和拖拽的方式在地图上完成配载。博主选择了前后端分离的方式，在这个过程中发现：首先，只要有依赖 jQuery 的组件，譬如 Kendoui，即使使用了 Vue，依然需要通过 jQuery 去操作 DOM。其次，只有有通过 Rozar 生成的 DOM，譬如 HtmlHelper，Vue 的双向绑定就突然变得尴尬起来，更不用说，Rozar 中的@语法和 Vue 中的@指令相互冲突的问题，原本可以直接用 v-for 生成列表，因为使用了 HtmlHelper，突然一下子变得厌恶起来，虽然 Rozar 语法非常强大，可我依然没有在 JavaScript 里写 C#的热情，因为实在太痛苦啦 Orz……\n所以，想做好前后端分离，首先需要分离出一套前端组件库，做不到这一点，前后端分离就无从谈起，就像我们公司的项目，即使框架切换到.NET Core，可是在很长的一段时间里，我们其实还是再写 MVC，因为所有的组件都是后端提供的 HtmlHelper/TagHelper 这种形式。我这次做项目的过程中，其实是通过 jQuery 实现了一部分组件，正因为如此，一个在前后端不分离时非常容易实现的功能，在前后端分离以后发现缺好多东西，就比如最简单的表单验证功能，即便你是在做一个新项目，为了保证产品在外观上的一致性，你还是得依赖老项目的东西，所以，这篇博客主要想说说前后端分离以后，Vue 的时代怎么去做表单的验证。因为我不想测试同事再给我提 Bug，问我为什么只有来自后端接口的验证，而没有来自前端页面的验证。我希望，在写下这篇博客之前，我可以实现和老项目一模一样的表单验证。如同 CRUD 之于后端，80%的前端都是在写 Form 表单，所以，这个事情还是挺有意思的。\n最简单的表单验证 OK，作为国内最接“地气”的前端框架，Vue 的文档可以说是相当地“亲民”啦！为什么这样说呢，因为其实在官方文档中，尤大已经提供了一个表单验证的示例，这个示例让我想起给某银行做自动化工具时的情景，因为这两者都是采用 MVVM 的思想，所以，理解起来是非常容易的，即：通过一个列表来存储错误信息，而这个错误信息会绑定到视图层，所以，验证的过程其实就是向这个列表里添加错误信息的过程。我们一起来看这个例子：\n\u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;你好，请登录\u0026lt;/h2\u0026gt; \u0026lt;div\u0026gt; \u0026lt;form id=\u0026#34;loginFrom\u0026#34;\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;邮箱\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;inputEmail3\u0026#34; placeholder=\u0026#34;Email\u0026#34; v-model=\u0026#34;email\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;密码\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;inputPassword3\u0026#34; placeholder=\u0026#34;Password\u0026#34; v-model=\u0026#34;password\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default login\u0026#34; v-on:click=\u0026#34;login()\u0026#34;\u0026gt;登录\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div v-if=\u0026#34;errorList.length \u0026gt; 0\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;alert alert-danger\u0026#34; role=\u0026#34;alert\u0026#34;\u0026gt;{{errorList.join(\u0026#39;;\u0026#39;)}}\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; var vm = new Vue({ el: \u0026#39;#loginFrom\u0026#39;, data: { email: \u0026#34;\u0026#34;, password: \u0026#34;\u0026#34;, errorList: [] }, methods: { validate: function () { this.errorList = [] if (this.email == \u0026#39;\u0026#39;) { this.errorList.push(\u0026#39;请输入邮箱\u0026#39;); } else { var reg = /^([a-zA-Z]|[0-9])(\\w|\\-)+@[a-zA-Z0-9]+\\.([a-zA-Z]{2,4})$/; if (!reg.test(this.email)) { this.errorList.push(\u0026#39;请输入有效的邮箱\u0026#39;); } } if (this.password == \u0026#39;\u0026#39;) { this.errorList.push(\u0026#39;请输入密码\u0026#39;); } else { if (this.password.length \u0026lt; 6) { this.errorList.push(\u0026#39;密码长度不得少于6位\u0026#39;); } } return this.errorList.length \u0026lt;= 0; }, login: function () { if (this.validate()) { alert(\u0026#39;登录成功\u0026#39;); } } } }); \u0026lt;/script\u0026gt; 为了排除无关内容对大家的影响，写这个例子的时候，博主排除了一切复杂的 HTML 结构和 CSS 样式，经过简单润色以后，这个例子的效果展示如下，果然 GUI 满足了人们颜控的一面，可让这个世界高速运行的是 CLI，Bootstrap 是博主这种“全栈工程师”的最爱之一。这种验证方式简直是人类本能的反应，可这恰好是最糟糕的一个例子，因为这个代码完全没法复用，可以想象得到，如果再继续增加针对密码强度，譬如大小写、数字等等的验证，这个代码会混乱成什么样子，所以，这是最简单的表单验证，同样是最糟糕的表单验证。\n第一个表单验证的例子\r基于 jQuery 的表单验证 其实，如果不是因为老项目依赖 jQuery，而新项目在某些地方又需要和老项目保持一致，有谁会喜欢在 Vue 的世界里使用 jQuery 呢？因为数据驱动和事件驱动，真的是两种不同的思想，我就见过因为监听不到某个事件而花费一整天时间的人……所以，这里使用 jQuery 的表单验证插件jQuery Validation，目的只有一个，即实现博主对自己的承诺，做一个和老项目一模一样的表单验证。官方这个示例最大的问题是，它的检验逻辑扩展性比较差，后端同学对这个应该有所体会啦，譬如实际业务中常常有邮箱、手机号、非空、数字、正则等等的验证规则，而后端常常采用基于 Attribute 的验证或者是 FluentValidation 这样的库，所以，核心问题是，能不能定义相应的验证规则。接下来，我们通过 jQuery 的表单验证插件来实现验证。\n通常情况下，jQuery Validation 支持面向控件和面向代码两种验证方式。所谓面向控件，就是指在控件里添加类似required、email、range等等的扩展属性，jQuery Validation 内置了十余种标准的验证规则，基本可以满足我们的日常使用。而面向代码，就是通过 JavaScript 来定义验证规则，这就非常符合 Vue 数据驱动的风格了，因为在 JavaScript 里一切皆是对象，而这些对象可以作为 Vue 中的数据来使用。自然而然地，在第一个示例的基础上，我们可以非常容易地扩展出基于 jQuery 的表单验证：\nvar vm = new Vue({ el:\u0026#39;#loginFrom\u0026#39;, data:{ email:\u0026#34;\u0026#34;, password:\u0026#34;\u0026#34;, validators:{ rules: { email: { required: true, email: true }, password: { required: true, minlength: 6, } }, messages:{ email:{ required:\u0026#34;请输入邮箱\u0026#34;, email:\u0026#34;请输入有效的邮箱\u0026#34; }, password:{ required:\u0026#34;请输入密码\u0026#34;, minlength:\u0026#34;密码长度不得少于6位\u0026#34; } } } }, mounted:function(){ $(\u0026#39;#loginFrom\u0026#39;).validate(this.validators); } }); 对于当前表单 loginFrom，其验证规则为 validators，它完全参照jQuery Validation的 API 文档而来，具体大家可以从jQuery Validation的文档来做进一步了解。这里唯一看起来不爽的就是#loginFrom，因为它和整个 Vue 看起来格格不入。不过，像博主目前项目的处境，如果老项目里使用jQuery来对表单进行验证，而使用 Vue 开发的新项目要兼容老项目的设计风格，使用 jQuery 有什么不可以呢？不得不说，Vue 作为一个渐进式的开发框架，真正照顾了各个\u0026quot;年龄\u0026quot;段的前端工程师。使用jQuery Validation以后的表单验证效果如下：\n基于jQuery的表单验证\r通过jQuery Validation，我们或许能感觉到一点不一样的地方，那就是表单验证其实还是蛮有意思的哈。也许是因为我原本是一个无聊的人，所以看到一点新的东西就觉得有趣。就像我虽然在提交数据时在后端做了校验，可牺牲的其实是整个前端的使用体验。而如果在前端对数据进行校验，是在输入过程中校验还是在输入完成校验，是通过表单自带的提交功能还是自己发起一个 AJAX 请求，这里面的确是有非常多的细节支撑的。第一种方案不支持远程校验，这更加能说明校验本身要考虑的不单单只有前端了，同理，有了前端的校验，不代表后端可以不做校验。前端时间有人在知乎上提问，大意是说前端该不该完全信任后端返回的数据，严格来说，我们不应该信任任何人提供的数据，而这就是校验这件事情本身的意义。\n基于 Vue 的表单验证 OK，如果说前面的两种校验是因为我们有一点历史包袱，那么，接下来，我们将尝试采用更“现代化”的表单验证方式。通过 Vue 文档中关于数据校验这一节的内容，我们了解到官方推荐的两个表单验证插件是vuelidate和VeeValidate，而实际上这篇博客中的第一个例子，就是由文档中的例子演化而来。我个人比较喜欢后者，所以，下面我们将使用这个插件来完成第三个例子。首先 ，我们通过Vue-Cli创建一个 Vue 项目，然后安装下面vee-validate和vue-i18n两个插件：\nnpm install vee-validate@2.0.0 --save npm install vue-i18n 注意到这里指定了版本号，这是因为最新的 3.x 超出了我这个新人的接受范围，一句话，太难了！接下来，我们在入口文件main.js中添加下面的代码，目的是启用这两个插件：\nimport VueI18n from \u0026#39;vue-i18n\u0026#39;; import VeeValidate from \u0026#39;vee-validate\u0026#39;; import zh_CN from \u0026#39;vee-validate/dist/locale/zh_CN\u0026#39; //启用Vue国际化插件 Vue.use(VueI18n) //配置VeeValidate const i18n = new VueI18n({ locale: \u0026#39;zh_CN\u0026#39;, }) Vue.use(VeeValidate, { i18n, i18nRootKey: \u0026#39;validation\u0026#39;, dictionary: { zh_CN } }); 接下来，编写一个单文件组件LoginForm.vue:\n\u0026lt;!-- template of LoginForm --\u0026gt; \u0026lt;template\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;text-center\u0026#34;\u0026gt;你好，请登录\u0026lt;/h2\u0026gt; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;form class=\u0026#34;form-horizontal col-md-offset-4 col-md-4\u0026#34; id=\u0026#34;loginFrom\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;inputEmail3\u0026#34; class=\u0026#34;col-sm-2 control-label\u0026#34;\u0026gt;邮箱\u0026lt;/label\u0026gt; \u0026lt;div class=\u0026#34;col-sm-10\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;email\u0026#34; name=\u0026#34;email\u0026#34; placeholder=\u0026#34;Email\u0026#34; v-model=\u0026#34;email\u0026#34; v-validate=\u0026#34;\u0026#39;required|email\u0026#39;\u0026#34; data-vv-as=\u0026#34;邮箱\u0026#34;/\u0026gt; \u0026lt;p class=\u0026#34;alert alert-danger\u0026#34; role=\u0026#34;alert\u0026#34; v-show=\u0026#34;errors.has(\u0026#39;email\u0026#39;)\u0026#34;\u0026gt;{{ errors.first(\u0026#39;email\u0026#39;) }}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34; name=\u0026#34;password\u0026#34; rules=\u0026#34;required\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;inputPassword3\u0026#34; class=\u0026#34;col-sm-2 control-label\u0026#34;\u0026gt;密码\u0026lt;/label\u0026gt; \u0026lt;div class=\u0026#34;col-sm-10\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;password\u0026#34; name=\u0026#34;password\u0026#34; placeholder=\u0026#34;Password\u0026#34; v-model=\u0026#34;password\u0026#34; v-validate=\u0026#34;\u0026#39;required|min:6\u0026#39;\u0026#34; data-vv-as=\u0026#34;密码\u0026#34;/\u0026gt; \u0026lt;p class=\u0026#34;alert alert-danger\u0026#34; role=\u0026#34;alert\u0026#34; v-show=\u0026#34;errors.has(\u0026#39;password\u0026#39;)\u0026#34;\u0026gt;{{ errors.first(\u0026#39;password\u0026#39;) }}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col-sm-offset-2 col-sm-10\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;checkbox\u0026#34;\u0026gt; \u0026lt;label\u0026gt; \u0026lt;input type=\u0026#34;checkbox\u0026#34; /\u0026gt;记住密码 \u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col-sm-offset-2 col-sm-10\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default login\u0026#34; v-on:click=\u0026#34;login()\u0026#34;\u0026gt;登录\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;!-- script of LoginForm --\u0026gt; \u0026lt;script\u0026gt; export default { name: \u0026#34;LoginForm\u0026#34;, components: {}, data: () =\u0026gt; ({ email: \u0026#34;\u0026#34;, password: \u0026#34;\u0026#34; }), methods: { login: function() { } } }; \u0026lt;/script\u0026gt; \u0026lt;!-- style of LoginForm --\u0026gt; \u0026lt;style scoped\u0026gt; .login { color: white; height: 38px; width: 300px; background-color: #2b669a; } \u0026lt;/style\u0026gt; 可以看到，我们在关键的两个 input 控件上添加了v-validate和data-vv-as这两个属性。比如我们这里需要验证用户输入的邮箱是否合法、邮箱是否为空，那么我们就可以使用下面的语法：\n\u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;email\u0026#34; name=\u0026#34;email\u0026#34; placeholder=\u0026#34;Email\u0026#34; v-model=\u0026#34;email\u0026#34; v-validate=\u0026#34;\u0026#39;required|email\u0026#39;\u0026#34; data-vv-as=\u0026#34;邮箱\u0026#34;/\u0026gt; \u0026lt;p class=\u0026#34;alert alert-danger\u0026#34; role=\u0026#34;alert\u0026#34; v-show=\u0026#34;errors.has(\u0026#39;email\u0026#39;)\u0026#34;\u0026gt;{{ errors.first(\u0026#39;email\u0026#39;) }}\u0026lt;/p\u0026gt; 这些语法在 Vue 中被称为指令，而data-vv-as则是 HTML5 中的一个特性，用来给提示信息中的字段起一个别名。实际上，这个插件里同样内置了一批常见的校验规则。当控件中的值不满足校验条件时，就会在errors中产生错误信息，所以，我们根据错误信息中是否包含指定字段来决定要不要展示错误信息，这就是这个插件的作用。运行这个例子，我们会得到下面的结果。\n基于Vue的表单校验\r既然提到这类表单验证最难的地方在于扩展性，那么下面我们再来看看如何扩展一个新的校验规则，这里以最常见的手机号校验为例, 个人以为这是这个插件最为强大的地方：\nValidator.extend(\u0026#39;isMobile\u0026#39;, { messages: { zh_CN: field =\u0026gt; field + \u0026#39;必须是11位手机号码\u0026#39; }, validate: value =\u0026gt; { return value.length === 11 \u0026amp;\u0026amp; /^((13|14|15|17|18)[0-9]{1}\\d{8})$/.test(value) } }) 相信通过今天这篇博客，大家应该对 Vue 里的表单验证有一点心得了。这类验证的库或者框架其实非常多，整合到 Vue 中要做的工作无外乎写一个插件，在控件触发相关事件或者表单提交的时候进行验证。作为一个 Vue 的新人，这个过程可谓是路漫漫其修远。你大概想不到，我是在凌晨加班加到凌晨两点半的情况下做完这几个示例的，最近这两三个月里加的班比我过去三年都多，这到底是好事还是坏事呢？有时候不知道自己还能不能坚持下去，往事已矣，人难免会感到迷茫的吧！\n本文小结 这篇博客主要通过三个示例分享了 Vue 下表单校验的实现，而促使博主对这一切进行研究的原始动力，则是源于一个实际工作中通过 Vue 开发的新项目。前后端要不要分离、项目里要不要继续使用 jQuery、该不该频繁地操作 DOM，这其实是毫无关联地三件事情，而这种事情 90%的人是完全不关心的，就像有一种看起来相当“成年人”的做法，出了事情第一时间不是去纠结谁的过错，而是问能不能马上解决以及解决问题需要多长时间。这看起来好像一点问题都没有，可不去在意事件本身对错的人，是因为这些问题不需要他去处理，利益相关和责任相关是完全不一样的，因为你不能一出问题全部都找到程序员这里，这项目又不是程序员一个人的。我关心这些无关紧要的问题，纯粹是因为我对自己做的东西有一种感情，我想做好它而已，我希望自己是个纯粹的人，而且可以一直纯粹下去，晚安！\n","date":"2019-09-06T14:53:46Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/169430744/","slug":"169430744","tags":["Vue","表单","验证"],"title":"Vue 快速实现通用表单验证"},{"categories":["开发工具"],"content":"最近忙里偷闲的博主，再次迷恋上折腾 Linux。话说自从微软推出 WSL 以后，我就彻底地停止了 Windows + Linux 的双系统组合。回想起从前使用过的各种 Linux 发行版，基本上每隔一段时间就会崩溃一次，所以，面对 WSL 这种近乎应用级别的方案，我个人是非常愿意去接受的。因为一不小心玩坏了的话，可以直接对应用程序进行重置，或者重新从应用商店下载，相比重装系统，我觉得这种方式要更友好一点。虽然说 Windows10 是有史以来最好的 Linux 发行版:smile:，可面对只有命令行的 Linux，果然还是有一丝丝的失望啊:beetle:。所以，在这篇博客里，主要想和大家分享下，关于在 WSL 下使用 Linux 桌面系统的一点点尝试和体会。虽然目前应用商店里已经提供了 Ubuntu、Debian、Kail Linux、OpenSUSE 这些常见的发行版，可当你熟悉了 Linux 的世界以后，就会明白这个世界对多元化的追求是永无止境的，我不想去 Judge 这些多元化间优劣，我只想自由地使用我喜欢的技术，比如 Linux Deepin、Elementary OS。这是我想要使用 Linux 桌面环境的理由。\n我们知道，目前应用商店里提供的 Linux 发行版都是\u0026quot;命令行\u0026quot;版本。因为 Windows 本身就提供了非常出色的桌面环境，虽然每一次设计都给人一种相当前卫的感觉。平时我们使用SSH登录远程服务器的时候，其实是使用它的终端环境即 CLI。Linux 和 Windows 最大的不同在于，Linux 的桌面环境并不是 Linux 本身的一部分，它和所有的 Linux 应用程序并没有什么区别，因为脱离桌面环境的 Linux 的单独运行，而脱离桌面环境的 Windows 则未必可以。那么，我们怎么样在 Windows 里使用 Linux 的桌面环境呢？常见的思路主要有XServer和远程桌面两种。这里我们主要介绍第一种方式，即XServer。什么是 XServer 呢？Linux 的 GUI 架构其实是 C/S 模式的，其中 XServer 负责显示，XClient 负责请求。所以，我们只要在宿主机上安装 XServer 就可以啦。在这里，常见的 XServer 主要有：VcXsrv、X410和MobaXterm。理论上，我们只需要在 WSL 里安装桌面环境，在 Windows 上安装 XServer，然后通过命令行启动相应桌面环境即可。\n作为一个最流行的 Linux 发行版，微软非常贴心地给出了 16.04 和 18.04 两个版本。不过随着博主不甘寂寞地一通升级以后，最终还是稳定在了 18.04 这个版本。既然选择从 Ubuntu 这个发行版开始折腾，不如从它默认的桌面环境 Gnome 开始折腾吧！虽然我个人一直不太喜欢这个风格，不然就不会有接下来针对Pantheon和Deepin两个桌面环境的作死啦。这个过程最有意思的事情，居然是发现了一个更轻量级的桌面环境，可能真的是\u0026quot;无心插柳柳成荫\u0026quot;吧。好了，关于如何开启 WSL 及安装 Linux 发行版的过程不再多说。首先，让我们把系统默认的源切换到阿里云，因为这样能节省博主和大家的时间。:slightly_smiling_face:\nsudo cp /etc/apt/sources.list /etc/apt/sources.list.2019016 sudo vim /etc/apt/sources.list 接下来，我们使用下面的命令对文件内容进行替换, 或者你可以手动逐行去编辑。\n:%s/security.ubuntu/mirrors.aliyun/g :%s/archive.ubuntu/mirrors.aliyun/g 除此以外，还推荐大家使用以下国内的镜像源：\n清华大学镜像源：https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\n网易开源镜像站：http://mirrors.163.com/.help/ubuntu.html\n完成镜像源的切换以后，我们就可以愉快地使用apt-get update刷一波存在感啦，话说最近看到一条微博，建议给sudo起一个别名plz或者pls。除了调侃以外，可能更多是想把冰冷的命令行变得充满人情味吧。Windows 下安装VcXsrv大家都轻车熟路啦，这个不再过多的说明。下面，我们来安装以下 Ubuntu 桌面环境：\necho \u0026#34;y\u0026#34;|sudo apt-get install ubuntu-desktop unity compizconfig-settings-manager sudo dpkg-reconfigure dbus \u0026amp;\u0026amp; service dbus restart 接下来配置XLaunch，这是我们安装完VcXsrv后自带一个应用程序：\n配置XLaunch\r按照默认配置直至完成后我，我们会发现桌面上出现了一个黑色的窗口，如下图所示：\nXLaunch经典黑屏\r此时，我们在 Ubuntu 的 Bash 窗口中输入sudo compiz命令并切回XLaunch界面。接下来，就是见证奇迹的时刻：\n经典的Ubuntu桌面\r如你所见，这是 Ubuntu 默认的 Unity 桌面，博主一开始是在 Ubuntu16.04 上研(折)究(腾)的，当时安装完以后桌面其实是黑色的，因为当时并没有保留下这历史性的一刻，所以，从网上找了张图来这里充数啦，这张图片出自：Run any Desktop Environment in WSL。\nOK，既然 Ubuntu 可以装桌面，那么，衍生自 Ubuntu 的 Elementary OS 和 Linux Deepin 应该同样可以吧，虽然目前应用商店里还有这两个发行版。本着不折腾就不会死的选择，先装个 Elementary OS 的桌面环境试试呗！我个人挺喜欢这个发行版的，理由是默认主题样式就很好看，同理，Linux Deepin 除了好看以外，本身就带有大量优秀的软件。所以说，人类果然还是始于颜值的啊！Elementary OS 使用的桌面环境是 Pantheon，我们可以通过下面的命令行快速安装：\nsudo add-apt-repository ppa:elementary-os/stable sudo apt update sudo apt install elementary-desktop 通常，每个桌面环境都会自带一部分“最佳”适配的应用程序，考虑到 WSL 并不是一个完整的 Linux 实现，我们在这里卸载掉一部分 WSL 下不支持的应用程序。而微软新推出的 WSL2，则是基于 VM 的实现，两种方式完全没有可比性，这里不做无意义的争论：\nsudo apt purge gnome-screensaver \\ switchboard-plug-power switchboard-plug-bluetooth switchboard-plug-networking \\ wingpanel-indicator-bluetooth wingpanel 参考Installing Pantheon Desktop On Ubuntu这篇文章中的建议，为了启动 Pantheon 桌面环境，我们需要 gala、 plank和wingpanel三个软件，它们的作用有点像前面的compiz。而关于X410，你可以把它理解为和VcXsrv类似的软件，不同的是这是一个付费软件，作者写了一系列的博客来推广它。接下来，在安装gala的过程中，你大概会遇到这个错误：\nThe following packages have unmet dependencies: gala : Depends: libmutter-2-0 (\u0026gt;= 3.28.4-0ubuntu18.04.1+elementary4~ubuntu5.0.1) but 3.28.4-0ubuntu18.04.1 is to be installed E: Unable to correct problems, you have held broken packages. 我向作者发邮件寻求帮助，作者非常热心地回复了我三次邮件，对方表示应该是 Elementary OS 团队正在基于 Ubuntu19.04 开发新版本，所以可能没有意识到elementary-desktop这个包已经 broken 了，并且他们在 18.04 版本上复现了这个问题，建议我直接联系官方。好吧，博主的英语表示受宠若惊，邮件在此为证：\n来自国外网友的热心指导\r总而言之，博主试图在 WSL 上体验 Elementary OS 的想法彻底失败，既然这个最美的 Linux 发行版已失败告终，并不打算就此罢手的博主，决定继续在命令行终端里折腾 Linux Deepin。这个发行版是我从大学时开始接触的 Linux 发行版，那时有个小学弟第一次给我介绍了 Linux Mint，不过我对这个版本实在爱不起来，因为太像 Windows 了啊，可谁能想到若干年后，Windows 反而变成了最好的 Linux 发行版呢(:smile:)，果真是“人生不相见，动如参与商”啊……\n好啦，继续敲命令。Linux Deepin 的桌面环境是 Deepin Desktop Environment，简称 dde：\nsudo add-apt-repository ppa:leaeasy/dde sudo apt-get update sudo apt install dde dde-file-manager Linux Deepin 安装是非常顺畅的，但即便安装完这个桌面环境，博主还是不知道怎么启动这个环境，因为你常规使用 Ubuntu 的话，安装完切换桌面管理器就可以了，可当你用 WSL 这种方式使用 Ubuntu 的时候，可能你就会感到非常困惑。相比之下，xfce就让人感觉友好得多，因为它只有一个命令startxfce4，而安装只需要安装xfce4和xfce4-terminal就可以了。在对比了 Gnome、KDE、Unity、Mint、xfce 等等的桌面环境以后，我觉得 Linux 在桌面市场输给 Windows 是理所当然的，因为实在太混乱了，WSL 下需要的应该是一个轻量级的桌面，因为越是华而不实东西，越会消耗大量资源。我最初想要折腾桌面环境，无非是为了下面这个结果，撒花完结，以上！\n简洁/简陋的xfce桌面\r","date":"2019-08-17T21:09:46Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3972610476/","slug":"3972610476","tags":["WSL","Linux","桌面"],"title":"在 WSL 中使用 Linux 桌面环境的尝试与总结"},{"categories":["编程语言"],"content":"我一直想吐槽下运维同事提供的 Jekins 项目模板，因为它居然不支持含有多个项目的解决方案。作为一个有追求的程序员，每个解决方案下最少应该含有两个项目，即项目本身和项目对应的单元测试。可惜在这样一种情形下，我没法再去坚持这样的原则，而这真正让我感到难过的是，为了在编译环节向 Jekins 妥协，大家在一个项目里极尽所能，在这一个项目里居然混合了MVC、WebApi和WebService三种技术，甚至到最后连传统三层的界限都越来越模糊了。这让我意识到一件事情，工程上的妥协和技术选型一样，在某种意义上它们都不能被称之为科学，因为确实没什么道理，完全是运维为了方便而制造出的问题。在我们意识到文档的重要性以后，写文档就变成了日常工作。我一直坚持的原则是，文档能通过工具生成就坚决不要手写，所以，看到项目目录里充斥着各种各样的文档格式，譬如 Word、Excel、Pdf、Viso 等等的时候，我毅然决然地选择了 Swagger。而今天这篇文章的原由，恰恰来自于这个\u0026quot;混搭\u0026quot;的项目。说到这里，你可能已经想到我想做什么了。不错，我们有部分 WebApi 是写在 MVC 的控制器里的，我希望使用者可以通过 Swagger 看到这部分接口的文档，这样我就有时间在这里写博客了。😄\n故事缘起 常规的 Swagger 的使用就不再说啦，因为基本上你通过 Nuget 安装完Swashbuckle以后，再配置下项目生成的 XML 注释文档就可以使用啦！不过，博主在这里遇到的第一个问题就是，按照常规套路配置好了以后，Swagger 页面打开完全就是空白的啊，反复尝试直至怀疑人生后，我突然意识到，莫非是因为我这是一个 MVC 的项目？立马跑到官方的 Issues 下面逐个扫视，果不其然，大佬们一致给出了答案：Swagger 是不支持 MVC 类型的项目的。这里补充说明，这里的 MVC 是指ASP.NET MVC。目前官方主推的ASP.NET Core是没有这种困惑的啦，因为微软在这个新版本中统一了 MVC 和 WebApi。对于这种历史“遗留问题”，既然 Swagger 官方都不愿意提供支持，那么，博主只好勉为其难的提供一个实现，我不得不承认，带着历史包袱的 ASP.NET 在扩展性上的确不如全新的“Core”系列，因为单单是System.Web系列的动态链接库版本就令人痛苦不堪，因此，博主在写这个扩展的时候，全部升到了最新的 5.2.7.0。\n实现 MvcApiExplorer 好了，Swagger 之所以能够生成友好、可交互的 API 文档，其核心依赖于 IApiExplorer 接口。这一点，我们可以通过 Swashbuckle 项目中的源代码来得到验证。其中，生成符合 Swagger 规范的 JSON 文档，是通过 SwaggerGenerator 这个类来实现的。而进一步研究这个类，我们就会发现它依赖IApiExplorer接口。这个接口位于System.Web.Http.Description命名空间下，而显然这是 WebApi 相关的命名空间，所以，对于一般的 WebApi 项目，因为微软已经帮我们实现了默认的 ApiExplorer，所以，Swagger 可以识别出项目中的 Controller 及其 Action，并通过 XML 注释文档进一步填充接口相关的描述信息。一旦想到这一层，我们就会明白，为什么 Swagger 不支持 MVC 项目，因为 MVC 里压根就没有实现 IApiExplorer 接口啊！那么，怎么办呢？我们的想法是通过反射取出所有的 MVC 控制器及其 Action，然后组织出这些接口的描述信息，再将它们添加到默认的 IApiExplorer 实现中去，这样 MVC 和 WebApi 都可以被 Swagger 识别，为此，我们继承默认的 ApiExplorer，并实现我们自定义的MvcApiExplorer：\npublic class MvcApiExplorer : ApiExplorer { /// \u0026lt;summary\u0026gt; /// HttpConfiguration /// \u0026lt;/summary\u0026gt; private HttpConfiguration _configuration; public MvcApiExplorer (Assembly assembly, HttpConfiguration configuration) : base (configuration) { _configuration = configuration; assembly.GetTypes () .Where (type =\u0026gt; typeof (IController).IsAssignableFrom (type) \u0026amp;\u0026amp; type.Name != \u0026#34;ErrorController\u0026#34; \u0026amp;\u0026amp; type.BaseType != typeof (ApiController)) .ToList ().ForEach (controller =\u0026gt; { base.ApiDescriptions.AddRange (BuildControllerApiDescription (controller)); }); } /// \u0026lt;summary\u0026gt; /// ApiExolorer for Action is visible /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;actionVariableValue\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;actionDescriptor\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;route\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; public override bool ShouldExploreAction (string actionVariableValue, HttpActionDescriptor actionDescriptor, IHttpRoute route) =\u0026gt; true; /// \u0026lt;summary\u0026gt; /// ApiExolorer for Controller is visible /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;controllerVariableValue\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;controllerDescriptor\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;route\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; public override bool ShouldExploreController (string controllerVariableValue, HttpControllerDescriptor controllerDescriptor, IHttpRoute route) =\u0026gt; true; private List\u0026lt;ApiDescription\u0026gt; BuildControllerApiDescription (Type type) { var controllerName = type.Name.Replace (\u0026#34;Controller\u0026#34;, \u0026#34;\u0026#34;); var methods = type.GetMethods (System.Reflection.BindingFlags.Instance | System.Reflection.BindingFlags.Public | BindingFlags.DeclaredOnly) .Where (m =\u0026gt; typeof (ActionResult).IsAssignableFrom (m.ReturnType)); var list = new List\u0026lt;ApiDescription\u0026gt; (); foreach (var method in methods) { var apiDescription = new ApiDescription (); apiDescription.ActionDescriptor = new MvcHttpActionDescriptor (method); apiDescription.ActionDescriptor.ControllerDescriptor = new HttpControllerDescriptor (_configuration, controllerName, type); apiDescription.HttpMethod = HttpMethod.Post; apiDescription.Route = new HttpRoute (string.Format (\u0026#34;{0}/{1}\u0026#34;, controllerName, method.Name)); apiDescription.RelativePath = string.Format (\u0026#34;{0}/{1}\u0026#34;, controllerName, method.Name); apiDescription.Documentation = string.Empty; typeof (ApiDescription).GetProperty (\u0026#34;ParameterDescriptions\u0026#34;).SetValue (apiDescription, BuildApiParameters (method)); typeof (ApiDescription).GetProperty (\u0026#34;ResponseDescription\u0026#34;).SetValue (apiDescription, new ResponseDescription () { ResponseType = method.ReturnType, DeclaredType = method.DeclaringType, Documentation = string.Empty }); list.Add (apiDescription); } return list; } private Collection\u0026lt;ApiParameterDescription\u0026gt; BuildApiParameters (MethodInfo methodInfo) { return new Collection\u0026lt;ApiParameterDescription\u0026gt; ( methodInfo.GetParameters ().Select (p =\u0026gt; new ApiParameterDescription () { Name = p.Name, Documentation = string.Empty, Source = ApiParameterSource.Unknown, ParameterDescriptor = new MvcHttpActionParameterDescriptor (p, new MvcHttpActionDescriptor (methodInfo)), }).ToList ()); } } 通过代码可以看出，实现 MvcApiExplorer 的过程，其实就是向 ApiDescriptions 集合中添加元素的过程。为此，我们通过程序集去反射所有实现了IController接口，同时其父类不是ApiController，并且方法返回值类型为ActionResult的所有类型，通过这个类型信息，我们进一步反射每个方法以及方法的参数，并把这些参数转换为 ApiDescription 类型需要的参数类型。在组织信息的过程中，有一部分属性被微软设计为只读，故而不得不通过反射的方式来解决。我们知道，MVC 里默认的路由模板是：{controller}/{action}，这是 WebApi 里的特性路由流行以前默认的、最基础的路由。我们这里基于沿用这个规则，所谓“约定大于配置”，这可以为我们节省不少时间。MVC 里的 HTTP 动词我全部使用了 POST，这是因为 MVC 里真正控制一个方法是 GET 还是 POST 请求，其实是JsonRequestBehavior这个参数，当它设置为 AllowGet 时，该方法可以同时支持这两种 HTTP 动词。同样，在模模型绑定阶段，我全部使用了 Unknown，因为 MVC 会尝试通过 Body 或者 Form 的形式来接受一个参数，这两个地方完全是来自 MVC 本身机制的限制，如果大家有更好的思路，欢迎大家在博客里留言。\n一旦实现了自定义的 MvcApiExplorer，我们就可以尝试用它来替换微软默认的实现。在 ASP.NET 中，我们通过GlobalConfiguration.Configuration.Services.Replace()方法来实现服务的替换。其实，这种思路在 ASP.NET Core 里依然存在，比如我们在实现动态 WebApi 时采用的ControllerFeatureProvider都属于服务替换，所不同的时，ASP.NET 时代是通过一个内置的 IoC 容器来实现服务替换，而 ASP.NET Core 时代，我们显然有了更多的选择，甚至依赖注入渗透到了整个.NET Core 的方方面面，这的确是一种相当大的进步。曾几何时，Javaer 嘲笑我们只会拖控件，可今天的我们，Java 里有的概念我们都有对应的实现，反倒是 Java 开始从 C#身上学习那些有点“甜”的语法糖啦！的确，我们写了这么多代码，其实最关键的就只有下面这一句，住口！这明明是三行：\nvar assembly = typeof(DefaultMvcProject.MvcApplication).Assembly; var apiExplorer = new MvcApiExplorer(assembly, GlobalConfiguration.Configuration); GlobalConfiguration.Configuration.Services.Replace(typeof(IApiExplorer), apiExplorer); OK，接下来我们简单写几个 MVC 的控制器，来验证下我们为 Swagger 编写的 MVC 控制。在此之前，请确保完成了 Swagger 的两步常规配置，即为 Swagger 引入 XML 注释文档、在项目属性中勾选 XML 注释文档。这是使用 Swagger 的最小配置，相信大家一定都知道啦！\nGlobalConfiguration.Configuration .EnableSwagger (c =\u0026gt; { c.SingleApiVersion (\u0026#34;v1\u0026#34;, \u0026#34;DefaultMvcProject\u0026#34;); c.IncludeXmlComments ($\u0026#34;{System.AppDomain.CurrentDomain.BaseDirectory}/bin/DefaultMvcProject.XML\u0026#34;); }) .EnableSwaggerUi (c =\u0026gt; { c.DocumentTitle (\u0026#34;My Swagger UI\u0026#34;); }); 两个非常简单的 Controller，这里就不再贴代码啦！\n非常简单的Controller\r可以注意到，一切都工作的很好，我们在 Swagger 里可以看到我们编写的 Api 接口，并且可以直接对接口进行调试。因为 MVC 本身的原因，这些 MVC 控制器的注释都不会生成到 XML 注释文档里。所以，稍微有一点遗憾的地方就是，这些接口都没有对应注释。不过，这已经达到了本文最初的目的，至少我不用再去写文档，告诉使用者这个接口里有哪些参数，以及这个接口的地址是什么啦，说到底啊，懒惰是人类进步的阶梯。这篇博客里实现的扩展，我已经发布到 Github 上，并附带了一个简单的示例(不要想太多哦，就是这篇文章里的示例)，感兴趣的朋友可以自助研究，仓库地址为：https://github.com/qinyuanpei/Swashbuckle.Extension.Mvc\n差一点就完美了\r本文小结 本文实现了一个针对 MVC 项目的 Swagger 扩展，它可以让你编写在 MVC 控制器里的 API 接口，像普通 WebApi 项目一样展示在 Swagger 里。其原理是继承并重写了ApiExplorer类，这是 Swagger 生成 API 文档的核心接口。好了，以上就是这篇文章的全部内容啦，写这种短小的文章没有那么累，希望大家读起来一样不会累吧，晚安，世界！\n","date":"2019-08-06T23:02:50Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/4116164325/","slug":"4116164325","tags":["Swagger","MVC","WebApi"],"title":"通过 ApiExplorer 为 Swagger 提供 MVC 扩展"},{"categories":["编程语言"],"content":"Hi，大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是：https://blog.yuanpei.me。在上一篇文章中，我和大家分享了 ASP.NET 中动态 Web API 的实现，这种方案的现实意义是，它可以让我们把任意一个接口转换为 Web API，所以，不单单局限在文章里提到的 WCF 迁移到 Web API，任意领域驱动开发(DDD)中的服务层，甚至是更为普遍的传统三层，都可以通过这种方式快速构建前后端分离的应用。可能大家会觉得直接把 Service 层暴露为 API，会引发一系列关于鉴权、参数设置(FromQuery/FromBody)等等的问题，甚至更极端的想法是，这样和手写的没什么区别，通过中间件反射能达到相同的目的，就像我们每天都在写各种接口，经常有人告诉我们说，不要在 Controller 层写太重的业务逻辑，所以，我们的做法就是不断地在 Service 层里增加新接口，然后再把 Service 层通过 Controller 层暴露出来，这样子真的是对的吗？\n可我个人相信，技术总是在不断向前发展的，大家觉得 RESTful 完全够用啦，结果 GraphQL 突然发现了。大家写了这么多年后端，其实一直都在绕着数据转，可如果数据库本身就支持 RESTful 风格的接口，或者是数据库本身就支持某种 ORM，我们后端会立马变得无趣起来。其实，在 ASP.NET Core 中已经提供了这种特性，这就是我们今天要说的 POCOController，所以，这也许是个正确的思路，对吧？为什么 Service 层本身不能就是 Controller 层呢？通过今天这篇文章，或许你会接受这种想法，因为 POCOController，就是弱化 Controller 本身的特殊性，一个 Controller 未必需要继承自 Controller，或者在名称中含有 Controller 相关的字眼，如果 Controller 同普通的类没有区别会怎么样呢？答案就是 Service 层和 Controller 层的界限越来越模糊。扪心自问，我们真的需要中间这一层封装吗？\n什么是 POCOController POCOController 是 ASP.NET Core 中提供的一个新特性，按照约定大于配置的原则，在 ASP.NET Core 项目中，所有带有 Controller 后缀的类，或者是使用了[Controller]标记的类，即使它没有像模板中一样继承 Controller 类，ASP.NET Core 依然会将其识别为 Controller，并拥有和普通 Controller 一样的功能，说到这里，你是不是有点兴奋了呢，因为我们在 ASP.NET 里花了大力气去做类似的事情，因为 ASP.NET 里一个普通的类是没有办法成为 Controller 的，即使通过 Castle 的 Dynamic Proxy 黑科技，我们依然需要去 Hack 整个 MVC 框架创建、筛选 Controller 和 Action 的过程。可在 ASP.NET Core 里这一切居然变成了一个新的 feature，所以，我预感到这篇文章应该不会像上一篇文章那么长，果然 9102 有 9102 的好处呢……好了，现在我们来写一个 POCOController：\npublic class MessageController { public string Echo(string receiver) { return $\u0026#34;Hello, {receiver}\u0026#34;; } } 接下来，我们通过浏览器访问：http://localhost:6363/Message/Echo?receiver=PayneQin，我们就会发现一件非常神奇的事情，那就是，我们并没有真的在写一个 Controller，它没有继承 Controller 类，虽然它的名字里带着 Controller 的后缀，可它确实实现了一个 Controller 所具备的功能，因为它返回了我们期望的信息。\n欢迎来到POCOController的世界\r可以注意到，这个 Controller 使用起来和普通的 Controller 是没有任何区别的，这正是我们想要的结果。对于.NET Core 而言，一个普通的类想要成为 POCOController，只需要满足以下任意一个条件：第一，继承自 Microsoft.AspNetCore.Mvc.Controller 类，无论是否带有 Controller 后缀，都可以作为 POCOController。第二，不继承自 Microsoft.AspNetCore.Mvc.Controller 类，同时引用了 Microsoft.AspNetCore.Mvc 相关的程序集。在这里，博主一开始就犯了这个错误，因为博主建的是一个 Web API 类型的项目。\nControllerFeatureProvider 那么，为什么 ASP.NET Core 里可以实现如此炫酷的功能呢？这里要介绍到 ControllerFeatureProvider。在.NET Core 中，微软引入了应用程序部件的概念，顾名思义，它是对应用程序资源的一种抽象，通过这些应用程序部件， .NET Core 提供了发现和加载 MVC 组件，如控制器、视图(View)、标记(TagHelper)、Razor 等等的功能。在 MVC 中，这些功能由 ApplicationPartManager 对象来进行管理，它维护着一个叫做 FeatureProviders 的列表，以上这些功能分别对应一个 Feature，所以，当我们希望引入一个新的功能的时候，只需要实现 IApplicationFeatureProvider\u0026lt;T\u0026gt; 接口即可，而这里的 ControllerFeatureProvider 显然是提供控制器相关的 Feature，它有一个最为关键的接口 IsController(TypeInfo)。\n回到一开始的话题，微软定义了一个类成为 POCOController 的规则，实际上我们同样可以定义自己的规则，譬如 ABP 框架中限定的接口约束是实现 IAppService 这个接口，那么我们就可以把一个程序集或者多个程序集里的类型识别为控制器，这就是 POCOController 的奥秘所在。在比如我们的项目中难免会有大量 CRUD 的垃圾需求，区别仅仅是它访问不同的仓储，我们可能会想写一个泛型的控制器来处理，可惜在过去的 ASP.NET 里，实现这一切并不太容易。为什么说不大容易呢？通过我们上一篇文章里动态路由的整个过程，大家就知道有多麻烦了啊，可在.NET Core 里要实现一个泛型的控制器就非常容易了啊，因为我们只需要告诉 ControllerFeatureProvider，这是一个控制器，并且控制器的类型就是这个泛型参数 T，所以，综上所述，ControllerFeatureProvider 主要做两个事情，第一，判定一个类型能不能算作 Controller；第二，对程序集里的类型进行筛选和过滤。下面，我们顺着这个思路来实现我们自己的 ControllerFeatureProvider。\npublic class DynamicControllerFeatureProvider : ControllerFeatureProvider { protected override bool IsController (TypeInfo typeInfo) { var type = typeInfo.AsType (); if (!typeof (IDynamicController).IsAssignableFrom (type) || !typeInfo.IsPublic || typeInfo.IsAbstract || typeInfo.IsGenericType) { return false; } return true; } } 如你所见，我们采用了一种简单粗暴的方式，任何非 Public、非抽象、非泛型并且实现了 IDynamicController 接口的类型，都可以被认为是一个 Controller，原谅我起了这样一个直白而普通的接口名称，因为一开始做的时候，真的就是想延续动态 Web Api 这个想法而已，所以，大家明白就好了，不用太过纠结这个接口的名字，甚至你还可以通过 Attribute 来打上标记，反正都是为了辨别哪些类型可以被当做控制器。\nIApplicationModelConvention OK，现在我们已经告诉.NET Core，怎么样去把一个类型识别为 Controller。因为 MVC 中有一些所谓“约定大于配置”的东西，比如默认的路由规则是：{area}/{controller}/{action}/{id}，相信从 ASP.NET 时代一起走过来的各位，对这个东西应该很熟悉啦，因为最早 App_Start 里会有 RouteConfig 和 WebApiConfig 这两个东西。我们在做 ASP.NET 版本的动态 Web API 的实现的时候，实际上就是配置了这样一个固定的路由，所以，理论上现在即使我们不讲下面这部分内容，现在我们已经实现了动态 Controller。可如果我们希望对路由进行全局配置，我们就不得不关注这个接口。简而言之，通过这个接口，我们可以修改 MVC 里约定俗成的这套规则，譬如在路由中带个版本号前缀，或者根据命名空间去生成某种规则的路由，我们都可以考虑去实现这个接口。一般情况下，我们会通过重写 Apply()方法来达到修改路由的目的。\n在这篇文章里，我们希望在 MVC 这套默认路由的基础上，增加对特性路由的支持。说到这里，我们又会回到一个旧话题，即基于配置的路由和基于特性的路由这两种路由。前者是 MVC 里的路由设计的基础，而后者是 Web API 里提出并在 RESTful 风格 API 的设计中发扬光大。所以，我们希望在提供默认路由的基础上，使用者可以自由配置路由风格，所以，我们需要通过这个接口来构造路由信息，值得一提的是，我们可以在这个过程中设置 ApiExplorer 是否可见，为接口参数设置合适的绑定模型等等，所以，我们会使用 HttpGet/HttpPost 等来标记接口的调用方式，使用 Route 来标记用户自定义的路由信息，使用 FromBody/FromQuery 等来标记参数的绑定信息，有了这些配合 Swagger 简直是无往不胜，并非是开发人员不愿意写文档，而是因为文档的更新速度往往赶不上需求的变化速度，一旦文档落后于实际业务，这样的文档实际是没有意义的，我真的讨厌所有人都来找你问接口的地址、参数这些东西，如果你写完了一个 Service，写好对应的方法注释，然后你就有了一个可用的 Web API，和一个可用的在线文档，何乐而不为呢？\n下面，是博主实现的一个动态路由，它主要涉及到 ConfigureApiExplorer()、ConfigureSelector() 和 ConfigureParameters() 这三个部分的实现，我们一起来看下面的代码，ASP.NET Core 版本相比 ASP.NET 版本，少了像 Castle DynamicProxy 这样的黑科技，因此，它的实现会更加纯粹一点。\nConfigureApiExplorer() 首先，是对 ApiExplorer 进行配置。通过 ApiExplorer，我们可以控制 Controller 级别和 Action 级别的 Web API 的可见性。一般情况下的用法是在 Controller 或者 Action 上添加 ApiExplorerSettings 标记，而在这里，我们只需要给 ControllerModel 和 ActionModel 的 ApiExplorer 属性赋值即可。\nprivate void ConfigureApiExplorer (ControllerModel controller) { if (string.IsNullOrEmpty (controller.ApiExplorer.GroupName)) controller.ApiExplorer.GroupName = controller.ControllerName; if (controller.ApiExplorer.IsVisible == null) controller.ApiExplorer.IsVisible = true; controller.Actions.ToList ().ForEach (action =\u0026gt; ConfigureApiExplorer (action)); } private void ConfigureApiExplorer (ActionModel action) { if (action.ApiExplorer.IsVisible == null) action.ApiExplorer.IsVisible = true; } ConfigureSelector() 接下来，是对路由进行配置。这部分的核心其实就是根据 AreaName、ControllerName、ActionName 来生成路由信息，我们会为没有配置过特性路由的 Action 生成默认的路由，这其实就是 MVC 里约定大于配置的一种体现啦。在这里会涉及到对 ControllerName 和 ActionName 的优化调整，主要体现在两个方面：其一，是对类似 XXXService、XXXController 等这样的后缀进行去除，使其构造出的 Api 路由更加短小精简；其二，是对 ActionName 里的 Get/Save/Update 等动词进行替换，使其构造出的 Api 路由更加符合 RESTful 风格。\nprivate void ConfigureSelector (ControllerModel controller, DynamicControllerAttribute controllerAttribute) { if (_dynamicControllerOptions.UseFriendlyControllerName) { var suffixsToRemove = _dynamicControllerOptions.RemoveControllerSuffix; if (suffixsToRemove != null \u0026amp;\u0026amp; suffixsToRemove.Any ()) suffixsToRemove.ToList ().ForEach (suffix =\u0026gt; controller.ControllerName = controller.ControllerName.Replace (suffix, \u0026#34;\u0026#34;)); } controller.Selectors.ToList ().RemoveAll (selector =\u0026gt; selector.AttributeRouteModel == null \u0026amp;\u0026amp; (selector.ActionConstraints == null || !selector.ActionConstraints.Any ()) ); if (controller.Selectors.Any (selector =\u0026gt; selector.AttributeRouteModel != null)) return; var areaName = string.Empty; if (controllerAttribute != null) areaName = controllerAttribute.AreaName; controller.Actions.ToList ().ForEach (action =\u0026gt; ConfigureSelector (areaName, controller.ControllerName, action)); } private void ConfigureSelector (string areaName, string controllerName, ActionModel action) { action.Selectors.ToList ().RemoveAll (selector =\u0026gt; selector.AttributeRouteModel == null \u0026amp;\u0026amp; (selector.ActionConstraints == null || !selector.ActionConstraints.Any ()) ); if (!action.Selectors.Any ()) { action.Selectors.Add (CreateActionSelector (areaName, controllerName, action)); } else { action.Selectors.ToList ().ForEach (selector =\u0026gt; { var routePath = $\u0026#34;{_dynamicControllerOptions.DefaultApiRoutePrefix}/{areaName}/{controllerName}/{action.ActionName}\u0026#34;.Replace (\u0026#34;//\u0026#34;, \u0026#34;/\u0026#34;); var routeModel = new AttributeRouteModel (new RouteAttribute (routePath)); if (selector.AttributeRouteModel == null || !_dynamicControllerOptions.UseCustomRouteFirst) selector.AttributeRouteModel = routeModel; }); } } 我们知道，每个 API 接口都会有相对应的 HTTP 动词，譬如 GET、POST、PUT 等等，那么，我们在构造路由的时候，如何知道当前的 Action 应该使用什么 HTTP 动词呢？实际上，我们有两个来源来组织这些信息。第一个来源，是根据方法本身的名称，比如 Get/Save/Update 等等，我们通过对应关系将其转化为对应的 HTTP 动词。第二个来源是根据用户在接口中配置的路由信息，比如 RouteAttribute、HttpMethod 等等，将其转化为对应的 HTTP 动词。这个方法，其实我们在分享 ASP.NET 下的实现的时候，就已经用过一次啦，所谓“万变不离其宗”，大概就是如此：\nprivate SelectorModel CreateActionSelector(string areaName, string controllerName, ActionModel action) { var selectorModel = new SelectorModel(); var actionName = action.ActionName; var routeAttributes = action.ActionMethod.GetCustomAttributes(typeof(HttpMethodAttribute), false); if (routeAttributes != null \u0026amp;\u0026amp; routeAttributes.Any()) { var httpVerbs = routeAttributes.SelectMany(s =\u0026gt; (s as HttpMethodAttribute).HttpMethods).ToList().Distinct(); var routePath = $\u0026#34;{_dynamicControllerOptions.DefaultApiRoutePrefix}/{areaName}/{controllerName}/{action.ActionName}\u0026#34;.Replace(\u0026#34;//\u0026#34;, \u0026#34;/\u0026#34;); selectorModel.AttributeRouteModel = new AttributeRouteModel(new RouteAttribute(routePath)); selectorModel.ActionConstraints.Add(new HttpMethodActionConstraint(httpVerbs)); return selectorModel; } else { var httpVerb = string.Empty; var methodName = action.ActionMethod.Name.ToUpper(); if (methodName.StartsWith(\u0026#34;GET\u0026#34;) || methodName.StartsWith(\u0026#34;QUERY\u0026#34;)) { httpVerb = \u0026#34;GET\u0026#34;; } else if (methodName.StartsWith(\u0026#34;SAVE\u0026#34;)) { httpVerb = \u0026#34;POST\u0026#34;; } else if (methodName.StartsWith(\u0026#34;UPDATE\u0026#34;)) { httpVerb = \u0026#34;PUT\u0026#34;; } else if (methodName.StartsWith(\u0026#34;DELETE\u0026#34;)) { httpVerb = \u0026#34;DELETE\u0026#34;; } var routePath = $\u0026#34;api/{areaName}/{controllerName}/{action.ActionName}\u0026#34;.Replace(\u0026#34;//\u0026#34;, \u0026#34;/\u0026#34;); selectorModel.AttributeRouteModel = new AttributeRouteModel(new RouteAttribute(routePath)); selectorModel.ActionConstraints.Add(new HttpMethodActionConstraint(new[] { httpVerb })); return selectorModel; } } 由此可见，无论多么令人惊诧的黑科技，当我们一层层地拨开它的迷雾时，常常有种豁然开朗的感觉。当然，和那些令人看起来神清气爽的代码相比，博主远远达不到返璞归真的境界，因为这段代码怎么看都觉得丑陋。古美门律师告诉我们，要爱上丑陋，或许每个程序员都是从写烂代码开始的吧！\nConfigureParameters() 接下来参数绑定相对简单，因为简单类型 MVC 自己就能完成绑定，所以，我们只需要关注复杂类型的绑定即可，最常见的一种绑定方式是 FromBody：\nprivate void ConfigureActionParameters(ActionModel action) { foreach (var parameter in action.Parameters) { if (parameter.BindingInfo != null) continue; var type = parameter.ParameterInfo.ParameterType; if (type.IsPrimitive || type.IsEnum || (type.IsGenericType \u0026amp;\u0026amp; type.GetGenericTypeDefinition() == typeof(Nullable\u0026lt;\u0026gt;))) { if (IsFromBodyEnable(action, parameter)) { parameter.BindingInfo = BindingInfo.GetBindingInfo(new[] { new FromBodyAttribute() }); } } } } 通过以上三个关键步骤，我们就能实现本文一开始的效果，感觉无形中我们又复习了一篇 MVC 匹配路由的原理呢！\n集成 Swagger 和 WebApiClient 今天这篇文章，本质上依然是 ABP 框架中 Dynamic WebAPI 这一特性的延伸，无非是因为.NET Core 中提供了更为友好的机制，可以让这一切实现起来更简单而已。还记得博主研究这个特性的“初心”是什么吗？因为我们在升级.NET Core 的过程中打算抛弃 WCF，我们需要一种方法，可以让现有的一个普通的 Service 变成一个 Controller。固然，我们可以一个一个的去重新封装，可这真的是比较好的实践方式吗？从内部 RPC 逐渐转变为 Web API 调用，这种转变就像从 Dubbo 换成了 Spring Cloud，可是 Spring Cloud 有注册中心啊，现在我们什么都没有，从 RPC 转变为 Web API，会面临诸如接口授权、地址配置、不同上下文等等的问题。你经常需要告诉别人某个接口的地址是什么，不出意外地话，你至少会有三套环境的地址，别人还会问你各个参数的含义，甚至更懒的会要求你提供示例报文。所以，我觉得做微服务，尤其是全部采用 Web API 进行通信的微服务，提供实时更新、在线查看的文档真的非常重要，每次看到同事在 Git 里提交 Word 或者 Excel，我就感到非常纠结，一来这种东西没法正常 Merge，压缩包合并个鬼啊，二来我必须下载下来看，君不见，我下载目录里一堆重复文件，所以，我更推荐努力维护好一家公司的 API 资产，在我们用 JWT 保护这些资产以前，至少要先了解它们吧！\n对于 API 文档，我个人推荐专门用一个站点来承载所有的 Web API，比如我们最常用的 Swagger，它在融合 OAuth2 以后可以更完美地去调试接口，了解每个接口的参数和返回值。尤其是在这篇博客的背景下，因为我们只需要把这些 POCOController 对应的注释文件(.XML)和程序集(.DLL)放到一起，同时把这些注释文件全部 Include 进来，Swagger 就可以把它们展示出来。这里用到一个非常重要的特性就是 IApiExploer 接口，你可以把它理解为，它是一切文档展示的核心，每个接口及其参数、返回值的描述信息都是由它提供的。在没有 Swagger 之前，微软提供了一个叫做 Web API HelpPage 的组件，它和 Swagger 的原理无出其右。这里剧透下，稍后我会专门写一篇博客来扩展 Swagger，目的是确保它可以为 ASP.NET MVC 提供文档支持。这里，我们使用 Swagger 来生成在线 API 文档，其核心配置如下：\nservices.AddMvcCore ().AddApiExplorer (); services.AddSwaggerGen (swagger =\u0026gt; { swagger.SwaggerDoc (\u0026#34;v1\u0026#34;, new Swashbuckle.AspNetCore.Swagger.Info () { Title = \u0026#34;DynamicController\u0026#34;, Version = \u0026#34;1.0\u0026#34;, }); swagger.DocInclusionPredicate ((docName, description) =\u0026gt; true); var xmlFile = $\u0026#34;{Assembly.GetExecutingAssembly().GetName().Name}.xml\u0026#34;; var xmlPath = Path.Combine (AppContext.BaseDirectory, xmlFile); swagger.IncludeXmlComments (xmlPath); }); 可以注意到，这篇文章里实现的动态 Controller 和默认的 ValuesController 都被展示了出来，两个字，完美，我们想要的就是这个效果。\n通过Swagger生成的在线Api文档\r说完了 API 文档的事情，我们再来说说调用 Web API 的问题。按理说，这应该没啥大问题，因为虽然我们会为 HttpWebRequest、WebClient、HttpClient 和 RestSharp 等等不同的 API 而感到纠结，可这丝毫不会影响我们调用 Web API。那么，问题来了，当你面对数不胜数的 API 接口的时候，你打算如何考虑这些问题？我的 API 地址应该配置在哪里？是存到 Web.Config 里还是存到数据库里？我调用 API 的时候，Token 应该从哪里获取？是每次都获取还是获取了缓存起来？如果 Token 过期了我又该怎么办？这几乎是所有全部采用 Web API 进行微服务设计时都会遇到的问题。\n此时，我们需要一种更优雅的方式，即 Retrofit，它能让我们像调用一个普通方法一样调用一个 Web API，这样，我们在调用方式上其实不会有太大的改变，因为 Web API 本质上是一种特殊的 RPC。在.NET 的世界里，我们有 WebApiClient 和 Refit 这样的轮子，我之前还专门为大家介绍过 WebApiClient。这里就不再展示它的具体细节了，所谓点到为止，希望大家可以自己去发现这种美，对博主而言，如果在定义 Service 的时候，就考虑到这一点，或许我们可以实现更理想的效果，即，服务端和客户端是一套代码，我们写完一个接口以后，它就是 Web API，而通过动态代理，它本身又会是客户端，此中乐趣，则不足为外人道也！\n本文小结 又是漫长的一个夏天，下雨并不能让这座城市温柔起来。这篇博客延续了上一篇博客中关于动态 Controller 的设想，而借助于.NET Core 框架提供的良好特性，它以一种更为简洁的方式被实现了，核心的内容有两个点，其一是 ControllerFeatureProvider，它能决定 MVC 会不会把一个普通的类当做控制器。其二是 IApplicationModelConvention 接口，它能对全局的路由规则进行修改，以满足我们特殊的定制化需要。再此基础上，继续引入 Swagger 和 WebApiClient 两个轮子，来解决微服务构建中的 API 文档和 API 调用问题。写博客真的是一件辛苦的事情诶，好啦，今天这篇博客就先写到这里，我们下一篇博客再见，晚安！本文中涉及到的代码可以通过：https://github.com/qinyuanpei/DynamicWCFProxy/tree/master/DynamicWebApi.Core 来做进一步的了解，以上！\n","date":"2019-08-01T16:44:59Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/116795088/","slug":"116795088","tags":[".NET Core","API","Web","技巧"],"title":".NET Core POCOController 在动态 Web API 中的应用"},{"categories":["生活感悟"],"content":"\r年少时未见长安，难以想象万国来朝的盛唐气象，心中最为仰慕的人物，是那个“好剑术、喜任侠“、二十五岁“仗剑去国、辞亲远游”的李白。人在年少轻狂的时候，容易因为一个人的豪迈不羁，而选择性地模糊一个时代的印象。于是，长安就蓦地变成了李太白大放异彩的舞台。印象里的长安，是一个可以让人吟咏“安能摧眉折腰事权贵，使我不得开心颜”的地方，是一个可以让“贵妃捧墨、力士脱靴”的地方，是一个“绣口一吐，就是半个盛唐”的地方。自此，喜欢上“大道如青天，我独不得出”，喜欢上“古来圣贤皆寂寞，唯有饮者留其名”，喜欢上“天生我才必有用，千金散尽还复来”，仿佛李白就是盛唐，而盛唐就是长安，若非如此，杨玉环便不会在极乐之宴上称赞李白的才华。\n可当历史脉络逐渐清晰起来的时候，我们突然发现，原本我们以为最为意气风发的李白，那一年(开元 18 年，即公元 730 年)李白已经 30 岁了，就在那一年李白谒见宰相张说及一干长安名流，均无结果，直到他结识贺知章并被对方称之为“谪仙”， 引出一段“金龟换酒”的传奇佳话。这位谪仙人曾两度离开长安，第一次是感慨“行路难，归去来”，那一年李白 38 岁；第二次是供奉翰林期间遭玄宗“贬谪”，那一年李白 43 岁。所以，年少时以为的李白，是否是真实的李白？年少时以为的长安，是否是真实的长安？李白生平豪放，有诗的地方必有酒、有剑，一首《侠客行》更是金庸武侠小说中接近玄学的存在，可李白生平最快乐的时候，或许只有白帝城两岸猿啼知道了。\n人们喜欢选择性地美化回忆，就像迪士尼重制的《狮子王》，我们曾以为木法沙和辛巴是正义的化身，而弑兄上位的刀疤则是邪恶的代表，其实用成人的眼光来看待这部电影，我们还真的找不出木法沙和辛巴的统治会比刀疤的统治好到哪里去的证据，这大概就是记忆本身的滤镜作用，长大了发现小时候常吃的熊毅武方便面，居然是陕西省出产的，而中萃方便面，则是江苏省出产的。当你未来的时光，有一小部分是和这两个地方有关，你不能不说，记忆真的是个奇妙的东西。《妖猫传》里绮丽璀璨的幻术表演，极力展现长安奢华的一面，虽然这一切都是活在两位主角的想象中，因为它想表达的是一种美的消逝和幻灭，此时的长安，其伟大和无与伦比体现在这三个符号上——空海代表的真理密法、李白代表的艺术创作、杨贵妃代表的绝对的美。\n正如我们已无从想象，当年矗立在兴庆宫的花萼相辉楼到底是什么样子，于是导演将其安排在一个看起来像是个洞穴的地方。我们更无从得知，那一年的白居易到底怀着什么样的抱负来到长安，当他从已为陈迹的花萼相辉楼里捡起李白用过的笔的时候，是否真的如他想说想要开创一个新的时代，谱写一曲荡气回肠的长恨歌。有人说，玄宗兴建花萼相辉楼的时候，在周遭兴建了五座宅邸供诸王居住，逢宴请娱游更是宴请诸位兄弟一同前去，一改盛唐自玄武门之变以来手足相残的情形。可正如《长安十二时辰》里战战兢兢的太子李亨一般，一个对兄弟手足颇具温情的帝王，为何又能冤杀三个皇子在前？我们不得不说，历史极具迷惑性。\n《妖猫传》开篇由玄宗驾崩引出，在经历了安史之乱以后，通过一只猫的视角，我们看到了在极尽繁华后满目苍凉。有人问，李白为何在吟咏《清平调》的时候泪流满面？也许以李白经天纬地的才华，他早就洞悉了帝王之爱的虚伪，早就看出了盛世之下的危机重重，早就明白了身为御用文人的悲哀，所谓盛世，或许仅仅是帝王权术操纵下的一场表演。可即使如此，李白依然愿意在洞悉这一切后去讴歌这种美，不论美本身多么的脆弱，至少这一刻它是真实存在的，就像罗曼罗兰说的那样：“这世界上只有一种真正的英雄主义，就是在看清生活之后，依然热爱生活”。\n今天在路上遇到一位外地来的游客，询问我关于陕博、大雁塔和兵马俑的种种，可其实我和他一样，都不过是这座城市的过客。我有时候会不由地起，一千多年前的长安，是否和今天一样向世界敞开大门？《妖猫传》里遣唐的日本僧人空海，今天依然可以在青龙寺的简介中找到姓名，而透过张天爱的胡旋舞，或许可以看到那个胡汉相融、开放包容的长安。《长安十二时辰》里，有突厥狼卫，有胡椒胡饼，有波斯王子，有圣拜火教，有粟特大秦，有拢右拨换……几乎可以媲美世界中心。而一千多年后的今天，各种商业中心不再局限于东西两市，而兴庆宫、曲江池不再是皇家宫殿园林，长安一百零八坊的格局依稀可见，皇城北侧空荡荡的大明宫遗址，是否会听到来自玄武门的阵阵杀伐之声？\n《长安十二时辰》里的主角张小敬，曾经是万年县不良帅，长安以朱雀大街为界，朱雀大街以西为长安县，朱雀大街以东为万年县，实则取“长安万年”之意。剧中靖安吏们聚集在一起八卦朝廷，有一个人说自古以来哪里有万年的江山。不管历史上的李亨是否提出过对赋税和藩镇进行改革的提议，我们都知道安史之乱是唐朝由盛转衰的开始。据说马亲王这本书的灵感来自“刺客信条”，我本人同样是这部游戏的忠实粉丝，可当你真正想在长安寻找鸟瞰点并进行同步的时候，你会发现小雁塔顶端早已残损，攀爬这样的建筑物简直就是在碰瓷儿，而大雁塔的高度早已被周遭的大悦城超越，按照书中的设定，只要在大悦城的天台上增设武侯，不要说同步鸟瞰点，分分钟就会被弓弩手射中失去同步，因为据说大雁塔下面只有音乐喷泉，并没有安置干草堆……\n对于《长安十二时辰》这部网剧而言，剧中的崔器或许是无数想留在长安的人的一个缩影，没有过人的背景，智力和胆识有限，因为担负着兄长的希望和光耀门楣的使命，渴望建功立业、想抓住一切能抓住的机会努力向上爬，在长安这座城市获得一种归属感。崔器的人设并不讨喜，甚至从一出场就在扮演猪队友的角色，属于那种有点蠢但本质不坏的人，靖安司一役被编剧写死完全是剧情需要，总体来说，这种小人物的设定，只要不是又蠢又坏，总能因为贴近底层而引起更多人的共鸣。《妖猫传》里的主角白乐天初到长安时，诗人顾况开他玩笑说：“居大不易”。同样地，在《长安十二时辰》里，有到长安来干谒的岑参，有出身贫寒的元载，各自的选择不同，最终的结果不同。虽然选择比努力更重要，可你的能力总要能配得上你的野心。每个时代都有每个时代的困境，今天在西安讨生活的你我，和一千多年前的这些前辈们有什么不同呢？你的选择又是什么呢？\n在浩如烟海的茫茫历史中，太多的人和事，最终都会变得像雪泥鸿爪一般无迹可寻。对史学家而言，那是震惊寰宇的历史发现，可对更多像你和我一样的普通人而言，那不过偶然想起的经年旧事。我们回头看这些历史的时候，一如空海和白居易回头瞥见八重樱下的杨玉环，所谓“一切有为法，皆化作泡影”。《长安十二时辰》中塑造了张小敬这样一个“刺客”形象，可历史不过是姚汝能笔下轻描淡写的一句话。安国柱，一个身在长安的粟特人，即使娶了美艳的长安女子做妻子，依然想着努力工作好配得上她；徐宾，一个身在长安的靖安司主事，为了让大家更好得整理案牍，积极改良造纸技术，甚至到生命的最后一刻还在保护案牍；焦遂，一个身在长安的布衣，悬挂金鱼袋只为进宫喝酒，一句“长安，焦遂”豪气干云……有这些可爱可敬的人，我愿意相信，这一切都曾在这个城市发生过，而一千多年后的今天，我在长安，可我见了长安，便懂了长安么？是少年豪气作祟的“赵客缦胡缨，吴钩霜雪明”，还是一个快三十岁的中年大叔“为赋新词强说愁”呢……三十岁的中年大叔“为赋新词强说愁”呢……\n","date":"2019-07-22T11:17:23Z","image":"/posts/1540537013/P2561551373.jpg","permalink":"https://qinyuanpei.github.io/posts/1540537013/","slug":"1540537013","tags":["长安","随笔","长安十二时辰"],"title":"长安十二时辰随想"},{"categories":["编程语言"],"content":"Hi，大家好，我是 Payne，欢迎大家一如既往地关注我的博客。今天这篇博客里的故事背景，来自我工作中的一次业务对接，因为客户方提供的是长达上百行的 XML，所以一度让更喜欢使用 JSON 的博主感到沮丧，我这里不是想讨论 XML 和 JSON 彼此的优缺点，而是我不明白 AJAX 里的 X 现在基本都被 JSON 替代了，为什么还有这么多的人坚持使用并友好的 XML 作为数据的交换协议呢？也许你会说，因为有这样或者那样等等的理由，就像 SOA、ESB、SAP 等等类似的技术在企业级用户依然大量流行一样，而这些正是“消费”XML 的主力军。我真正想说的是，在对接这类接口时，我们会遇到一个异步化的 HTTP 协议场景，这里的异步和多线程、async/await 没有直接关系，因为它描述的实际上是业务流程上的一种“异步”。\n引子-想对 XML 说不 我们知道，HTTP 协议是一个典型的请求-响应模型，由调用方(Client)调用服务提供者(Server)提供的接口，在理想状态下，后者在处理完请求后会直接返回结果。可是当后者面对的是一个“耗时”任务时，这种方式的问题就立马凸显出来，此时调用者有两个选择：一直等对方返回直至超时(同步)、隔一会儿就看看对方是否处理完了(轮询)。这两种方式，相信大家都非常熟悉了，如果继续延伸下去，我们会联想到长/短轮询、SignalR、WebSocket。其实，更好的方式是，我们接收到一个“耗时”任务时，立即返回表明我们接收了任务，等任务执行完以后再通知调用者，这就是我们今天要说的 HTTP 异步化方案。因为对接过程中，客户采用的就是这种方案，ESB 这类消息总线本身就提供了这种功能，可作为调用方的博主就非常难受啦，因为明明能“同步”地处理完的事情，现在全部要变成“异步”处理，就像一个习惯了 async/await 语法糖的人，突然间就要重新开始写 APM 风格的代码，宝宝心里苦啊，“异步”处理就异步处理嘛，可要按人家要求去返回上百行的 XML，博主表示想死的心都有了好嘛……\n好了，吐槽归吐槽，吐槽完我们继续梳理下 HTTP 异步化的方案，这种方式在现实生活中还是相当普遍的，毕竟人类都是“异步”做事，譬如“等你哪天有空一起吃个饭”，测试同事对我说得最多的话就是，“等你这个 Bug 改完了同我说一声”，更不用说，JavaScript 里典型的异步单线程的应用等等……实现“异步”的思路其实是非常多的，比如同样在 JavaScript 里流行的回调函数，比如通过一张中间表存起来，比如推送消息到消息队列里。在面向数据库编程的时候，我听到最多的话就是，没有什么问题是不能用一张中间表来解决的，如果一张不行那就用两张。项目上我是用 Quartz+中间表的方式实现的，因为这是最为普通的方式。这里，我想和大家分享下，关于使用 Hangfire 来实现类似 Quartz 定时任务的相关内容，果然，我这次又做了一次标题党呢，希望大家会对今天的内容感兴趣。简单来说，我们会提供一个接口，调用方提供参数和回调地址，调用后通过 Hangfire 创建后台任务，等任务处理结束后，再通过回调地址返回结果给调用方，这就是所谓的 HTTP 异步化。\n开箱即用的 Hangfire 我们项目上是使用 Quartz 来实现后台任务的，因为它采用了反射的方式来调用具体的 Job，因此，它的任务调度和任务实现是耦合在同一个项目里的，常常出现单个 Job 引发整个系统卡顿的情况，尤其是是它的触发器，常常导致一个 Job 停都停不下来，直到后来才渐渐开始通过 Web API 来分离这两个部分。Quartz 几乎没有一个自己的可视化界面，我们为此专门为它开发了一套 UI。我这里要介绍的 Hangfire，可以说它刚好可以作为 Quartz 的替代品，它是一个开箱即用的、轻量级的、开源后台任务系统，想想以前为 Windows 开发定时任务，只能通过定时器(Timer)来实现，尚不知道 CRON 为何物，而且只能用命令行那种拙劣的方式来安装/卸载，我至今都记得，测试同事问我，能不能不要每次都弹个黑窗口出来，这一起想起来还真是让人感慨啊。好了，下面我们开始今天的实践吧！首先，第一步自然是安装 Hangfire 啦，这里我们新建一个 ASP.NET Core 的 Web API 项目就好，然后通过 NuGet 依次安装以下库：\nInstall-Package HangFire Install-Package Hangfire.MySql.Core 这里我们选择了 MySQL 来实现任务的持久化，从官方的流程图中可以了解到，Hangfire 有服务端、持久化存储和客户端三大核心部件组成，而持久化存储这块儿，除了官方默认的 SQLServer(可以集成 MSMQ)以外，还支持 Redis、MongoDB 等，Hangfire 使用起来是非常简单哒，首先在 Startup 类的 ConfigureServices()方法中注入 Hangfire 相关的服务，然后在 Configure()方法中使用 HangfireServer 和 UseHangfireDashboard 即可：\npublic void ConfigureServices (IServiceCollection services) { //为了简化说明，已忽略该方法中无关的代码 services.AddHangfire (x =\u0026gt; x.UseStorage (new MySqlStorage (Configuration.GetConnectionString (\u0026#34;Hangfire\u0026#34;))) .UseFilter (new HttpJobFilter ()) .UseSerilogLogProvider () ); } public void Configure (IApplicationBuilder app, IHostingEnvironment env) { //为了简化说明，已忽略该方法中无关的代码 app.UseHangfireServer (new BackgroundJobServerOptions () { Queues = new string[] { \u0026#34;default\u0026#34; }, WorkerCount = 5, ServerName = \u0026#34;default\u0026#34;, }); app.UseHangfireDashboard (); app.ApplicationServices.GetService\u0026lt;ILoggerFactory\u0026gt; ().AddSerilog (); } 注意到在配置持久化的部分，我们使用了一个数据库连接字符串 Hangfire，它需要我们在 appsettings.json 中配置 ConnectionStrings 部分。这里我们为 Hangfire 设置了默认队列 default、默认服务器 default、并发数目为 5。与此同时，我们开启了 Hangfire 中自带的 Dashboard，可以通过这个界面来监控后台任务的执行情况。此时运行项目，输入以下地址：http://locahost:/hangfire，就会看到下面的画面，这说明 Hangfire 配置成功：\nHangfire Dashboard\rHangfire 中默认支持四种类型的后台任务，他们分别是Fire-and-forget jobs、Delayed jobs、Recurring jobs和Continuations。严格来说，Fire-and-forget jobs和Delayed jobs并不能算后台任务，因为它们在执行一次后就会从队列中移除，属于一次性“消费”的任务，这两者的不同在于Delayed jobs可以在设定的时间上延迟执行。而Recurring jobs和Continuations则是周期性任务，任务在入队后可以按照固定的时间间隔去执行，周期性任务都是支持 CRON 表达式的，Continuations类似于 Task 中的 ContinueWith()方法，可以对多个任务进行组合，我们现在的项目中开发了大量基于 Quartz 的 Job，可当你试图把这些 Job 相互组合起来的时候，你就会觉得相当尴尬，因为后台任务做所的事情往往都是大同小异的。从官方文档中 ，我们会发现 Hangfire 的关键代码只有下面这四行代码，可以说是相当简洁啦！\n//Fire-and-forget jobs var jobId = BackgroundJob.Enqueue( () =\u0026gt; Console.WriteLine(\u0026#34;Fire-and-forget!\u0026#34;)); //Delayed jobs var jobId = BackgroundJob.Schedule( () =\u0026gt; Console.WriteLine(\u0026#34;Delayed!\u0026#34;), TimeSpan.FromDays(7)); //Recurring jobs RecurringJob.AddOrUpdate( () =\u0026gt; Console.WriteLine(\u0026#34;Recurring!\u0026#34;), Cron.Daily); //Continuations BackgroundJob.ContinueWith( jobId, () =\u0026gt; Console.WriteLine(\u0026#34;Continuation!\u0026#34;)); Hangfire 除了这种偏函数式风格的用法以外，同样提供了泛型版本的用法，简而言之，泛型版本是自带依赖注入的版本。众所周知，稍微复杂点的功能，常常会依赖多个服务，比如后台任务常常需要给相关人员发邮件或者是消息，此时，Job 的实现就会依赖 MailService 和 MessageService。Hangfire 内置了基于 Autofac 的 IoC 容器，因此，当我们使用泛型版本时，它可以自动地从容器中 Resolve 相应的类型出来。事实上，我们可以通过重写 JobActivator 来实现自己的依赖注入，譬如博主就喜欢 Castle。下面是一个简单的例子：\n//Define a class depends on IDbContext \u0026amp; IEmailService public class EmailSender { private IDbContext _dbContext; private IEmailService _emailService; public EmailSender() { _dbContext = new DbContext(); _emailService = new EmailService(); } // ... } //When it is registered in Ioc Container BackgroundJob.Enqueue\u0026lt;EmailSender\u0026gt;(x =\u0026gt; x.Send(\u0026#34;Joe\u0026#34;, \u0026#34;Hello!\u0026#34;)); 可扩展的 Hangfire OK，在对 Hangfire 有了一个初步的了解以后，我们再回到本文的题目，我们希望实现一个基于 HTTP 方式调用的 HttpJob。因为我们不希望任务调度和具体任务放在一起，我们项目上采用 Quartz 来开发后台任务，它要求我们实现一个特定接口 IbaseJob，最终任务调度时会通过反射来创建 Job，就在刚刚过去的这周里，测试同事向我反馈了一个 Bug，而罪魁祸首居然是因为某个 DLL 没有分发，所以，我希望实现一个基于 HTTP 方式调用的 HttpJob，这既是为了将任务调度和具体任务分离，同时为了满足这篇文章开头描述的场景，得益于 Hnagfire 良好的扩展性，我们提供了一组 Web API，代码如下：\n/// \u0026lt;summary\u0026gt; /// 添加一个任务到队列并立即执行 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;jobDescriptor\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpPost (\u0026#34;AddEnqueue\u0026#34;)] public JsonResult Enqueue (HttpJobDescriptor jobDescriptor) { try { var jobId = string.Empty; jobId = BackgroundJob.Enqueue (() =\u0026gt; HttpJobExecutor.DoRequest (jobDescriptor)); return new JsonResult (new { Flag = true, Message = $\u0026#34;Job:#{jobId}-{jobDescriptor.JobName}已加入队列\u0026#34; }); } catch (Exception ex) { return new JsonResult (new { Flag = false, Message = ex.Message }); } } /// \u0026lt;summary\u0026gt; /// 添加一个延迟任务到队列 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;jobDescriptor\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpPost (\u0026#34;AddSchedule\u0026#34;)] public JsonResult Schedule ([FromBody] HttpJobDescriptor jobDescriptor) { try { var jobId = string.Empty; jobId = BackgroundJob.Schedule (() =\u0026gt; HttpJobExecutor.DoRequest (jobDescriptor), TimeSpan.FromMinutes ((double) jobDescriptor.DelayInMinute)); return new JsonResult (new { Flag = true, Message = $\u0026#34;Job:#{jobId}-{jobDescriptor.JobName}已加入队列\u0026#34; }); } catch (Exception ex) { return new JsonResult (new { Flag = false, Message = ex.Message }); } } /// \u0026lt;summary\u0026gt; /// 添加一个定时任务 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;jobDestriptor\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpPost (\u0026#34;AddRecurring\u0026#34;)] public JsonResult Recurring ([FromBody] HttpJobDescriptor jobDescriptor) { try { var jobId = string.Empty; RecurringJob.AddOrUpdate (jobDescriptor.JobName, () =\u0026gt; HttpJobExecutor.DoRequest (jobDescriptor), jobDescriptor.Corn, TimeZoneInfo.Local); return new JsonResult (new { Flag = true, Message = $\u0026#34;Job:{jobDescriptor.JobName}已加入队列\u0026#34; }); } catch (Exception ex) { return new JsonResult (new { Flag = false, Message = ex.Message }); } } /// \u0026lt;summary\u0026gt; /// 删除一个定时任务 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;jobName\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpDelete (\u0026#34;DeleteRecurring\u0026#34;)] public JsonResult Delete (string jobName) { try { RecurringJob.RemoveIfExists (jobName); return new JsonResult (new { Flag = true, Message = $\u0026#34;Job:{jobName}已删除\u0026#34; }); } catch (Exception ex) { return new JsonResult (new { Flag = false, Message = ex.Message }); } } /// \u0026lt;summary\u0026gt; /// 触发一个定时任务 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;jobName\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpGet (\u0026#34;TriggerRecurring\u0026#34;)] public JsonResult Trigger (string jobName) { try { RecurringJob.Trigger (jobName); return new JsonResult (new { Flag = true, Message = $\u0026#34;Job:{jobName}已触发执行\u0026#34; }); } catch (Exception ex) { return new JsonResult (new { Flag = false, Message = ex.Message }); } } /// \u0026lt;summary\u0026gt; /// 健康检查 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpGet (\u0026#34;HealthCheck\u0026#34;)] public IActionResult HealthCheck () { var serviceUrl = Request.Host; return new JsonResult (new { Flag = true, Message = \u0026#34;All is Well!\u0026#34;, ServiceUrl = serviceUrl, CurrentTime = DateTime.Now }); } 你可以注意到，这里用到其实还是四种后台任务，在此基础上增加了删除 Job 和触发 Job 的接口，尤其是触发 Job 执行的接口，可以弥补 Quartz 的不足，很多时候，我们希望别人调了接口后触发后台任务，甚至希望在编写 Job 的过程中使用依赖注入，因为种种原因，实施起来总感觉有点碍手碍脚。这里我们定义了一个 HttpJobExecutor 的类，顾名思义，它是执行 Http 请求的一个类，说来惭愧，我写作这篇博客时，是一边看文档一边写代码的，所以，等我实现了这里的 HttpJobExecutor 的时候，我忽然发现文档中关于依赖注入的内容，简直相见恨晚啊。这里直接给出它的实现，我要再一次安利 RestSharp 这个库，比 HttpWebRequest、HttpClient 这两套官方的 API 要好用许多，可还是有人喜欢一遍又一遍地封装啊，话说自从我们把 WCF 换成 Web API 后，看着相关同事在 Git 上的折腾历史，果然还是回到了写 Http Client 的老路上来，话说在纠结是手写代理还是动态代理的时候，Retrofit 了解下啊！\n[HttpJobFilter] public static void DoRequest (HttpJobDescriptor jobDestriptor) { var client = new RestClient (jobDestriptor.HttpUrl); var httpMethod = (object) Method.POST; if (!Enum.TryParse (typeof (Method), jobDestriptor.HttpMethod.ToUpper (), out httpMethod)) throw new Exception ($\u0026#34;不支持的HTTP动词：{jobDestriptor.HttpMethod}\u0026#34;); var request = new RestRequest ((Method) httpMethod); if (jobDestriptor.JobParameter != null) { var json = JsonConvert.SerializeObject (jobDestriptor.JobParameter); request.AddParameter (\u0026#34;application/json\u0026#34;, json, ParameterType.RequestBody); } var response = client.Execute (request); if (response.StatusCode != HttpStatusCode.OK) throw new Exception ($\u0026#34;调用接口{jobDestriptor.HttpUrl}失败，接口返回：{response.Content}\u0026#34;); } 在这里，我们以 HealthCheck 这个接口为例，来展示 HttpJob 是如何工作的。顾名思义，这是一个负责健康检查的接口。我们现在通过 Postman 来触发健康检查这个后台任务。在这里，该接口是一个 GET 请求：\n通过Postman创建后台任务\r接下来，我们我们就会在 Hangfire 的 Dashborad 中找到对应的记录，因为这是一个Fire \u0026amp; Forget类型的任务，因此我们几乎看不到中间的过程，它就已经执行结束啦。我们可以在 Dashboard 中找到对应的任务，然后了解它的具体执行情况。值得一提的是，Hangfire 自带了重试机制，对于执行失败的任务，我们可以重试栏目下看到，这里是其中一条任务的执行记录。可以注意到，Hangfire 会把每个 Job 的参数序列化为 JSON 并持久化起来，仔细对照的话，你会发现，它和我们在 Postman 中传入的参数是完全一样的！\nHangfire中Job执行详情查看\r在执行 Job 的过程中，我们可能会希望记录 Job 执行过程中的日志。这个时候，Hangfire 强大的扩展性再次我们提供了这种可能性。注意到在 HttpJobExecutor 类上有一个 [HttpJobFilter]的标记，显然这是由 Hangfire 提供的一个过滤器，博主在这个过滤器中对 Job 的 ID、状态等做了记录，因为在整个项目中博主已经配置了 Serilog 作为 Hangfire 的 LogProvider，所以，我们可以在过滤器中使用 Serilog 来记录日志，不过博主个人感觉这个 Filtre 稍显鸡肋，这里还是给出代码片段吧！\npublic class HttpJobFilter : JobFilterAttribute, IApplyStateFilter { private static readonly ILog Logger = LogProvider.GetCurrentClassLogger (); public void OnStateApplied (ApplyStateContext context, IWriteOnlyTransaction transaction) { if (context.NewState is FailedState) { var failedState = context.NewState as FailedState; if (failedState != null) { Logger.ErrorException ( String.Format (\u0026#34;Background Job #{0} 执行失败。\u0026#34;, context.BackgroundJob.Id), failedState.Exception); } } else { Logger.InfoFormat ( \u0026#34;当前执行的Job为：#{0}, 状态为：{1}。\u0026#34;, context.BackgroundJob.Id, context.NewState.Name ); } } public void OnStateUnapplied (ApplyStateContext context, IWriteOnlyTransaction transaction) { } } 为什么我说这个 Filter 有点鸡肋呢？因为你看下面的图就会明白了啊！\n使用Serilog记录日志\r本文小结 果然，我还是不得不承认，这又是一篇彻彻底底的\u0026quot;水文\u0026quot;啊,因为写着写着就发现自己变成了标题党。这篇文章总结下来其实只有两句话，一个不喜欢写 XML 报文的博主，如何与 ERP、SAP、ESB 里的 XML 报文斗智斗勇的故事，在这样一个背景下，为了满足对方的\u0026quot;异步\u0026quot;场景, 不得不引入一个后台任务系统来处理这些事情，其实，这个事情用消息队列、用 Redis、甚至普通的中间表都能解决，可惜我写这篇文章的时候，是有一点个人化的情绪在里面的，这种情绪化导致的后果就是，可能我越来越难以控制一篇文章的写作走向啦，大概是写东西越来越困难，而又没有时间取吸收新的知识进来，这让我觉得自己的进步越来越少，Hangfire 的有点说起来就是挺好用的，以上！\n","date":"2019-07-04T08:56:28Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1071063696/","slug":"1071063696","tags":[".NET Core","Hangfire","HTTP"],"title":"使用 ASP.NET Core 和 Hangfire 实现 HTTP 异步化方案"},{"categories":["编程语言"],"content":"在《又见 AOP 之基于 RealProxy 实现 WCF 动态代理》 这篇文章中，我和大家分享了关于使用动态代理来简化 WCF 调用过程的相关内容，当时我试图解决的问题是，项目中大量通过 T4 生成甚至手动编写的“代理方法”。今天，我想和大家分享的是，如何通过动态的 Controller 来实现从 WCF 到 Web API 的迁移。为什么会有这个环节呢？因为我们希望把一个老项目逐步迁移到.NET Core 上面，在这个过程中首当其冲的就是 WCF，它在项目中主要承担着内部 RPC 的角色，因为.NET Core 目前尚未提供针对 WCF 服务端的支持，因此面对项目中成百上千的 WCF 接口，我们必须通过 Web API 重新“包装”一次，区别于那些通过逐个 API 进行改造的方式，这里我们通过 Castle 动态生成 Controller 来实现从 WCF 到 Web API 的迁移。\n如何对类和接口进行组合 首先，我们来思考这样一个问题，假设现在有一个类 BaseClass、一个接口 IBaseService 及其实现类 BaseService，我们有没有什么办法，可以让这个类和接口组合起来呢？联系面向对象编程的相关知识，我们应该可以想到最常见的两种方式，即 BaseService 继承 BaseClass(或者反过来)、BaseClass 实现 IBaseService 接口。考虑到语言本身是否支持多继承的因素，第二种方式可能会更具有适用性。可如果这个问题，就仅仅到这种程度，我相信大家一定会感到失望，因为这的确没有什么好说的。现在的问题是，假如 BaseClass 类、BaseService 类都已经存在了，我们有没有什么思路，可以把它们组合到一个类中呢？这又和我们今天要讨论的内容有什么关系呢？\n好了，不卖关子啦，下面隆重请出 Castle 中的 Dynamic Proxy，我们曾经介绍过 Castle 中的动态代理，它可以为指定的类和接口创建对应的代理类，除此以外，它提供了一种称为AdditionalInterfaces的接口，这个接口可以在某个代理对象上“组合”一个或者多个接口，换句话说，代理对象本身包含被代理对象的全部功能，同时又可以包含某个接口的全部功能，这样就实现了一个类和一个接口的组合。为什么我们会需要这样一个功能呢？因为假如我们可以把一个 ApiController 类和指定的接口类如 CalculatorService 进行组合，在某种程度上，CalculatorService 就变成了一个 ApiController，这样就实现了我们的目标的第一步，即动态生成一个 ApiController。与此同时，它会包含我们现有的全部功能，为了方便大家理解，我们从下面这个简单的例子开始：\n/// \u0026lt;summary\u0026gt; /// IEchoService定义 /// \u0026lt;/summary\u0026gt; public interface IEchoService { void Echo (string receiver); } /// \u0026lt;summary\u0026gt; /// IEchoServicee实现 /// \u0026lt;/summary\u0026gt; public class EchoService : IEchoService { public void Echo (string receiver) { Console.WriteLine ($\u0026#34;Hi，{receiver}\u0026#34;); } } /// \u0026lt;summary\u0026gt; /// 空类EmptyClass /// \u0026lt;/summary\u0026gt; public class EmptyClass { } public class EchoInterceptor : IInterceptor { private IEchoService _realObject; public EchoInterceptor (IEchoService realObject) { _realObject = realObject; } public void Intercept (IInvocation invocation) { invocation.Method.Invoke (_realObject, invocation.Arguments); } } var container = new WindsorContainer (); container.Register ( Component.For\u0026lt;EchoService, IEchoService\u0026gt;(), Component.For (typeof (EchoInterceptor)).LifestyleTransient(), Component.For (typeof (EmptyClass)).Proxy.AdditionalInterfaces (typeof(IEchoService)) .Interceptors (typeof (EchoInterceptor)).LifestyleTransient() ); var emptyClass = container.Resolve\u0026lt;EmptyClass\u0026gt; (); var methodInfo = emptyClass.GetType().GetMethod (\u0026#34;Echo\u0026#34;); methodInfo.Invoke (emptyClass, new object[] { \u0026#34;Dynamic WebApi\u0026#34; }); 此时，我们会发现通过 Castle 动态生成的代理类，同时具备了类和接口的功能。\n通过Castle实现类和接口的组合功能\r重温 ASP.NET MVC 原理 OK，通过第一个例子，我们已经达到了第一个目的。接下来，顺着这个思路，我们不妨想象一下，如果把这个 BaseClass 换成 BaseController 会怎么样呢？因为在一个 OO 的语言里，一切都是 Class，所以，Web 开发中的 Controller 同样不会脱离这个体系。不过，在这之前，我们需要复习下 ASP.NET MVC 的原理，为什么要说这个呢？因为接下来的内容，都和它有重大的关联，我们实际上是自己实现了 ASP.NET MVC 中几个关键的环节，所以，在博主看来，这部分内容是非常重要的，这几乎是这篇文章中实现的最细节部分，因为第一个目标，说句实话，Castle 帮我们简化到了只有 4 行代码。\n一张图了解MVC 一张图了解MVC\r通常来讲，当我们在 MVC 中接收到一个 Url 请求后，这个请求会被 UrlRoutingModule 拦截。此时，请求的上下文 HttpContext 会被封装到 HttpContextWrapper 对象中。而根据当前请求的 HttpContext，则可以提取出符合当前 Url 的路由对象 RouteData，它会被进一步封装为 RequestContext 对象。接下来，从 RequestContext 对象中获取 RouteData，它对应一个 RouteHandler，是 IHttpHandler 的一个实现类。对于 MVC 而言，则对应 MvcHandler。通过调用 MvcHandler，对应的 Controller 会被反射激活，进而调用具体的 Action。以上就是整个 MVC 请求的过程描述，可以看出最关键的两个组件是 UrlRoutingModule 和 MvcHandler，前者的作用是解析 Controller 和 Action 名称，后者的作用则是根据 Controller 名称去反射调用具体的 Action，大家可以通过上面的图来理解这个过程。\n在这里，其实我们只需要关注第二部分:-D，即 MvcHandler，因为我们会在默认路由的基础上，增加一个自定义路由来“标记”这些动态的 Controller，所以，我们集中关注 MvcHandler 这部分即可，虽然这里提到它会根据 Controller 的名称来反射激活相应的 Controller 实例、调用具体的 Action，但这仅仅是宏观上的一种认识。我们来看一下，它具体是怎么反射出 Controller 以及调用 Action 的。\nIControllerFactory 接口 第一个关键的组件是 IControllerFactory 接口，顾名思义，它是作用是创建 Controller，可实际上，这个组件除了完成创建 Controller 的工作以外，还会涉及到 Controller 类型的解析、Controller 实例激活、Controller 实例释放、会话状态行为选项获取等多个功能。这里有一个激活的过程，我们可以将其理解为 Controller 的初始化，因为 Controller 在使用的过程中往往会通过 IoC 容器来注入相关服务，所以，你可以理解为在构造 Controller 的过程中，我们需要一个 IoC 容器来完成依赖注入相关的事情，微软默认提供了一个 DefaultControllerFactory 的实现，它内部是通过 IHttpControllerActivator 接口来完成依赖注入的，而这恰恰是我们要关注的第二个组件。\nIHttpControllerActivator 接口 老实说，通过自定义 IHttpControllerActivator 的方式实现依赖注入的方式并不常见，因为更一般的情况是，大家在 Global.asax 里初始化像 Unity、Autofac 等等类似的容器，然后在 Controller 里通过容器去 Resolve 一个服务出来，对于 IHttpControllerActivator 接口而言，它只有一个 Create()方法，在这篇文章中，我们是通过 Castle 这个容器来实现依赖注入的，所以，你大概可以想象出它的过程，首先把所有动态生成的 Controller 全部注入到 Ioc 容器中，然后再根据传入的类型获取对应 Controller 的实例。在本文中，我们重写了默认的 HttpControllerActivator，这里主要指 Create()方法，因为我们希望实现的效果是，动态的 Controller 全部从 Castle 容器中获取，而静态的 Controller 依然按照微软的设计来获取。\nIHttpControllerSelector 接口 OK，现在有了 Controller 以后，我们怎么让 MVC 路由到正确的 Controller 上面去呢？这时候，必然需要有人来解析路由啊，这就是第三个组件——IHttpControllerSelector。这又是一个顾名思义的接口，充分说明命名是件多么重要的事情。在这里我们重写了 SelectController()方法，当路由信息中存在 ServiceName 和 ActionName 时，就去检查容器中是否存在对应的 Controller，如果存在就返回一个 HttpControllerDescriptor，这是一个用以描述控制器上下文信息的类型。反之，会调用默认的 base.SelectController()方法，这样做还是为了兼容微软原来的设计，因为我们不希望在引入动态 Controller 后，导致普通的 Controller 无法正常工作。\nIhttpActionSelector 接口 同理，我们还需要告诉 MvcHandler，它应该调用哪个方法，这时候我们需要 IHttpActionSelector，因为从路由信息中我们可以提取到 ActionName 参数，因此，通过通过 typeof(Controller).GetMethod(ActionName)，就可以获得对应 ActionName 对应的方法，熟悉反射的朋友应该都知道，它会返回 MethodInfo 这个类型，实际上 IHttpActionSelector 所做的事情，就是把 MethodInfo 传给 MvcHandler，因为此时只要通过反射调用这个方法即可，Controller 的实例在上一步就创建好了，而调用方法所需要的参数，则被存储在当前请求的上下文 HttpContext 里面，至此万事具备！我们要做的，就是顺着这些思路去实现以上组件。\n关键组件的自定义实现 OK，下面我们来看看如何针对这些组件， 来分别实现我们的自定义组件，实现这些自定义组件并对 MVC 中的默认组件进行替换，这就是我们这篇文章中实现动态 Controller 的一个基本原理。\nDynamicControllerActivator DynamicControllerActivator 实现了 IHttpControllerActivator 接口，这里我们通过单例模式获得了 DynamicHttpControllerManager 对象的一个实例，其内部封装了 Castle 的容器接口 IWindsorContainer，所以，在这里我们直接通过 controllerType 从容器中 Resolve 对应的 Controller 即可，而默认情况下，所有的 Controller 都实现了 IHttpController 接口，所以，这一步我们需要做一个显示的类型转换，后面我们会通过它替换微软默认的实现，这样，当一个请求被发送过来的时候，我们实际上是从这个自定义容器中获取对应 Controller 的实例。\npublic class DynamicHttpControllerActivtor : IHttpControllerActivator { public IHttpController Create(HttpRequestMessage request, HttpControllerDescriptor controllerDescriptor, Type controllerType) { return (IHttpController)DynamicHttpControllerManager.GetInstance().Resolve(controllerType); } } DynamicHttpControllerSelector 如果说 DynamicControllerActivator 是真正实现控制器的**\u0026ldquo;激活\u0026rdquo;部分，那么在此之前，我们需要实现控制器的\u0026ldquo;筛选\u0026rdquo;部分，换言之，一个请求被发送过来的时候，到底应该用哪一个 Controller 去处理这个请求呢？所以，我们来看看 DynamicHttpControllerSelector 这个组件是如何实现的，这里我们重写 SelectController()这个方法来完成控制器的\u0026ldquo;筛选\u0026rdquo;部分的工作。可以注意到，我们首先会判断路由信息中是否存在 ServiceName 和 ActionName 这两个值，因为对于动态的 Controller，我们默认使用的路由模板是services/{ServiceName}/{ActionName}**，这里使用 services 前缀是为了区别于微软默认的 api 前缀，当然，强迫症的你同样可以使用相同的前缀。\n接下来，我们会判断 ServiceName 是否在容器中注册过，如果注册了就从容器里取出对应的服务，并构造 DynamicHttpControllerDescriptor 对象，否则调用父类方法按微软默认实现去处理。那么，这个 DynamicHttpControllerDescriptor 对象，又是何方神圣呢？从名称上我们大概可以了解，这应该是一个对控制器相关信息进行描述的类型，它继承了 HttpControllerDescriptor 这个父类，目前没有任何扩展性的实现。对于 DynamicHttpControllerDescriptor，它最重要的参数是构造函数中第三个参数，即 controllerType，因为 DynamicControllerActivator 实际上就是根据它来工作的。\npublic class DynamicHttpControllerSelector: DefaultHttpControllerSelector { private HttpConfiguration _configuration; /// \u0026lt;summary\u0026gt; /// 构造函数 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;configuration\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; public DynamicHttpControllerSelector(HttpConfiguration configuration) : base(configuration) { _configuration = configuration; } public override HttpControllerDescriptor SelectController(HttpRequestMessage request) { var routeData = request.GetRouteData().Values; if (routeData.ContainsKey(\u0026#34;ServiceName\u0026#34;) \u0026amp;\u0026amp; routeData.ContainsKey(\u0026#34;ActionName\u0026#34;)) { var serviceName = routeData[\u0026#34;ServiceName\u0026#34;].ToString(); var actionName = routeData[\u0026#34;ActionName\u0026#34;].ToString(); if (DynamicHttpControllerManager.GetInstance().ContainsService(serviceName)) { var controllerInfo = DynamicHttpControllerManager.GetInstance().GetControllerInfo(serviceName); var controller = DynamicHttpControllerManager.GetInstance().Resolve(serviceName); if (controller == null) return base.SelectController(request); var controllerDescriptor = new DynamicHttpControllerDescriptor(_configuration, serviceName, controllerInfo.ControllerType); controllerDescriptor.Properties[\u0026#34;ServiceName\u0026#34;] = serviceName; controllerDescriptor.Properties[\u0026#34;ActionName\u0026#34;] = actionName; controllerDescriptor.Properties[\u0026#34;IsDynamicController\u0026#34;] = true; controllerDescriptor.Properties[\u0026#34;ServiceType\u0026#34;] = controllerInfo.ServiceType; controllerDescriptor.Properties[\u0026#34;ControllerType\u0026#34;] = controller.GetType(); return controllerDescriptor; } } return base.SelectController(request); } } DynamicHttpActionSelector 既然通过路由中的 ServiceName 可以对 Controller 进行**\u0026ldquo;筛选\u0026rdquo;，那么，我们自然可以通过路由中的 ActionName 来对 Action 进行筛选\u0026quot;**。Action 是控制器中的概念，对应一般的接口或者类，我们称之为方法，因此，DynamicHttpActionSelector 在这里实现针对 Action 的筛选，它继承 ApiControllerActionSelector 类并重写了 SelectAction()方法，下面给出具体的实现：\npublic class DynamicHttpActionSelector : ApiControllerActionSelector { public override HttpActionDescriptor SelectAction(HttpControllerContext controllerContext) { var isDynamicController = controllerContext.ControllerDescriptor.Properties.ContainsKey(\u0026#34;IsDynamicController\u0026#34;); if (isDynamicController) { var controllerType = new object(); if (controllerContext.ControllerDescriptor.Properties.TryGetValue(\u0026#34;ControllerType\u0026#34;, out controllerType)) { var actionName = controllerContext.ControllerDescriptor.Properties[\u0026#34;ActionName\u0026#34;].ToString(); var methodInfo = ((Type)controllerType).GetMethod(actionName); if (methodInfo == null) return base.SelectAction(controllerContext); return new DynamicHttpActionDescriptor(controllerContext.ControllerDescriptor, methodInfo); } } return base.SelectAction(controllerContext); } } 和筛选 Controller 的过程类似，首先我们会判断这是不是一个动态的 Controller，请注意在 DynamicHttpControllerSelector 中，我们为 ControllerDescriptor 添加了大量的 Properties，这些 Properties 可以在这里继续使用。显然，我们只需要关注动态的 Controller 即可，如果可以通过 ActionName 找到对应的 MethodInfo，那就说明当前 Controller 中存在指定的 Action，反之则需要调用父类方法按微软默认的实现去处理。其实，这里不好的一点就是，我们的通过反射获取 MethodInfo 时，需要传入 ActionName 即方法的名字，而方法的名字是区分大小写的，这会导致我们的 URL 必须区分大小写，这不太符合 RESTful API 风格。同样额，这里定义了一个类型 DynamicHttpActionDescriptor，它继承自 ReflectedHttpActionDescriptor，它需要传入 MethodInfo，这样 MVC 就知道应该去调用控制器的哪一个方法了。\n容器注册及服务替换 在我们实际的业务系统中，存在着大量的 WCF 接口，它们都是通过 ServiceHost 这种方式来托管，然后在调用端通过代理类的方式来相互调用，因此把 WCF 迁移到 Web API 上，被抛弃的仅仅是这些.svc 的文件，而这些 WCF 接口依然可以继续使用。在之前的文章中，我们用 Castle 的 Dynamic Proxy 来代替各种手写的代理类，在这篇文章中我们继续沿用 ICalculator 这个接口示例，它包含着最为简单加减乘除四个方法，那么，我们应该怎样把这个接口变成一个 Web API 呢？这就是所谓的容器注册和服务替换啦！首先我们来注册 ICalculator 这个服务，它的代码只有一行：\nDynamicHttpControllerManager.GetInstance().RegisterType\u0026lt;CalculatorService, ICalculator\u0026gt;(); 这是一个典型的依赖注入，其中 CalculatorService 是 ICalculator 的实现类，它到底做了什么呢？我们来看看本质：\npublic void RegisterType\u0026lt;TImplement, TInterface\u0026gt;(string serviceName = \u0026#34;\u0026#34;) { if (string.IsNullOrEmpty(serviceName)) serviceName = GetServiceName\u0026lt;TImplement\u0026gt;(); _container.Register( Component.For(typeof(TImplement), typeof(TInterface)), Component.For\u0026lt;DynamicApiInterceptor\u0026lt;TInterface\u0026gt;\u0026gt;().LifestyleTransient(), Component.For\u0026lt;BaseController\u0026lt;TInterface\u0026gt;\u0026gt;().Proxy.AdditionalInterfaces(typeof(TInterface)) .Interceptors\u0026lt;DynamicApiInterceptor\u0026lt;TInterface\u0026gt;\u0026gt;().LifestyleTransient() .Named(serviceName) ); _controllerInfoList.Add(serviceName, new DynamicControllerInfo(typeof(TInterface))); } 有没有觉得这段代码非常熟悉，实际上这就是我们这篇文章最开始提出的问题：怎么样对一个类和接口进行租户。一开始我们是用一个最普通的类、一个最普通的接口来演示这种可能性，而这里我们不过将其推广到一个特殊的场景，如果这个类是一个继承了 ApiController 的 BaseController 呢？这是一个由一般到特殊的过程。如你所见，内部的确使用了 Castle 的容器来处理依赖注入，而_controllerInfoList 则存储了 Controller 相关的信息，方便我们在整个流程中随时获取这些信息。完成容器注册以后，我们就可以着手对 MVC 中的默认组件进行替换工作啦，我个人建议，替换工作放在整个 Global.asax 的最前面：\nvar configuration = GlobalConfiguration.Configuration; var dynamicControllerSelector = new DynamicHttpControllerSelector(configuration); var dynamicHttpControllerActivtor = new DynamicHttpControllerActivtor(); var dynamicActionSelector = new DynamicHttpActionSelector(); GlobalConfiguration.Configuration.Services.Replace(typeof(IHttpControllerSelector), dynamicControllerSelector); GlobalConfiguration.Configuration.Services.Replace(typeof(IHttpActionSelector), dynamicActionSelector); GlobalConfiguration.Configuration.Services.Replace(typeof(IHttpControllerActivator), dynamicHttpControllerActivtor); 假设现在我希望调用 ICalcultor 接口中的 Add 方法，理论上它的 URL 应该是http://localhost/Service/Calculator/Add，因为截至到目前为止，所有的接口默认都是通过 Get 来访问的，下面是整个流程第一次跑通时的截图：\n迁移后的ICalculator接口\r接口迁移后的二三事 现在，我们完成了 ICalculator 接口的改造，它从一个 WCF 服务变成了一个 Web API，而在这个过程中，我们发现一点点问题。首先，Web API 中的 URL 是不区分大小写的，而我们这里的 ServiceName、ActionName 都是严格区分大小写的。其次，接口方法中的 out、ref、params 等关键字不适用于 Web API 语境，需要进一步对接口进行改造。再者，Web API 需要区分 GET、POST、PUT、DELETE 等动词，返回值需要统一调整为 JSON 格式。最后，完成改造的动态 API 需要通过 RestSharp 或者 HttpClient 等 HTTP 客户端来调用，以替换原有的 WCF 代理方法。这里简单对后面这两个问题做下说明，因为前两个问题，都是历史遗留问题啦，哈哈😄。\nHTTP 动词支持 为了让接口支持不同的 HTTP 动词，我们需要对整个设计进行进一步优化。为什么我会把这件事情看得如此重要呢？因为在我看来，RESTful 风格的 API 大概会有这样几种级别，第一种级别指仅仅使用了 HTTP 协议来设计 API，第二种级别是在 API 设计中引入资源的概念，第三种级别是合理地使用 HTTP 动词如 GET、POST、PUT 等，第四种级别是使用 HATEOSA 来返回用户接下来可能的意图。可惜在实际的应用种，能做到第二种级别的都相当不容易啦。比如某接口不支持 GET 操作，原因是它需要附加 token 在 Body 中，因此在改造接口的过程中，哪怕参数是最简单的值类型，它还是必须要用 POST 方式来请求。可其实这种问题，如果把 token 附加在 HTTP 首部中，或者干脆就使用标准的 Authorizatin 字段完全就能解决啊。为了让这个方案更完美一点，我们对 DynamicHttpActionDescriptor 进行改造，重写它的 SupportedHttpMethods 属性：\nvar isDynamicController = controllerDescriptor.Properties.ContainsKey(\u0026#34;IsDynamicController\u0026#34;); if (isDynamicController) { var serviceType = controllerDescriptor.Properties[\u0026#34;ServiceType\u0026#34;]; var httpVerbAttributes = ((Type)serviceType).GetMethod(methodInfo.Name).GetCustomAttributes\u0026lt;Attribute\u0026gt;() .Where(t =\u0026gt; typeof(IActionHttpMethodProvider).IsAssignableFrom(t.GetType())) .ToList(); if (httpVerbAttributes.Any()) { //根据路由来获取Http动词 if (httpVerbAttributes.Count \u0026gt; 1) throw new Exception($\u0026#34;Multiple http verb matched in method {methodInfo.Name} of {((Type)serviceType).Name}\u0026#34;); _httpVerbs = GetHttpVerbByRoute(httpVerbAttributes); } else { //根据方法名称获取Http动词 _httpVerbs = GetHttpVerbByMethod(methodInfo); } } } 其原理说起来并不复杂，检查方法上是否有 HTTPGet、HttpPost、HttpPut 等标签，如果存在，则添加相应的 HTTP 动词到**_httpVerbs集合中；如果不存在，则根据方法的名字来构建相应的 HTTP 动词。譬如以 Add、Create 等开头的方法对应 POST 请求，以 Get 开头的方法对应 GET 请求，以 Update 开头的方法对应 PUT 请求，以 Delete 开头的方法对应 DELETE 请求等。最终，我们只需要把_httpVerbs**作为 SupportedHttpMethods 属性的返回值即可。\n接口返回值优化 通常在编写控制器的时候，我们会使用 JSON 作为接口的返回值，这是因为 JSON 在信息冗余度上相比 XML 更低，而且 JSON 和 JavaScript 有着密不可分的联系，所以使用 JSON 作为返回值会流行起来一点都不奇怪。我们知道，WCF 是可以实现 Web Service 这种所谓的 SOAP 架构的，而 WebService 本质上是使用 XML 进行通信的 HTTP，在调用 WCF 接口的时候，接口的参数、返回值都会被序列化为 XML。平时我们手写 Controller 的时候，通常是在 Controller 层调用一层薄薄的 Service 层，然后对结果进行封装，使其成为对前端更友好的数据类型，可当我们调用动态的 Controller 时，其接口的返回值是在接口中定义好的，我们不可能去修改已经在使用中的 Service 定义。\n虽然微软的 Web API 中可以自动对返回值进行序列化，参考最经典的 ValuesController，它是微软对 RESTful 风格的一种标准实现，具体表现为 Get()、Post()、Put()、Delete()四个方法，分别对应 GTE、POST()、PUT()、DELETE(四个 HTTP 动词，这就是所谓的约定大于配置，并且这些方法的返回值都不是 ActionResult 或者 IHttpActionResult，但整个框架依然可以帮我们将其序列化为 JSON，这一切是为什么呢？其实，我们只需要重写 DynamicHttpActionDescriptor 的 ReturnType 属性，同时重写 DynamicHttpActionDescriptor 的 ExecuteAsync()方法就可以达到这一目的：\npublic override Type ReturnType { get { return typeof(DynamicApiResult); } } public override Task\u0026lt;object\u0026gt; ExecuteAsync(HttpControllerContext controllerContext, IDictionary\u0026lt;string, object\u0026gt; arguments, CancellationToken cancellationToken) { return base.ExecuteAsync(controllerContext, arguments, cancellationToken) .ContinueWith(task =\u0026gt; { try { if (task.Result == null) { return new DynamicApiResult() { Flag = true }; } if (task.Result is DynamicApiResult) { return task.Result; } return new DynamicApiResult() { Flag = true, Result = task.Result }; } catch (AggregateException ex) { throw ex; } }); } 从代码中大家大致可以猜出 DynamicApiResult 的结构了，它包含三个属性：Flag、Msg、Result。这是一个最常见的 Web API 的返回值封装，即通过 Flag 判断方法是否调用成功，通过 Msg 来返回异常信息，通过 Result 来返回具体的返回值。最近对接某公司的 API 接口的时候，发现一个非常奇葩的现象，一般没有返回值可以返回 null 或者空字符串，可这家公司居然返回的是**”无数据\u0026quot;**，你以为这是放在 Msg 里的吗？不，人家是放在 Result 里的。对此，我只能说，互联网发展到 2019 年了，那些年野蛮生长留下的坑却还一直都在。好了，现在我们来看看接口调用的结果，喏，这次是不是感觉顺眼多啦！\n优化后的ICalculator接口返回值\rPOCOController 其实，这篇文章写到这里就已经结束啦，因为对于一般的 ASP.NET 项目，这篇文章里所分享这些内容，基本上可以实现我们最初的目标，即把老系统中的 WCF 接口迁移到 Web API 上，从长远的角度来看，这是为了后续迁移到.NET Core 上做准备，其实不单单是 WCF，任何的接口、服务都可以顺着这种思路去做扩展，手写 Controller 虽然是最容易上手的实践方式，可随着业务的不断推进，无一例外地出现接口爆炸的现象，在没有注册中心的情况下，业务系统间互相调对方的 Web API 简直叫一个混乱，你能想象一家公司里的不同业务系统，居然没有通用的网关去做接口的授权吗？反正我最近是见识到了某友的混乱。这篇文章中的思路，其实是参考了 Abp 这个框架中的 DynamicApiController 这一功能，可我们大多数人都没怎么好好用过这项技术，.NET Core 就来了，Abp 官方着手开发的 Abp vNext 就是基于.NET Core 的下一代 Abp 框架，不知道届时会不会有这个功能。\n既然说到了,NET Core，那么就不可避免地要说到.NET Core 里的 POCOController。因为 ASP.NET 与 ASP.NET Core 的机制完全不同，所以，我们在这篇文章里的实现是无法直接用到 ASP,NET Core 里的，这听起来有点遗憾是不是，就在我写这篇博客的前几天，我看到有人把 Abp 的 DynamicApiController 移植到了.NET Core 下面，还是熟悉的味道，但内部的原理已然大为不同。具体来讲, .NET Core 下的 POCOController 特性会让这一切更简单。所谓 POCOController，就是指任意一个类都可以是 Controller。我们都知道在 ASP.NET 下，要写一个 Web API 必须继承 ApiController，就是说这个类必须实现了 IHttpController 接口，就是因为有这个限制，所以，我们不得不通过 Castle 来动态生成一个 Controller，既然现在 ASP.NET Core 里可以打破这一限制，那么实现起来自然会非常简单。限于这篇文章的篇幅(截至到这里已经将近 6000 余字)，我将在下一篇文章中和大家分享这一特性的相关内容。\n本文小结 在传统的 ASP.NET 项目向 ASP.NET Core 迁移的过程中，我们遇到的第一个阻力就是作为内部 RPC 使用的 WCF。因此，收到上一篇文章基于 Castle 动态代理这一思路的影响，参考 Abp 框架中的 DynamicApiController 功能，我们分享了一种可以为任意接口动态生成 Controller 的思路，其核心原理是通过 Castle 中的AdditionalInterfaces功能，将指定接口和 ApiController 进行组合，使得一个普通的接口可以像 Controller 一样被调用。在这个过程中，我们回顾了 ASP.NET MVC 的基本原理，了解了 MVC 是如何根据路由筛选 Controller、激活 Controller 和筛选 Action，在此基础上，我们对微软的 MVC 进行了一次 Hack，使用我们自定义的组件替换了微软的默认实现，从而可以让原来托管在 ServiceHost 上的接口，通过 Web API 来访问和调用。当然，这篇文章里没有实现自定义的路由、过滤器的支持，所谓抛砖引玉，Abp 的代码本身在 Github 上就可以找到，大家如何对此感兴趣的话，可以做进一步的研究和扩展。我们实现了服务端的切换，这意味着在客户端同样需要一次切换，预知后事如何，请大家关注我的下一篇博客，以上就是我这篇博客的全部内容了，谢谢大家！\n参考文章 Castle 中 AdditionalInterfaces 用法介绍 ABP 源码分析三十五：ABP 中动态 WebAPI 原理解析 https://github.com/FJQBT/ABP ","date":"2019-06-08T13:48:41Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/4236649/","slug":"4236649","tags":["RESTful","WebApi","动态代理"],"title":"通过动态 Controller 实现从 WCF 到 Web API 的迁移"},{"categories":["读书笔记"],"content":"设计优美的 Web API 易于使用、便于更改、健壮性好、不怕公开\nREST 的两层含义 指符合 Fielding 的 REST 架构风格的 Web 服务系统 指使用符合 RPC 风格的 XML 或 JSON + HTTP 接口的系统(不使用 SOAP) 端点的基本设计 短小便于输入的 URI- 人可以读懂的 URI 没有大小写混用的 URI 修改方便的 URI 不暴露服务端架构的 URI 规则统一的 URI HTTP 方法和端点 GET 获取资源 POST 新增资源 PUT 更新已有资源 DELETE 删除资源 PATCH 更新部分资源 查询参数和路径的使用区别 表示唯一资源时，放在路径中 当参数可以忽略时，放在查询参数中 RESTful 的设计级别 使用 HTTP 引入资源的概念 引入 HTTP 动词 引入 HATEOAS 如何指定数据格式 查询参数：url?format=xml 扩展名：/url.json Accept 头部字段 让用户决定响应的内容 GraphQL 通过状态码表示错误信息 1xx：消息 2xx：成功 3xx：重定向 4xx：客户端原因造成的错误 5xx：服务端原因造成的错误\n缓存与 HTTP 协议规范 RFC7234：过期模型/验证模型 过期模型：Cache-Control/Expires 验证模型：Last-Modified/ETag Vary 首部：指定缓存单位 Conent-Type/Accept：指定媒体类型\nAPI 版本控制 在 URI 中嵌入版本号 在查询字符串中加入版本信息 通过媒体类型指定版本 API 安全问题 推荐使用 HTTPS XSS/XSRF 注入漏洞 返回正确的数据格式 使用安全相关首部 采用 KVS 实现访问限制 提供 API 文档 API Blueprint API Console/Apigee 提供 SDK ","date":"2019-05-28T12:00:53Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3677280829/","slug":"3677280829","tags":["Web API","RSETful","笔记","提纲"],"title":"《Web API 的设计与开发》读书笔记"},{"categories":["编程语言"],"content":"最近一直在研究 Mongodb 和 ElasticSearch 之间同步数据的问题，苦于到目前为止，并没有取得任何实质性的进展。偶尔“趁得浮生半日闲暇”，看一看 Web API 设计方面的书籍，和前辈交流下项目中的历史遗留问题，最为直观的感受就是，这个世界上任何方案的最终落地，都经过理想和现实的无数次挣扎，比如我们希望迁移项目到.NET Core 平台上，初步分析大概有将近 1000 多个无法兼容的地方，维持现状固然可以保证整个项目的稳定，可如果真到了不得不升级的地步，面临的问题可能会越来越多，所谓“凡事预则立，不预则废”，早一点准备总是好的。既然说到里历史问题，那么，今天这篇文章就来说一说，基于 RealProxy 实现 WCF 动态代理。\n故事背景 在我们的业务系统中，对内是使用 WCF 来进行相互通信的，而对外则是使用 Web API 来进行数据交换。关于 RPC 还是 REST 的争论由来已有，严格地来说，两者没有绝对的高下之分，从风格上而言，RPC 倾向于让接口映射到一个方法上，而 REST 则倾向于让接口映射到一个资源上。从我们实际的使用情况来看，REST 在系统中应用得并不是很完美，因为大多数情况下，我们实现的仅仅是 HTTP+JSON 这样一种协议组合，因此业务系统中存在着大量的 WCF 接口供系统内部调用。\n内部服务调用示意图\r最早的时候，是通过 T4 模板来生成针对某个接口的代理类，而代理类中通常封装了 ChannelFactory 的创建、释放等等 WCF 相关的代码，实际应用中还会对 WCF 接口的异常进行捕获、记录日志、统计调用时间等，因此早期的 T4 模板实际上承担了生成代理类的职责。虽然业务的不断推进，接口中加入的新方法越来越多，导致具体业务类中的代码越来越多，动辄出现单个文件中代码行数达 3000 行以上，与此同时，每当 WCF 接口中增加了新方法，就不得不在其相关的代理类中增加代理方法。坦白地讲，就是增加一个看起来差不多的方法，因为你依然要处理 ChannelFactory 的创建、释放、异常处理、日志记录等等的工作。\n其实，WCF 可以直接生成客户端代码，因为每个 WCF 的服务都可以以 WebService 服务的形式暴露出来，而只要是 WebService，总可以通过 WSDL 生成一个代理类。不过这显然不利于团队间的协作，更不利于服务终结点配置的集中化，更失去了异常处理、日志记录等等这些“通用”工作的可能性。T4 应该可以基于“工作”，可显然大家觉得手写比生成要来得更容易些，所以，这个故事最终演变成这样一个局面，我们不得不通过局部类(Partial Class)的方式来开辟新的类文件。\n系统中充斥着大量类似的代码\r那么，说了这么多，从一个历史遗留问题入手，它真正的痛点在哪里呢？在我看来，主要有两点：第一，是手写代理类的“此恨绵绵无绝期”，明明就是对接口的简单封装，看起来是增加一个代理方法，其实最多就是复制黏贴，因为代理方法的核心代码就是调用接口，而剩下的都是重复的“服务型”代码；第二，是异常处理、日志记录的“哀鸿遍野”，同核心代码交织在一起，一遍又一遍的“重复”，为什么不考虑让它统一地去处理呢？难道每个人都抄着同一段代码，这样就实现了某种意义上的复用吗？\nRealProxy 介绍 既然像我这样懒惰的人，不愿意像别人一样手写代理类，那么我的思路又是什么呢？显然，从这篇文章的题目，你就可以看出，我这里要说的是动态代理，原来的代理类同样属于代理，它是在编译时期间生成了一个代理类，我们以为在调用这个代理类，可其实真正去工作的是 ChannelFactory，这种方式称之为“静态代理”。如果你了解过设计模式，应该会知道相对应的代理模式，这里不再展开开来讲这这个设计模式，可以明确的是，动态代理就是在运行时期间动态创建一个代理对象的实例，它可以完全模拟被代理对象的行为，而我们的目的，就是要和手写的代理类永远地说再见！\n好了，下面隆重介绍本文的主角——RealProxy。相信大家一定听说过 AOP，即所谓的面向切面编程。它可以让我们在某一个所针对的横切面编程，并讲这种功能应用到所有相同的横切面上。譬如对方法级别的横切面增加拦截器，那么所有的方法都可以在执行前后具备相同的逻辑，典型的如日志记录、指定操作前的检验等等。而 RealProxy 类恰恰提供最基本的代理功能，它是一个抽象类，必须通过重写其 Invoke()方法并添加新功能来继承，该类位于 System.Runtime.Remoting.Proxies 命名空间中，通过重写 Invoke()方法，我们就可以在被代理对象调用前后插入相关逻辑，而通过 GetTransparentProxy()方法，则可以返回实际的代理对象。所以，通过这个原理，我们就可以在运行时期间，动态创建出指定类型的实例。这里，我们从一个简单的例子来开始，以帮助大家更好的理解 RealProxy。\npublic interface ICalculator { double Add(double n1, double n2); double Subtract(double n1, double n2); double Multiply(double n1, double n2); double Divide(double n1, double n2); } public class CalculatorService : ICalculator { public double Add(double n1, double n2) { return n1 + n2; } public double Subtract(double n1, double n2) { return n1 - n2; } public double Multiply(double n1, double n2) { return n1 * n2; } public double Divide(double n1, double n2) { return n1 / n2; } } 首先，我们定义一个简单的接口 ICalculator，它含有加、减、乘、除四种基本运算，我们希望记录每个方法调用的参数、结果和执行时间，因此通过 RealProxy 对现有类型 CalculatorService 进行代理，并动态地创建代理对象来供调用方使用，下面给出关键代码：\npublic class CalculatorServiceProxy : RealProxy { private Server.Service.ICalculator _calculator; public CalculatorServiceProxy(Server.Service.ICalculator calculator) : base(typeof(Server.Service.ICalculator)) { _calculator = calculator; } public override IMessage Invoke(IMessage message) { var methodCall = message as IMethodCallMessage; var methodInfo = methodCall.MethodBase as MethodInfo; var startTime = DateTime.Now; var serviceName = _calculator.GetType().Name; var methodName = methodInfo.Name; try { Console.WriteLine(\u0026#34;调用{0}服务的{1}方法开始...\u0026#34;, serviceName, methodName); var argsInfo = new Dictionary\u0026lt;string, object\u0026gt;(); for (int i = 0; i \u0026lt; methodCall.ArgCount; i++) { argsInfo.Add(methodCall.GetArgName(i), methodCall.Args[i]); } Console.WriteLine(\u0026#34;当前传入参数:{0}\u0026#34;, JsonConvert.SerializeObject(argsInfo)); var result = methodInfo.Invoke(_calculator, methodCall.InArgs); if (result != null) Console.WriteLine(\u0026#34;当前返回值:{0}\u0026#34;, JsonConvert.SerializeObject(result)); return new ReturnMessage(result, null, 0, methodCall.LogicalCallContext, methodCall); } catch (Exception ex) { Console.WriteLine( \u0026#34;调用{0}服务的{1}方法失败,失败原因：{2}\u0026#34;, serviceName, methodName, ex.Message ); throw ex; } finally { Console.WriteLine( \u0026#34;调用{0}服务的{1}方法结束,共耗时{2}秒\u0026#34;, serviceName, methodName, DateTime.Now.Subtract(startTime).TotalSeconds ); Console.WriteLine(\u0026#34;----------------------------------\u0026#34;); } } } 可以注意到，最核心的代码是在 Invoke()方法中，在这里我们增加了我们想要的功能，但这些功能丝毫不会影响到 CalculatorService，当我们通过构造函数给 RealProxy 传入被代理对象后，它就会对被代理对象的特定方法进行拦截，这里实际上就是加、减、乘、除四个方法。OK，到现在为止，这些都是我们的想像而已，具体我们实现执行结果来看。\nvar serviceProxy = new CalculatorServiceProxy(new CalculatorService()); var calculator = (ICalculator)serviceProxy.GetTransparentProxy(); calculator.Add(12, 24); calculator.Subtract(36, 10); calculator.Multiply(12, 35); calculator.Divide(36, 12); 现在，我们可以说，刚刚所说的一切都是真的，因为我们真的创建了一个 ICalculator 接口的实例，它真的记录了每个方法调用的参数、结果和执行时间。\nRealPrxoy牛刀小试\rWCF 动态代理 现在，我们来考虑 WCF，WCF 需要通过 ChannelFactory 来创建和释放，而这恰恰是代理类所做的事情，就像下面的代码一样，我们通常会把所有的 WCF 集中配置在一个地方，并通过构造 Binding 和终结点地址来创建一个 WCF 服务，在调用服务的过程中，会对调用时间、异常信息等进行记录，这其实和我举的第一个例子完全一致，那么我们能不能用 RealProxy 来实现这些功能呢？\npublic class ServiceInfo\u0026lt;TService\u0026gt; { private readonly ChannelFactory _channelFactory; public ServiceInfo(ChannelFactory channelFactory) { _channelFactory = channelFactory; } public TService Service { get; set; } public void Close() { if (_channelFactory != null) _channelFactory.Close(); } } private ServiceInfo\u0026lt;TService\u0026gt; FindService() { ChannelFactory\u0026lt;TService\u0026gt; channelFactory = new ChannelFactory\u0026lt;TService\u0026gt;(_binding, _endpointAddress); var serviceInfo = new ServiceInfo\u0026lt;TService\u0026gt;(channelFactory); serviceInfo.Service = channelFactory.CreateChannel(); return serviceInfo; } 顺着这样的思路，如果我们可以把 ChannelFactory 注入到 RealProxy 中，就可以在接口调用过程中记录相关信息，这样我们就可以关注调用本身，因为所有的我们不想写的代码，现在全部都由代理类接管了，更重要的是，所有通过这种方式调用的 WCF 服务，都可以以一种统一而简洁的方式去处理，永远不用担心因为某个人忘记写代理方法而出现问题，下面给出整个实现的关键代码：\npublic class DynamicServiceProxy\u0026lt;TService\u0026gt; : RealProxy { private readonly Binding _binding; private readonly EndpointAddress _endpointAddress; public DynamicServiceProxy(Binding binding, EndpointAddress endpointAddress) : base(typeof(TService)) { _binding = binding; _endpointAddress = endpointAddress; } public DynamicServiceProxy(Binding binding, string serviceUrl) : this(binding, new EndpointAddress(serviceUrl)) { } public override IMessage Invoke(IMessage message) { var serviceInfo = FindService(); var methodCall = message as IMethodCallMessage; var methodInfo = methodCall.MethodBase as MethodInfo; var startTime = DateTime.Now; var serviceName = serviceInfo.Service.GetType().Name; var methodName = methodInfo.Name; try { Console.WriteLine(\u0026#34;RealProxy调用{0}服务{1}方法开始...\u0026#34;, serviceName, methodName); var argsInfo = new Dictionary\u0026lt;string, object\u0026gt;(); for (int i = 0; i \u0026lt; methodCall.ArgCount; i++) { argsInfo.Add(methodCall.GetArgName(i), methodCall.Args[i]); } Console.WriteLine(\u0026#34;RealProxy当前传入参数:{0}\u0026#34;, JsonConvert.SerializeObject(argsInfo)); var result = methodInfo.Invoke(serviceInfo.Service, methodCall.InArgs); if (result != null) Console.WriteLine(\u0026#34;RealProxy当前返回值:{0}\u0026#34;, JsonConvert.SerializeObject(result)); return new ReturnMessage(result, null, 0, methodCall.LogicalCallContext, methodCall); } catch (Exception ex) { Console.WriteLine( \u0026#34;RealProxy调用{0}服务{1}方法失败,失败原因：{2}\u0026#34;, serviceName, methodName, ex.Message ); throw ex; } finally { serviceInfo.Close(); Console.WriteLine( \u0026#34;调用{0}服务{1}方法结束,共耗时{2}秒\u0026#34;, serviceName, methodName, DateTime.Now.Subtract(startTime).TotalSeconds ); Console.WriteLine(\u0026#34;----------------------------------\u0026#34;); } } } 对于 WCF 服务端的实现，我们依然使用 ICalculator 这个接口，需要注意的是为其添加[ServiceContract]和[OperationContract]标签，在这个例子中，我们共有 CalculatorService 和 MessageService 两个服务，为了简化这个实例，我们采用 BasicHttpBinding 的方式进行绑定，并为其指定各自的终结点地址。可以注意到，现在我们的动态代理实现了和原来代理类一样的效果。\nvar binding = new BasicHttpBinding(); var serviceUrl = \u0026#34;http://localhost:8502/Calculator.svc\u0026#34;; var calculator = ServiceProxyFactory.CreatePorxy\u0026lt;Server.Service.ICalculator\u0026gt;(binding, serviceUrl); 通过RealPrxoy动态代理WCF服务\r在调用 WCF 的时候，因为超时、网络等原因造成的调用异常，此时，我们可以为 WCF 添加异常处理相关的标签，而相应地，我们可以在异常中对异常的种类进行判断和处理，以便于及时地关闭 ChannelFactory，因为如果它不能正确地关闭，会导致后续的通信出现问题，而这恰好是当初的代理类想要解决的问题，考虑到创建 ChannelFactory 是需要付出一定的性能代价的，因此，可以适当地考虑对 ChannelFactory 进行缓存，而这恰好是原来业务中的一个盲点。\nCastle.DynamicProxy 通过 RealProxy，我们已经实现了 WCF 服务的动态代理，这里介绍第二种方式，即 Castle.DynamicProxy，Castle 和 AspectCore、Unity 等项目一样，提供了 AOP 相关的能力，可以让我们对接口、虚方法、类等进行拦截。Castle 中的动态代理使用的是透明代理，而.NET Remoting 的动态代理必须继承自 MarshalByRefObject。博主暂时没有搞清楚，这两种是否属于同一种技术上的实现，作为延伸，我们来一起看看如何使用 Castle 中的 DynamicProxy 实现类似的功能，首先我们定义一个拦截器，它需要实现 IInterceptor 接口中的 Intercept()方法：\npublic void Intercept(IInvocation invocation) { var serviceInfo = FindService(); var methodInfo = invocation.Method; var startTime = DateTime.Now; var serviceName = serviceInfo.Service.GetType().Name; var methodName = methodInfo.Name; try { Console.WriteLine(\u0026#34;CastleProxy调用{0}服务{1}方法开始...\u0026#34;, serviceName, methodName); var argsInfo = new Dictionary\u0026lt;string, object\u0026gt;(); var parameters = methodInfo.GetParameters(); for (int i = 0; i \u0026lt; invocation.Arguments.Length; i++) { argsInfo.Add(parameters[i].Name, invocation.Arguments[i]); } Console.WriteLine(\u0026#34;当前传入参数:{0}\u0026#34;, JsonConvert.SerializeObject(argsInfo)); var result = methodInfo.Invoke(serviceInfo.Service, invocation.Arguments); if (result != null) { Console.WriteLine(\u0026#34;当前返回值:{0}\u0026#34;, JsonConvert.SerializeObject(result)); invocation.ReturnValue = result; } } catch (Exception ex) { Console.WriteLine( \u0026#34;CastleProxy调用{0}服务{1}方法失败,失败原因：{2}\u0026#34;, serviceName, methodName, ex.Message ); throw ex; } finally { serviceInfo.Close(); Console.WriteLine( \u0026#34;CastleProxy调用{0}服务{1}方法结束,共耗时{2}秒\u0026#34;, serviceName, methodName, DateTime.Now.Subtract(startTime).TotalSeconds ); Console.WriteLine(\u0026#34;----------------------------------\u0026#34;); } } 接下来，我们通过 ProxyGenerator 来生成新的代理类，我们需要告诉 ProxyGenerator 要创建的类型是什么，是一个接口还是类，以及要应用哪一个拦截器。这里我们用到的方法是 CreateInterfaceWithoutTarget()，它在这里的作用就是动态创建 ICalculator 接口的代理类。而通过查看 Castle 的 API，我们会发现它可以在以下几种情况下创建某个类型的实例。首先是 CreateInterfaceWithoutTarget()这个方法，当你希望创建一个接口的代理而又不想提供具体的实现时可以使用。其次是 CreateInterfaceProxyWithTarget()这个方法，当你希望创建一个接口的代理同时又有提供具体实现时使用可以使用。接下来，是 CreateInterfaceProxyWithTargetInterface()这个方法，它的命名看起来让人感到迷惑，甚至在某种角度来看，它和 CreateInterfaceProxyWithTarget()这个方法还有点相似，其实。这两者最大的不同就是：后者允许你将调用目标替换为目标接口的不同实现。这种在实际场景中使用得不多，从 Castle 官方的使用场景来看，唯一用到这种技术的是 Castle.Facilities，它可以和 Windsor 这样的容器整合在一起使用，这个时候调用者就可以把 WCF 服务当作一个普通接口来使用，果然，大家都想到这一点，英雄所见略同啊，哈哈。好了，下面我们来看具体的代码实现：\nProxyGenerator generator = new ProxyGenerator(); var interceptor = new CastleServicePorxy\u0026lt;ICalculator\u0026gt;(binding, serviceUrl); var calculator = (ICalculator)generator.CreateInterfaceProxyWithoutTarget(typeof(ICalculator),interceptor); 迁移至.NET Core 其实，我对 WCF 是不太感冒的，因为第一个字母 W 表明，它是一个只能运行在 Windows 平台的产物，现在依然有大量的 Web Service 存在，如果可以让我像使用普通接口一样使用 WCF 接口，我还是非常愿意去使用它的，毕竟系统中有大量依赖 WCF 的东西。可话又说回来，现在到.NET Core 这个版本，微软并没有把 WCF 的服务端移植到.NET Core 上，仅仅是提供了客户端调用的支持，或许还是因为 WCF 里有太多平台相关的东西吧！如果希望自己的.NET 应用可以跨平台，越早摆脱这些 Windows 平台东西越好，譬如 IIS、SQLServer 等等。不过我这里想说的是，RealProxy 在.NET Core 中有类似的实现，我们可以用下面这种方式来进行迁移，当然，如果你直接 Castle 就更没有问题啦！\npublic class InvokeSerice { public static T Proxy\u0026lt;T\u0026gt;() { return DispatchProxy.Create\u0026lt;T, InvokeProxy\u0026lt;T\u0026gt;\u0026gt;(); } } public class InvokeProxy\u0026lt;T\u0026gt; : DispatchProxy { private Type type = null; public InvokeProxy() { type = typeof(T); } protected override object Invoke(MethodInfo targetMethod, object[] args) { //TODO: 在这里实现拦截逻辑 } } 本文小结 这篇博客再次让大家领略了 AOP 的魅力，通过动态代理来创建相关的服务接口，让我们逐渐摆脱了手写代理类的深渊。本文主要分享了两种动态代理的实现方式，一种是基于.NET Remoting 的 RealProxy，一种是基于 Castle 的 DynamicProxy。两种方式在使用上是非常相近的，通过这种方式。我们实现了 WCF 服务创建细节的隐藏，调用者不再需要去关心 ChannelFactory 相关的底层细节，可以像使用普通接口一样调用 WCF 服务，并且可以用一种统一的方式去记录调用相关的细节、对异常进行处理等等。早期的 T4 模板本质上是一种静态代理的方式，其缺点是难以适应快速迭代的变化，必须人手编写代理方法，而通过动态代理，这一切只需要写一次就好了，从而做到了真正意义上的“一次编写，到处运行”，这就是所谓的面向横切面编程的思路。关于 Castle 动态代理更多的应用场景，以及 Castle.Facilities 相关的内容，大家可以从各自的文档中去了解，以上就是这篇博客的全部内容了。\n","date":"2019-05-10T16:27:50Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2954591764/","slug":"2954591764","tags":["AOP","Castle","Dynamic Proxy"],"title":"又见 AOP 之基于 RealProxy 实现 WCF 动态代理"},{"categories":["编程语言"],"content":"众所周知，Win10 中开始提供 Linux 子系统，即 Windows Subsystem for Linux，简称 WSL，它可以让我们在 Windows 系统使用 Linux 系统，自从有了这个新功能以后，博主果断地放弃双系统的方案，因为折腾起来实在花费时间。关于如何使用 WSL，网上有非常多的文章可以参考，这里不再赘述。今天想说的是，WSL 下使用 Docker 遇到的各种坑。\n装完 WSL 以后，对各种编译环境的使用相当满意，最近在研究日志可视化平台 ELK，其中需要使用 Docker 来搭建环境，一顿 sudo 操作猛如虎，快速安装完 Docker 环境，结果发现熟悉的命令行居然无法正常工作，是可忍孰不可忍。\nsudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \\ \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026#34; sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io 第一个错误是，你按照官方文档安装完 Docker，输入 docker -v，一切显示正常的时候，此时，如果会执行 docker run hello-world 命令，会出现以下错误：\n$ docker run hello-world docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. See \u0026#39;docker run --help\u0026#39;. 此时，你可能会尝试通过执行 systemctl start docker 命令来启动 Docker 服务，因为错误信息告诉我们，Docker 的守护进程没有启动，可你会发现这样依然报错。可是为什么呢？明明 Docker 都在 WSL 里安装成功了啊，事实上除了 docker -v 不需要依赖守护进程，其余的命令都需要依赖守护进程，而 WSL 恰恰是不支持 docker-engine 的，所以，一种曲线救国的思路就是，让 WSL 去连接宿主机上的 docker engine。果然，还是要安装 Docker for Windows 啊！那么，剩下的事情变得就非常简单啦，确保系统开启 Hyper-V，然后安装 Docker for Windows，并打开对宿主机 Docker 的监听，这些相信玩过 Docker 的人都会啦！\n暴露宿主机器Docker端口\r接下来，我们给 WSL 中的 Docker 设置宿主机的地址，在终端中输入下列命令即可：\nexport DOCKER_HOST=tcp://localhost:2375 此时，我们执行 docker run hello-world 命令，如果不出意外的话，我们会看到下面的画面，这说明我们的 Docker 环境已经正常工作啦：\nWSL中完美运行的Docker\r博主按捺不住内心的激动，果断安装 ELK 全家桶，体验了下 Kibana 的可视化界面，开始思考：如何把存储在 Mongodb 中的日志数据放到 ElasticSearch 中。当然，这都是后话啦，因为博主马上发现了 WSL 中 Docker 的第二个坑，那就是终端关闭以后，针对宿主机的 Docker 连接就结束了。\nELK全家桶\rOK，为了解决这个问题，我们继续在终端中输入以下命令：\necho \u0026#34;export DOCKER_HOST=tcp://localhost:2375\u0026#34; \u0026gt;\u0026gt; ~/.bashrc \u0026amp;\u0026amp; source ~/.bashrc 在使用 Docker 的过程中，最令人困惑的部分当属分区的挂载，因为你时刻要搞清楚，它到底表示的是容器内部的分区，还是宿主机上的分区。对于运行在 WSL 中的 Docker 而言，它会采用类似/mnt/c/Users/Payne/这样的更符合 Linux 习惯的路径，而 Docker for Windows 则会使用类似/c/Users/Payne/这样更符合 Windows 习惯的路径。因此，如果你在使用 Docker 的过程中，需要处理分区挂载相关的东西，一个比较好的建议是修改 WSL 的配置文件(如果不存在需要自行创建)：\nsudo nano /etc/wsl.conf [automount] root = / options = \u0026#34;metadata\u0026#34; 好了，以上就是在使用 WSL 中的 Docker 搭建 ELK 全家桶过程中遇到的问题的梳理，从体验上来讲，我个人会把 Linux 平台相关的工作渐渐转移到 WSL 上，因为安装双系统总会分散你的精力去处理维护相关的事情，虽然装系统对程序员来说都不算是个事儿，可我内心依旧排斥自己被贴上“修电脑”的标签。我会在后续的博客中分享.NET Core 下日志分析平台构建相关内容，希望大家可以继续关注我的博客，这篇文章到此结束，谢谢大家！\n","date":"2019-04-22T22:13:36Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/4159187524/","slug":"4159187524","tags":["WSL","Docker","Linux"],"title":"WSL 下 Docker 使用踩坑小记"},{"categories":["数据存储"],"content":"因为工作中需要同时面向 MySQL、Oracle 和 SQLServer 三种数据库进行开发，所以，大概从去年国庆节开始，我开始使用一个开源的数据库管理工具——DBeaver。\n使用这个工具的初衷，是因为我不想在同一台电脑上安装三个客户端工具，尤其是 Oracle 和 SQLServer 这种令人恐惧的、动辄需要重装系统的应用程序。我不想再使用类似 Navicat 这样的软件，因为它的画风像是上个世纪 VB6.0 的产品一样，同理，我不喜欢用 PL/SQL，因为我每次都要瞪大眼睛，在它狭窄而拥挤的画面上找表、找视图，更有甚者，有时要去找触发器、找存储过程。直到我同事给我发了一个几十 M 的文档，我突然间意识到，这货居然还要安装 Oracle 的客户端，配置数据库连接要手动去改配置文件，我一点都不喜欢 PL/SQL。\n除了这三种经典的关系型数据库，我们还会用 Memcache 和 Redis 这样的内存数据库，Mongodb 这样的非关系型数据库，所以，我希望有一个统一的入口来管理这些连接，毕竟我身边的同事会使用三种以上的工具，譬如 Sqlyog、PL/SQL、SQLServer 等来处理这些工作，恰好 DBeaver 可以满足我 80% 的工作需要。目前，DBeaver 企业版支持关系型数据库和非关系型数据库，而社区版仅支持关系型数据库。\n可最近在写 Oracle 环境的触发器(存储过程和触发器都是万恶之源)时，我发现 DBeaver 和 PL/SQL 在面对同一段 SQL 脚本时，居然因为一点点语法上的差异而不兼容，这让我内心深处不由得想对 Oracle 吐槽一番。这是一个什么样的 SQL 脚本呢？我们一起来看下面的例子：\nCREATE OR REPLACE TRIGGER \u0026#34;TRI_SYNC_ITEM_VALUE\u0026#34; BEFORE DELETE ON \u0026#34;or_line\u0026#34; FOR EACH ROW DECLARE v_item_value NUMBER(18,6); BEGIN SELECT ITEM_VALUE INTO v_item_value FROM \u0026#34;order_info\u0026#34; WHERE ORDER_GID = :OLD.ORDER_GID; v_item_value := v_item_value - :OLD.PACKAGE_COUNT * NVL(to_number(:OLD.OL_UDF7),0); IF v_item_value \u0026lt; 0 THEN v_item_value:= 0; END IF; UPDATE \u0026#34;order_info\u0026#34; SET ITEM_VALUE = v_item_value WHERE ORDER_GID = :OLD.ORDER_GID; END \u0026#34;TRI_SYNC_ITEM_VALUE\u0026#34;; INSERT INTO \u0026#34;sys_upgrade_history\u0026#34;(UPGRADE_TYPE,VERSION_NO,UPDATE_DATE,REMARK) VALUES(\u0026#39;版本更新\u0026#39;,\u0026#39;10005\u0026#39;,SYSDATE,\u0026#39;Normal\u0026#39;); 这是实际业务中编写的一个简单触发器脚本，我们通常的编写习惯是，在写完触发器或者存储过程以及函数后，会在升级历史中插入一天新纪录，所以，这个脚本实际上由两部分组成。如果这段脚本分两次执行，那么在 DBeaver 和 PL/SQL 中效果是一样的。可如果我们希望一次执行整个脚本，根据 PL/SQL 的规范，一个 PL/SQL 脚本由如下结构组成：\nDECLARE [声明部分] BEGIN [过程部分] END; / 这个时候，我们就要在这两部分脚本间增加一个分隔符——/。可尴尬的是，这种写法在 DBeaver 中是无法编译执行的，因为它认为**/**是个无效的 SQL 关键字。我一直疑心这是个 Bug，因为 Github 上曾有人提过类似的 Issue，作者回复说，DBeaver 并没有完全实现 PL/SQL 语法的解析，而最近更新的 6.0 版本中提到：对 Oracle 环境的存储过程编译进行了强化。博主尝试升级到最新版本，发现这个问题依然存在，哪怕用 Ctrl+Enter 来执行一样会报错，于是我想从这件事吐槽下某数据库，从哪里说起呢，就从 PL/SQL 说起吧！\n标准与私货 我想一开始学习 SQL 语法的时候，大家绝对不会想到，看起来和谐而统一的结构化查询语言，其实是貌合神离。为什么这样说呢？因为我真的不知道，一个时间函数居然可以有 SYSDATE、NOW()和 GETDATE()三种写法，我更不知道，有一天会因为不知道 ROWNUM 而被面试官鄙视，更不必说每种数据库都会定义一两种不一样的数据类型，这东西号称是有一个标准吗？比如 SQL92/99 这个标准定义了 DML(数据操作语言)、DDL(数据定义语言)、DCL(数据控制语言)和 TCL(事务控制语言)四种分类，所以，SQL 的定位其实更接近于交互式命令行，它是命令式的查询语言，而非过程式的声明语言。\n可在标准化进程缓慢的大背景下，每一家数据库厂商都在往自家产品里夹藏私货，以甲骨文为首的 Oracle 发展出了 PL/SQL、以微软为首的 SQLServer 发展出了 T-SQL。其实，我很能理解这种标准跟不上时代发展需要的阵痛，就像我们的 Web 领域直到 10 年前后才提出了 HTML5 标准，在此之前，我们为不同的浏览器的兼容性煞费苦心，兼容 IE8 与否甚至成为了评价技术好坏的一个隐性标准，可说句实话，浏览器的 Bug 难道不应该让浏览器厂商来修复吗？关前端工程师什么事？同样的，数据库间的差异，让我们的脚本失去了可移植性，触发器、存储过程这种严重依赖数据库的东西，一旦更换了数据库，基本等于要重头再写一遍，如今的小程序让 Web 变成信息孤岛，甚至 Chrome 正在变成下一个 IE，这就是所谓“屠龙少年战胜恶龙，自身亦化为恶龙吗”？\n这种不统一带来的弊端就是，我们永远写出可以完美“跨”数据库的 SQL，现在跨平台基本成为了大家的共识，因为操作系统间的差异越来越小，以我个人为例，我使用的大多数软件都可以找到对应的 Linux 版本，这样做的好处是，我可以在无差别地从 Windows 切换到 Linux。可现在，我们必须在 MySQL 里使用 VARCHAR、而在 Oracle 里使用 NVARCHAR，而在 SQLServer 里又要使用 NVARCHAR2，可明明它们都是表示一样的东西啊，类似的还有 MediumText 和 CLOB，是不是起一个不一样的名字会显得与众不同呢？更不必说在 DDL 中表约束相关的语法存在差别了。我被告知 Oracle 脚本中表名要用双引号括起来，理由是 Oracle 区分大小写，加上双引号就可以让它忽略大小写，忽略大小写不应该给 Oracle 一个设置吗？为什么要让我再写个多余的双引号呢？诸如此类，举不胜举。\nSQL 是个好 DSL 吗 SQL 标准定义的 SQL，就是一个以集合论为基础的结构化查询语言，它天生适合的场景就是，你在命令行中输入 SQL 语句，然后它去执行你输入的 SQL 语句，它就像我们大多数情况下使用的交互式命令行，不然，为什么 MySQL 要提供命令行版本，主流的数据库管理工具都提供了输入 SQL 语句的窗口。可我们同样能意识到，SQL 的表达能力有限，它无法表达顺序、条件、循环这种基本的程序结构，所以，数据库厂商几乎都对 SQL 标准进行了扩展，像 PL/SQL 和 T-SQL 中都提供了这些语法，进而催生出函数、触发器、存储过程一系列“万恶之源”，可从编程语言的角度来看，SQL 算是个好 DSL 吗？\nSQL 试图从编程语言中获得“灵感”的思路是正确的，但总给人一种买椟还珠的感觉，譬如使用大量的英文关键字来作为保留关键字，可你很难想象，像 GROUP BY 和 ORDER BY 这样的关键字，居然可以保留中间一个甚至多个空格，既然是关键字，为什么不选择一个单词，而选择一个组合词呢？这个世界上用 Begin 和 End 的编程语言，我使用过的有 Pascal 和 Basic，但现在我几乎不会再用它们，为什么呢？因为使用花括号({})更符合这个世界的发展趋势，你看 Python 居然用缩进代替花括号，是打算时刻用游标卡尺写代码吗？\n全世界都默认用分号作为一个语句的结束，那么，当多个语句放在一起的时候，直接相互间用分号隔开，编译器或者解释器都能识别，就算不喜欢写分号的 JavaScript，最新的标准提案里不还是建议要写吗？可为什么到了 PL/SQL 这里，明明已经用分号作为结束符了，偏偏还要再用一个/作为分隔符。我们都知道/会被当做是注释的开始，那么如果我在 PL/SQL 里恰好在 End;后写上一句/，你告诉我，这到底代表什么意思？明明像\u0026amp;\u0026amp;、||、^等这样的运算符，都是有固定含义，并且大家所有编程需要都默认了这个原则，可偏偏有人用||来连接字符串，你告诉我，用+不好吗？就像从小到大，÷都会被认为表示一个除法运算，结果突然有一天，有人用这个符号来表示加法运算，你说你是不是有种被当做傻子的感觉。全世界都用=表示赋值运算，结果 PL/SQL 自作聪明地搞了个:=，我想说，你真的考虑过使用者的体验吗？\n你甚至连分页、排序、分组这种事情，都无法在不同的数据库上获得一致的书写体验，读取指定数目的数据库记录，居然要纠结用到底用 Limit 还是 Top，像 Select Into 这样把指定列存储到指定变量中的操作，居然要求使用者来限制结果集的数目，从函数的角度来看，返回的必然是结果集中的一个元素，只有这样才可以赋值给指定的变量，可问题是存在多条记录的时候，你必须用游标去循环读取，而不能像大多数编程语言一样，直接 Map()到一个类型上然后 ToList()，可能是我对 SQL 的要求太高了吧，毕竟它就是个面向过程的语言，OO 不 OO 的没那么重要，可明明你可以抛出异常啊，可以对字符串做截取啊正则啊，可以在控制台里输出日志啊，可以调用各种有的没的的内部函数啊，elsif 可能是因为 e 不发音，就像 usr 绝对不是拼写错误……\nPython 的缩进虽然为人所不齿，但它至少和大部分编程语言一样，单独一行的程序语句和由多行程序组成的程序块之间，并不需要明显的分割符号。可 MySQL 需要用 DELIMITER $$这种奇怪的符号，PL/SQL 需要用/这种奇怪的符号，SQLServer 需要用@这种奇怪的符号，还有大名鼎鼎的虚拟表 DUAL。也许这些东西写多了就可以记住，就像我现在可以分清 SYSDATE、NOW()和 GETDATE()，可它带来的问题是什么呢，大多数的触发器、存储过程、函数都是没有移植性可言的，很多年前，我们讲设计模式，最喜欢觉的例子就是，如果项目发生变动，需要更换数据库，我们要怎么设计能不改动代码，现在看起来，当时还是太天真了，真要换了数据库，估计就是重新做了，敢把全部业务写到数据库里，Web 就做一个展示层的项目，有生之年应该是不会换数据库啦！\n多元与统一 这个世界的离奇之处在于，人们一边渴望在标准的庇护下幸福生活，又一边渴望可以超脱标准去发展独立的个性，如你我所见，多元与统一，构成了这个世界永恒的旋律，或许是因为那句名言——没有永远的敌人，只有永远的利益。可对比 Web 的标准化与 SQL 的标准化，我们却看到了截然不同的场景，虽然 Chrome 浏览器市场份额的不断提高，加上微软、Mozilla 等“浏览器巨头”一起推动，HTML5 和 CSS4，让大量的工作得到了简化，尤其像 WebSocket、Drag\u0026amp;drop、Canvas 等 API 的推出，这带来的好处是什么呢？大家不再去重点关注浏览器的兼容性问题，各种天花乱坠的炫酷特效不再通过 JavaScript 去控制。一个标准的 API + 一个支持降级的 profily，基本就可以覆盖到主流的浏览器，就算有小程序这种偏离标准的解决方案，回顾近几年整个前端领域的趋势，可以说，一切都在向着好的方向发展。\n可数据库领域发生了什么，依稀记得甲骨文和 Google 因为 Android 使用了 Java 而官司连连，Google 不得不推出一种新的基于 JVM 的语言——Kotlin；依稀记得甲骨文在开源社区的强烈反对下收购了 MySQL，社区不得不继续维护 MySQL 的开源分支——MariaDB。从这两件事情，我完全提不起对甲骨文这家公司的好感，虽然大家都说 Oracle 品质卓越，可实际使用下来，经常出问题的 Oracle。从 LAMP 时代开始，MySQL 就以其免费、轻量的特点广泛应用在互联网产品中，直至今天有大量的云产品使用着 MySQL，而 Oracle 和 SQLServer 则被更多地使用在私有部署的场景中。虽然，我承认把数据掌握在自己手里会放心些，可当你没有能力去维护这些东西时，付出的时间和精力远远要比这多。甲骨文收购了那么多公司的产品，时至今日，对整个行业的标准化有什么推动呢？Oracle 数据库依然难装、难用，PL/SQL 同样难用得要命，可我们这世界一直都很奇怪，最流行的偏偏未必是最好的，据说 Oracle 的代码写得非常差，开发人员表示不会在为它继续开发新功能。\n可能有时候，我们完全说不出来，一件东西是好还是坏，就像 JavaScript 能在前端开发流行，是因为没有其它的选择，你说这门语言没有缺点吗？当然有，JavaScript 里各种“骚操作”和“黑科技”，甚至吐槽三天三夜都说不完。同样，还有 Python 这门语言，大家都觉得它的解释器慢腾腾的，动态语言遇上大型项目简直就是火葬场，还有神来之笔—— 通过缩进来代替花括号。我最终还是在 PL/SQL 里执行了我的脚本，只要我在使用 DBeaver 的时候，人肉地区分/前后的 SQL 语句就可以了。果然，我骨子里还是一个不喜欢写 SQL 脚本的人，因为我认为这么别扭的东西简直不能称之为脚本，你看看 Lua，再看看 Python，有哪一门脚本语言有 SQL 脚本这样别扭呢？数据库对我而言，就是一个存取数据的“潘多拉魔盒”，索引啊，触发器啊，数据库任务啊，执行计划啊，存储过程啊，难道不属于暴露了太多细节给用户吗？我天天用这个数据库，我每天用哪些表，我每天用哪些字段，你作为一个成熟的数据库了，居然不能自己去解决这些问题，我对你很失望啊，请记住，程序员比任何人都喜欢偷懒。\n","date":"2019-04-19T12:52:10Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/337943766/","slug":"337943766","tags":["DBeaver","PL/SQL","Oracle"],"title":"由 DBeaver 与 PL/SQL 引发的数据库吐槽"},{"categories":["前端开发"],"content":"最近需要在项目中实现报表的自定义设置功能，即用户可以针对报表新建自定义分组，分组间可以互相嵌套，分组及分组内的报表需要支持拖拽排序、编辑、删除……相信听到这里，你大概明白我要实现一个什么样的功能了。不错，我要实现一个集美观、功能于一身的树形菜单。本着“不要重复制造轮子”的原则，我在考察了 JQuery EasyUI、layui、Bootstrap、Kendo UI 等不同框架提供的“树形菜单”组件以后，最终选择了zTree这样一个插件，虽然这个官网看上去相当复古，虽然最终的成品依然被同事吐槽丑，可它的确完美得实现了我想要的功能，是当之无愧的“树形菜单”王者。\nzTree 的 API 相当复杂，尤其是属性和事件的种类，简直叫一个繁杂，这是大部分基于 jQuery 插件的一个特点。不过 zTree 的使用还是比较简单的，我们只需要提供一个 DOM 节点，一份 JSON 数据，zTree 就可以帮我们在界面上渲染出一个完整的树形菜单：\nvar data = res.Data; var zNodes = JSON.parse(data.TreeData); $.fn.zTree.init($(\u0026#34;#reportTree\u0026#34;), setting, zNodes); zTree 的节点是由 JSON 结构来定义的，其基本结构是{name:\u0026ldquo;节点名称\u0026rdquo;,children:[]}，父子节点采用相同的结构相互嵌套。例如，下面是博主所使用的数据结构：\n[ { \u0026#34;id\u0026#34;: null, \u0026#34;name\u0026#34;: \u0026#34;全部报表\u0026#34;, \u0026#34;url\u0026#34;: null, \u0026#34;pId\u0026#34;: null, \u0026#34;viewUrl\u0026#34;: null, \u0026#34;children\u0026#34;: [ { \u0026#34;id\u0026#34;: null, \u0026#34;name\u0026#34;: \u0026#34;示例报表A\u0026#34;, \u0026#34;url\u0026#34;: null, \u0026#34;pId\u0026#34;: null, \u0026#34;viewUrl\u0026#34;: null, \u0026#34;children\u0026#34;: [ { \u0026#34;id\u0026#34;: null, \u0026#34;name\u0026#34;: \u0026#34;示例报表B\u0026#34;, \u0026#34;url\u0026#34;: null, \u0026#34;pId\u0026#34;: null, \u0026#34;viewUrl\u0026#34;: \u0026#34;/MyReport/List?menuid=38c0e1ce7442419f9e3305a03b819128\u0026#34;, \u0026#34;children\u0026#34;: null }, { \u0026#34;id\u0026#34;: null, \u0026#34;name\u0026#34;: \u0026#34;示例报表C\u0026#34;, \u0026#34;url\u0026#34;: null, \u0026#34;pId\u0026#34;: null, \u0026#34;viewUrl\u0026#34;: \u0026#34;/MyReport/List?menuid=e88ae4a5c07445a59c2f04ec405e6158\u0026#34;, \u0026#34;children\u0026#34;: null } ] } ] } ] 参考官网上的 DEMO，我们基本上就可以快速上手 zTree，博主这里就是结合了节点的编辑、拖拽这两个功能。不过，按照官网上的 DEMO 会存在两个 Bug，与我们实际的期望有所不同，**其一，是当一个分组下的子节点被全部删除后，这个分组的图标会变成一个子节点的图标；其二，是当个一个分组下的节点被全部拖拽到分组以外的地方，这个分组的图标会变成一个子节点的图标。**这两个 Bug 是由测试小姐姐们发现的，zTree 是我引入到项目中来的，这个 Bug 哪怕跪着都要改完，说多了都是泪啊，下面给出解决方案：\nfunction onRemove(e, treeId, treeNode) { var zTree = $.fn.zTree.getZTreeObj(reportTreeId); var root = zTree.getNodes()[0]; if (treeNode.isParent) { reports = GetReportsByNode(treeNode) var parentNode = treeNode.getParentNode(); if (parentNode != null \u0026amp;\u0026amp; (parentNode.children == null || parentNode.children.length == 0)) { parentNode.isParent = true; parentNode.isOpen = true; zTree.updateNode(parentNode); } } } var emptyNode; function beforeDrop(treeId, treeNodes, targetNode, moveType, isCopy) { var zTree = $.fn.zTree.getZTreeObj(reportTreeId); for (var i = 0; i \u0026lt; treeNodes.length; i++) { var treeNode = treeNodes[i]; var parentNode = treeNode.getParentNode(); if (parentNode != null \u0026amp;\u0026amp; (parentNode.children == null || parentNode.children.filter(function (s) { return s.name != treeNode.name; }).length == 0)) { emptyNode = parentNode; break; } } return true; } function onDrop(event, treeId, treeNodes, targetNode, moveType, isCopy) { var zTree = $.fn.zTree.getZTreeObj(reportTreeId); if (emptyNode != null) { emptyNode.isOpen = true; emptyNode.isParent = true; zTree.updateNode(emptyNode); emptyNode = null; } } OK，实际项目中可能需要存储这个树形结构，因为你能想象，用户编辑完这样一个“个性化”的设置以后，我们还要根据这个设置来加载树形菜单，以达到个性化的目的。那么，怎么获得这个树形结构呢，理论上我们只需要通过 zTree.getNodes()方法获得整个树的节点信息，然后将其序列化为 JSON 即可，可实际上 zTree 会在树上附加“冗余”信息，所以，博主的做法是，通过递归来遍历整个树的节点，获取其中的关键信息，这里以 name 字段为例：\nfunction GetTreeData(zTree) { var data = []; for (var i = 0; i \u0026lt; zTree.length; i++) { var treeNode = zTree[i]; if (!treeNode.isParent) { var obj = new Object(); obj.name = treeNode.name; data.push(obj) } else { var obj = new Object(); obj.name = treeNode.name; obj.children = GetTreeData(treeNode.children) data.push(obj) } } return data; } 好了，最近接触到都是些零碎的东西，大家都讲究着看看吧，可以说没有什么干货。折腾前端最大的感悟就是，做一个页面其实并不难，真正难的是集成到一个系统里，像 iframe 和 tab 这种“垃圾”的东西，集成到一起就像猜地雷，你永远不知道别人埋了什么坑在里面，以上就是这篇博客啦，晚安！\n","date":"2019-04-12T12:37:10Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1397717193/","slug":"1397717193","tags":["JavaScript","zTree","前端"],"title":"zTree 删除/拖拽子节点保留父节点分组样式"},{"categories":["前端开发"],"content":"Hi，大家好，在经历了两周多的 “写 Bug”、“改 Bug” 死循环后，又一个迭代终于在习以为常的加班生活中结束啦！联想到最近在 Github 上发起的 \u0026ldquo;996.icu\u0026rdquo; 事件，不禁令人由衷地感慨生活不易，所谓“”起风了，唯有努力生存”。其实，我反对是加班常态化所导致的无效加班，既然努力工作是为了更好的生活，可如果因此而模糊了工作和生活的界限，这到底是一件好事还是一件坏事呢？想想每个周末被工作群里消息支配的失落感，我希望我有可以自由支配的时间，即使我看起来比别人年轻，即使我下班后依旧孤身一人，因为用时间来换钱这件事情，着实是件性价比不高的事情，货币会一天天地贬值直至我们老去，可那些失去的时间就永远地失去了。好了，“业精于勤荒于嬉”，今天我们来说前端中实现拖拽排序这件事情。\n其实，这件事情说起来挺尴尬的，我们曾经为用户提供过某种**”智能“**的体验，我们通过对用户的行为进行分析，为其推荐了个性化的菜单项，甚至根据用户的使用频率对菜单进行了排序。可事实上用户的反响并不是非常强烈，在经过一段时间的使用后，用户依然觉得这个功能相当地”鸡肋“，这件事情告诉我们一个真相，即无论是产品设计还是需求研讨，最好不要轻易地代入用户的角色。最终的结果是我们打算为用户提供自定义的功能，考虑到操作的便利性问题，我们放弃了那种通过上下箭头按钮进行排序的方案，这样就回到了本文的主题，如何在前端中对一组列表进行拖拽排序，最终我们选定了两组方案，它们分别是Nestable和Sortable。\nNestable 方案 Nestable 是一个基于 jQuery 的插件，是一个在 Github 上开源的项目，据作者声称，这是一个\u0026quot;拖放具有鼠标和触摸兼容性的分层列表\u0026quot;的方案。这里针对触摸兼容性的支持可以忽略不计，因为如今都 9012 年了，博主依然在做传统前端页面的开发，这里博主最感兴趣的一点是，它可以支持分层列表，换言之，我们的列表元素是可以有层级关系、是可以嵌套的，唯一令人有点不爽的就是它依赖 jQuery 了，在这样一个连 Github 和 Bootstrap 都在努力移除 jQuery 的时代，没有 jQuery 的历史包袱，意味着我们可以大胆地去做现代前端应该做的事情。好了，我们来看看 Nestable 具体是怎么使用的吧！首先，我们定义一个简单的 HTML 结构：\n\u0026lt;div class=\u0026#34;dd\u0026#34;\u0026gt; \u0026lt;ol class=\u0026#34;dd-list\u0026#34;\u0026gt; \u0026lt;li class=\u0026#34;dd-item\u0026#34; data-id=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;dd-handle\u0026#34;\u0026gt;Item 1\u0026lt;/div\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;dd-item\u0026#34; data-id=\u0026#34;2\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;dd-handle\u0026#34;\u0026gt;Item 2\u0026lt;/div\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;dd-item\u0026#34; data-id=\u0026#34;3\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;dd-handle\u0026#34;\u0026gt;Item 3\u0026lt;/div\u0026gt; \u0026lt;ol class=\u0026#34;dd-list\u0026#34;\u0026gt; \u0026lt;li class=\u0026#34;dd-item\u0026#34; data-id=\u0026#34;4\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;dd-handle\u0026#34;\u0026gt;Item 4\u0026lt;/div\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;dd-item\u0026#34; data-id=\u0026#34;5\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;dd-handle\u0026#34;\u0026gt;Item 5\u0026lt;/div\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;/div\u0026gt; 接下来，我们可以使用如下的 JavaScript 代码来初始化整个列表，果然，一股 jQuery 风扑面而来：\n$(\u0026#39;.dd\u0026#39;).nestable({ /* config options */ }); 然后，我们就可以看到下面的效果：\nnestablejs-demo\r怎么样？看起来效果还不错吧！不过博主在前期调研的过程中发现，它对于复杂的层级关系就无能为力啦，可能是博主打开的姿势不对吧！如果希望对列表做更深层次的定制，它需要配置的属性会非常非常的多，而且它有一套内在约束在里面，譬如 className、nodeName 等等，虽然这些都可以去配置，但要想像作者一样运用得好，依然是需要花费大量时间来学习它的 API。\n说到这里，对于 Nestable，我唯二喜欢的一个 feature 是，它可以实时地获取到排序后的节点信息，而且是序列化后的 JSON 格式哦，因为当我们要保存用户的排序结果时，有这样一个接口简直太棒啦有木有！这里需要说明的是，所有具备类似**data-**属性的节点都可以被序列化，熟悉前端的朋友一定知道，这是一个 HTML5 中的扩展功能，可以让我们在节点上附带更多的数据信息，在 Bootstrap 中经常需要用到这一特性。\n$(\u0026#39;.dd\u0026#39;).nestable(\u0026#39;serialize\u0026#39;); 继续以这个例子为例，我们将会得到下面的 JSON 信息：\n[{\u0026#34;id\u0026#34;:1},{\u0026#34;id\u0026#34;:2},{\u0026#34;id\u0026#34;:3,\u0026#34;children\u0026#34;:[{\u0026#34;id\u0026#34;:4},{\u0026#34;id\u0026#34;:5}]}] 不过，遗憾的是，貌似作者已经不打算维护这个项目啦，最后一次维护时间已经是 6 年前，毕竟属于 jQuery 的辉煌时代都已经过去，何况是基于 jQuery 的一个插件呢？可这种频繁修改 DOM 结构引发浏览器重绘的操作，在大前端时代会消失吗？或许并不会。关于这个项目更多的使用细节，大家可以到它的Github主页去了解。\nSortable 方案 Sortable相比 Nestable 好的一点就是，它对自己的定位是“一个用于可重新排序的拖放列表的 JavaScript 库”。它不再局限于 jQuery 这样一个方案上，事实上它支持 Vue、React、Angualr、Knockout 等将近 7 个框架，除了支持常规的列表以外，还支持 Grid 中元素的拖拽，文档相比 Nestable 要更为完善一点，所以要在项目中使用的话，我个人更推荐 Sortable。我们一起来看看如何使用 Sortable 吧，这里我们选择 Bootstrap 作为基础样式。首先，我们写一个简单的“列表组”：\n\u0026lt;ul class=\u0026#34;list-group\u0026#34; id=\u0026#34;items\u0026#34;\u0026gt; \u0026lt;li class=\u0026#34;list-group-item\u0026#34; data-id=\u0026#34;0\u0026#34;\u0026gt; Menu1 \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;list-group-item\u0026#34; data-id=\u0026#34;1\u0026#34;\u0026gt; Menu2 \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;list-group-item\u0026#34; data-id=\u0026#34;2\u0026#34;\u0026gt; Menu3 \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;list-group-item\u0026#34; data-id=\u0026#34;3\u0026#34;\u0026gt; Menu4 \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; 接下来，我们通过 JavaScript 来给这个列表“施加”魔法——巴拉能量：\nvar ele = document.getElementById(\u0026#39;items\u0026#39;); var sortable = Sortable.create(ele); 然后我们就可以发现，这个基于 Bootstrap 的列表居然可以拖拽啦！\nsortablejs-demo-1\rOK，我们继续给这个例子来点魔法，可以让列表元素在拖动的时候高亮显示：\nvar sortable = Sortable.create(ele, { animation: 150, ghostClass: \u0026#39;blue-backgroun-class\u0026#39; }); 可以注意到，拖拽时动画会变得更流畅，被拖拽的元素会以蓝底白字高亮显示：\nsortablejs-demo-2\r和 Nestable 类似，我们可以指定一个回调函数来获得排序后的结果，注意到我们这里指定一个 dataIdAttr，它告诉 Sortable 我们将用哪一个值作为数据的主键，从 data-text 这个命名就可以看出，它的数据是维护在类似**data-**的属性上的，假设我们这里希望获得排序后的菜单，那么，它的打开方式是这样的：\nvar ele = document.getElementById(\u0026#39;items\u0026#39;); var result = document.getElementById(\u0026#39;result\u0026#39;); var sortable = Sortable.create(ele, { animation: 150, dataIdAttr: \u0026#39;data-text\u0026#39;, onUpdate: onUpdate, ghostClass: \u0026#39;blue-backgroun-class\u0026#39; }); function onUpdate(evt){ var data = sortable.toArray(); result.innerText = \u0026#34;当前排序结果为：\u0026#34; + JSON.stringify(data); } 好了，现在可以看到，随着我们对列表进行拖拽，每次都会获得更新以后的列表数据，显然，我们可以将这个结果存到任何地方，这样就可以按用户定义的方式去加载一个列表。\nsortablejs-demo-3\r以上就是 Soratble 的基本用法，关于更多的使用细节，官方文档了解一下。\nHTML5 原生方案 OK，说完了 Nestable 和 Sortable 这两个第三方的解决方案，下面我们来说说基于 HTML5 的原生方案。HTML5 标准问世以来，有很多有意思的东西被吸收到标准之中，拖放(drag \u0026amp; drop)就是其中之一。在此之前，我们需要写大量的 JavaScript 代码来实现这个功能。现在，HTML5 中原生支持拖放 API，我们不妨考虑通过它来实现一个可拖拽的列表，这里我们继续沿用基于 Bootstrap 的例子。\nvar dragElement = null; var source = document.querySelectorAll(\u0026#39;.list-group-item\u0026#39;); for(var i = 0; i \u0026lt; source.length; i++){ source[i].addEventListener(\u0026#39;dragstart\u0026#39;,function(ev){ dragElement = this; },false); source[i].addEventListener(\u0026#39;dragenter\u0026#39;, function(ev){ if(dragElement != this){ this.parentNode.insertBefore(dragElement,this); } }, false) source[i].addEventListener(\u0026#39;dragleave\u0026#39;, function(ev){ if(dragElement != this){ if(this == this.parentNode.lastElementChild || this == this.parentNode.lastChild){ this.parentNode.appendChild(dragElement); } } }, false) }; document.ondragover = function(e){e.preventDefault();} document.ondrop = function(e){e.preventDefault();} 这里唯一需要注意的地方，就是要给每一个 className 为 list-group-item 的元素添加 draggable 属性，并设置该属性为 true，这是使用 HTML5 拖放 API 的一个前提，换言之，只有 draggable 的元素才可以被拖拽。那么，HTML5 中针对拖放的 API 有哪些呢？针对拖放事件，我们可以抽象出三种角色，它们分别是：\n源对象：即对拖拽的对象。它有 dragstart、drag 和 dragend 三个事件。\n过程对象：即被拖拽的对象，在拖拽过程中经过的中间对象，它有 dragenter、dragover 和 dragleave 三个事件。\n目标对象：即被拖拽的对象，最终所放置的对象，它只有一个 drop 事件。\n而在所有的拖拽事件中，都提供了一个数据传递对象 dataTransfer，用于在源对象和目标对象间传递数据。例如，我们可以通过 setData()来向 dataTransfer 存入数据，通过 getData()来从 dataTransfer 读取数据，通过 clearData()来清理 dataTransfer 中的数据。此外，还可以通过 setDragImage()、effectAllowed 属性 和 dropEffect 属性来设置拖拽过程中的图标、拖放的视觉效果等。这里需要注意的是，IE 浏览器不支持 dataTransfer 对象。了解了这下，我们就可以做出一个**”简陋“**的拖拽排序功能：\nsortablejs-demo-4\r本文小结 这篇文章主要分享了三种实现列表拖拽排序的方案，在技术选型阶段，主要选择 Nestable 和 Sortable 这两种方案，前者对层级节点提供的序列化支持非常好，但经过一番折腾后，发现要想像作者一样用好这个插件，着实是件困难的事情，而且貌似作者已经不再维护这个项目了，最近的代码提交历史大概是 6 年前，毕竟属于 jQuery 的辉煌时代已经过去，何况是一个基于 jQuery 的插件呢？所以，个人不建议在正式项目中使用 Nestable。相比之下，Sortable 的定位要更高一点，它不再局限于某个 UI 框架上，理论上任何前端项目都可以使用，从文档的完整性和易用性上，都要比 Nestable 要更胜一筹。原本一开始打算写这两种方案的，后来觉得 HTML5 中提供了拖拽相关的 API 接口，这种方式不失为一种解决方案。虽然提到 HTML5 就让人联想到兼容性，可都 2019 年了，连浓眉大眼的微软(巨硬)都开始在 Edge 里使用 Chrome 内核了，兼容性问题还算是个问题吗？所以，这篇文章实际上介绍了三种解决方案，具体使用哪一种，大家可以根据实际情况来决定，好啦，这篇博客就写到这里，谢谢大家，晚安！\n","date":"2019-03-31T12:49:37Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2436573863/","slug":"2436573863","tags":["前端","HTML5","拖拽"],"title":"分享两种实现前端拖拽排序的方案"},{"categories":["编程语言"],"content":"最近利用闲暇时间从图书馆借了两三本书来“充电”，因为如果不及时摄取新的营养，感觉会越来越难有新的想法输出出来，尤其是像 ServerLess、组件化、分布式等等这样的场景慢慢开始接触，就势必无法再用从前的眼光去看待。大概去年的时候，阿里巴巴发布了「阿里巴巴开发手册」这本小册子，大概不到 100 页的样子，这次我就挑选了我觉得还不错的关键点，和大家简单分享一下，所以，这是一篇“典型”的读书笔记，下面的编号代表的是指定章节下的第几条规范，例如，1.1.2 表示的是第一章第一节中的第二条规范，欢迎大家一起讨论。\n编程规范 1.1.2 代码中的命名严禁使用拼音与英文混合的方式，不允许直接使用中文的方式，纯拼音命名方式更要避免采用。\n说明：英文不好可以去查，禁止使用纯拼音或者拼音缩写的命名方式，除了不能“望文生义”以外，对导致别人在调用接口的时候，向这种“丧心病狂”的编码风格妥协，这里不点名批评某 SAP 提供的 OA 接口，除了超级难用以外，每次都要花大量时间去对字段。\n1.4.3 相同参数类型，相同业务含义，才可以使用 Java 的可变参数，避免使用 Object，可变参数必须放置在参数列表最后。\n说明：例如一个接口同时支持单条更新或者批量更新，此时，完全就可以使用 param 关键字来声明相同的参数类型，而无须定义 InsertOne 和 InsertMany 两个方法。\n1.4.4 对外部正在使用或者二方库依赖的接口，不允许修改方法签名，以避免对接口调用方产生影响。若接口过时，必须加@Deprecated 注解，并清晰地说明采用的新接口或者新服务是什么。\n说明：对于过期的接口可以通过 Obsolete 特性来声明过期，这样在编译时期间可以告知使用者该接口已过期。对于 WebAPI 接口，除非有版本控制机制，否则一律不允许修改已上线的接口签名、参数和返回值。\n1.4.17 在循环体内，字符串的连接方式使用 StringBuilder 的 append 方法进行扩展。\n说明：这一点，在 C#中同样适用，因为字符串类型是 Immutable 的，对字符串进行拼接会产生大量的临时对象。\n1.5.7 不要在 foreach 循环内进行元素的 remove/add 操作。remove 元素请使用 Iterator 方式，如果并发操作，需要对 Iterator 对象加锁。\n说明：因为 foreach 是基于迭代器(IEnumerator)的，在 foreach 循环内部修改集合，会导致 Current 和 MoveNext()发生混乱，早期的集合使用 SynRoot 来解决线程安全(内部原理是使用了 Interlocked 锁)，现在我们使用 CurrentBag 等线程安全的集合。\n1.6.1 获取单例对象需要保证线程安全，其中的方法同样要保证线程安全。\n说明：只要类型中有静态成员存在，就要考虑线程安全，因为静态成员隶属于类型而非类型的实例。\n1.6.5 SimpleDataFormat 是线程不安全的类，一般不要定义为 static 变量，如果定义为 static，必须加锁，或者使用 DateUntils 工具类。\n说明：无论所声明的静态成员是否线程安全，都应该考虑到在竞态条件下，可能会出现多个线程同时修改静态成员的风险，此时最好对其进行加锁。\n1.6.8 在并发修改同一条记录时，为避免更新丢失，需要加锁。要么在应用层加锁，要么在缓存层加锁，要么在数据库层使用乐观锁，使用 version 作为更新依据。\n说明：所谓“悲观锁”，是指认为数据一定会被篡改，此时，在一个作用域结束前对其进行加锁，典型的如 lock 关键字。而所谓“乐观锁”，是认为数据不一定会被篡改，此时，通过一个 version 来作为更新的依据。\n1.6.12 在并发场景下，通过双重检查锁(double-checkedlocking)实现延迟初始化的优化问题隐患，推荐解决方案中较为简单的一种(JDK5 及以上版本)，即目标属性声明为 volatile 型。\n1.7.4 在表达异常的分支时，尽量少用 if-else 方式。如果不得不使用 if……elseif…else 方式，请勿超过 3 层。\n说明：当条件不满足时可以直接 return，或者先判断不满足的条件，则剩余逻辑默认就是满足条件的分支，尽量避免使用 if……elseif……else 方式，同时保证分支里的代码足够简单，复杂的逻辑应考虑封装或者用 switch……case 甚至多态来重构。\n1.7.8 循环体的语句要考量性能，以下操作请尽量移至循环体外处理，如定义对象或变量、获取数据库连接，避免进行不必要的 try……catch 操作。\n说明：抛出一个异常是非常简单的，然而捕获一个异常需要付出一定的性能代价，因为它需要捕捉程序异常时的上下文信息，建议在进入循环内部合理检验，覆盖到每一种考虑到的情况，考虑不到的请让它向上抛出。\n2.1.2 对大段代码进行 try-catch，使得程序无法根据不同的异常做出正确的应激反应，不利于定位问题，这是一种不负责任的表现。\n说明：对大段代码进行 try-catch，或许可以保证应用程序不崩溃，但在程序异常的一瞬间，可能业务数据已经出错，此时再让程序继续运行下去，不仅无法快速定位出错原因，而且会对下一流程的业务产生“污染”。\n异常日志 2.1.2 捕捉异常是为了处理它，不要捕捉了却什么都不处理而抛弃之，如果不想处理它，请将该异常抛给它的调用者，然后由调用者在最外层业务中处理异常，并将其转化为用户可以理解的内容。\n说明：捕捉了异常一定要去处理它，如果单单是为了记录个错误日志，完全可以通过 AOP 来记录，底层抛出的异常不允许被“吞掉”，必须将其抛给它的调用者，异常最终需要转化为友好的界面提示。\n2.2.5 finally 块必须对资源对象、流对象进行关闭操作，如果有异常同样要做 try-catch 操作(JDK7 及以上版本可以使用 try-with-resource 方式)\n说明：因为 finally 块一定会在 return 前执行，所以，无论程序是否发生了异常，我们都可以在 finally 块中对资源对象、流对象、数据库连接等进行关闭或者释放，using 其实是 try……finally 的语法糖，它会自动地在 finally 块里调用 Dispose 方法(因为它要实现 IDispose 接口)。\n2.2.7 不能在 finally 块中使用 return。\n说明：这里涉及到一个 return 和 finally，尽管 return 可以提前“跳出”，但对 finally 来说，不管是否发生异常，它都会执行，在此之前 return 会把返回值写入内存，等 finally 块执行结束后，return 再“跳出”。C#中 finally 块中不允许写 return，否则会导致编译错误。通常，finally 块用来做清理相关的工作。\n数据库 5.1.1 表达是否的概念时，必须使用 is_xxx 的命名方式，数据类型是 unsigned tinyint。其中，1 表示是，0 表示否。\n说明：表达是否最好用 0 和 1 来表示，我们用 Y 和 N 时经常会出现，开发人员忘记给模型赋值，导致进入到数据库里的数据出现错误数据，而领域模型里又不建议给字段默认值，可如果使用 unsigned tinyint 类型，它本身就自带默认值 0，这样就可以避免这种问题的出现。\n5.1.2 表名、字段名必须使用小写字母或数字，禁止出现数字开头，禁止两个下画线中间只出现数字。\n说明：建议全部使用小写，因为主流 SQL 教程的里关键字都采用大写，但在 PLSQL/SQLyog 里编写 SQL 语句时，字段会自动地变成小写，而且不区分大小写，为了避免人格分裂，建议所有字段都用小写。我们表名用小写，表字段用大写，输出的 SQL 语句看起来特别奇怪。\n5.1.13 字段允许适当冗余，以提高查询性能，但必须考虑数据一致。冗余字段应遵循：\n不是频繁修改的字段 不是 varchar 超长字段，更不能是 text 字段。 说明：冗余字段是个好东西，但主表和扩展表间的一致性保证需要经过良好的设计，那种把相关表都放在一个事务里处理的做法，都声称是为了保证数据的一致性，可实际过程中依然会存在数据不一致的情况。\n5.1.15 当单表行数超过 500 万行活着单表容量超过 2G 时，才推荐进行分库分表。\n说明：多租户架构下，不同租户采用不同的库，是最简单的数据隔离方案，但缺点是增加了维护多个库的成本。如果要分库分表，最好从框架层面来“切库”，而不要让开发人员自行维护数据库的连接字符串。\n5.2.1 业务上具有唯一特性的字段，即使是多个字段的组合，也必须建成唯一索引。根据墨菲定律，只要没有唯一索引，必然会有脏数据产生，即使在应用层做了非常完善的检验控制。\n5.2.2 超过三个表禁止 join，需要 join 的字段，类型必须绝对一致；当多表关联查询时，保证被关联的字段需要有索引。\n说明：关系型数据库最值得炫耀的地方就是表多，超过三张表禁止 join，实际中根本不现实，所以，建议以业务场景为准，我就曾经 join 了 5 张表，大概客户就喜欢看这些东西吧！\n5.3.7 禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\n说明：存储过程和触发器是万恶之源，不同数据库下的 SQL 语句千姿百态，同样的业务逻辑，Oracle、MySQL 和 SQLServer 基本上是三种语法，更不用说$$、/和@这种奇葩的东西了，查询语言就老老实实写查询，写业务了逻辑，SQL 真的不行，太垃圾，虽然做权限划分非常容易……\n5.3.9 in 操作能避免则避免，如实在避免不了，需要仔细评估 in 后面的集合元素数量，最好控制在 1000 之内。\n说明：这一点表示认同，我们经常遇到这样的情况，先筛选出 A 表符合条件的所有记录，然后根据 A 表中某一列(通常是外键)，通过 IN 操作来筛选出 B 表中符合条件的所有记录，这个时候，应该注意控制 IN 后面集合内元素的数目，总之不要太大……\n","date":"2019-03-20T12:49:37Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1122710277/","slug":"1122710277","tags":["阿里","Java","笔记"],"title":"《阿里巴巴 Java 开发手册》读书笔记"},{"categories":["编程语言"],"content":"今天是过完春节以后的第二周啦，而我好像终于回到正常工作的状态了呢，因为突然间就对工作产生了厌倦的情绪，Bug 就像无底洞一样吞噬着我的脑细胞。人类就像一颗螺丝钉一样被固定在整部社会机器上，除了要让自己看起来像个正常人一样，还要拼命地让所有人都像个正常人一样。过年刚经历过被催婚的我，面对全人类近乎标准的“幸福”定义，大概就是我此刻这种状态。其实，除了想自己定义“幸福”以外，我还想自己定义“问题”，因为，这样就不会再有“Bug”了。言归正传，今天我想说的是前端跨域这个话题，相信读完这篇文章，你就会明白，这个世界上太多太多的问题，都和你毫无瓜葛。\n故事缘起 年前被安排去做一个 GPS 相关的需求，需要通过百度地图 API 来计算预计到达时间，这并不是一个有难点的需求，对吧？就在博主为此而幸灾乐祸的时候，一个非常醒目的错误出现在 Chrome 的控制台中，相信大家都见过无数次啦，大概是说我们的请求受到浏览器的同源策略的限制。那么，第一个问题，什么是同源策略呢？我们知道，一个 URL 通常有以下几部分组成，即协议、域名、端口和请求资源。由此我们就可以引申出同源的概念，当协议、域名和端口都相同时，就认为它们是在同一个域下，即它们同源。相反地，当协议、域名和端口中任意一个都不相同时，就认为它们在不同域下，此时就发生了跨域。按照排列组合，我们可以有以下常见的跨域场景：\nURL 说明 是否允许跨域 www.abc.com/a.js vs www.abc.com/b.js 相同域名下的不同资源 允许 www.abc.com/1/a.js vs www.abc.com/2/b.js 相同域名下的不同路径 允许 www.abc.com:8080/a.js vs www.abc.com:8081/b.js 相同域名下的不同端口 不允许 http://www.abc.com vs https://www.abc.com 相同域名采用不同协议 不允许 http://www.abc.com vs http://wtf.abc.com 相同域名下的不同子域 不允许 http://www.abc.com vs http://www.xyz.com 两个完全不同的域名 不允许 http://192.168.100.101 va http://www.wtf.com 域名及其对应的 IP 地址 不允许 那么，我们就不仅要问啦，现在微服务啊、RESTful 啊这些概念非常流行，在我们实际的工作中，调用第三方的 WebAPI 甚至 WebService，这难道不是非常合理的场景吗？前端的 Ajax，即 XMLHttpRequest，和我们平时用到的 RestSharp、HttpClient、OkHttp 等类似，都可以发起一个 Http 请求，怎么在客户端里用的好好的东西，到了前端这里就突然出来一个**“跨域”的概念呢？这是因为从原理上来说，这些客户端都是受信的“用户”(好吧，假装是被信任的)，而浏览器的环境则是一个“开放”**的环境。\nURI_Syntax_Diagram\r举一个例子，你在家的时候，可以随意地把手插进自己的口袋，因为这是你的私有环境。可是当你在公共环境中时，你是不允许把手插进别人口袋的。所以，浏览器有“跨域”限制，本质上是为了保护用户的数据安全，避免危险地跨域行为。试想，没有跨域的话，我们带上 Cookie 就可以为所欲为了，不是吗？实际上，同源限制和 JavaScript 没有一丁点关系，因为它是 W3C 中的内容，是浏览器厂商要这样做的，我们的请求其实是被发出去了，而它的响应则被浏览器给拦截了，所以我们在控制台中看到“同源策略限制”的错误。\n喜闻乐见的跨域拦截\r十八般武艺 好了，既然现在浏览器有这个限制，那为了客户着想，我们还是要去解决这个问题(对吧？)，虽然我至今想不明白，适配浏览器为什么会成为我们的工作之一[doge]。打开 Google 搜索“前端跨域”，于是发现了解决跨域问题的各种方案，这里选取最具代表性的 JSONP 和 CORS。\nJSONP 首先，我们来说说 JSONP，什么是 JSONP 呢？我们知道，通常 RESTful 接口返回的都是 JSON，而 JSONP 返回的是一段可以执行的 JavaScript 代码，我们所需要的数据就被“包裹”在这段代码中，这就是 JSONP，即JSON Padding的得名由来。在实际应用中，服务的提供方会根据调用方传入的回调函数(callback)来组织返回数据，譬如callback({\u0026ldquo;name\u0026rdquo;:\u0026ldquo;tom\u0026rdquo;,“gender”:\u0026ldquo;male\u0026rdquo;})。这就说到一个点，并不是所有的 API 接口在调用的时候出现跨域问题，都可以通过 JSONP 的方式来解决，因为它需要后端来配合组织返回数据。这里我们以“不蒜子”这个静态博客中使用最多的访问量统计工具为例，通过查看页面源代码，我们了解到它是通过 JSONP 来返回数据的。为什么它要用这种方式来返回数据呢？其实，我们仔细想想就能明白其中的缘由，因为像 Hexo、Jekyll 这种静态博客大多都是没有后端服务支持的，所以，它要访问“不蒜子”的统计服务，就必然会存在跨域的问题啊！那怎么解决这个问题呢？当然是选择 JSONP 啦！这里我们以 Postman 调用不蒜子接口为例，可以发现它的返回值是下面这个样子：\n在Postman中调用不蒜子接口\r博主计划在接下来的时间里，迁移不蒜子的统计数据到 LeanCloud 上，届时博主会使用最喜欢的 Python，来抓取这些访问量数据，因为 JSONP 返回的都不是 JSON 数据，因此再处理这些数据的时候，需要用正则来匹配这些结果。为什么在前端领域没有这些问题呢，因为 JSONP 返回的是世界上最**“任性”**的语言——JavaScript，当然，这些会是下一篇甚至下下一篇里的内容啦。\nCORS 好了，下面我们说说 CORS 这种方案。CORS，即跨域资源共享，是一种利用 HTTP 头部信息访问不同域下的资源的机制。我们在前面提到过，发生跨域访问时，其实请求已经发出去了，但响应则被浏览器给拦截住了。那么，CORS 说白了就是它可以通过 HTTP 头部信息，告诉浏览器来自哪些域的请求可以被允许，来自哪些域的请求应该被禁止。如果说 JSONP 多少带着点“hack”的意味儿，那么 CORS 就可以说是被官方认可的跨域解决方案啦！这种方案需要启用新的 HTTP 头部字段，具体可以参考这里。\n按照定义，浏览器会将 CORS 请求分为简单请求和非简单请求两类。对于简单请求，浏览器会对请求的头部进行“魔改”，即增加一个 Origin 字段，这样只要后端接口支持 CORS 跨域，就可以接收这些跨域请求，并做出回应，即在响应的头部信息中返回 Access-Control-Allow-Origin 等字段。而对于非简单请求，通常会先发出一个 OPTIONS 的“预检请求”，只有这个验证过程通过以后，主请求才会被发起。那么浏览器是怎么验证请求是否通过的呢？答案就是：检查Origin字段是否包含在Access-Control-Allow-Origin中。当验证不通过时，浏览器就会输出同源策略限制的错误。这就是 CORS，浏览器和服务端分别通过响应、请求的 HTTP 头部信息来**“商量”**要不要跨域。\n没有银弹 说了这么多关于“跨域”的话题，其实博主想说的是，没有银弹。这是一位前辈高人，曾经对博主反复说过的话。现在我们来看 JSONP，会发现它本质上是利用了浏览器的**“漏洞”**。为什么这样说呢？因为在浏览器中，所有具备 src 属性的 HTML 都是可以跨域的，譬如 script、img、iframe、link 这四个标签，我们赖以生存的 CDN 加速、图床、插件等等都是基于这一“漏洞”的产物。所以，很多人问为什么$.ajax 可以跨域，但原生的 XMLHttpRequest 则不可以呢？因为 jQuery 实际上把 JSONP 做成了一种语法糖，这就就会给人一种 ajax 可以跨域的错觉。\nJSONP？其实就是 JS JSONP 实际上返回的是可以执行的 JavaScript，即 text/javascript，它和我们所使用的大多数 JavaScript 并无区别，所以，你可以想到，当我们把一个远程地址赋值给 script 标签的 src 属性时，它和我们引用 CDN 上的医院文件并无区别，这正是 JSONP 的秘密所在，显然它只支持 Get 方式，当我们想要支持更多方式的时候，我们需要的是 CORS，一起来看下面这段代码，我们首先来写一个简单的 API 接口：\n// GET api/user/5?callback= [HttpGet(\u0026#34;{id}\u0026#34;)] public IActionResult Get(string id, string callback) { var userInfo = UserInfoService.Find(x =\u0026gt; x.UserId == id); if (userInfo == null) return NotFound(); if (string.IsNullOrEmpty(callback)) { //返回JSON Response.ContentType = \u0026#34;application/json\u0026#34;; return Json(userInfo); } else { //返回JSPNP Response.ContentType = \u0026#34;application/javascript\u0026#34;; return Content($\u0026#34;{callback}({JsonConvert.SerializeObject(userInfo)})\u0026#34;); } } OK，写完这个接口以后，我们首先来尝试在前端页面中调用这个接口，为了尽可能地减少依赖，我们这里用最新 Fetch API 来代替$.ajax()，毕竟现在都是 2019 年了呢，Github 和 Bootstrap 相继宣布从代码中移除 jQuery。大家都知道，原生的 xhr 和 Date 对象一样，简直难用得要命，而这一切在新的 Fetch API 下，会变得非常简单：\n//基于Fecth API调用JSONP showUserByFetch:function(){ fetch(\u0026#34;https://localhost:5001/api/user/1\u0026#34;) .then(function(response) { return response.json(); }) .then(function(user) { showUser(user); }); } 果然，就算使用最新的 Fetch API，浏览器还是会因为同源限制策略而拦截我们的请求\n浏览器中再次出现同源限制错误\r那么，试试用 JSONP 的思路来解决这个问题。注意到，为了兼容 JSONP 方式调用，我们在 API 接口中增加了一个 callback 参数，这个参数实际上就是预先在客户端中定义好的方法的名字啦！既然 JSONP 返回的是可执行的 JavaScript，那么我们在页面里增加一个 Script 标签好了：\n\u0026lt;script src=\u0026#34;https://localhost:5001/api/user/1?callback=showUser\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 其中，showUser 是一个预先定义好的 JS 函数，其作用是输出用户信息到页面上：\n//展示用户信息 function showUser(user){ var result = document.getElementById(\u0026#39;jsonp-result\u0026#39;); result.innerText = \u0026#39;用户ID：\u0026#39; + user.uid + \u0026#34;, 姓名：\u0026#34; + user.name + \u0026#39;, 性别：\u0026#39; + user.gender; } 现在，我们可以注意到，在控制台中输出了我们期望的结果，这说明页面中定义的 showUser()方法确实被执行了，所以，到这里我们可以对 JSONP 做一个简单总结：JSONP 是一种利用 script 标签实现跨域的方案，它需要对后端接口进行适当改造以返回可以执行的 JavaScript，客户端需要事先定义好接收数据的方法，两者通过 callback 参数建立起联系，返回类似 callback({\u0026ldquo;name\u0026rdquo;:\u0026ldquo;tom\u0026rdquo;,“gender”:\u0026ldquo;male\u0026rdquo;})结构的数据，因此 JSONP 请求必然且只能是一个 GET 请求。\n通过Script标签调用JSONP\r既然通过 Script 标签可以调用一个 JSONP 接口，那么我们不妨试试动态创建 Script 标签，然后你就会发现这两种方式的效果是一样的，都可以调用一个 JSONP 接口，前提是 JS 中已经存在 showUser()方法：\n//动态创建scipt调用JSONP showUserByDynamic:function(){ var self = this; var script = document.createElement(\u0026#34;script\u0026#34;); script.src = \u0026#34;https://localhost:5001/api/user/1?callback=showUser\u0026#34;; document.body.appendChild(script); }, 事实上，jQuery 中针对 JSONP 的支持正是基于这种原理，虽然 jQuery 的时代终将过去，可我相信这些背后的原理永远不会过时。顺着这个思路，我们不妨来看看 jQuery 中是如何实现 JSONP 的，以下代码可以在这里找到：\n// Bind script tag hack transport jQuery.ajaxTransport(\u0026#34;script\u0026#34;, function(s) { // This transport only deals with cross domain or forced-by-attrs requests if (s.crossDomain || s.scriptAttrs) { var script, callback; return { send: function(_, complete) { script = jQuery(\u0026#34;\u0026lt;script\u0026gt;\u0026#34;).attr(s.scriptAttrs || {}).prop({ charset: s.scriptCharset, src: s.url }).on(\u0026#34;load error\u0026#34;, callback = function(evt) { script.remove(); callback = null; if (evt) { complete(evt.type === \u0026#34;error\u0026#34; ? 404 : 200, evt.type); } }); // Use native DOM manipulation to avoid our domManip AJAX trickery document.head.appendChild(script[0]); }, abort: function() { if (callback) { callback(); } } }; } }); 可以注意到，它和我们这里的思路一致，即动态创建一个 script 标签，然后设置其 src 属性为目标地址，当其加载完成或者加载失败时，就会从页面的 DOM 节点中删除该标签，因为数据已经通过指定的 callback 处理过了。jQuery 甚至可以替我们生成对应的 callback 函数，例如，在这里我们可以这样使用 jQuery 来实现 JSONP 跨域，具体使用细节这里不再深究：\n//基于$.ajax()调用JSONP showUserByAjax:function(){ $.ajax({ type: \u0026#34;get\u0026#34;, url: \u0026#34;http://localhost:5000/api/user/1\u0026#34;, dataType: \u0026#34;jsonp\u0026#34;, jsonp: \u0026#34;callback\u0026#34;, data: \u0026#34;\u0026#34;, success: function (user) { showUser(user); } }); }, CORS，跨域新标准 相对 JSONP 来说，CORS 实现起来就非常简单啦，因为主流的 Web 框架中几乎都提供了 CORS 的支持，因为 CORS 可以实现除了 GET 以外的譬如 POST、PUT 等请求，所以，它比 JSONP 这种”Hack“的方式有更为广阔的适用性，而且随着 Web 标准化的不断推荐，目前 CORS 可以说是官方主推的跨域方案。这里我们以.NET Core 为例来讲解 CORS 跨域。\nCORS，即同源资源共享，其实早在 ASP.NET 时代，这一机制就已经得到了支持，现在我们以.NET Core 来讲，无非是希望大家放下历史包袱，在跨平台的新道路上轻装上阵。好了，在.NET Core 中我们有两种 CORS 方案，一种是在 Startup 类中以全局配置的方式注入到整个中间件管道中，一种是以特性的方式在更小的粒度上控制 CORS。这其实和之前配置路由的思路相近，即我们可以配置全局的路由模板，同样可以在 Controller 和 Action 级别上定义路由。在这里，我们先定义两种 CORS 策略，AllowAll 和 AllowOne，并以此来测试 CORS 实际的使用效果。\n//CORS策略：简单粗暴一刀流 services.AddCors(opt=\u0026gt;{ opt.AddPolicy(\u0026#34;AllowAll\u0026#34;, builder =\u0026gt; { builder.AllowAnyOrigin(); builder.AllowAnyHeader(); builder.AllowAnyMethod(); }); }); //CORS策略：允许指定域 services.AddCors(opt=\u0026gt;{ opt.AddPolicy(\u0026#34;AllowOne\u0026#34;, builder =\u0026gt; { builder.WithOrigins(\u0026#34;http://localhost:8888\u0026#34;) .AllowAnyHeader() .AllowAnyMethod() .WithExposedHeaders(\u0026#34;X-ASP-NET-Core\u0026#34;,\u0026#34;X-UserName\u0026#34;) .AllowCredentials(); }); }); 可以注意到，在全局范围内应用 AllowAll 以后，我们的后端接口将支持来自任意域/端口的跨域访问，这意味着我们之前必须使用 JSONP 来跨域的地方，现在都可以直接发起跨域请求。到底是不是和我们想得一样呢？答案啊，那必须是肯定的啊！\n[EnableCors(\u0026#34;AllowOne\u0026#34;)] [Route(\u0026#34;api/[controller]\u0026#34;)] [ApiController] public class UserController:Controller { //... } 好了，现在我们来测试在 UserController 上应用局部的 CORS 请求，在这个实例中，我们指定只有来自 localhost:8888 的请求可以跨域，为此博主这里用 Python 临时开了一个服务器，本文中的前端页面，实际上就是运行在这个服务器上的。你知道我想说什么，“人生苦短，我用 Python”。因为我们这里返回的是 application/json，所以它是一个非简单请求，这里复习一下简单请求与非简单请求。\n简单请求 根据MDN中关于 CORS 的定义，若请求满足所有下述条件，则该请求可视为“简单请求”，简单请求意味着不会触发CORS 预检请求：\n使用下列方法之一：GET、HEAD、POST。 Fetch 规范定义了对 CORS 安全的首部字段集合，不得人为设置该集合之外的其他首部字段。该集合为：Accept、Accept-Language、Content-Language、Content-Type (需要注意额外的限制)、DPR、Downlink、Save-Data、Viewport-Width、Width。 Content-Type 的值仅限于下列三者之一：text/plain、multipart/form-data、application/x-www-form-urlencoded。 MDN中对简单请求的图解\r非简单请求 非简单请求和简单请求相反，即不满足简单请求中任一条件的请求都被成为非简单请求。非简单请求，相对简单请求多了一次CORS 预检请求。其过程是，首先由浏览器自动发起一个 OPTION 请求，该请求中携带 HTTP 头部字段 Origin。在本例中，前端页面部署在http://localhost:8888服务器上，所以，它的 Origin 字段即为http://localhost:8888。接下来，服务端会返回 Access-Control-Allow-Origin/Access-Control-Allow-Headers/Access-Control-Allow-Methods 等字段，它对应我们后端定义的 AllowOne，注意到这里我们有两个自定义字段 X-ASP-NET-Core 和 X-UserName。在通过预检以后，我们在发起正式请求(本例中为 GET 请求)的时候，设置后端允许的源，即http://localhost:8888，这样就可以实现基于 CORS 的跨域请求啦！\nMDN中对非简单请求的图解\r所以，我们可以注意到，这里会有一个 OPTION 请求，即“预检请求”。对于 AllowOne 这个 CORS 策略而言，它允许来自 localhost:8888 的跨域请求，允许的请求方法有 GET、PUT、POST 和 OPTION，客户端必须携带一个自定义 HTTP 头：X-ASP-NET-Core。当这三个条件满足时，即表示通过“预检”。此时，服务端会返回 Access-Control-Allow-Origin/Access-Control-Allow-Methods/Access-Control-Allow-Headers 等字段。接下来，浏览器发起的正式请求会带上这些字段，并返回我们所需要的 JSON 数据，这就是 CORS 跨域的实际过程。\nOPTION预检请求\r本文小结 这篇文章主要梳理了目前前端跨域的两种主流方案(事实上，在奇葩的前端领域里，最不缺的就是解决方案)，即 JSONP 和 CORS。其中，JSONP 本质上是返回可以执行的 JS，其基本套路是 callback({\u0026ldquo;foo\u0026rdquo;:\u0026ldquo;bar\u0026rdquo;})，利用了 HTML 中含 src 的属性天生具备跨域能力的“漏洞”，是一种相对\u0026quot;hack\u0026quot;的方案，要求预先定义好 callback，需要改造后端接口，仅支持最简单的 GET 请求。而 CORS，是比较“官方”的跨域解决方案，其原理是利用 HTTP 头部字段对请求的来源进行检验，CORS 支持除 GET 以外的请求动词，在使用中间件的情况下，无需修改后端接口，可以在全局或者局部配置 CORS 跨域策略，对后端开发相对友好。自从接触前端领域，对这个领域里的“黑科技”、“骚操作”吐槽无数次了，不过，前后端分离过程中这些事情还是挺有意思的，对吧？好了，以上就是这篇博客里的全部内容了，欢迎大家吐槽！本文中的示例请从：https://github.com/qinyuanpei/dotnet-sse/blob/master/server/index.html这里来获取，谢谢大家！\n","date":"2019-02-26T15:03:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3846545990/","slug":"3846545990","tags":["跨域","CORS","JSONP"],"title":"聊聊前端跨域的爱恨情仇"},{"categories":["编程语言"],"content":"前段时间，为客户定制了一个类似看板的东西，用户可以通过看板了解任务的处理情况，通过 APP 扫面页面上的二维码就可以领取任务，而当任务被领取以后需要通知当前页面刷新。原本这是一个相对简单的需求，可是因为 APP 端和 PC 端是两个不同的 Team 在维护，换句话说，两个 Team 各自有一套自己的 API 接口，前端页面永远无法知道 APP 到底什么时候扫描了二维码，为此前端页面不得不通过轮询的方式去判断状态是否发生了变化。这种方式会发送大量无用的 HTTP 请求，因此在最初的版本里，无论是效率还是性能都不能满足业务要求，最终博主采用一种称为 服务器推送事件(Server-Sent Events) 的技术，所以，在今天这篇文章里，博主相和大家分享下关于 服务器推送事件(Server-Sent Events) 相关的内容。\n什么是 Server-Sent Events 我们知道，严格地来讲，HTTP 协议是无法做到服务端主动推送消息的，因为 HTTP 协议是一种 “请求-响应” 模型，这意味着在服务器返回响应信息以后，本次请求就已经结束了。可是，我们有一种变通的做法，即首先是服务器端向客户端声明，然后接下来发送的是流信息。换句话说，此时发送的不是一个一次性的数据包，而是以数据流的形式不断地发送过来，在这种情况下，客户端不会关闭连接，会一直等着服务器端发送新的数据过来，一个非常相似而直观的例子是视频播放，它其实就是在利用流信息完成一次长时间的下载。那么，Server-Sent Events(以下简称SSE)，就是利用这种机制，使用流信息像客户端推送信息。\n说到这里，可能大家会感到疑惑：WebSocket 不是同样可以实现服务端向客户端推送信息吗？那么这两种技术有什么不一样呢？首先，WebSocket 和 SSE 都是在建立一种浏览器与服务器间的通信通道，然后由服务器向浏览器推送信息。两者最为不同的地方在于，WebSocket 建立的是一个全双工通道，而 SSE 建立的是一个单工通道。所谓单工和双工，是指数据流动的方向上的不同，对 WebSocket 而言，客户端和服务端都可以发送信息，所以它是双向通信；而对于SSE而言，只有服务端可以发送消息，故而它是单向通信。从下面的图中我们可以看得更为直观，在 WebSocket 中数据\u0026quot;有来有往\u0026quot;，客户端既可以接受信息亦可发送信息，而在 SSE 中数据是单向的，客户端只能被动地接收来自服务器的信息。所以，这两者在通信机制上不同到这里已经非常清晰啦！\nWebSocket与SSE对比\rSSE 服务端 下面我们来看看SSE是如何通信的，因为它是一个单工通道的协议，所以协议定义的都是在服务端完成的，我们就从服务端开始吧！协议规定，服务器向客户端发送的消息，必须是 UTF-8 编码的，并且提供如下的 HTTP 头部信息：\nContent-Type: text/event-stream Cache-Control: no-cache Connection: keep-alive 这里出现了一个一种新的MIME类型，text/event-stream。协议规定，第一行的 Content-Type 必须是text/event-stream，这表示服务端的数据是以信息流的方式返回的，Cache-Control 和 Connection 两个字段和常规的HTTP 一致，这里就不再展开说啦！OK，现在客户端知道这是一个 SSE 信息流啦，那么客户端怎么知道服务端发送了什么消息呢？这就要说到 SSE 的消息格式，在 SSE 中消息的基本格式是：\n[field]: value\\n 其中，field 可以取四个值，它们分别是：data、event、id、retry，我们来一起看看它们的用法。\ndata 字段表示数据内容，下面的例子展示 SSE 中的一行和多行数据，可以注意到，当数据有多行时，可以用 \\n 作为每一行的结尾，只要保证最后一行以 \\n\\n 结尾即可。\n：这是一行数据内容 data: SSE给你发了一行消息\\n\\n ：这是多行数据内容 data: {\\n data: \u0026#34;foo\u0026#34;: \u0026#34;foolish\u0026#34;,\\n data: \u0026#34;bar\u0026#34;, 2333\\n data: }\\n\\n event 字段表示自定义事件，默认为 message，在浏览器中我们可以用 addEventListener() 来监听响应的事件，这正是为什么SSE被称为服务器推送事件，因为我们在这里既可以发送消息，同样可以发送事件。\n: GameStart 事件 event: GameStart\\n data: 敌军还有30秒到达战场\\n\\n data: Double Kill\\n\\n : GameOver 事件 event: GaneOver\\n data: You Win！\\n\\n id 字段是一个数据标识符，相当于我们可以给每一条消息一个编号。\nid: 1\\n data: 敌军还有30秒到达战场\\n\\n id: 2\\n data: Double Kill\\n\\n id: 3\\n data: You Win！\\n\\n retry 字段可以指定浏览器重新发起连接的时间间隔，所以，SSE 天生就支持断线重连机制。\nretry: 10000\\n SSE 客户端 SSE 目前是 HTML5 标准之一，所以，目前主流的浏览器(除了IE和Edge以外)都天然支持这一特性，这意味着我们不需要依赖 前端娱乐圈 推崇的各种工具链，就可以快速地使用 SSE 来投入开发。这里需要使用地是 EventSource 对象，我们从下面这个例子开始了解：\nif (\u0026#39;EventSource\u0026#39; in window) { var source = new EventSource(url, { withCredentials: true }); /* open事件回调函数 */ source.onopen = function(){ console.log(\u0026#39;SSE通道已建立...\u0026#39;); }; /* message事件回调函数 */ source.onmessage = function(evt){ console.log(evt.data); } /* error事件回调函数 */ source.onerror = function(evt){ console.log(\u0026#39;SSE通道发生错误\u0026#39;); } /* 自定义事件回调 */ source.addEventListener(\u0026#39;foo\u0026#39;, function (event) { var data = event.data; // handle message },false); /* 关闭SSE */ source.close() } 和各种各样的 HTML5 接口一样，我们需要判断当前的浏览器环境是否支持SSE。建立 SSE 只需要后端提供一个 Url 即可，当存在跨域时，我们可以打开第二个参数：withCredentials，这样 SSE 会在建立通道时携带 Cookie。我们通过实例化后的 source 对象来判断通道是否建立，该对象有一个重要的属性：readyState。当它的取值为 0 时，表示连接还未建立，或者断线正在重连；当它的取值为 1 时，表示连接已经建立，可以接受数据；当它的取值为 2 时，表示连接已断，且不会重连。\n好了，当 SSE 被成功建立以后，首先会触发 open 事件。这里介绍下 SSE 中的关键事件，即 open、message 和 error，我们可以分别通过 onopen、onmessage 和 onerror 这三个回调函数来监听相应的事件。对于 SSE 而言，它是一个单工通道，客户端不能主动向服务端发送信息，所以，一旦建立了 SSE 通道，客户端唯一需要关注的地方就是 onmessage 这个回调函数，因为客户端只需要负责处理消息即可，甚至我们可以连 onerror 都不用关注，因为 SSE 自带断线重连机制，当然你可以选择在发生错误的时候关掉连接，此时你需要 close() 方法。\n我们在上面提到，SSE 在服务端可以定义自定义事件，那么，在浏览器中我们该如何接收这些自定义事件呢？这当然要提到无所不能的 addEventListener，在人肉操作 DOM 的 jQuery 时代，jQuery 中提供的大量 API 在协调不同浏览器间差异的同时，让我们离这些底层的知识越来越远，时至今日，当 qerySelector/querySelectorAll 完全可以替换 jQuery 的选择器的时候，我们是不是可以考虑重新把某些东西捡起来呢？言归正传，在 SSE 中，我们只需要像注册普通事件一样，就可以完成对自定义事件的监听，只要客户端和服务端定好消息的协议即可。\n在 .NET 中集成 Server-Sent Events OK，说了这么多，大家一定感觉有一个鲜活的例子会比较好一点，奈何官方提供的示例都是 PHP 的，难道官方默认 PHP 是世界上最好的编程语言了吗？所谓万变不离其宗\u0026quot;，下面我们以 .NET 为例来快速集成 Server-Sent Events，这里需要说明的是，博主下面的例子采用 ASP.NET Core 2.0 版本编写，首先，我们建一个名为 SSEController 的控制器，在默认的 Index() 方法中，按照 SSE 规范，我们首先组织 HTTP 响应头，然后发送了一个名为 SSE_Start 的自定义事件，接下来，我们每隔 10 秒钟给客户端发送一条消息，请原谅我如此敷衍的 Sleep()：\n[Route(\u0026#34;api/[controller]\u0026#34;)] [ApiController] public class SSEController : Controller { [HttpGet] public IActionResult Index() { //组织HTTP响应头 Response.Headers.Add(\u0026#34;Connection\u0026#34;, \u0026#34;keep-alive\u0026#34;); Response.Headers.Add(\u0026#34;Cache-Control\u0026#34;, \u0026#34;no-cache\u0026#34;); Response.Headers.Add(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/event-stream\u0026#34;); //发送自定义事件 var message = BuildSSE(new { Content = \u0026#34;SSE开始发送消息\u0026#34;, Time = DateTime.Now }, \u0026#34;SSE_Start\u0026#34;); Response.Body.Write(message, 0, message.Length); //每隔10秒钟向客户端发送一条消息 while (true) { message = BuildSSE(new { Content = $\u0026#34;当前时间为{DateTime.Now}\u0026#34; }); Response.Body.Write(message, 0, message.Length); Thread.Sleep(10000); } } } 我们提到，SSE 的数据是按照一定的格式，由 id、event、data 和 retry 四个字段构成的，那么，织消息格式的代码我们放在了 BuildSSE() 方法中，我们来一起看看它的实现：\nprivate byte[] BuildSSE\u0026lt;TMessage\u0026gt;(TMessage message, string eventName = null, int retry = 30000) { var builder = new StringBuilder(); builder.Append($\u0026#34;id:{Guid.NewGuid().ToString(\u0026#34;N\u0026#34;)}\\n\u0026#34;); if (!string.IsNullOrEmpty(eventName)) builder.Append($\u0026#34;event:{eventName}\\n\u0026#34;); builder.Append($\u0026#34;retry:{retry}\\n\u0026#34;); builder.Append($\u0026#34;data:{JsonConvert.SerializeObject(message)}\\n\\n\u0026#34;); return Encoding.UTF8.GetBytes(builder.ToString()); } 可以看到，完全按照 SSE 规范来定义的，这里每次生成一个新的 GUID 来作为消息的 ID，客户端断线后重连的间隔为 30 秒，默认发送的是 \u0026ldquo;消息\u0026quot;，当指定 eventName 参数时，它就表示一个自定义事件，这里我们使用 JSON 格式来传递信息。好了，这样我们就完成了服务端的开发，怎么样，是不是感觉非常简单呢？我们先让它跑起来，下面着手来编写客户端，这个就非常简单啦！\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;DotNet-SSE\u0026lt;/h1\u0026gt; \u0026lt;div id=\u0026#34;result\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; if (\u0026#39;EventSource\u0026#39; in window) { var source = new EventSource(\u0026#39;http://localhost:5000/api/SSE/\u0026#39;); /* open事件回调函数 */ source.onopen = function(){ document.getElementById(\u0026#34;result\u0026#34;).innerHTML+= \u0026#34;SSE通道已建立...\u0026lt;br/\u0026gt;\u0026#34;; }; /* message事件回调函数 */ source.onmessage = function(evt){ document.getElementById(\u0026#34;result\u0026#34;).innerHTML+= \u0026#34;Message: \u0026#34; + event.data + \u0026#34;\u0026lt;br/\u0026gt;\u0026#34;; } /* error事件回调函数 */ source.onerror = function(evt){ document.getElementById(\u0026#34;result\u0026#34;).innerHTML+= \u0026#34;SSE通道发生错误\u0026lt;br/\u0026gt;\u0026#34;; } /* SSE_Start事件回调 */ source.addEventListener(\u0026#39;SSE_Start\u0026#39;, function (event) { document.getElementById(\u0026#34;result\u0026#34;).innerHTML += \u0026#34;SSE_Start: \u0026#34; + event.data + \u0026#34;\u0026lt;br/\u0026gt;\u0026#34;; },false); } \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 此时，不需要任何现代前端方面的技术，我们直接打开浏览器，就可以看到：\nSSEDemo\r更为直观的，我们可以通过Chrome开发者工具观察到实际的请求情况，相比普通的 HTTP 请求，SSE 会出现一个名为 EventStream 的选项卡，这是因为我们在服务端设置的 Content-Type 为 text/event-stream 的缘故，可以注意到，我们定义的id(GUID)会在这里显示出来：\n有点与众不同的SSE\r同类技术优劣对比 OK，这篇文章写到这里，相信大家已经对 SSE 有了一个比较具体的概念，那么，我们不妨来梳理下相关的同类技术。一路走过来，我们大体上经历了 (短)轮询、长轮询/Comet、SSE 和 WebSocket。\n(短)轮询这个比较容易理解了，它从本质上来讲，就是由客户端定时去发起一个 HTTP 请求，这种方式是一种相对尴尬的方式，为什么这样说呢？因为时间间隔过长则无法保证数据的时效性，而时间间隔过短则会发送大量无用的请求，尤其是当客户端数量比较多的时候，这种方式很容易耗尽服务器的连接数。\n而长轮询 则是(短)轮询的一个变种，它和(短)轮询最大的不同在于，服务端在接收到请求以后，并非立即进行响应，而是先将这个请求挂起，直到服务器端数据发生变化时再进行响应。所以，一个明显的优势是，它相对地减少了大量不必要的HTTP请求，那么，它是不是就完美无暇了呢？当然不是，因为服务端会将客户端发来的请求挂起，因此在挂起的那些时间里，服务器的资源实际上是被浪费啦！\n严格地说，SSE 并不是一门新技术，为什么这样说呢？因为它和我们基于 HTTP 长连接的 Push 非常相似。这里又提到一个新概念，HTTP 长连接，其实，这个说法病逝非常严谨，因为我们知道 HTTP 最早就是一个请求-响应模型，直到 HTTP1.1 中增加了持久连接，即 Connection:keep-alive 的支持。所以，我们这里说的长连接、短链接实际上都是指 TCP 的长连接还是短连接，换句话说，它和客户端没有关系，只要服务端支持长连接，那么在某个时间段内的 TCP 连接实际上复用的，进而就能提高 HTTP 请求性能，曾经我们不是还用 iframe 做过长连接吗？\nWebSocket 作为构建实时交互应用的首选技术，博主曾经在《基于WebSocket和Redis实现Bilibili弹幕效果》一文中有所提及，WebSocket 相比前面这些技术，最大的不同在于它拥有专属的通信通道，一旦这个通道建立，客户端和服务端就可以互相发送消息，它沿用了我们传统的 Socket 通讯的概念和原理，变被动为主动，无论是客户端还是服务端，都不必再被动地去\u0026rdquo;拉\u0026quot; 或者 \u0026ldquo;推\u0026rdquo; 。在这个过程中，出现了像 SignalR/SocketIO 等等的库，它们主打的兼容性和降级策略，曾经一度让我们感到亲切，不过随着 WebSocket 标准化的推进，相信这些最终都会被原生 API 所替代吧，也许是有生之年呢？谁知道未来是什么样子呢？\n下面给出针对以上内容的 \u0026ldquo;简洁\u0026rdquo; 版本：\n（短)轮询 长轮询/Comet SSE WebSocket 浏览器支持 全部 全部 除IE/Edge 现代浏览器 是否独立协议 HTTP HTTP HTTP WS 是否轻量 是 否 是 否 断线重连 否 否 是 否 负载压力 占用内存/请求数 同（短)轮询 一般 同SSE 数据延迟 取决于请求间隔 同（短)轮询 实时 实时 本文小结 正如本文一开始所写，博主使用SSE是因为业务上的需要，在经历了轮询带来的性能问题以后，博主需要一款类似 WebSocket 的东西，来实现服务端主动向客户端推送消息，究其原因，是因为浏览器永远都不知道，App 到底什么时候会扫描二维码，所以，从一开始我们试图让网页去轮询的做法，本身就是不太合理的。那么，为什么没有用 WebSocket 呢？因为 WebSocket 需要一点点 框架 层面的支持，所以，我选择了更为轻量级的 SSE，毕竟，这比让其它 Team 的同事去调整他们的后端接口要简单的多。我之前参与过一部分 WebSocket 相关的项目，我深切地感受到，除了在浏览器的兼容性问题以外，因为 WebSocket 使用的是独有的 WS 协议，所以，我们常规的 API 网关其实在这方面支持的都不是很好，更不用说鉴权、加密等等一系列的问题啦，而 SSE 本身是基于 HTTP 协议的，我们目前针对 HTTP 的各种基础设施，都可以直接拿过来用，这应该是我最大的一点感悟了吧，好了，这篇文章就是这样啦，谢谢大家，新的一年注定要重新开始的呢\u0026hellip;\u0026hellip;\n参考文章 IBM - Comet：基于 HTTP 长连接的“服务器推”技术 Mozilla - 使用服务器发送事件 阮一峰 - Server-Sent Events 教程 呆呆_小茗 - Ajax轮询，Ajax长轮询和Websocket(详细使用) hrhguanli - HTTP长连接和短连接 ","date":"2019-01-18T13:46:44Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3175881014/","slug":"3175881014","tags":["WebSocket","SSE","后端"],"title":"基于 Server-Sent Events 实现服务端消息推送"},{"categories":["独立博客"],"content":" 去年国庆的时候，七牛官方开始回收测试域名，这直接导致博客中大量图片出现无法访问的情况，虽然博主第一时间启用了新的域名：https://blog.yuanpei.me，可是因为七牛官方要求域名必须备案，所以，这件事情一直耽搁着没有往下进行。至于为什么会一直拖到 2019 年，我想大家都能猜到一二，没错，我就是懒得去弄域名备案这些事情:joy:。最近花了点时间，把博客里的图片从七牛和 CSDN 迁移了出来，所以，今天这篇博客，主要想和大家分享下这个折腾的过程，如果能帮助到和我一样，因为七牛官方回收了域名而无法导出图片的朋友，在下开心之至。虽然今天没有回望过去，没有给新的一年立 flag，就如此平淡地过渡到了 2019 年，可或许这才是生活本来的样子吧！\n七牛的测试域名被官方回收了以后，我们有两种思路去导出这些图片，其一，是临时像官方提工单申请一个测试域名，这样在测试域名被回收前，我们可以直接使用官方提供的qrsctl或者qshell工具进行批量导出，因为此时我们可以直接在配置文件里配置测试域名，具体可以参考这篇文章：跑路之后七牛图片如何导出备份至本地，甚至你可以直接到七牛的管理控制台手动下载，可这样就一点都不极客了对吗？我们是一生追求做极客的人好伐。其二，同样是借助官方提供的qshell工具，因为没有域名，我们没有办法批量导出，可是工具中提供了两个非常有用的命令，它们分别是：qshell listbucket、qshell get。通过这两个命令，我们就可以列举出指定 bucket 中的文件以及下载指定文件，所以，这就是我们的第一步，首先把图片从七牛那里导出到本地。以博主的blogspace为例：\nqshell account \u0026lt;ak\u0026gt; \u0026lt;sk\u0026gt; \u0026#39;qinyuanpei@163.com\u0026#39; /* 请使用你的ak/sk，谢谢 */ qshell listbucket blogspace 使用listbucket列举指定bucket内文件\r事实上，通过第一列的 Key，即文件名，我们就可以下载该资源到本地，因为七牛实际上是采用对象存储的方式来组织资源的，这里我们以第一张图片05549344-BF85-4e8c-BCBC-1F63DFE80E43.png为例：\nqshell get blogspace 05549344-BF85-4e8c-BCBC-1F63DFE80E43.png 默认情况下，该图片会下载到当前目录下，本地文件和远程文件名保持一致。当然，我们还可以通过-o 参数来指定输出文件：\n使用get命令下载指定文件\r好了，有了这个基础，我们就可以着手博客图片的迁移啦。博主最初的想法是，先获取到指定 bucket 下的全部文件，然后再对结果进行拆分，循环执行 qshell get 命令，可惜再 PowerShell 下并没有类似 grep 的命令，所以，这个想法放弃。其实，你仔细观察七牛图片外链的格式就会发现，除了域名部分以外，剩下的就是该文件在 bucket 里对应的 key 啦，所以，博主的想法开始从 Markdown 文件入手，最终我们的思路是，解析博客对应的 Markdown 文件，通过正则匹配所有的图片链接，截取出图片的文件名并通过 qshell 下载到本地。人生苦短，我用 Python。具体写出来，大概是下面这个样子：\ndef sync(root,ak,sk,account,bucket): files = [] children = os.listdir(root) for child in children: path = os.path.join(root,child) if os.path.isfile(path): files.append(path) for file in files: links = [] newContent = \u0026#39;\u0026#39; with open(file,\u0026#39;rt\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) as fp: content = fp.read() matches = re.compile(\u0026#39;!\\\\[.*?\\\\]\\\\((.*?)\\\\)\u0026#39;).findall(content) if(len(matches)\u0026gt;0): links.extend(matches) for link in links: fileKey = link.split(\u0026#39;/\u0026#39;)[-1] if(\u0026#39;http://img.blog.csdn.net\u0026#39; in link): newLink = sync_csdn(link) newContent = content.replace(link,newLink) elif(\u0026#39;clouddn.com\u0026#39; in link): newLink = sync_qiniu(ak,sk,account,bucket,fileKey) newContent = content.replace(link,newLink) if(newContent != \u0026#39;\u0026#39; and len(links) \u0026gt; 0): with open(file,\u0026#39;wt\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) as fp: fp.write(newContent) print(\u0026#39;已自动完成对{0}中图片链接的自动更新\u0026#39;.format(file)) 因为博主的博客，在此之前(指 2012 年~2018 年，暴露年龄啦ORZ)，主要都在：https://blog.csdn.net/qinyuanpei这里维护，所以，这次就一并通过脚本处理啦。这部分我们这里不用太关注，对于托管在七牛上的图片资源，我们通过sync_qiniu方法来完成同步：\ndef sync_qiniu(ak,sk,account,bucket,fileKey): os.system(\u0026#39;qshell account {0} {1} {2} -w\u0026#39;.format(ak,sk,account)) outfile = root + \u0026#34;\\\\download\\\\blogspace\\\\\u0026#34; + fileKey outfile = outfile.replace(\u0026#39;\\\\\u0026#39;,\u0026#39;/\u0026#39;) if(os.path.exists(outfile)): os.remove(outfile) os.system(\u0026#39;qshell get {0} {1} -o {2}\u0026#39;.format(bucket,fileKey,outfile)) print(\u0026#34;同步七牛图片{0}完成\u0026#34;.format(fileKey)) pid = upload(outfile) if(pid != None): print(\u0026#39;同步后的图片链接为:\u0026#39; + upload(outfile)) return pid 博客中的图片导出到本地以后，我们就要开始考虑第二个问题，这些图片要放到哪里去，直接和博客放在一起，估计早晚会突破 Github 单个仓库 1G 的限制。七牛增加域名备案的限制以后，像又拍云这种同类产品必须会跟进这个feature，原因嘛，我想大家都知道不必多说。大概考虑了像阿里云和腾讯云的 OSS 类产品，因为我司的产品最近正在着手从自有的 FTP 上切换到阿里云的 OSS 上，主要是考虑服务器的维护成本，但对博主这样的个人用户而言，这类 OSS 产品实在太贵了，最终，博主选择国内某知名社交平台提供的**\u0026ldquo;图床服务\u0026rdquo;**。参考这篇文章：PHP 上传图片到微博图床，最终实现了一个简洁(陋)的版本：\ndef upload(src_file): url = \u0026#34;http://picupload.service.weibo.com/interface/pic_upload.php\u0026#34; fileExt = src_file.split(\u0026#39;.\u0026#39;)[-1] if(fileExt == \u0026#39;png\u0026#39;): fileExt = \u0026#39;jpg\u0026#39; timestamp = str(int(time.time())) mimes = {\u0026#34;gif\u0026#34;:\u0026#39;image%2Fgif\u0026#39;,\u0026#39;jpg\u0026#39;:\u0026#39;image%2Fjpeg\u0026#39;,\u0026#39;jpeg\u0026#39;:\u0026#39;image%2Fjpeg\u0026#39;} querystring = {\u0026#34;mime\u0026#34;:mimes[fileExt],\u0026#34;data\u0026#34;:\u0026#34;base64\u0026#34;,\u0026#34;url\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;markpos\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;logo\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;nick\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;marks\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;app\u0026#34;:\u0026#34;miniblog\u0026#34;,\u0026#34;cb\u0026#34;:\u0026#34;http://weibo.com/aj/static/upimgback.html?_wv=5\u0026#34;,\u0026#34;callback\u0026#34;:\u0026#34;STK_ijax_\u0026#34; + timestamp} headers = { \u0026#39;Cookie\u0026#39;: \u0026#34;在这里填入Cookie\u0026#34;, } files = {\u0026#39;pic1\u0026#39;:open(src_file,\u0026#39;rb\u0026#39;).read()} response = requests.request(\u0026#34;POST\u0026#34;, url, headers=headers, params=querystring,files=files) if(response.status_code == 200): result = re.sub(r\u0026#34;\u0026lt;meta.*\u0026lt;/script\u0026gt;\u0026#34;, \u0026#34;\u0026#34;, response.text, flags=re.S) image_result = json.loads(result) image_id = image_result.get(\u0026#39;data\u0026#39;).get(\u0026#39;pics\u0026#39;).get(\u0026#39;pic_1\u0026#39;).get(\u0026#39;pid\u0026#39;) return \u0026#39;https://ww1.sinaimg.cn/large/{0}.{1}\u0026#39;.format(image_id,fileExt) 如果你现在访问我的博客，大概就会发现，之前那些无法显示的图片，现在基本上都可以显示啦，而我所做的事情，就是执行这些 Python 脚本，让它帮我完成从图片下载、上传再到替换链接的所有事情。感觉配合 Typora 在插入图片时可以拷贝到指定目录的功能，完全可以支持本地图片链接自动替换的功能，这样子以后写博客的时候，只要插入准备后的本地图片就好了，真是想想都觉得美好呢？我和一位朋友分享了这个想法，他觉得**\u0026ldquo;微博图床\u0026rdquo;**并不靠谱，这样说起来，它最不好的地方大概就是没有办法保留原有的文件名，所以，万一将来有一天它挂了，你要恢复这些图片会比较麻烦，一个好的建议是维护一个数据库，譬如 SQLite 足矣，把本地文件名和远程文件名对应起来，甚至你可以把图片的 Base64 编码存储到数据库里呢，对吧？\n好了，以上就这 2019 年第一篇碎碎念啦，为了证明我的思维的确是跳跃的，最后的最后，强烈地向大家安利两个图床工具：WeiBox、PicGo，它们都支持微博图床，所以，你猜我是用哪个工具上传这篇文章里地图片的呢？当然是脚本啊，那样不就不那么极客了嘛，我们是一生追求做极客的人好伐！:v:\n","date":"2019-01-18T09:27:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3444626340/","slug":"3444626340","tags":["七牛","图床","Python"],"title":"博客图片迁移折腾记"},{"categories":["数据存储"],"content":" 各位朋友，大家好，欢迎大家关注我的博客，我是 Psyne，我的博客地址是https://blog.yuanpei.me。在上一篇博客中，我们提到了通过 DbCommandInterceptor 来实现 EF 中 SQL 针对 SQL 的“日志”功能。我们注意到，在这个拦截器中，我们可以获得当前数据库的上下文，可以获得 SQL 语句中的参数，更一般地，它具备“AOP”特性的扩展能力，可以在执行 SQL 的前后插入相应的动作，这就有点类似数据库中触发器的概念了。今天，我们主要来说一说，基于 EF 实现数据库主从复制和读写分离，希望这个内容对大家有所帮助。\n主从复制 ＆ 读写分离 首先，我们先来了解一个概念：主从复制。那么，什么是主从复制呢？通常，在只有一个数据库的情况下，这个数据库会被称为主数据库。所以，当有多个数据库存在的时候，数据库之间就会有主从之分，而那些和主数据库完全一样的数据库就被称为从数据库，所以，主从复制其实就是指建立一个和主库完全一样的数据库环境。\n那么，我们为什么需要主从复制这种设计呢？我们知道，主数据库一般用来存储实时的业务数据，因此如果主数据库服务器发生故障，从数据库可以继续提供数据服务，这就是主从复制的优势之一，即作为数据提供灾备能力。其次，从业务扩展性上来讲，互联网应用的业务增长速度普遍较高，随着业务量越来越大，I/O 的访问频率越来越高，在单机磁盘无法满足性能要求的情况下，通过设置多个从数据库服务器，可以降低磁盘的 I/O 访问频率，进而提高单机磁盘的读写性能。从业务场景上来讲，数据库的性能瓶颈主要在读即查询上，因此将读和写分离，能够让数据库支持更大的并发，这对优化前端用户体验很有意义。\n通常来讲，不同的数据库都在数据库层面上实现了主从复制，各自的实现细节上可能会存在差异，譬如 SQLServer 中可以通过“发布订阅”来配置主从复制的策略，而 Oracle 中可以通过 DataGurd 来实现主从复制，甚至你可以直接把主库 Dump 出来再导入到从库。博主没有能力详细地向大家介绍它们的相关细节，可博主相信“万变不离其宗”的道理，这里我们以 MySQL 为例，因为它在互联网应用中更为普遍，虽然坑会相应地多一点:)……\nMySQL 中有一种最为重要的日志 binlog，即二进制日志，它记录了所有的 DDL 和 DML(除查询以外)语句，通过这些日志，不仅可以作为灾备时的数据恢复，同样可以传递给从数据库来达到数据一致的目的。具体来讲，对于每一个主从复制的连接，都有三个线程，即拥有多个从库的主库为每一个从库创建的binlog 输出线程，从库自身的IO 线程和SQL 线程：\n当从库连接到主库时，主库就会创建一个线程然后把 binlog 发送到从库，这是 binlog 输出线程。 当从库执行 START SLAVER 以后，从库会创建一个 I/O 线程，该线程连接到主库并请求主库发送 binlog 里面的更新记录到从库上。从库 I/O 线程读取主库的 binlog 输出线程发送的更新并拷贝这些更新到本地文件(其中包括 relay log 文件)。 从库创建一个 SQL 线程，这个线程读取从库 I/O 线程写到 relay log 的更新事件并执行。 EF 中主从复制的实现 虽然从数据库层面上做主从复制会更简单一点，可在很多时候，这些东西其实更贴近 DBA 的工作，而且不同数据库在操作流程上还都不一样，搞这种东西注定不能成为“通用”的知识领悟。对开发人员来说，EF 和 Dapper 这样的 ORM 更友好一点，如果可以在 ORM 层面上做触发器和存储过程，可能 SQL 看起来就没有那么讨厌了吧！博主的公司因为要兼顾主流的数据库，所以，不可能在数据库层面上去做主从复制，最终我们是通过 EF 来实现主从复制。\n其实，讲了这么多主从复制的原理，对我们来说，这篇文章的实现则是非常简单的。因为通过 DbCommandInterceptor 我们能拦截到 SQL 命令，所以，只要是 Select 命令全部走从库，Insert/Update/Delete 全部走主库，这样就实现了读写分离。怎么样，是不是感觉相当简单啊！当然，前提是要准备好主从库的屋里环境，这些就让 DBA 去折腾吧(逃。好了，下面一起来看具体代码，首先我们定义一个主从库管理类 MasterSlaveManager：\npublic static class MasterSlaveManager { private static MasterSalveConfig _config =\u0026gt; LoadConfig(); /// \u0026lt;summary\u0026gt; /// 加载主从配置 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;fileName\u0026#34;\u0026gt;配置文件\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; public static MasterSalveConfig LoadConfig(string fileName = \u0026#34;masterslave.config.json\u0026#34;) { if (!File.Exists(fileName)) throw new Exception(string.Format(\u0026#34;配置文件{0}不存在\u0026#34;, fileName)); return JsonConvert.DeserializeObject\u0026lt;MasterSalveConfig\u0026gt;(File.ReadAllText(fileName)); } /// \u0026lt;summary\u0026gt; /// 切换到主库 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;command\u0026#34;\u0026gt;DbCommand\u0026lt;/param\u0026gt; public static void SwitchToMaster(DbCommand command, string serverName = \u0026#34;\u0026#34;) { var masterServer = string.IsNullOrEmpty(serverName) ? _config.Masters.FirstOrDefault() : _config.Masters.FirstOrDefault(e =\u0026gt; e.ServerName == serverName); if (masterServer == null) throw new Exception(\u0026#34;未配置主库服务器或者服务器名称不正确\u0026#34;); //切换数据库连接 ChangeDbConnection(command, masterServer); } /// \u0026lt;summary\u0026gt; /// 切换到从库 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;command\u0026#34;\u0026gt;DbCommand\u0026lt;/param\u0026gt; public static void SwitchToSlave(DbCommand command, string serverName = \u0026#34;\u0026#34;) { var salveServer = string.IsNullOrEmpty(serverName) ? _config.Slaves.FirstOrDefault() : _config.Slaves.FirstOrDefault(e =\u0026gt; e.ServerName == serverName); if (salveServer == null) throw new Exception(\u0026#34;未配置从库服务器或者服务器名称不正确\u0026#34;); //切换数据库连接 ChangeDbConnection(command, salveServer); } /// \u0026lt;summary\u0026gt; /// 切换数据库连接 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;command\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;dbServer\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; private static void ChangeDbConnection(DbCommand command, DbServer dbServer) { var conn = command.Connection; if (conn.State == System.Data.ConnectionState.Open) conn.Close(); conn.ConnectionString = dbServer.ConnectionString; conn.Open(); } } 接下来，和之前关于 EF 中的 SQL 拦截器类似，我们定义一个名为 MasterSlaveDbInterceptor 的拦截器：\npublic class MasterSlaveDbInterceptor : DbCommandInterceptor { public override void NonQueryExecuting(DbCommand command, DbCommandInterceptionContext\u0026lt;int\u0026gt; interceptionContext) { //Insert/Update(写操作)走主库 MasterSlaveManager.SwitchToMaster(command); base.NonQueryExecuting(command, interceptionContext); } public override void ScalarExecuting(DbCommand command, DbCommandInterceptionContext\u0026lt;object\u0026gt; interceptionContext) { //Select(读操作)走从库 var sqlText = command.CommandText; if (!sqlText.ToUpper().StartsWith(\u0026#34;INSERT\u0026#34;) || !sqlText.ToUpper().StartsWith(\u0026#34;UPDATE\u0026#34;)) MasterSlaveManager.SwitchToSlave(command); base.ScalarExecuting(command, interceptionContext); } public override void ReaderExecuting(DbCommand command, DbCommandInterceptionContext\u0026lt;DbDataReader\u0026gt; interceptionContext) { //Select(读操作)走从库 var sqlText = command.CommandText; if (!sqlText.ToUpper().StartsWith(\u0026#34;INSERT\u0026#34;) || !sqlText.ToUpper().StartsWith(\u0026#34;UPDATE\u0026#34;)) MasterSlaveManager.SwitchToSlave(command); base.ReaderExecuting(command, interceptionContext); } } 至此，我们就实现了基于 EF 的数据库主从复制、读写分离。其实，更严谨的说法是，主从复制是在数据层面上完成的，而读写分离则是在代码层面上完成。当然，实际应用中需要考虑事务、数据库连接等因素，这里我们仅仅提供一种思路。这里我们的配置文件中，对主、从数据库进行了简单配置，即一主一从。在实际应用中，可能我们会遇到一注多从的情况，在这个基础上，我们又可以延申出新的话题，譬如在存在多个从库的情况下，通过心跳检测来检查从库服务器的健康状态，以及如何为不同的从库服务器设置权重，实现多个从库服务器的负载均衡等等。我们在微服务中提出的**“健康检查”和“负载均衡”**等概念，其实都可以映射到这里来，我想这是真正值得我们去深入研究的地方。\n本文小结 并没有，いじょう\n","date":"2018-10-18T08:41:08Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2418566449/","slug":"2418566449","tags":["EF","读写分离","主从复制"],"title":"基于 EF 的数据库主从复制、读写分离实现"},{"categories":["生活感悟"],"content":"\r上周国庆节放假，在家里看了黄渤的导演处女座《一出好戏》，虽然网上对它的评价还不错，可当我真正看完了这部电影，我反而觉得它更适合一个人的时候去看，因为在“演而优则导”的大趋势下，越来越多的人都在尝试“触电”，前有角逐奥斯卡的《逐梦演艺圈》，后有倒卖情怀的《爱情公寓》大电影，在这样一个充斥着“浮躁”的娱乐时代，有这样一类充满哲学意味的电影，可以说这是中国电影里的“净土”。不知道是不是因为年龄大了的缘故，对打打杀杀的动作戏显得视觉疲劳，有时候更愿意看点安静平淡的东西，这部电影所吸引我的，是某个镜头里无比空旷而迷离的故事和人们。\n“一出好戏”这四个字一出，陡然间有种开了上帝视角的感觉。其实，我们总以为在看戏里的别人，我们又何尝不是这戏里的人？就像人们觉得大街上耍猴有意思，人们看着被驯化的猴子，遵从人们的指令做出各种动作，人们瞬间有了种睥睨众生的感觉，可明明人类就是由猿猴进化而来，想到这一点，你忽然觉得人类世界的奇妙之处，即人类以自身有限的认知，所勾勒和描绘出的这个世界，其实永远都是冰山一角，在某种情况下，这种体系的崩塌，我让我们不得不重新审视自我的合理性，所谓“你站在桥上看风景，看风景的人在楼上看你 。明月装饰了你的窗子，你装饰了别人的梦”。\n整个故事架设在一个现代文明被摧毁的世界里，因为天外陨石落入海洋引发巨大海啸，男主马进及公司参与团建的众人，被困在一个与世隔绝的孤单上面。故事从男主颇具“屌丝”气质的自嘲开始，男主名叫马进，是一个在公司里基本没有存在感的人，连想给心仪的女生买饮料都要拐弯抹角，渴望通过买彩票实现一夜暴富的理想。这的确是一个普通社会阶层的真实写照，如男主所言，即便陨石真的落到地球上，像男主这样一穷二白的人，真的是穷到无所畏惧，可转眼间发现自己中了 6000 万彩票，同时流落到一个荒无人烟的孤单，如此喜剧性的一幕，无非是在质问我们，如果整个世界都遭遇崩塌，我们所在乎的这些，又到底该算作什么呢？\n我们不妨看看众人的反应，面对这样一个突如其来的灾难，首先被诘难的是史教授，它代表的是某种社会权威。接下来，被诘难的是张总，它代表是组织权威。这其实可以理解为一个群体引发的信任危机，史教授作为这个集体中的高端人才，他必须在抚慰群众人情绪和顾及自身尊严这两点上找到一个平衡，所以，他的做法是向大家“部分地”承认错误。而张总从一开始的认为“有钱可以搞到船”，到最后在岛上绝望地散尽“千金”，这同样是因为群体性的信任危机，所以，第一个点是，群体性的“信任危机”往往代表着某种体系崩溃的开始。马进身上的矛盾恰恰就在于此，他带着一个可能已经崩塌的世界里的产物——彩票，来到了一个全新的世界里，而在这个世界里，彩票是 undefined，彩票映射的是作为一般等价物的货币，同样，货币在这个世界里是 undefined。\n所以，电影所描绘的，其实更像是人类从无到有的演化过程。在众人缺乏食物和淡水的情况下，因为服过兵役而具备野外生存能力的小王，第一次成为了这个群体中的领袖。这象征的是人类历史上，以“体力”作为主要角逐指标的蛮荒时代。这里想吐槽下编剧的恶趣味，王根基这个名字实在是一言难尽。小王的策略是“想吃就自己干”，这其实是人类早期以采集狩猎为主的生活场景的反映，在这个阶段，对体力的重视超过对智力的重视，所以，史教授作为知识分子的身份第一次遭遇解离。人们普遍尊重劳动的价值，所以，两位没有参与劳动的“老总”，作为职场上司的身份遭遇第一次解离。同样，集体中身材最好的女性 Lucy，被老潘一把推给“王”，这是早期推崇“体力”的体系中，对性资源的一种绝对占有。\n那么，在推崇“体力”的体系中，满足了人们的口腹之欲后，人们很快厌倦了这种满足感。以张总为代表的“智力”派，开始向着推崇“智力”的文明时代演变。在这个过程中，主要有两点，第一，是通过“期权”和“蓝图”发展出第一批员工。第二，是以扑克牌的形式定义了货币体系。这两点是整个社会转变的重要历程，第一点张总可以和“王”分庭抗礼的基础，而第二点则使得人们告别了“以物易物”的时代。张总利用马进想要离开孤单的心理，拉拢了岛上半数左右的人，可实际上他从未想过要带大家离开，这像不像现代职场里给期权的画饼方式？第二点，货币的出现让整个岛上的物品出现了流通。电影里有个人问，一张扑克牌能换多少鱼，张总回答说，这是有它的价值来决定的，这可以理解为经济学中的一个理论，即价格是由供需关系来决定的，故事到这里，我们已然看到了人类经济社会的雏形。\n如果说在“体力”时代，人们角逐方式就是争斗，小到人与人之间的决斗，大到部落与部落间的战争，那么，到了“智力”时代，人们开始追求更高阶的竞争。或许竞争是万物的天性，所谓“物竞天择，适者生存”，从小就被教育这套弱肉强食的丛林法则，更是在宫斗、商斗等一众“争斗”里不亦乐乎，当真是“与天奋斗，其乐无穷；与地奋斗，其乐无穷；与人奋斗，其乐无穷”啊……那么，当岛上两拨人为了食物而大打出手的时候，到底是生存更重要还是道德更重要？为什么进化到“智力”时代的人们，依然要面对像强盗一样的“体力”时代的人们？人类历史上有多少自诩正义的行为，其实带来的都是流血和牺牲？所谓“鹬蚌相争，渔翁得利”，最终是马进在这场争斗中获利。\n回头来看马进的行为，其实是群体中对优质资源的逐步占有的过程。如马进所言，只要是岛上生产不出的东西，都是宝贝。这里有两层意思，第一，物以稀为贵。第二，社会中的原始资源是有限的，只有不断创造新资源，整个社会的资源才能得以流通。虽然在电影中马小兴修坏了小王的水陆两用车，但其实电影想说的是，马小兴是所有人里唯一一个懂得现代科技的人，所以，马小兴的黑化，其实是马进人性中的阴暗面而已，当你真的拥有了摆布他人的能力，你是否又能守住道德的底线。\n马进成为取代“王”和张总，成为新的领袖，从故事上来说，多少有点宗教的意味。首先，在光影的掩映下以高姿态俯视众人，这是神化的过程，马进从一个不被人待见的无名之辈，瞬间转变为充满神性的人，神到了什么地步呢？甚至连女神珊珊都开始主动向他求爱。众人穿着白色竖纹衣服的那一幕真的很美好，就连 Lucy 跳绳的镜头我都看了好几遍😉。有没有觉得马进一手持书的样子像极了耶稣，如果说耶稣通过圣经来向世人传道，那么马进对着黑暗中众人的一番演讲，是否具有类似的教化的作用，马进再一次“刷新”了活着的定义，引导人们去重见光明、寻找新大陆和重建家园，至此，我们仿佛过渡到了追求精神文明的阶段。\n如果说，故事永远按照现在的方向去发展，那么或许在某个时刻人们就会回到“新大陆”，重新经历一次体系的重建。可生活从来都不是童话，导演用马小兴的黑化，完美地打破了这种想当然。或许有人会说，这是人类演化的必然，因为集体主义消亡以后，必然会导致精致的利己主义的产生，可即便如此，人们依然怀念着那个“罗曼蒂克”的时代，不是因为人们想要这样对别人，仅仅是希望别人这样对待自己而已。在张总主导的这个阶段，马进曾因为张总不愿意带大家离开，而被张总手下一通暴打，可其实他同样迷失在这份“岁月静好”中，所以，他不愿意让大家相信真的有大船经过，而这种内心的纠结，则完全交由马小兴来完成，主要事件有两个，其一是勒索张总资产，其二是小王“被得精神病”。电影快结束的时候，马进一直在重复“真的”还是“假的”，当一个人认识到这个社会的复杂性，当一个人在真真假假中来回穿梭的时候，或许他就会和马进一样，真的就分不出“真假”，可让马进走上神坛的电灯，和让小王变成“精神病”的电，难道不是一样的吗？\n在圣经故事里，耶稣最后被他的门徒犹大背叛，最终被人们钉上十字架，这是不是和马进被马小兴背叛特别相似？最后，岛上只有马进和珊珊的那一幕，我个人更倾向于理解为，这是马进内心的真实写照，他愿意和珊珊一起在这个岛上生活，可他并不愿意让其他人永远困在这个岛上，所以，这是他心中的一种美好的想象，真实的结局是大家一起乘船离开了荒岛，可据说船上放烟花，据说是泰坦尼克号事件以后的一种约定成俗，意思是一种求救信号，所以，这艘大船并不存在？结尾，众人一起到医院看马小兴，马小兴暂时失忆，大概导演是不想他想起那些“阴暗”的记忆，可如果这是暂时失忆，就是说终究会再想起来的，那么，结合马小兴的经历，或许马小兴内心深处就是这样的？众人惊愕地停在一圈儿精神病人面前，大概每个人面对荒诞的时候，都不能确定自己这算不算荒诞吧，可谁叫这就是个反乌托邦的荒诞故事呢？你说，这故事在那只蜥蜴的眼睛里又会是什么样子呢？\n","date":"2018-10-11T09:02:05Z","image":"/posts/1127467740/P2531096332.jpg","permalink":"https://qinyuanpei.github.io/posts/1127467740/","slug":"1127467740","tags":["影评","电影","一出好戏"],"title":"戏里戏外的一出好戏"},{"categories":["开发工具"],"content":" 使用 SourceTree 有一段时间啦，从界面舒适度和易用性两个方面来看，的确要比小乌龟更好一点，日常配合命令行来使用，基本能覆盖到各种使用场景，譬如分支、版本、变基、合并等等。我本人在工作中接触到的 Git 工作流，大体上可以分为两类，从最早是官方所推崇的 5 个分支的 Git Workflow，到如今在 Github 上更为流行的 PR(Pull Request)。这两种方式，实际使用中各有优劣吧，而且这个话题似乎更适合专门写一篇文章来说。\n我真正想说的是，我需要一个优雅的 Diff 和 Merge 工具。虽然，对一个使用命令行的人来说，使用 git diff 来展示差异对比已经完全足够啦，可在某些需要解决冲突的场合，命令行就显得有点力不从心。我个人一直不习惯小乌龟的合并工具，因为使用起来总觉得相当别扭。直到我发现，VSCode 可以在打开冲突文件的时候，自动提示解决冲突的选项，我觉得我开始喜欢上这个工具啦。所以，平时我解决冲突的做法是，在命令行里找到冲突的文件，然后逐一用 VSCode 打开来解决冲突。\n现在，使用 SourceTree 的时候，周围同事大部分都习惯 GUI 操作，所以，就想能不能把 SourceTree 和 VSCode 结合着来用，因为我发现 SourceTree 可以支持外部的 Diff 和 Merge 工具。其实，小乌龟一样是支持的，关键是配置太难用啦！SourceTree 支持的 Merge 工具里有鼎鼎大名的 P4Merge，不过我发现一来官网完全打不开(需要翻墙)，二来界面相当复古我不喜欢，而 SourceTree 默认的 Merge 工具其实就是小乌龟里的，所以，请允许我如此任性的折腾吧！\n首先，确保你安装了 VSCode，这显然是一句废话，可对于博主来说，这是唯一可以替代 Sublime Text 的代码编辑器，想想可以写 Markdown、写 Python、写 JS、写.NET Core，简直不能更美好了好嘛？然后，我们在 SourceTree 里做如下配置，这里我们直接让 VSCode 作为我们的 Diff 和 Merge 工具，具体参数如图所示：\nSourceTree配置图示\r好了，现在我们就可以在 SourceTree 里愉快地使用 VSCode 啦，感受一下这如德芙一般的纵想丝毫，从现在开始，彻底忘掉小乌龟那丑陋的合并工具吧！\nVSCode解决冲突\rVSCode差异比较\r既然，作为 Git 可视化工具的 SourceTree 可以使用 VSCode 作为 Diff 和 Merge 的工具，那么，我们干脆一鼓作气，将 VSCode 作为 Git 默认的 Diff 和 Merge 的工具吧！熟悉 Git 命令行的朋友一定遇到过这样的场景，有时候，我们执行完 git merge 以后，命令行会采用 Vim 的方式来进行交互，这是因为 Git 默认的编辑器就是 Vim，为什么是 Vim 呢？因为 Git 和 Linux 一样，都出自 Linus 大神之手啊！所以，这句话的意思是，我们可以给 Git 配置外部工具，譬如小乌龟、P4Merge 等等，这里直接给出相关命令：\n//Merge时不创建备份文件 git config --global al mergetool.keepBackup fap false //配置Diff工具 git config --global al diff.tool cod code git config --global al difftool.prompt mpt false git config --global al difftool.code.cmd \u0026#39;\u0026#34;C \u0026#39;\u0026#34;C:\\Program Files\\Microsoft VS Code\\de\\Code.exe\u0026#34; \u0026#34;-\u0026#34; \u0026#34;--wait --diff\u0026#34; \u0026#34;$LOCAL\u0026#34; \u0026#34;$REMOTE\u0026#34;\u0026#39; //配置Merge工具 git config --global al merge.tool cod code git config --global al mergetool.prompt mpt false git config --global al mergetool.code.cmd \u0026#39;\u0026#34;C \u0026#39;\u0026#34;C:\\Program Files\\Microsoft VS Code\\de\\Code.exe\u0026#34; \u0026#34;-\u0026#34; \u0026#34;--wait\u0026#34; \u0026#34;$MERGED\u0026#34;\u0026#39; git config --global al mergetool.code.trustexitcodecode true OK，配置完 Git 以后，遇到用到需要 Diff 的场景，我们只需要执行 git difftool；而需要用到 Merge 的场景，我们只需要执行 git mergetool。直接合理搭配工具，Git 一样可以变得非常可爱，而不是一堆枯燥乏味的命令行，好啦，Enjoy it，难得写一篇不那么技术向的博客，以后记得早点睡觉~zZ，晚安！\n","date":"2018-09-30T08:43:44Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3222622531/","slug":"3222622531","tags":["Git","VSCode","SourceTree"],"title":"使用 VSCode 作为 SourceTree 的 Diff 和 Merge 工具"},{"categories":["编程语言"],"content":"接触新项目有段时间了，如果让我用一句话来形容此刻的感受，大概就是**“痛并快乐着”。痛苦之一是面对 TFS，因为它的分支管理实在是一言难尽，无时无刻不在体验着人肉合代码的“趣味”。而痛苦之二是同时维护三套数据库的脚本，这让我想到一个梗，在讲到设计模式的时候，一个常常被提到的场景是，怎么样从设计上支持不同数据库的切换。我想，这个问题是非常容易回答的，真正的问题是我们真的需要切换数据库吗？原谅我的年少无知，我们的产品因为要同时支持公有云和私有化部署，所以在数据库的选择上，覆盖到了主流 MySQL、Oracle 和 SQL Server，这直接导致我们要维护三套数据库的脚本，你说这样子能不痛苦吗？而快乐的地方在于，终于有机会在一个有一定用户体量的产品上参与研发，以及从下周开始我们将从 TFS 切换到 Git。好了，今天这篇文章的主题是，通过 EF 来生成不同数据库的 SQL 脚本，这是痛苦中的一次尝试，所谓“痛并快乐着”**。\n基本原理 我们知道数据库和面向对象这两者间存在着天然阻抗，这是因为两者在事物的认知上存在差异，数据库关注的是二维表、是集合间的关系，而面向对象关注的是封装、是细节的隐藏，所以，不管到什么时候，这两者都只能以某种尴尬的方式共存，SQL 执行效率高，这是以牺牲可读性为代价的； ORM 迎合了面向对象，这是以牺牲性能为代价的，所以，即使到了今天，关于 SQL 和 ORM 的争论从来没有停止过，甚至写 SQL 的人不知不觉间“造”出了 ORM，而使用 ORM 的人有时需要 SQL。所以，面对这样一个需要同时维护三套数据库脚本的工作，我个人倾向于用工具去生成，或许是出于程序员对“懒”这种美德的极致追求，或许是出于我对 SQL 这种“方言”天生的排斥，总而言之，我不是很喜欢手写 SQL 除非特别必要，因为它和正则一样，只有写得人懂它真正的含义。\n那么，说到这里，我们就知道了一件事情，ORM 可以帮助我们生成 SQL，所以，我们为什么不让它帮我们生成不同数据库的 SQL 脚本呢？虽然 ORM 的性能总是为人所诟病，因为它严格遵循某种规则，所以注定做不到像人类一样“灵活”。我们始终认为不“灵活”的就是“笨拙”的，可即便如此 ORM 生成的 SQL 依然比人类写得要好看。故而，我们的思路是，在 ORM 生成 SQL 语句的时候将其记录下来，然后按照一定规则生成不同数据库的脚本。毕竟 SQL 语言更接近“方言”，每一种数据库的 SQL 脚本都存在着细微的差别。所以，后来人们不得不发明 T-SQL，可任何东西归根结底不都是权力和利益带来的附属品吗？人类为了互相竞争而形成差异化，可当一切差异都不甚明显时，最终又不得不花费精力来解决这些差异。可一个只有垄断存在的世界，除了让人想起 1984 里的 Big Brother 以外，还能想起什么呢？\n尝试过程 好了，顺着这个思路，我们就会想到在 ORM 中添加拦截器或者是日志的方式，来获得由 ORM 生成的 SQL 语句，这里我们以 Entity Framework(以下简称 EF)为例，这是.NET 中最常见的 ORM，因为目前官方的 Web 开发框架有 ASP.NET 和 ASP.NET Core 两个版本，所以这里我们分别以 ASP.NET 和 ASP.NET Core 为例来说明具体的实现过程，相应地，我们分别使用了 EF6 和 EF Core 作为各自的 ORM。\nEF6 对于 EF6，我们可以通过继承DbCommandInterecptor类来编写一个拦截器。而在拦截器中重写相应的方法，就可以对数据库中的常见操作(CURD)进行拦截。所以，根据这个思路，我们会联想到，通常数据库迁移都是针对“写”这个操作，因此，我们的想法是记录 INSERT 和 UPDATE 两种 SQL 语句。这里我们通过下面的示例来验证这个想法，需要说明的是，本文中所有数据库相关的示例，均采用 Code First 的方式来创建。\npublic class SQLGenInterceptor : DbCommandInterceptor { public override void NonQueryExecuting(DbCommand command, DbCommandInterceptionContext\u0026lt;int\u0026gt; interceptionContext) { var sqlText = FormatSQL(command); Log.Info(sqlText); } public override void ReaderExecuting(DbCommand command, DbCommandInterceptionContext\u0026lt;DbDataReader\u0026gt; interceptionContext) { var sqlText = FormatSQL(command); Log.Info(sqlText); } public override void ScalarExecuting(DbCommand command, DbCommandInterceptionContext\u0026lt;object\u0026gt; interceptionContext) { var sqlText = FormatSQL(command); Log.Info(sqlText); } private string FormatSQL(DbCommand command) { var sqlText = command.CommandText; foreach (DbParameter sqlParam in command.Parameters) { sqlText = sqlText.Replace(sqlParam.ParameterName, sqlParam.Value.ToString()); } return sqlText; } } 在这个示例中，我们使用 NLog 来记录由 EF 生成的 SQL 语句，可以注意到它比我们想象中的要稍微复杂些，所以，人们说 ORM 性能差并不是没有道理。可当你见过那些由人手写出的天书一般的 SQL 语句后，也许两者在可读性上来说不过是五十步笑百步。实际上 EF 生成的 SQL 是一种叫做 T-SQL 的东西，你可以把它理解为一种标准的 SQL 语言。譬如在 PowerBuilder 这个数据库建模软件中，我们可以通过 T-SQL 转换出主流数据库的 SQL 语句。博主在工作中需要维护三套 SQL 脚本，而这些脚本间细小的语法差异，就变成了这个过程中最难忘的记忆，这里我们不考虑去做语法转换的事情，因为实际上通过传入不同的连接字符串，我们就能得到不同数据库的 SQL 脚本，所以接下来的工作就交给各位了(逃……\n//注入SQLGen拦截器 DbInterception.Add(new SQLGenInterceptor()); using (var context = new DataContext()) { context.Users.Add(new User() { UserName = \u0026#34;PayneQin\u0026#34;, UserRole = \u0026#34;Administrator\u0026#34; }); context.SaveChanges(); } 现在，我们需要将这个拦截器注册到 EF 中，注册过程非常简单，一旦拦截器注册完成，当我们在 EF 中执行相应操作的时候，就可以在日志中看到相对应的 SQ 语句了，这样我们就达到了用 EF 生成 SQL 语句的目的，虽然说这样可能还没手写来快，可它至少让你知道了，这个世界上有一种不需要手写 SQL 的可能性啊，你说对吗？\nEF生成SQL语句比想象中更为复杂\rEF Core 对于 EF Core 来说，它并没有提供像 EF6 那样的拦截器，虽然官方曾经说过后续会做这方面的工作[摊手]……不过办法终究是人想出来的，对于 EF Core 我们可以通过注入日志的方式来实现。我们知道，微软在.NET Core 中大力地发展了依赖注入、中间件等一系列特性，所以，这对于我们这种喜欢搞事情的人来说，简直太方便了有木有啊！.NET Core 中日志注入主要集中在 ILogger、ILoggerFactory 和 ILoggerProvider 三个接口，简单来说，ILoggerFactory 是日志工厂，负责返回具体的 Logger；而 ILoggerProvider，则决定在什么情况下应该提供什么样的 Logger。最常见的两种 LoggerProvider 是 Console 和 Debug，它们分别通过 AddConsole()和 AddDebug()来注入。具体到这里，我们通过下面的方式实现：\npublic class SQLGenLogger : ILogger { private readonly string categoryName; public SQLGenLogger(string categoryName) =\u0026gt; this.categoryName = categoryName; public IDisposable BeginScope\u0026lt;TState\u0026gt;(TState state) =\u0026gt; null; public bool IsEnabled(LogLevel logLevel) =\u0026gt; true; public void Log\u0026lt;TState\u0026gt;(LogLevel logLevel, EventId eventId, TState state, Exception exception, Func\u0026lt;TState, Exception, string\u0026gt; formatter) { Log.Info(state) } } 首先定义 SQLGenLogger，顾名思义，它是用来记录生成的 SQL 语句的，同样，我们选择了 NLog。这里有一点要说明，平时我们在控制器中使用 ILogger 的时候，通常会在控制器的构造函数中注入 ILogger，一旦我们使用泛型的 ILogger 接口，Log()方法中的参数 state 实际上就是当前类型，这里和 SQL 语句相关的类型 DbCommandData，实际上是博主试出来的，因为如果不限定 ILogger中的参数 T，我们将得到所有的执行日志，显然，这不是我们想要的结果。\npublic class SQLGenLoggerProvider : ILoggerProvider { public ILogger CreateLogger(string categoryName) { if(categoryName == \u0026#34;Microsoft.EntityFrameworkCore.Database.Command\u0026#34;) return new SQLGenLogger(categoryName); return NullLogger.Instance; } public void Dispose() { } } 接下来来看，ILoggerProvider 接口的实现。我们说过，ILoggerProvider 接口决定在什么情况下应该提供什么样的 Logger，我们注意到它提供了一个 CreateLogger()的方法，它会根据 categoryName 来返回不同的 Logger，而参数 categoryName 实际上等价与 nameof(FooController)，所以，到这里我们就会明白，为什么这里要判断 categoryName 了，它实际上起一个过滤的作用，因为我们只需要 SQL 相关的日志，它和 SQLGenLogger 中的 state 相对应，我们已经说过，这是博主试出来的。\npublic class DataContext : DbContext { public virtual DbSet\u0026lt;User\u0026gt; Users { get; set; } protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) { var loggerFactory = new LoggerFactory(); loggerFactory.AddProvider(new SQLGenLoggerProvider()); //在这里注入日志工厂 optionsBuilder.UseLoggerFactory(loggerFactory) .EnableSensitiveDataLogging() .UseSqlServer(@\u0026#34;Data Source=(LocalDb)\\MSSQLLocalDB;Initial Catalog=SQLGen.DataContext;Integrated Security=True;MultipleActiveResultSets=True;App=EntityFramework\u0026#34;); } protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.ApplyConfiguration(new UserTypeMap()); modelBuilder.Entity\u0026lt;User\u0026gt;().ToTable(\u0026#34;Users\u0026#34;); } } 好啦，接下来就非常简单啦，我们在 DbContext 里对 EF 的 Logger 进行配置，把我们定义的 SQLGenLoggerProvider 注入到 EF 里，可以注意到，它可以如我们期望得那样，输出由 EF 生成的 SQL 脚本，这实在是有趣，Ok，打完收工！\n通过注入日志获取EF生成的SQL\r本文小结 我一直相信，懒惰是工程师的一种美德，因为为了让自己有机会懒惰，你就必须要先让自己勤奋起来。我一直怕自己在舒适区里温水煮青蛙，明明一直在重复做一件事情，还要安慰自己说：“做好这一件事情一样是成功“，有时候，一味地重复自己并不见得会有太多收获，所以，就像这篇文章一样，我本来像偷懒少写一点 SQL，结果意外地发现了给数据库记录日志的方法。当有了意外收获以后，曾经的初衷到底是什么可能就没那么重要了，如“雨血”中左殇所说，当你赢了的时候，你说曾经有十成把握亦不为过。\n这篇文章主要介绍如何利用 EF 来生成不同数据库的 SQL 脚本，对 EF6 来说，需要继承 DbCommandInterecptor 类编写拦截器；对于 EF Core 来说，需要注入 ILogger 来记录日志。本文的延伸之一是记录 SQL 执行日志，这一点在本文已经有所体现。本文更深层次的延伸是，在这个基础上实现数据库的主从复制、读写分离，这一点我会在下一篇博客中讲解，欢迎大家继续关注我的博客，好啦，以上就是这篇文章的全部内容啦，晚安！\n","date":"2018-09-17T09:42:23Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/795474045/","slug":"795474045","tags":["EF",".NET Core","Logger"],"title":"记通过 EF 生成不同数据库 SQL 脚本的一次尝试"},{"categories":["编程语言"],"content":"Hi，大家好，欢迎大家关注我的博客，我是 Payne，我的博客地址是https://qinyuanpei.github.io。今天这篇博客，我们来说说文件上传相关的内容。看到这里，大家一定觉得博主在技术上越来越没追求了吧，文件上传这种再简单不过的东西，真的值得博主你专门写篇博客吗？在介绍声明式 RESTful 客户端 WebApiClient 的这篇文章中，博主曾经提到，HTTP 协议中对文件上传的支持，主要是通过 multipart/form-data 来实现。因为这种方式是将文件视为一种特殊的键值对，所以对这种方式我本人不太喜欢。可作为标准的意义就是要忽略个人的情感因素，所以，在今天这篇文章中，博主更多的是想从 HTTP 协议(RFC2388)的角度来看待这个问题，即为什么它选择了 multipart/form-data 来实现上传，以及伴随着前端技术的发展它经历了哪些变化。\n从 Form 表单说起 圣经上开篇就点明主旨，“起初神创造天地。地是空虚混沌。渊面黑暗”。一切的一切，都要从神创造天地开始，神说，要有光，这世上便有了光。那么，对于 HTTP 协议我们要从哪里开始说起呢。HTTP 的全称是超文本传输协议，所以，它设计的初衷是传输超文本类型的数据。什么是超文本类型的数据呢？从现代网页的组成，我们就可以知道，它不单单是指文本类信息，同时指图片、音频、视频等等一切可能的信息形式。可神奇的地方就在于，HTTP 协议是基于文本的协议，这意味着我们在网页中的信息交换，是借助某种文本类型的通信协议。顺着这个思路，最早我们在网页中交换信息的方式是什么呢？我认为是 Form 表单。想想看，我们在 Form 表单中输入信息，然后通过一个按钮将数据提交到服务器，服务器会对我们的请求做出响应。事实上，直到今天，我们的前端依然在采用这一机制。所不同的是，我们今天用各种组件替代了 Form 表单。\n如果我们讲各种语言的\u0026quot;打印\u0026quot;理解为 Hello World，那么对前端而言最浅显的 Hello World 是什么呢？我个人以为是登录，想象一下，这是任何一个 Web 应用里都有的功能，我们输入用户名和密码以后，点击“登录”按钮就可以登录到系统。虽然，此时此刻的你我，都知道这是一个简单的 POST 请求，甚至对于用户名和密码这两个字段，我们有多种方法可以将其传递到服务器上。那么，各位是否知道，我们通过 Form 表单来登录时，这个过程中到底发生了什么呢？既然提到了登录，那么我们这里通过下面的例子来分析。\n如你所见，这是一个相当“简陋”的 Web 页面。对一名后端开发人员而言，精致的 Web 页面就是一段被套在华丽外壳里的代码(不知道这样会不会被前端网红们打死)。所以，排除了样式相关的 CSS，可以让我们更加专注于核心原理。同样地，我们编写了一个简单的 Web API，来处理前端发送来的 HTTP 请求，这不是本文的重点，我们假设它存在且可以工作就好。\nHTML结构/界面\r这里已经说过，比起炫酷的 Web 页面和后端接口，我们这里更关心的是，登录时到底发生了什么。所以，大家都猜对了，通过 Chrome 自带的开发人员工具，我们可以捕捉到点击“登录”按钮时发出的 HTTP 请求，我们一起来看看它的报文内容是什么吧，相信大家都会有一种恍然大悟的感觉，让我们拭目以待吧！ encrypt为x-www-form-urlencode时的请求报文\r通过这个报文内容，我们可以发现，“登录”实际上是一个 POST 请求，这是因为我们在 HTML 结构中声明了，Form 表单用什么样的方式去提交数据。而实际上呢，Form 表单默认的行为是 GET。我们同样会注意到报文中的 Content-Type 为 application/x-www-form-urlencode，它的特点是采用类似 key1=value1\u0026amp;key2=value2……的形式来提交数据，并且每一个 value 都会被编码。这样，我们就不得不提到 Form 表单的 encrypt 属性，它有三种基本取值：text/plain、application/x-www-form-urlencode 和 multipart/form-data。其中，text/plain 这种不必再说，因为它传递的是纯文本数据。而对于 multipart/form-data 来说，它的特点是采用一系列的 boundary 来分割不同的值，如果我们将示例中 Form 表单的 encrypt 属性设为 multipart/form-data，就会得到下面的报文内容，可以注意到，它和我们预期是一致的。 encrypt为multipart/form-data时的请求报文\r或许大家会说，现在我们用 AJAX 来请求 RESTful 风格的 API 时，不都是用 JSON 作为数据交换的格式吗？对于这一点，或许我们可以理解为，Form 表单是封装了有限的 3 种 Content-Type 的 XHR 对象，所以，Form 表单足以让我们一窥 AJAX 最初的样子。虽然，我们今天已经不再主张使用 jQuery，但是熟悉 jQuery 的朋友一定知道这一点，即 jQuery 中默认的 Content-Type 示例上是 application/x-www-form-urlencoded。所以，即使我们今天有了全新的 Fetch API，可它依然脱离不了 HTTP 协议的范畴。可或许正因为如此，HTTP 中的文件上传多少像是某种妥协的产物。\n神奇的 Input 控件 OK，在本文的第一节，我们使用的是最简单的 Input 控件，即它的 type 属性为“text”。事实上，Input 控件是一个神奇的控件，因为不同的 type 会有不同的作用。例如，type 为 password 对应密码域；type 为 checkbox 对应复选；type 为 radio 对应单选域；type 为 button 对应按钮域等等……有很多朋友可能会问，你说的这个和这篇文章有什么关系吗？我想说的是，当然有关系而且关系密切，因为我们下面要提到的这种 Input 控件，和本文想要说明的 HTTP 上传，在本质上有着千丝万缕的联系。具体是什么样的联系呢？我们来一起看下面的这个例子。\nHTTP_Upload_06\rHTTP_Upload_05\r通过这个例子，我们很容易发现的一点是，当我们采用 type 为 file 的 Input 控件上传一个文件时，它会采用 multipart/form-data 来传递数据，报文中使用了和第二个示例类似的结构，即第一部分负责描述文件信息，譬如文件的名称、扩展名类型等等；第二部分表示文件数据流，可以理解为二进制形式的内容。既然它采用 multipart/form-data 来传递数据，那么这是否意味着，我们可以在这个结构中携带更多的信息呢？譬如，有时候我们需要将文件和用户提交的信息关联起来，这个时候就需要将这些信息一切提交到服务器端，如果我们将其拆分为两个 API 来实现，那么就需要去花精力维护这个关联的 id 啦。答案自然是可以的，只要把文件视为一种特殊的键值对即可。\nHTTP 与文件上传 好了，说了这多么内容，是时候来说说 HTTP 与文件上传啦！现在大家都知道了，HTTP 上传实际上是在 multipart/form-data 基础上扩展而来的。早期人们在制定 HTTP 协议的时候，并没有想到用它来作为文件上传的协议，因为事实上 TPC/IP 或者 FTP 都可以提供更好的上传支持。当我们回顾 Form 表单中关于 HTTP 的部分，我们就会发现，HTTP 中具备上传文件可能性的方式只有两种，即 multipart/form-data 和 x-www-form-urlencode。这里为什么不考虑 text/plain 呢？尽管从理论上来讲，它可以作为文件上传的一种方式，此时，它相当于把整个文件的内容全部放在请求体(body)中。从实用性角度来讲，text/pain 在实际应用中并不多见，因为采用纯文本意味着客户端与服务端必须按照某种规则去解析报文。而从功能性角度来讲，把整个文件的内容全部放在请求体中，则会造成文件信息的不完整，因为此时文件名等信息是没有办法传输到服务器端的，所以，这样综合下来再看的话，HTTP 协议本身留给我们的选择的空间并不大，我们能够选择的就只有 multipart/form-data 和 x-www-form-urlencode 这两种啦，下面着重来分析下这种数据加密方式。\nHTTP_Upload_07\r对于 Content-Type 为 multipart/form-data 而言，首先，它会在请求头部的 Content-Type 字段中，声明当前的内容类型为 multipart/form-data，并指定一个名为 boundary 的随机字符串，它的含义是说，从现在开始，请求中的每一个“字段”都会用这个名为 boundary 的随机字符串进行分割。而对于每一个“字段”而言，它可以拥有部分子头部字段，一个最为常见的头部字段是 Content-Disposition，其取值为 form-data。除此之外，每一个“字段”可以在**Content-Disposition: form-data;**后追加若干个字段，譬如 name、filename 以及用以指定文件类型的 Content-Type(假如这个“字段”是一个文件的话)。HTTP 协议中还规定这里可以支持扩展字段。我们通过 type 为 file 的 Input 控件进行上传时，默认的 name 为 multipartfile，当服务器端接受到类似的字段时，就会根据报文对文件进行拼接，所以，对于 HTTP 上传来说，它可以支持多个文件并发上传，但并不直接支持断点续传。注意这里我说的是，不直接支持断点续传，实际上它可以通过请求头部中的 Range 字段来实现，当然这已经超出了这篇文章的范畴。\nHTTP_Upload_08\r对于 Content-Type 为 x-www-form-urlencode 而言，它会将请求中的每一个字段以 key1=value1\u0026amp;key2=value2……的形式串联起来，并对每一个 value 进行编程，这种传值方式我们一般称为 QueryString，而更为一般的场景是，我们在通过 GET 方式请求数据的时候，QueryString 是唯一的传参方式，不同之处是 GET 请求的参数是附加在 URL 上，而 POST 请求的参数是附加在 body 里。如果我们用这种方式来上传文件会怎么样呢？答案是，当我们试图将一个文件以 x-www-form-urlencode 方式进行传输时，文件流会被彻底忽略，它实际传输的是对应文件的名称。所以，从这个角度来讲，它不能用于文件的上传。事实上，它是被设计用来传输非二进制数据的，那么可能有人要问啦，那我如果有 JSON 来传输文件可不可以呢？理论上应该没有问题，曾经我们在一个项目中用 JSON 描述图片，当然这是经过 Base64 编码以后的图片。回过头来看 text/plain，我们把 JSON 字符串直接放到 body 里可不可以呢？当然没有问题，因为问题全部转移到服务器端。所以，官方建议用它来作为调试的一种选择。\n现在，我们可以来总结下 Form 表单和 HTTP 协议间的关系啦！首先，Form 表单可以提交非二进制数据和二进制数据。非二进制数据，比如一般表单中提交的各种文本信息，用户名、密码这一类等等。二进制数据，主要指各种不同类型的文件等等。对于非二进制数据，可以通过 x-www-form-urlencode 或者 multipart/form-data 两种编码方式来提交。对于二进制数据，只能通过 multipart/form-data 这种方式来提交。所以，当我们需要混合提交二进制数据和非二进制数据的时候，我们就只有 multipart/form-data 这一种选择啦！更一般的结论是，只要我们的 Form 表单里有一个 type 为 file 的 Input 控件，对应 POST 请求的 Content-Type 就会变为 multipart/form-data。我不喜欢这种方式的原因之一，就是构造它的 HTTP 报文非常难受。如果用 HttpClient，痛苦会降低很多；而如果用 HttpWebRequest，我会感到绝望。当然，你此时已明白了这个原理，相信 Postman 可以帮到你的忙。\nForm 表单消失以后 熟悉前端演变历程的朋友，应该对我下面要说的历史表示怀念。在很久很久以前，我们的网页三剑客分别是 Dreamwave、Fireworks 和 Flash。那个时候我们用 Dreamwave 制作的网页充斥着大量的 Form 表单，通过 JS 实现对数据的校验，就像这篇文章里描述的一样，我们做几个 type 为 submit 的按钮，就可以把数据提交到服务器端。按理说，这样子很没完美啊，我们可以提交用户输入的信息，可以上传用户选择的文件，何乐而不为呢？为什么大家要用 Div + CSS 淘汰 Form 表单呢？我认为主要有两点，传统的基于表格的布局无法满足现代 Web 程序的布局要求，RESTful 风格 Web API 的出现让开发者希望前后端交互可控。换言之，开发者希望通过 FormData 这样的对象，精细地控制整个请求的细节，而不是交给一个由浏览器发出的 POST 请求。所以，我们看到了前端文件上传的新思路。\n首先，最常见的方式，是通过监听 Input 控件的 onchange 方法，通过 files 属性即可获得当前用户选择的文件。我们知道，在大多数情况下，前端是无法和本地文件系统进行交互的。因此通过这种方式获得文件路径，实际上是一个指向本地数据的 blob，前端将文件相关的 type 和 size 组织到一个 FormData 对象的实际中，即可完成对文件的上传。其次，可以利用 HTML5 中的“拖拽”和“粘贴”，其核心依然是监听相关的事件，然后从中获取 File 对象或者 blob 对象的实例，一旦获得了这些实例，就可以将其添加到 FormData 中。到了这一步，接下来的就和第一种方法完全一样啦！最后，是类似百度出品的 WebUploader 这类 HTML5 和 Flash 混合的插件，主打兼容性，不过随着大家对 IE8 以下版本兼容问题的逐步放弃，这类产品的使用场景会越来越少，我们大概知道就可以啦！归根到底一句话，Form 表单和 FormData 对象，其实是可以相互转化的，Form 表单里每一个 Input 控件的 name，其实就是 FormData 里的 key 啦，到了这一步，我想 HTTP 上传就没有什么好神秘了的吧！\n本文小结 本文从 Form 表单说起，首先探讨了 Form 表单和 HTTP 之间的关系，即 Form 表单在提交数据的时候，背后的本质其实是一条 HTTP 请求，相对应地，Form 表单默认的请求方式是 GET，在第一个示例中，我们分别展示了使用 x-www-form-urlencode 和 multipart/form-data 时请求报文实际的内容。接下来，我们提到了 HTML 中的 Input 控件，它可以通过指定不同的 type 达到不同的效果，作为第一个示例的延伸，我们尝试通过 Form 表单上传文件并重点关注其报文的结构。接下来，我们从协议的角度分析了为什么要选用 multipart/form-data 来上传文件以及它的原理是什么。最后，我们从前端常见的文件上传方式入手，简要分析了 Form 表单和 FormData 对象间的内在联系，即 Form 表单和 FormData 对象，其实是可以相互转化的，Form 表单里每一个 Input 控件的 name，其实就是 FormData 里的 key。好啦，又是一个难以入眠的夜晚，这篇博客先写到这里，大家晚安！\n","date":"2018-09-05T12:57:36Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2463121881/","slug":"2463121881","tags":["HTTP","Form","RFC"],"title":"漫谈前端进化史之从 Form 表单到文件上传"},{"categories":["编程语言"],"content":"嗨，大家好，欢迎大家关注我的博客，我是 Payne，我的博客地址是https://qinyuanpei.github.io。在上一篇博客中，我们使用了.NET Core 和 Vue 搭建了一个基于 WebSocket 的聊天室。在今天这篇文章中，我们会继续深入这个话题。博主研究 WebSocket 的初衷是，我们的项目上有需要实时去推送数据来完成图表展示的业务，而博主本人对这个内容比较感兴趣，因为博主有对爬虫抓取的内容进行数据可视化(ECharts)的想法。可遗憾的是，这些数据量都不算太大，因为难以支持实时推送这个想法，当然更遗憾的是，我无法在项目中验证以上脑洞，所以，最终退而求其次，博主打算用 Redis 和 WebSocket 做一个弹幕的 Demo，之所以用 Redis，是因为博主懒到不想折腾 RabbitMQ。的确，这世界上有很多事情都是没有道理的啊……\n其实，作为一个业余的数据分析爱好者，我是非常乐意看到炫酷的 ECharts 图表呈现在我的面前的，可当你无法从一个项目中收获到什么的时候，你唯一的选择就是项目以外的地方啦，所以，在今天这样一个精细化分工的时代，即使你没有机会独立地完成一个项目，我依然鼓励大家去了解项目的“上下文”，因为单单了解一个点并不足以了解事物的全貌。好了，下面我们来简单说明下这个 Demo 整体的设计思路，即我们通过 Redis 来“模拟”一个简单的消息队列，客户端发送的弹幕会被推送到消息队列中。当 WebSocket 完成握手以后，我们定时从消息队列中取出弹幕，并推送到所有客户端。当客户端接收到服务端推送的消息后，我们通过 Canvas API 完成对弹幕的绘制，这样就可以实现一个基本的弹幕系统啦！\n编写消息推送中间件 首先，我们来实现服务端的消息推送，其基本原理是：在客户端和服务端完成“握手”后，我们循环地从消息队列中取出消息，并将消息群发至每一个客户端，这样就完成了消息的推送。同上一篇文章一样，我们继续基于“中间件”的形式，来编写消息推送相关的服务。这样，两个 WebSocket 服务可以独立运行而不受到相互的干扰，因为我们将采用两个不同的路由。在上一篇文章中，我们给“聊天”中间件 WebSocketChat 配置的路由为**/wsws。这里，我们将“消息推送”中间件 WebSocketPush 配置的路由为/push**。这块儿我们做了简化，不再对所有 WebSocket 的连接状态进行维护，因为对一个弹幕系统而言，它不需要让别人了解某个用户的状态是否发生了变化。所以，这里我们给出关键的代码。\npublic async Task Invoke(HttpContext context) { if (!IsWebSocket(context)) { await _next.Invoke(context); return; } var webSocket = await context.WebSockets.AcceptWebSocketAsync(); _socketList.Add(webSocket); while (webSocket.State == WebSocketState.Open) { var message = _messageQueue.Pull(\u0026#34;barrage\u0026#34;,TimeSpan.FromMilliseconds(2)); foreach(var socket in _socketList) { await SendMessage(socket,message); } } await webSocket.CloseAsync(WebSocketCloseStatus.NormalClosure, \u0026#34;Close\u0026#34;, default(CancellationToken)); } 同样地，我们需要在 Startup 类中添加 WebSocketPush 中间件。按照 ASP.NET Core 中的惯例，我们为 IAppBuilder 接口增加一个名为 UseWebSocketPush 的扩展方法。这样，可以让我们直接使用该方法完成中间件的注册。\npublic static void UseWebSocketPush(this IApplicationBuilder app) { app.UseMiddleware\u0026lt;WebSocketPush\u0026gt;(); } Redis 打造的消息队列 OK，在编写“消息推送”中间件的时候，我们会注意到，我们使用了一个名为 SimpleMessageQueue 的类来取得消息，而服务端会负责将该消息群发到所有的客户端。这个其实就是博主写的一个简单的消息队列啦，如此简洁直白的命名证明它的确非常简单。有多简单呢？我想一会儿大家就会找到答案。在此之前，我想和大家讨论这样一个问题。其实，聊天室和弹幕挺像的吧，理论上服务端接收到客户端发的消息，就可以直接群发过去啊，为什么要搞一个消息队列在这里呢？而且更扯的一点是，既然博主你选择用 Redis 啦，你难道不知道 Redis 天生就支持发布订阅(Pub-Sub)吗？为什么要搞一个消息队列在这里呢？\n对这个问题，我的想法其实是这样的，我最初想做的是：后端定期推送数据到前端，再由前端通过这些数据来绘制图表。此时，无论后端还是前端，其实都是数据的消费者，这些数据当然不能一股脑儿全给它们啊，这吃撑着了可怎么办，所以，为了避免它们消化不良，我得有一个东西帮助它维持秩序啊，这就是消息队列啊。简单来说，如果数据量超过程序的处理能力，这个时候我们就需要消息队列在前面帮忙“挡”一下。想象一下，如果去银行办理业务的人，都不排队一股脑儿涌向柜台，银行柜员大概会感到崩溃。我们的程序模拟的是现实生活，所以，我们需要消息队列。\n为什么需要消息队列\r那么，有朋友要问啦，就算你要用消息队列，那博主你为什么不用 RabbitMQ，再不济可以考虑微软自带的 MQ 啊，为什么要用 Redis 做一个 MQ 呢？就算你坚持要用 Redis 做 MQ，为什么不考虑用的 Redis 的发布-订阅(Pub-Sub)呢？对于第一个问题，你可以理解为我穷或者懒(穷个什么鬼啊，你特么就是懒_(:з」∠)_)。我就是懒得去搞 RabbitMQ，谁让我电脑 C 盘都快爆炸了呢，自从我把玩了几次Docker for Windows以后，而且我们项目上还真有不被允许用 MQ 的情况。所以，基于以上原因，我选择了 Redis。\nRedis中的Pub-Sub\r那么，为什么不用发布-订阅(Pub-Sub)呢，因为观察者模式的一个前提是，订阅者和主题必须在同一个上下文，即消息的发送方和接受方都必须同时“在线”。可 Bilibili 的弹幕和用户的在线与否无关，这意味着发弹幕与接收弹幕可以不在同一个时刻，所以，在设计上我们是提供了一个 API 接口来发送弹幕，而不是直接通过 WebSocket 来发送。否则，消息都到达服务端了，再通过一个消息队列来取消息，这就真的有点奇怪了不是吗？\n下面给出这个消息队列的实现，原理上是这样的，每一个消息所在的 Channel，实际上都是一个列表，我们使用 Channel 的名称作为这个列表的键。接下来，ServiceStack 提供的 Redis 客户端中，提供了名为 BlockingListItem()的方法，它可以提供类似消息队列的功能，我们在这个基础上实现了一个简单的消息队列。\npublic class SimpleMessageQueue { private string _connectionString; private readonly BasicRedisClientManager _clientManager; public SimpleMessageQueue(string connectionString) { _connectionString = connectionString; _clientManager = new BasicRedisClientManager(_connectionString); } public void Push(string channel, string messsage) { using (var client = _clientManager.GetClient()) { client.PushItemToList(channel, messsage); } } public void Push(string channel, IEnumerable\u0026lt;string\u0026gt; messages) { using (var client = _clientManager.GetClient()) { client.AddRangeToList(channel, messages.ToList()); } } public string Pull(string channel,TimeSpan interval) { using (var client = _clientManager.GetClient()) { return client.BlockingDequeueItemFromList(channel,interval); } } } 相应地，在 WebSocketPush 中间件中，我们通过 Pull()方法来取得消息，时间间隔为 2s。在 MessageController 中，我们提供了用以发送弹幕的 API 接口，它实际上调用了 Push()方法，这个非常简单啦，我们不再做详细说明。\n[HttpPost] [Route(\u0026#34;/api/message/publish/barrage\u0026#34;)] public IActionResult Publish() { Stream stream = HttpContext.Request.Body; byte[] buffer = new byte[HttpContext.Request.ContentLength.Value]; stream.Read(buffer, 0, buffer.Length); string message = System.Text.Encoding.UTF8.GetString(buffer); _redisPublisher.Push(\u0026#34;barrage\u0026#34;, message); Response.Headers.Add(\u0026#34;Access-Control-Allow-Origin\u0026#34;, \u0026#34;*\u0026#34;); return Ok(); } 使用 Canvas 绘制弹幕 好啦，截止到目前为止，我们所有后端的开发已基本就绪。现在，我们来关注下前端的实现。关于 WebSocket 原生 API 的使用，在上一篇文章中，我们已经讲过啦，这里我们重点放在客户端提交弹幕以及绘制弹幕。\n首先来说，客户端提交弹幕到服务器，因为我们已经编写了相应的 Web API，所以这里我们简单调用下它就好。和上一篇文章一样，我们继续使用 Vue 作为我们的前端框架，这对一个不会写 ES6 和 CSS 的伪前端来说，是非常友好的一种体验。因为现在是 2018 年，所以，我们要坚决地放弃 jQuery，虽然它的 ajax 的确很好用，可这里我们还是要使用 Axios：\naxios.post(\u0026#34;http://localhost:8002/api/message/publish/barrage\u0026#34;,{ value: self.value, color: self.color, time: self.video.currentTime }).then(function (response) { console.log(response); }) .catch(function (error) { console.log(error); }); 接下来，说说弹幕绘制。我们知道，HTML5 中提供了基于 Canvas 的绘图 API，所以，我们这里可以用它来完成弹幕的绘制。基本思路是：根据 video 标签计算出弹幕出现的范围，然后让弹幕从右侧向左逐渐移动，而弹幕的垂直位置则可以是顶部/底部/随机，当弹幕移动到屏幕左侧时，我们从弹幕集合中移除掉这个元素即可。下面给出基本代码，绘图相关的接口可以参考这里，弹幕相关参考了这篇文章：\nvar context = canvas.getContext(\u0026#39;2d\u0026#39;); context.shadowColor = \u0026#39;rgba(0,0,0,\u0026#39; + this.opacity + \u0026#39;)\u0026#39;; context.shadowBlur = 2; context.font = this.fontSize + \u0026#39;px \u0026#34;microsoft yahei\u0026#34;, sans-serif\u0026#39;; if (/rgb\\(/.test(this.color)) { context.fillStyle = \u0026#39;rgba(\u0026#39; + this.color.split(\u0026#39;(\u0026#39;)[1].split(\u0026#39;)\u0026#39;)[0] + \u0026#39;,\u0026#39; + this.opacity + \u0026#39;)\u0026#39;; } else { context.fillStyle = this.color; } context.fillText(this.value, this.x, this.y); 翻滚吧，弹幕 OK，现在我们来一起看看最终的效果，如你所见，在视频播放过程中，我们可以通过视频下方的输入框发送弹幕，弹幕会首先经由 Redis 缓存起来，当到达一定的时间间隔以后，我们就会将消息推送到客户端，这样所有的客户端都会看到这条弹幕，而对于客户端来说，它在和服务端建立 WebSocket 连接以后，唯一要做的事情就是在 onmessage 回调中取得弹幕数据，并将其追加到弹幕数组中，关于弹幕绘制的细节，我们在本文的第三节已经做了相关说明，在此不再赘述。 弹幕效果展示\r这里，我们采用了前后端分离的设计，即使我们没有并使用主流的 ES6 去实现客户端。因此，这是客户端实际上是一个静态页面，在本地开发阶段，我们可以通过打开多个浏览器窗口来模拟多用户。那么，如果我们希望让更多人来访问这个页面该怎么做呢？这就要说到 ASP.NET Core 中的静态文件中间件。无论是 IIS 还是 Apache，对静态页面进行展示，是一个 Web 服务器最基本的能力。在 ASP.NET Core 中，我们是通过静态文件中间件来实现这个功能，简而言之，通过这个功能，我们就可以让别人通过 IP 或者域名来访问 wwwroot 目录下的内容。具体代码如下：\napp.UseDirectoryBrowser(); app.UseStaticFiles(); 当然，这里有一个细节是为了让别人可以通过 IP 或者域名来访问你的服务，你需要修改下 WebHostBuilder 中 URL。此外，因为我们在前端界面中使用了绝对的 URL 去访问 WebAPI，因此，当前端页面和 WebAPI 不在一个域中时，就会出现所谓垮域的问题，这方面的内容非常丰富，因为这是一个再常见不过的问题，身处在这个时代，80%的问题都已经被解决过了，这到底是我们的幸运还是不幸呢？\nWebHost.CreateDefaultBuilder(args) .UseStartup\u0026lt;Startup\u0026gt;() .UseUrls(\u0026#34;http://*:8002\u0026#34;); 本文小结 本文在上一篇的基础上，借助 Redis 和 WebSocket 实现了一个简单的弹幕系统。博主的初衷是想一个数据可视化的小项目，可以通过 WebSocket 实时地刷新图表，因为在博主看来，数据分析同样是有趣的事情。这篇文章选取博主在工作中遇到的实际场景作为切入点，试图发掘出 WebSocket 在实时应用方面更多的可能性。首先，我们编写了“消息推送”中间件，并通过不同的路由来处理各自的业务，实现了模块间的相互独立。接下来，我们讨论了 Redis 作为消息队列的可行性，并基于 Redis 编写了一个简单的消息队列。最终，通 Canvas API 完成客户端弹幕的绘制，实现了从后端到前端的方案整合。藉由这个小项目，可以引出 ASP.NET Core 相关的话题，譬如静态文件中间件、部署、跨域等等的话题，感兴趣的朋友可以自己去做进一步的了解，以上就是这篇博客的全部内容啦，谢谢大家！\n","date":"2018-08-22T14:07:23Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3269605707/","slug":"3269605707","tags":["Redis","WebSocket",".NET Core"],"title":"基于 WebSocket 和 Redis 实现 Bilibili 弹幕效果"},{"categories":["生活感悟"],"content":"“大道如青天，我独不得出” 。这是唐朝大诗人李白在 《行路难·其二》 中的感慨，相比“长风破浪直济沧海”的豪迈，这首诗反而显得矫情啦，仿佛生活就应该陪着大家一起笑，我这种神经兮兮的文艺 Boy，就只有这片手机屏幕大小的地方，来说些不合时宜的冷笑话。这大概是博客能让我坚持写下去的理由，因为它的的确确是属于你的，那么请原谅我，因为我要在这里写点矫情的话。\n上周一个人独自从公司办理完离职手续，原来那天下午是可以不用下班打卡的，这种感觉就好像是，你坚持并且在乎了很久的一件事情，在某一瞬间突然变得不再重要。我突然意识到，我居然已经在这个城市里生活了两年多。两年多是种什么体验呢？或者是曾经说过再见的朋友再没有见过面，又或者是曾经拒绝过你的女孩子终于结了婚，又或者是青龙寺里的樱花们开了一年又一年，又或者是遗址公园里石榴又从青色变成了红色……你脑海中的记忆越来越浅，而额头上的皱纹越来越深，记忆果真都被时间从额头上凿了去吗？\n和朋友们聊天的时候，他们一如既往地感慨着自己婚后的月光生活，一如既往地羡慕着我接近他们三倍的高薪工资，总要在聊天快要结束时侯，一如既往地问我：真的不打算回来了吗？也许，等到银西高铁通车的时候，我终于不用再坐将近 12 个小时的火车回家。“鸟倦飞而知还”，可我是否是 《阿飞正传》 里的那一只，那只没有脚的鸟，它只能够一直飞呀飞呀，飞累了就在风里睡觉。我认识的人里，有从大学时代就在这个城市驻足的人，有远离故乡在这个城市扎根的人，无一例外的是，我们都离故乡越来越远，交通的便利和发达，并不足以弥补这种心灵上的距离，就像你从地图上看各个省份好像都离得不远。可没法在一起的人们啊，连最后一步都会觉得遥远啊！我不得不承认，没有人天生会是一个无忧无虑的漂泊者，无论是洛阳还是长安，对我而言都不是故乡。\n早已忘记是从什么时候开始不吃辣的，真正令我感到神奇的，是这种习惯终于让我带去到不同的城市，就像你很难说清楚，喜欢一个人有多少是来自喜欢，又有多少是来自习惯。周末基本固定的去书店看书，看书的同时亦看匆匆的行人。小寨和钟楼永远不乏光鲜亮丽的男男女女，俨然是这个城市里最繁华的地带，人们的自拍无一不透露着时尚与精致，在某一瞬间，让我这个来自三四线小城市的人，相形见绌到沉默不言。想象下大唐盛世里的长安城，最繁华的市集无外东西两市，马亲王的 《长安十二时辰》 所展现的盛唐风物，对真实的历史而言，不过是雪泥鸿爪、惊鸿一瞥。如今西市为商业街所包围，一座大唐西市博物馆悄立其中，其形堪称寂寥否？西安遍地都是商场、购物中心，其盛堪比大唐否？\n我曾经开玩笑地和朋友说，我现在喜欢观察路上行人们的穿搭，仿佛这样能让我喜欢的女孩子愿意看我一眼，朋友不无嘲讽地说，“你这是在东施效颦啊”，就连我喜欢的女孩子都说，“每个人都会有自己喜欢的风格啊”。可其实，我只有一点能确定，我明确知道我不喜欢那种风格，倘若真要问我喜欢什么，我真的不知道啊！不要以为只有女孩子，会在面对琳琅满目的商品时选择困难，在这个选择多样化的时代，明确知道自己想要什么，对每一个人而言，反倒是一种相当稀缺的品质。就像人只有长大了以后才会明白，做一个优秀的男人是多么困难的事情，做学生的时候比的是学习成绩，做男人的时候则是比社会化综合测评。初到长安“居之不易”的白居易，和此时的你我是何其相似，彼时长安是大唐的首都，此时西安是新新一线城市，历史啊，果然都是相似的嘛，所不同的只是当事人。\n朋友们都希望我可以“自信”点，可终于有这样一天，你做到了曾经想做而不敢做的事情，这一切又是否真的会如你所愿。人啊，总是情愿活在借口里。我有位朋友常常“一语惊人”，简直就是“语不惊人死不休”的典型代表，他说，“不管男人的话还是女人的话，都不要相信”。大意就是说，人家就是那么随口一说，你这还当真了不是。姜文《让子弹飞》里有一个情节，小六子被诬陷吃了两碗凉粉却给了一碗凉粉钱，百口莫辩的小六子，不得已剖开肚子来自证清白。其实，世上好多事情都是没有道理的，你证明了你可以做到某一件事情又能怎么样呢？时过境迁，当一切都重新归于平静，也许人家就是那么随口一说，也许人家早都忘记了说过这句话，而你却守着这个可笑的执念等到花儿都谢了。人呐，偶尔狠下心来，是因为这样很爽吗？就像人们喜欢暴力一样。其实，真正的自信应该是温柔的，很多时候你以为的自信，无非是任性罢了。一个小孩子，会因为你帮他捡起掉在地上的扇子，而对你微笑，即使他还不会说话。\n有时候，我会想人们对于一件事物的评价标准为何会存在差异，这是否是因为我们根本不了解自己。人类其实和猴子差不了多少，对这个世界总是充满好奇，似乎什么都想要去尝试。喜欢吃喝玩乐会被认为是懂得享受生活，而喜欢独处内省则会被认为是乏味无趣，可其实大家都是第一次做人，都是第一次面对这个存活了上亿年的星球。当北极圈里开始出现 30 度的高温，大概在这个世界上并不存在绝对的事情。当所有的标准都被满足，这是否意味着无论对方是谁都可以，我们明明都在执着于找寻唯一的东西，却为何选择了原本和唯一无关的标准。因为但凡有标准存在的地方，它们就注定难以成为独一无二的东西。就像 《小偷家族》 里的“父亲”，自认为什么都不会除了盗窃，却教会了祥太关于青春期的一切。很多时候爱不像我们想象地拥有一个标准模板，就像这个并不“标准”的家庭，却拥有足以打动我们的情感一样。或许，追求“标准”本来就是件愚蠢的事情，我们自以为个性独立，实际上永远被各种“虚拟”的东西束缚着，正如卢梭所言：“人生而自由，而无往不在枷锁之中”。\n我想，李白抒发“不见长安”的愁绪时，大抵不会想到日后遇赦时的快意，更不会想到被玄宗逐出长安时的失意。可当你真的了解了你所要面对的人生，是否还有勇气会像现在这样选择。对于我的人生，我不知道今后会是什么样子，我唯一能做的，就是接受我已经失去的一切，长安并不足以安，你会有一个可以令你心安的人出现吗？长安不见，你愿意让我见到你吗？\n","date":"2018-08-10T20:42:23Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3417652955/","slug":"3417652955","tags":["西安","感悟","矫情"],"title":"长安不见使人愁"},{"categories":["编程语言"],"content":" Hi，大家好，我是Payne，欢迎大家关注我的博客，我的博客地址是：https://qinyuanpei.github.io。今天这篇博客，我们来说说WebSocket。各位可能会疑惑，为什么我会突然间对WebSocket感兴趣，这是因为最近接触到了部分“实时”的业务场景，譬如：用户希望在远程视频通话过程中，实时地监控接入方的通话状态，实时地将接入方的响应时间、通话时长以及接通率等信息推送到后台。与此同时，用户可以通过监控平台看到实时变化着的图表。坦白地讲，这种业务场景陌生吗？不，每一年的双11，都能见到小伙伴们实时地“剁手”。所以，在今天这篇文章中，我们会以WebSocket聊天室为例，来讲解如何基于WebSocket构建实时应用。\nWebSocket概述 WebSocket是HTML5标准中的一部分，从Socket这个字眼我们就可以知道，这是一种网络通信协议。WebSocket是为了弥补HTTP协议的不足而产生的，我们知道，HTTP协议有一个重要的缺陷，即：请求只能由客户端发起。这是因为HTTP协议采用了经典的请求-响应模型，这就限制了服务端主动向客户端推送消息的可能。与此同时，HTTP协议是无状态的，这意味着连接在请求得到响应以后就关闭了，所以，每次请求都是独立的、上下文无关的请求。这种单向请求的特点，注定了客户端无法实时地获取服务端的状态变化，如果服务端的状态发生连续地变化，客户端就不得不通过“轮询”的方式来获知这种变化。毫无疑问，轮询的方式不仅效率低下，而且浪费网络资源，在这种背景下，WebSocket应运而生。\nWebSocket协议最早于2008年被提出，并于2011年成为国际标准。目前，主流的浏览器都已经提供了对WebSocket的支持。在WebSocket协议中，客户端和服务器之间只需要做一次握手操作，就可以在客户端和服务器之间实现双向通信，所以，WebSocket可以作为**服务器推送**的实现技术之一。因为它本身以HTTP协议为基础，所以对HTTP协议有着更好的兼容性，无论是通信效率还是传输的安全性都能得到保证。WebSocket没有同源限制，客户端可以和任意服务器端进行通信，因此具备通过一个单一连接来支持上下游通信的能力。从本质上来讲，WebSocket是一个在握手阶段使用HTTP协议的TCP/IP协议，换句话说，一旦握手成功，WebSocket就和HTTP协议再无瓜葛，下图展示了它与HTTP协议的区别：\nHTTP与WebSocket的区别\r构建一个聊天室 OK，在对WebSocket有了一个基本的认识以后，接下来，我们以一个最简单的场景来体验下WebSocket。这个场景是什么呢？你已经知道了，答案就是网络聊天室。这是一个非常典型的实时场景。这里我们分为服务端实现和客户端实现，其中：服务端实现自豪地采用.NET Core，而客户端实现采用Vue的双向绑定特性。现在是公元2018年了，当jQuery已成往事，操作DOM这种事情交给框架去做就好，而且我本人很喜欢MVVM这种模式，Vue的渐进式框架，非常适合我这种不会写ES6的伪前端。\n.NET Core与中间件 关于.NET Core中对WebSocket的支持，这里主要参考了官方文档，在这篇文档中，演示了一个最基本的Echo示例，即服务端如何接收客户端消息并返回消息给客户端。这里，我们首先需要安装Microsoft.AspNetCore.WebSockets这个库，直接通过Visual Studio Code内置的终端安装即可。接下来，我们需要在Startup类的Configure方法中添加WebSocket中间件：\napp.UseWebSockets() 更一般地，我们可以配置以下两个配置，其中，KeepAliveInterval表示向客户端发送Ping帧的时间间隔；ReceiveBufferSize表示接收数据的缓冲区大小：\nvar webSocketOptions = new WebSocketOptions() { KeepAliveInterval = TimeSpan.FromSeconds(120), ReceiveBufferSize = 4 * 1024 }; app.UseWebSockets(webSocketOptions); 好了，那么怎么接收一个来自客户端的请求呢？这里以官方文档中的示例代码为例来说明。首先，我们需要判断下请求的地址，这是客户端和服务端约定好的地址，默认为**/，这里我们以/ws为例；接下来，我们需要判断当前的请求上下文是否为WebSocket请求，通过context.WebSockets.IsWebSocketRequest来判断。当这两个条件同时满足时，我们就可以通过context.WebSockets.AcceptWebSocketAsync()**方法来得到WebSocket对象，这样就表示“握手”完成，这样我们就可以开始接收或者发送消息啦。\nif (context.Request.Path == \u0026#34;/ws\u0026#34;) { if (context.WebSockets.IsWebSocketRequest) { WebSocket webSocket = await context.WebSockets.AcceptWebSocketAsync(); //TODO } }); 一旦建立了Socket连接，客户端和服务端之间就可以开始通信，这是我们从Socket中收获的经验，这个经验同样适用于WebSocket。这里分别给出WebSocket发送和接收消息的实现，并针对代码做简单的分析。\nprivate async Task SendMessage\u0026lt;TEntity\u0026gt;(WebSocket webSocket, TEntity entity) { var Json = JsonConvert.SerializeObject(entity); var bytes = Encoding.UTF8.GetBytes(Json); await webSocket.SendAsync( new ArraySegment\u0026lt;byte\u0026gt;(bytes), WebSocketMessageType.Text, true, CancellationToken.None ); } 这里我们提供一个泛型方法，它负责对消息进行序列化并转化为byte[]，最终调用**SendAsync()方法发送消息。与之相对应地，客户端会在onmessage()**回调中就会接受到消息，这一点我们放在后面再说。WebSocket接收消息的方式，和传统的Socket非常相似，我们需要将字节流循环读取到一个缓存区里，直至所有数据都被接收完。下面给出基本的代码示例：\nvar buffer = new ArraySegment\u0026lt;byte\u0026gt;(new byte[bufferSize]); var result = await webSocket.ReceiveAsync(buffer, CancellationToken.None); while (!result.EndOfMessage) { result = await webSocket.ReceiveAsync(buffer, default(CancellationToken)); } var json = Encoding.UTF8.GetString(buffer.Array); json = json.Replace(\u0026#34;\\0\u0026#34;, \u0026#34;\u0026#34;).Trim(); return JsonConvert.DeserializeObject\u0026lt;TEntity\u0026gt;(json, new JsonSerializerSettings() { DateTimeZoneHandling = DateTimeZoneHandling.Local }); 虽然不大清楚，为什么这里反序列化后的内容中会有大量的**\\0**，以及这个全新的类型ArraySegment到底是个什么鬼，不过程序员的一生无非都在纠结这样两个问题，“it works” 和 “it doesn\u0026rsquo;t works\u0026quot;，就像人生里会让你纠结的无非是”她喜欢你“和”她不喜欢我“这样的问题。有时候，这样的问题简直就是玄学，五柳先生好读书而不求甚解，我想这个道理在这里同样适用，截止到我写这篇博客前，这个代码一直工作得很好，所以，这两个问题我们可以暂时先放在一边，因为眼下还有比这更为重要的事情。\n通过这篇文档，我们可以非常容易地构建出一个”实时应用“，可是它离我们这篇文章中的目标依然有点距离，如果各位足够细心的话，就会发现这样一个问题，即示例中的代码都是写在app.Use()方法中的，这样会使我们的Startup类显得臃肿，而熟悉OWIN或者ASP.NET Core的朋友，就会知道Startup类是一个非常重要的东西，我们通常会在这里配置相关的组件。在ASP.NET Core中，我们可以通过Configure()方法来为IApplicationBuilder增加相关组件，这种组件通常被称为中间件。那么，什么是中间件呢？\n中间件示意图\r从这张图中可以看出，中间件实际上是指在HTTP请求管道中处理请求和响应的组件，每个组件都可以决定是否要将请求传递给下一个组件，比如身份认证、日志记录就是最为常见的中间件。在ASP.NET Core中，我们通过app.Use()方法来定义一个Func\u0026lt;RequestDelegate,RequestDelegate\u0026gt;类型的参数，所以，我们可以简单地认为，在ASP.NET Core中，Func\u0026lt;RequestDelegate,RequestDelegate\u0026gt;就是一个中间件，而通过app.Use()方法，这些中间件会根据注册的先后顺序组成一个链表，每一个中间件的输入是上一个中间件的输出，每一个中间件的输出则会成为下一个中间件的输入。简而言之，每一个RequestDelegate对象不仅包含了自身对请求的处理，而且包含了后续中间件对请求的处理，我们来看一个简单的例子：\napp.Use(async (context,next)=\u0026gt; { await context.Response.WriteAsync(\u0026#34;这是第一个中间件\\r\\n\u0026#34;); await next(); }); app.Use(async (context,next)=\u0026gt; { await context.Response.WriteAsync(\u0026#34;这是第二个中间件\\r\\n\u0026#34;); await next(); }); app.Use(async (context,next)=\u0026gt; { await context.Response.WriteAsync(\u0026#34;这是第三个中间件\\r\\n\u0026#34;); await next(); }); 通过Postman或者任意客户端发起请求，我们就可以得到下面的结果，现在想象一下，如果我们在第一种中间件中不调用next()会怎么样呢？答案是中间件之间的链路会被打断，这意味着后续的第二个、第三个中间件都不会被执行。什么时候我们会遇到这种场景呢？当我们的认证中间件认为一个请求非法的时候，此时我们不应该让用户访问后续的资源，所以直接返回403对该请求进行拦截。在大多数情况下，我们需要让请求随着中间件的链路传播下去，所以，对于每一个中间件来说，除了完成自身的处理逻辑以外，还至少需要调用一次next()，以保证下一个中间件会被调用，这其实和职责链模式非常相近，可以让数据在不同的处理管道中进行传播。\nASP.NET Core中间件示例\rOK，这里我们继续遵从这个约定，将整个聊天室相关的逻辑写到一个中间件里，这样做的好处是，我们可以将不同的WebSocket互相隔离开，同时可以为我们的Startup类”减负“。事实证明，这是一个正确的决定，在开发基于WebSocket的弹幕功能时，我们就是用这种方式开发了新的中间件。这里，我们给出的是WebSocketChat中间件中最为关键的部分，详细的代码我已经放在Github上啦，大家可以参考WebSocketChat类，其基本原理是：使用一个字典来存储每一个聊天室中的会话(Socket)，当用户打开或者关闭一个WebSocket连接时，会向服务器端发送一个事件(Event)，这样客户端中持有的用户列表将被更新，而根据发送的消息，可以决定这条消息是被发给指定联系人还是群发：\npublic async Task Invoke(HttpContext context) { if (!IsWebSocket(context)) { await _next.Invoke(context); return; } var userName = context.Request.Query[\u0026#34;username\u0026#34;].ToArray()[0]; var webSocket = await context.WebSockets.AcceptWebSocketAsync(); while (webSocket.State == WebSocketState.Open) { var entity = await Receiveentity\u0026lt;MessageEntity\u0026gt;(webSocket); switch (entity.Type) { case MessageType.Chat: await HandleChat(webSocket, entity); break; case MessageType.Event: await HandleEvent(webSocket, entity); break; } } await webSocket.CloseAsync(WebSocketCloseStatus.NormalClosure, \u0026#34;Close\u0026#34;, default(CancellationToken)); } 其中，HandleEvent负责对事件进行处理，HandleChat负责对消息进行处理。当有用户加入聊天室的时候，首先会向所有客户端广播一条消息，告诉大家有新用户加入了聊天室，与此同时，为了让大家可以和新用户进行通信，必须将新的用户列表推送到客户端。同理，当有用户离开聊天室的时候，服务器端会有类似的事件推送到客户端。事件同样是基于消息来实现的，不过这两种采用的数据结构不同，具体大家可以通过源代码来了解。发送消息就非常简单啦，给指定用户发送消息是通过用户名来找WebSocket对象，而群发消息就是遍历字典中的所有WebSocket对象，这一点我们不再详细说啦！\nVue驱动的客户端 在实现服务端的WebSocket以后，我们就可以着手客户端的开发啦！这里我们采用原生的WebSocket API来开发相关功能。具体来讲，我们只需要实例化一个WebSocket类，并设置相应地回调函数就可以了，我们一起来看下面的例子：\nvar username = \u0026#34;PayneQin\u0026#34; var websocket = new WebSocket(\u0026#34;ws://localhost:8002/ws?username=\u0026#34; + username); 这里我们使用**/s这个路由来访问WebSocket，相应地，在服务端代码中我们需要判断context.Request.Path**，WebSocket在握手阶段是基于HTTP协议的，所以我们可以以QueryString的形式给后端传递一个参数，这里我们需要一个用户名，它将作为服务端存储WebSocket时的一个键。一旦建立了WebSocket，我们就可以通过回调函数来监听服务器端的响应，或者是发送消息给服务器端。主要的回调函数有onopen、onmessage、onerror和onclose四个，基本使用方法如下：\nwebsocket.onopen = function () { console.log(\u0026#34;WebSocket连接成功\u0026#34;); }; websocket.onmessage = function (event) { console.log(\u0026#34;接收到服务端消息：\u0026#34; + event.data) }; websocket.onerror = function () { console.log(\u0026#34;WebSocket连接发生错误\u0026#34;); }; websocket.onclose = function () { console.log(\u0026#34;WebSocket连接关闭\u0026#34;); }; 原生的WebSocket API只有两个方法，即send()和close()，这两个方法非常的简单，我们这里不再说明。需要说明的是，客户端使用了Vue来做界面相关的绑定，作为一个不会写CSS、不会写ES6的伪前端，我做了一个相当简洁(简陋)的前端页面，下面给出主要的页面结构，ViewModel层的代码比较多，大家可以参考这里：\n\u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; Hi，{{ username }}。欢迎来到WebSocket聊天室！ \u0026lt;hr/\u0026gt; 发送给： \u0026lt;select v-model=\u0026#34;sendTo\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;All\u0026#34;\u0026gt;全部\u0026lt;/option\u0026gt; \u0026lt;option v-for=\u0026#34;user in userList\u0026#34; :value=\u0026#34;user\u0026#34;\u0026gt;{{user}}\u0026lt;/option\u0026gt; \u0026lt;/select\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;input id=\u0026#34;text\u0026#34; type=\u0026#34;text\u0026#34; v-model=\u0026#34;message\u0026#34; /\u0026gt; \u0026lt;button v-on:click=\u0026#34;sendMessage\u0026#34;\u0026gt;发送消息\u0026lt;/button\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;button v-on:click=\u0026#34;openWebSocket\u0026#34;\u0026gt;打开WebSocket连接\u0026lt;/button\u0026gt; \u0026lt;button v-on:click=\u0026#34;closeWebSocket\u0026#34;\u0026gt;关闭WebSocket连接\u0026lt;/button\u0026gt; \u0026lt;button v-on:click=\u0026#34;clearMessageList\u0026#34;\u0026gt;清空聊天记录\u0026lt;/button\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;div id=\u0026#34;messageList\u0026#34; v-html=\u0026#34;messageList\u0026#34;\u0026gt; {{ messageList }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 下面是实际的运行效果，果然是非常简洁呢，哈哈:laughing:\nWebSocket聊天室展示\r再看Websocket 好了，我们花了如此大的篇幅来讲WebSocket，那么你对WebSocket了解了多少呢？或许通过这个聊天室的实例，我们对WebSocket有了一个相对直观的认识，可你是否想过换一个角度来认识它呢？我们说过，WebSocket是以HTTP协议为基础的，那么至少可以在握手阶段捕获到相关请求吧！果断在Chrome中打开”开发者工具“，在面板上选择监听”WebSocket\u0026quot;，然后我们就会得到下面的内容。\nWebSocket的秘密-请求\r相比HTTP协议，WebSocket在握手阶段的请求有所变化，主要体现在Upgrade、Connection这两个字段，以及Sec-WebSocket系列的这些字段。下面来分别解释下这些字段的含义，Upgrade和Connection这两个字段，是最为关键的两个字段，它的目的是告诉Apache、Nginx这些服务器，这是一个WebSocket请求。接下来，是Sec-WebSocket-Key、Sec-WebSocket-Protocol和Sec-WebSocket-Version这三个字段，其中Sec-WebSocket-Key是一个由浏览器采用Base64算法随机生成的字符串，目的是验证服务器是否真的支持WebSocket；Sec-WebSocket-Protocol则是一个由用户指定的字符串，目的是区分同一URL下，不同服务所需要的协议；Sec-WebSocket-Version是告诉服务器浏览器支持的WebSocket版本，标准规定9-12的版本号是保留字段，所以在这里我们看到的版本号是13.\nWebSocket的秘密-响应\r那么，对于这个浏览器发起的这个请求，服务端是如何做出响应的呢？这就要来看看服务端返回的内容。 和客户端发起的请求类似，服务端返回的内容中依然会有Upgrade和Connection这两个字段，它们和请求中的含义是完全一致的。这里需要说明的是Sec-WebSocket-Accept这个字段，我们前面提到，浏览器会通过WebSocket-Key检验服务器是否真的支持WebSocket，具体怎么检验呢？是通过下面的算法。除此之外，一个特殊的地方是这个Response的状态码是101，这表示服务端说：下面我们就按照WebSocket协议来通信吧！当然，一个更为残酷的现实是，从这里开始，就不再是HTTP协议的势力范围了啊：\nsec-websocket-accept = base64(hsa1(sec-websocket-key + 258EAFA5-E914-47DA-95CA-C5AB0DC85B11)) 本文小结 这篇文章选取了“实时应用”这样一个业务场景作为切入点，引出了本文的主题——WebSocket。WebSocket是一种建立在HTTP协议基础上的双向通信协议，它弥补了以“请求-响应”模型为基础的HTTP协议先天上的不足，客户端无需再通过“轮询”这种方式来获取服务端的状态变化。WebSocket在完成“握手”后，即可以长连接的方式在客户端和服务端间构建双向通道，因而WebSocket可以在实时应用场景下，作为服务器推送技术的一种方案选择。本文以一个WebSocket聊天室的案例，来讲解WebSocket在实际项目中的应用，在这里我们使用ASP.NET Core来完成服务端WebSocket的实现，而客户端选用原生WebSocket API和Vue来实现，在此基础上，我们讲解了ASP.NET Core下中间件的概念，并将服务器端WebSocket以中间件的形式实现。在下一篇文章中，我们将偏重于服务器端的数据推送，客户端将作为数据展现层而存在。好了，以上就是这篇文章的全部内容啦，谢谢大家，让我们一起期待下一篇文章吧！\n","date":"2018-08-01T15:42:23Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1989654282/","slug":"1989654282","tags":["WebSocket",".NET Core","Vue"],"title":"使用 .NET Core 和 Vue 搭建 WebSocket 聊天室"},{"categories":["生活感悟"],"content":" 最近一直在看 《逃避虽可耻但有用》(逃げるは恥だが役に立つ) 这部日剧，当我们感慨各种脑洞都满足不了人类的好奇心时，日剧依然在老老实实地讲述着故事，即使这个故事离普通人依旧很遥远。可我认为，这是一部以轻喜剧为载体的温情剧，不管你是单身、恋爱中还是已婚，你都能从这部剧中找到自己对应的部分。所以，对于这部日剧而言，我个人是推荐大家去看一看的。原谅我不肯用我贫乏的语言去评价这部电视剧，因为我相信“此中不足为外人道也”。所谓“如人饮水，冷暖自知”，感情这件事情，懂的人自然会懂，不懂的人假装懂。\n剧中男主津崎平匡是一个“典型”的程序员，因为外表无攻击性，一脸的人畜无害，而被女主森山实栗称为“草食系”男人。男主的长相在主流审美中或许谈不上帅，因为这个世界更欣赏的，是风见君这样帅气的男人。程序员群体木讷而内向的性格，其实都是大众给贴上去的标签。人们不喜欢被贴上各种标签，可人们喜欢给别人贴各种标签，因为这样子区分不同的人最省事儿。我们无法指责这个世界用五官和三观来割裂地看待一个人，我们唯一能做的，就是去改变留在人们心中的刻板印象。剧中男主在很多方面是比我们优秀的，向他学习不能保证我们会娶到 Gakki，可能让我们变得更优秀。\n好了，下面就由我带大家一起来盘点男主在剧中的穿搭，所以，这是一篇总结向的草食系程序员穿搭指南。考虑到这部剧中室内场景比室外场景更多，季节主要集中在秋冬季，所以，我们将从环境、季节、种类等多个维度，对男主在剧中的穿搭进行盘点。活在一个看脸的时代最大的悲哀就是，那些长得比你好看，明明可以靠颜值，非要靠才华的人，永远都比你更努力。虽然津崎先生经常被人说“低情商”和“屌丝”，可我相信他比我们大多数“屌丝”要优秀得多。当然，这些优点需要大家在剧中去发掘。我只是希望，通过这种方式来提升自我。面对来自这个世界的恶意，争辩是没有意义的，你只能努力去纠正这种偏见。\n室内篇 20180724012456548-101-2018725\r1、深蓝色衬衣 + 深绿色休闲裤。作为职场日常穿搭，在第一集中出现，中年已婚男士池日在男主津崎面前炫耀“爱妻便当”，高情商的田沼先生替男主解围，安慰男主要好好吃饭。建议搭配：休闲皮鞋 + 一条优质皮带。同样地，我想说的是，一个人更要好好吃饭。\n20180724012544572102-2018725\r2、蓝色衬衣 + 西裤，俨然是雇主与雇员的上下级关系。女主森山实栗通过试用期考核，指令清晰、有条不紊给女主留下良好印象。作为职场常规搭配，搭配黑框眼睛，给人一种斯文儒雅的感觉，建议根据个人肤色，选择合适的颜色，具体来讲，如果你皮肤较白，建议选择明亮的色彩；如果你皮肤较黑，建议选择中性的色彩。\n20180724012633880103-2018725\r3、因为业务需求发生变更，男主被公司安排加班，在大家的共同努力下，项目终于按时完成，男主小心翼翼地在同事面前测试程序，衬衣领口的双色纹路，避免了视觉上的枯燥感，同事们在身后欢呼，男主深藏功与名，穿一件白衬衣，幻想自己是阿泰尔，千军万马避白袍，写程序没有 Bug。\n20180724012656554104-2018725\r4、每个程序员都会有一件格子衬衫，仙剑之父姚壮宪更是穿了一辈子格子衬衫。讲道理，男主穿格子衬衫难看吗？为什么程序员穿格子衬衫和特步鞋就要被黑到异次元呢？其实，只要不是浮夸的大格子衬衫，穿起来一样萌萌哒，关键是合体！当然，只要一胖就完啦。所以，穿搭是技巧，健身是根本啊。\n20180724012839470105-2018725\r5、女人变美只需要一只口红，而男人变帅只需要一条领带。男女主决定协议结婚后，召集双方父母商议结婚事宜。一套贴合肩线的西装，搭配一件白色衬衫，视觉上给人成熟稳重的感觉，男主虽然在剧中表现得很“怂”，可这并不影响他的“帅”啊，这套衣服最多算彩排，真正的新郎礼服请关注第 11 集……(嗯，这是最后一集，日剧追起来很快呦)\n20180724012746230106-2018725\r6、简洁到不能再简洁的短袖衬衣 + 牛仔裤。前一秒的踌躇满志，同下一秒的惊慌失措，莫名地戳中萌点，明明同事就在眼前，非要学人家卷福发短信。请女生们不要再吐槽男生穿衣服“土”，你告诉我，除了长裤和短裤我们还有什么？对了，短裤是不能穿的哦……，尤其是花花绿绿的那种🙃\n20180724012906742107-2018725\r7、蓝白相间的衬衣，相比普通蓝色衬衣，平添了一种活泼的感觉，就连工牌卡的绳子都来凑热闹。你知道怎么快速从人群中识别一名程序员吗？牛仔裤 + 双肩包 + 工牌卡。不，我拒绝这种符号化的穿搭，大隐隐于市，忘了这套新手村装备吧……当然，如果你包里还是各种数据线……好像换汤不换药啊(逃\n20180724013156508108-2018725\r8、任何领域都会鄙视链的存在，像津崎先生这样优秀的工程师，自然远非某某培训班的学生们。如何做一名优雅的学院派呢？你需要一件毛衣或者是一件马甲，而且一定要套在衬衣上。你问我为什么这么穿，因为通常教授们都这样穿，请参考卷福主演的电影《模仿游戏》，负责破译德军恩尼格码密码机的专家们，都是这样的穿着，同样的，还有《万物李军》里剑桥的教授们……\n20180724013332938109-2018725\r9、同样是毛衣和衬衣的搭配，圆领和 V 领是一种风格，是否翻出衬衣领又是一种风格。而我们的男主，显然可以同时驾驭这两种风格，再搭配一件休闲外套，试问还有谁？风见君帅是帅了，不过他的衣服好像永远都是针织衫啊，难道说有钱人都喜欢买一堆一样的衣服？恩，我说的就是老乔和小扎这种有钱人……\n20180724013332940110-2018725\r10、果然，有圆领就会有 V 领，强迫症对工牌卡挂绳莫名地充满好感，这个“V”字完美地贴合衣领。针织衫和衬衣，需要有一定的层次感，比如备受我们嫌弃的格子衬衫，如果搭配针织衫效果还是非常不错的，唯一的要求或许是肩膀不能过宽，因为这样会显得整体线条僵硬。我有一个问题，像女主这样宽肩膀的女生，穿一字肩真的不怕滑下来吗？😂\n20180724013332943111-2018725\r11、这种“假领”的毛衣，穿出来同样好看，我严重怀疑，这个创意是来自上海静安区同福里的老马。如果你的脖子比较长，可以考虑尝试下高领毛衣，请注意，我不是在教你，去做一名女装大佬。话说回来，衬衣上套毛衣最大的缺点是，需要挤上衬衣最上面的扣子，所以买衬衣时，请确保可以放入两根手指，这样子不会像《杀破狼 2》里的张晋一样被“帅”死。\n20180724013332944112-1-2018725\r20180724013332946112-2-2018725\r12、这里分别是针织衫和毛背心搭配格子衬衫的正确示例，简而言之，衣服的搭配上需要体现出层次感，切忌选择色调过于接近的颜色，衬衣一定要修身，否则搭配毛衣会让你显得臃肿不堪。我要立一个 flag，等我瘦到 120 斤，我就奖励自己一件针织衫。\n20180724013332949113-1-2018725\r20180724013332951113-2-2018725\r13、毛衣和针织衫真的是搭配率超级高的优质单品，穿出来真的非常好看。我知圆领 T 恤是夏天最常见的穿搭，可如果你想尝试下不同的风格，我建议你买一件衬衣或者是 Polo 衫或者是针织衫，这些都能带给你不一样的感觉。我一直想尝试皮夹克或者是牛仔外套，可我自我感觉不适合这样硬朗的风格，谁让我是一个温柔的蓝孩纸呢……\n室外篇 20180724013619267201-2018725\r1、这种材质的衣服应该很容易脏，而且大概率会让你显得臃肿(胖)，可不得不说，这一身和女主站一起挺搭的，我们学习穿搭只有两个目的，找到女朋友和不给女朋友丢脸(🙃)。作为围巾控，这身搭配我觉得可以尝试一下。\n20180724013619269202-2018725\r2、一个男人，只要有一件合身的西装，就已经在变帅的路上迈出一大步。这一款的话，毛衣黑白两种颜色，和衬衣蓝灰白的色调蛮接近的，所以基本上看不出层次感来。其实一直不明白男主为什么如此沉闷的颜色，难道是因为向女主表白以后变成熟了吗？😂\n20180724013619273203-2018725\r3、你看，这件衣服再次发挥了格子衬衫的伟大魅力，而在这件蓝色的针织衫的衬托下，可以明显地感觉到男主变“白”了，当 90 后们开始步入中年职场，不妨尝试穿一点靓丽的颜色，因为我们还可以再年轻一下。池日先生又讲了一句“名言”，你看津崎先生这震惊的小眼神。\n20180724013619275204-1-2018725\r20180724013619277204-2-2018725\r4、这一次，男女主在众人“陪伴”下开展了一次小旅行，男主所穿的这件短袖衬衣真的是最普通的衣服，目测在某澜之家就可以找到同款，搭配这斜挎的帆布包简直是减龄神器，我真心羡慕那些三十多岁还会被认成学生的“大叔”们，在下高中刚毕业就被叫叔叔到现在，人家明明想被叫做“哥哥”😓\n20180724013619279205-1-2018725\r20180724013619281205-2-2018725\r20180724013619283205-3-2018725\r5、为什么这三件毛衣给人越来越帅的感觉？因为你发现它的颜色越来越纯粹，纯粹到最后就剩下一种颜色，所以，人家建议衣服上不要有 Logo 不无道理啊，在下有一位朋友，喜欢穿各种印有二次元图案的 T 恤，30 多岁了永远都给人萌萌哒的感觉，你说到底听谁的好呢？总之，我计划今天买一件纯色毛衣，你呢？\n20180724013619285206-2018725\r6、呃……这件应该被称为棒球衫还是夹克呢？我个人不太喜欢这种拼接的样式，我更喜欢那种纯色的简洁的夹克。说起这一集，男主因为错过女主的生日而自责，独自到商城里为女主挑选礼物，面对琳琅满目的商品，男主一脸茫然……有时，女生会嫌弃男生分不清口红色号什么的，并送给男生一个“直男”称号，其实，面对不熟悉的领域，谦虚而大方的承认就好了，我们当然是直的，难道你们喜欢弯的吗？\n20180724013619286207-2018725\r7、你一定觉得像男主这样西装革履的高薪人士，每天都是坐在电脑前喝喝咖啡写写代码。其实，我们是一群连星巴克都不舍得去的人，每一次紧急加班，都是咖啡因转换为二进制代码的过程。我们并不是不会花钱，我们在数码产品、电子设备等方面的投入，完全不亚于你们买衣服、做美甲等等。有判词云：钱多、话少、死得快\n20180724013619288208-2018725\r8、嗯，这件怎么评价呢？中规中矩的秋冬款外套。我一直有一个愿望，等瘦下来以后买件卫衣穿，因为我实在怀疑自己，穿任何套头的衣服都会显得胖。不过好在这是秋冬季节，大家一起胖呀，这种衣服应该会比较容易脏，因为在下就有一件差不多的，果然直男审美啊，呵呵🙃\n20180724013619290209-2018725\r9、这个世界对长得高的人相当宽容，即使他们长得并不好看，可他们长得高穿衣服好看啊。从此刻开始，我希望你打破这种认知，谁说矮个子男生不能穿长款的衣服，男主这就是活生生的例子啊，我知道女生都喜欢 175+的男生，可我希望你能找到我除了不能举高高以外的优点，女主到菜场就买了棵葱回来，男主赶紧接过来拎在手里，真是适合过日子的人啊\n20180724233648400210-2018725\r10、这是整部剧出镜率最高的一套衣服。什么？你问我这是哪一集？话说，你们都不看片尾曲的吗？由男主演唱的单曲《恋》，着实为抖音贡献了大量流量，这舞蹈难道不可爱吗？这衣服难道不好看吗？我说过了，衬衣 + 背心是学院派的典型穿法，男主果然是个文艺的男孩子，他的帅你 Get 到了吗？\n本文小结 这或许是我写过的最“八卦”的一篇博客啦，有时候，越是轻松的东西越容易被人接受。程序员，他们并不是情商低，并不是内向，并不是不会撩妹，仅仅是因为这个世界不单单需要娱乐精神，同样需要严谨和专注。搅动一个人的情绪，无非是分泌出某种荷尔蒙；而真正驱动这个世界的，是严格甚至苛刻的规则。面对不熟悉的领域，应该保持敬畏心，而非以标签化的定义以讹传讹；如果靠贴标签就可以给人分类，那么谁是好人谁又是坏人？谁代表了正义谁又代表了邪恶？哦哦，对了，我们不会修电脑以及做任何你认为简单的事情……\n","date":"2018-07-25T10:11:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/94443781/","slug":"94443781","tags":["日剧","程序员","穿搭","八卦"],"title":"草食系程序员的穿搭指南"},{"categories":["生活感悟"],"content":"\r一直想约朋友去看场电影，可是要找一部两个人都喜欢看的电影，当真是一件非常困难的事情。直到遇上了姜文的新片《邪不压正》，愿望终于在这个周末达成。说到姜文的电影，总是不可避免地提到“政治隐喻”这个词汇，所以，对这部电影而言，导演自成一体的独特风格，让其在与普通商业片拉开差距的同时，更将观众推向了一个略显尴尬的境地，以至于散场时朋友的第一反应是：好像完全没有看懂。\n电影一开始，茫茫雪地里闪现出两个模糊的背影，向着雪地深处无限地延伸。而此时此刻，在火红的灯笼的映衬下，屋内一众人正忙着为师父庆贺寿辰，两位不速之客的到访，让一切瞬间化为烈焰中的修罗场。可以说，开篇这一场极具暴力美学的戏份，的确是可以吸引人眼球的戏份。姜文电影里有一种与生俱来的英雄主义，所谓的硬汉精神，于是你看到了身负东西方文化的李天然，是握着一把武士刀参与刺杀任务，而信奉武士道精神的根本一郎，果真是单刀赴会，说让三刀就是三刀。可这位武术名家，甚至连出手的机会都没有，就被手枪击中了头颅，武术在坚船利炮前又算得了什么呢？\n不知道大家有没有注意到这样一个细节，朱潜龙和根本一郎闯到师父家里时，师父说了句：没听到狗叫，这是否是因为，在向师父祝寿时，跪拜的声音掩盖了炸弹的声音。联想到《让子弹飞》里，张牧之到鹅城上任，对老百姓说，“不许跪，皇帝都没有了，没有人值得你们跪……”。同样地，朱潜龙在师父面前，一样跪得可谓是以头抢地，可下一秒子弹就在师父脑袋上留下弹孔……其实，人蠢一点没有关系，毕竟都跪了几千年，可偏偏人还有点儿坏。师父问朱潜龙为什么日本人不在日本种植鸦片，朱潜龙说日本是文明的国家。\n日本从明治维新以后，自上而下全面效仿西方国家，因为他们看到曾经最为强大的中华帝国，在鸦片和战争的侵蚀下早已满目疮痍。日本大河剧《坂上之云》里有一个片段，男主秋山真之在东京街头看到英国人欺负日本人，他愤怒地质问老师，为什么英国人在这里不讲绅士文化，他的老师不无遗憾地说，唯有强者有资格讲绅士文化。当时的日本不见得有多么文明，但那种全民参与到战争中的举动，在当时的世界格局里无出其右者，譬如日本曾担心和美国发生战争，起初民众讨论的是如何避免这场战争，后来则变成能否打赢这场战争，最后则变为如何打赢这场战争。\n廖凡饰演的朱潜龙\r朱潜龙在影片中是一个典型的汉奸，他帮日本人种鸦片，是希望在日本人的扶持下做个傀儡皇帝。在七七事变前，日本人借助麻姑囤事件，杀死了不愿意合作的张作霖，而朱潜龙自认为是大明后裔，一心想着要反清复明，可讽刺的是，溥仪在日本人的扶持下建立了伪满洲国政权，他居然天真地相信，日本人会允许两个傀儡政权同时存在。于是，在裁缝铺里李天然看到“龙袍”，导演不无幽默地说，这是准备去参加巴黎时装周的，仔细想起来，这是否是在讽刺某位穿着“龙袍”去参加电影节的演员呢？可朕的大清都灭亡了，你反什么清复什么明嘛，真有种《天龙八部》里慕容世家妄图兴复一个灭亡 100 余年的大燕国的痴狂劲儿。\n姜文镜头下的北平城\r姜文一心想要还原一个老北京的全貌，可我感觉在这部电影里看到的北京整体偏“白”一点，印象最深刻的地方是，老亨得利带着儿子从火车站回来，镜头里的北京好像刚下过雪一样。可或许是我们本不了解北京，故宫那种红墙青瓦的印象是从新中国成立以后的啦。梁思诚夫妇当年在战争中保护下来的古建筑群，或许本来就是这个样子的。于是，在姜文导演的镜头里，我们看到带着京味儿的北京胡同，看到了发生过无数故事的东交民巷，看到了曲折蜿蜒的八达岭长城，看到了古香古色的钟楼牌坊。李天然在屋顶跟踪朱潜龙的汽车时，我开玩笑地对朋友说，“以后刺客信条要出中国近代史系列游戏，完全可以参考李天然这个设定”。\n姜文饰演的蓝青峰\r这一次姜文饰演的蓝青峰，这个角色在我看来相当复杂：想要除掉朱潜龙和根本一郎，但私底下跟这两个人都有来往；和老亨得利有 25 年的交情，因为李天然身份暴露对其痛下杀手；被朱潜龙禁锢在家中无法自救，个人实力强弱被敌人查探地一清二楚；作为参加过辛亥革命的前辈，有且只有李天然一个下级……凡次种种，不一而足。从他的名字，我联想到“青出于蓝而胜于蓝”以及“青峰侠”，电影里李天然的英文名字叫做布鲁斯，他和师兄比武时致敬了李小龙的《龙争虎斗》，黑色的中国传统服饰，李小龙标志性的步法动作。可其实说到底，蓝青峰在精神上是懦弱的，因为他完全不清楚自己要做什么，那时国内外形势风起云涌，可他到底能真正地依赖谁，或许连他自己都不知道，他觉得李天然对他有用，就花了十余年时间去布局，李天然不过是他的一枚棋子……\n蓝青峰的计划是让朱潜龙和根本一郎产生矛盾，朱潜龙杀死根本一郎后，再用李天然做交换。按照这个计划，李天然回国的确是来送死的，除非他可以在交换后杀死朱潜龙。蓝青峰害怕杀死根本一郎会引发战争，可从电影中来看，根本一郎并不是日军的高级军官。或许很多时候，人们都相信刺杀一两个人就可以让战争结束。全智贤在《暗杀》里说过这样一句话，“刺杀一两个日本人，能不能结束一场战争，我是不知道的，但我总要告诉人们，我们一直在战斗”。所以，即使李天然终于手刃仇人，卢沟桥的炮火依旧会在这个城市轰鸣。李天然凭借一腔热血，毫无来由地杀死了几个日本人，固然会让人激昂澎湃，可真的就是邪不压正吗？李天然的复仇，在我看来，是杀死懦弱的“自我”的过程，因为无父无母，李天然其实一直生活在“我是谁”、“我要去哪里”、“我要做什么”的自我怀疑之中，\n许晴饰演的唐凤仪\r唐凤仪，一个愿意陪着朱潜龙做皇帝梦的女人，习惯了被男人驱使和奴役，可被李天然恶作剧般在屁股上以后，她终于明白，自己在朱潜龙心中不过是一个玩物，尤其是六国饭店里的那场戏，看似不露痕迹地打朱潜龙耳光，实则这个敢爱敢恨的女人形象立了起来，回敬李天然的“凤仪之宝”，通过关巧红给李天然通风报信，日军进城时城墙上的一跃，都是这个角色留给人的深刻印象。所谓“商女不知亡国恨，隔江犹唱后庭花”，风尘女子的这种刻板印象，在姜文的电影里是不存在的，她们不单有性感的身姿，更有热血的灵魂。侵略者端坐在石狮子上准备拍照，被从城墙上一跃而下的唐凤仪撞倒在地上，当时电影院里发出一阵笑声，可这无非是一个女子的反抗而已。\n姜文对老婆是真爱\r与之相对的关巧红，她美好得宛如江南恬静的女子，她同样是一种独特的美感，和唐凤仪这种艳丽的画风不同，她是像迷一样的女子，背后有太多故事没有说完，看似惊鸿一瞥地讲了放脚、开裁缝铺这些琐碎的事情，但永远给人一种“这个女人不简单”的感觉，她好像无论什么时候，都能找得到李天然；她好像对李天然有种莫名的情愫，可又清楚地知道自己想要做什么……喜欢上这样的女人，就像喜欢上一朵云，你看云时很近，而云看你时很远。即使到了故事的结尾，她依然像阵风飘然远去，留下原地惆怅的李天然，明明李天然爬屋顶比她要好，可要寻找她时，又要去哪里寻找呢？有时候，这像是朴树的《那些花儿》散落天涯，有时候，又像是泰戈尔的“生如夏花般灿烂，死如秋叶般静美”……\n说实话，这一次彭于晏的角色设定让人很出戏，因为这个角色本身的真实感并不强，即使他可以飞檐走壁，即使他可以躲开子弹。究其本质，是因为李天然身上有着勇敢而又懦弱的矛盾性格，未回国时，他一心想杀根本一郎和朱潜龙报仇；等回国后，他突然像被定住一般不知所措。第一次莽撞间接造成老亨得利被杀害，第二次莽撞直接导致蓝青峰被软禁。彭于晏一直都是一个“孤儿”，无论是师父、老亨得利还是蓝青峰，其实都不见得有多爱他。一个心中带着复仇愿望的人，一旦真正地手刃了仇人，他存在的意义又会是什么呢？所以，他怕自己因为复仇而变得迷茫，李天然看似身负正义之名，可对于师门武学的传承并无实际意义，相反，是那个杀死师父的朱潜龙，为师父塑像扬名，让师父成为大家所称赞的武术名家，到底谁是正？谁是邪？当周围人都是在利用你，杀了朱潜龙，李天然将失去存在感；而杀了李天然，日本人可以随时除掉朱潜龙。跪在岳飞目前的秦桧夫妇，和被塑成一条狗跪在武术名家塑像前，是否具有异曲同工之妙？普通人会在乎真相到底是什么样子的吗？\n一个再简单不过的寻找“爸爸”的故事，对于那时的中国，是否就像年轻而莽撞的李天然，在探索着“我是谁”、“我要去哪里”、“我要做什么”的终极哲学命题。亨得利父子出城遇见正在演习的日本军官，对方声称亨得利父子的驴子挡住了坦克，破坏了军方的演戏计划。亨得利父子以美国护照作为挡箭牌，日本军官不得不去找这两头驴子的晦气。多年以后，吴京在《战狼》系列里重复着美国护照和海军陆战队的老梗，只是此时的中国早已不再是那个家国积弱的中国。日本军官质问李天然为什么穿着日本和服，可彼时彼刻，根本一郎自作聪明地曲解论语中的含义，又是否是在告诉我们，从外表上模仿何其容易，可一旦要张嘴说话，就很容易被人识破。曾经日本在全面欧化的过程中，被西方人讥讽为穿着衣服的猴子，我们都曾经模仿过他人，一如今天“韩式审美”在中国流行。这是一个时代里的众生相，愿每个人都能找到“真我”，不再犹豫，不再怯懦，勇敢地面对自己，发现自己。\n","date":"2018-07-23T10:48:48Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1099762326/","slug":"1099762326","tags":["影评","电影","邪不压正"],"title":"邪不压正：本我的发现之旅"},{"categories":["编程语言"],"content":"自从项目上采用敏捷开发的流程以后，我们的开发任务中出现了不少“联调”的任务，而所谓的“联调”任务，完全是拜前后端分离所赐。通常来讲，按照前后端分离的思想，我们的团队会被分成前端和后端两个组，前端负责页面内数据的展示，后端负责提供相关服务的接口。这样听起来非常合理，对吧？可问题在于，后端常常在等前端联调这些接口，因为后端不知道具体有哪些异常需要处理；同样，前端常常在等后端接口稳定，因为一旦出现问题，就会导致接口发生变更。虽然在此之前，我们早已花了一周左右的时间去讨论接口，接口文档早已伴随着 API 部署到线上，可我们依然需要大量的时间去沟通每个接口的细节。用一种什么样的语言来描述这种状态呢？大概就是人们并不是真的需要接口文档，因为真的不会有人去看这东西。\n从敏捷开发到产品架构 为什么会出现这种情况呢？我想，可以从三个方面来考虑，即设计不当、进度不一、沟通不畅。有时候集思广益去讨论一个接口，可能并不是一件好事，因为考虑的因素越多，问题就会变得越复杂，相应地妥协的地方就会越多。我并非不懂得做人需要适当妥协，事实是从妥协的那一刻起，我们的麻烦越来越多。有人问怎么能消灭 Bug，我说消灭需求就可以了。现代人被各种各样的社交网络包围着，以至于隐私都被赤裸裸地暴露在空气中，可你很难想象人与人之间的沟通会越来越困难，难道是因为社交网络加剧了人类本身的孤独？没有人是一座孤岛，可前后端分离好像加剧了这种界限。现在动辄讲究全栈，可当你把精力都耗费在这些联系上去，你如何去追求全栈？相反，我们像电话接线员一样，在不停地切换上下文，因为我们要“敏捷”起来，可作为工程师就会知道，切换上下文需要付出相应的代价。\n我之所以提到这样一个场景，是出于对当前项目的一种整体回顾。我们的项目是一个客户端产品，但是它依然体现了前后端分离的思想。受业务背景限制，这个客户端采用了 Native + Web 的技术架构。如果你了解整个互联网产品形态的演变历程，就会对这种技术架构非常的了解，从曾经的 Native 和 Web 之争，到所谓的 Hybrid App，再到如今的 React Native 及小程序，这种技术架构其实一直都存在，譬如 Electron、Atom、Node-Webkit、Cordova、Ionic、VSCode 等等，其实都是非常相近的技术。对应到我们的项目，我们提供了一个 JSBridge 来完成 Native 层和 Web 层之间的通信，而客户端的渲染实际上是由前端来完成的，所以你可以想到，我们通过一个 WebView 来加载页面，而平台相关的交互由 C++/C#来完成，所以，理论上客户端是是一个和 Electron 类似的壳子(Shell)，它可以展示来自任何页面的内容。\n以JSBridge为核心的系统架构图\r从客户端的角度来讲，它是 Native 层接口的提供者，连接着平台相关的 API，并集成了第三方的硬件设备，所以，理论上它是和具体业务无关的。可实际上，因为 Web 层不能直接和文件系统交互，所以，像上传、下载这样本该由前端调用的接口，部分地转移到了客户端这边，所以，客户端无可避免地受到后端 API 变化的影响，因为业务上需求存在差异，上传接口前后共发生了三次变化，所以，客户端中存在三个版本的上传，当然，我相信这是一个设计上的问题，通过改进设计可以得到完美的解决。关于上传为什么会这么复杂，感兴趣的朋友可以通过留言来一起交流。这里我想说的是什么呢？因为客户端希望与具体业务无关，所以，客户端注定是以功能来划分服务，然后通过 JSBridge 暴露给 Web 层。可是对后端的微服务架构而言，它的服务是以业务为主导的，它的一个业务就是一个接口。由此导致一个问题，后端接口的数量不断增加，客户端面临频繁地改动。\n不做平庸的 ApiCaller 有很多人说，今天的编程工作变得越来越简单，对于这一点我非常认同。因为，无论是无论是语言、工具、生态、平台，都获得空前的繁荣，所以，我们大多数人的工作，可能就是调用现成的 API，而少数人的工作，可能就是提供友好的 API，甚至连代码你都可以在 Google 上找到，你唯一要做的就是 Ctrl + C \u0026amp; Ctrl + V。当初想要改变世界的你我，突然有一天就变成了 ApiCaller，甚至大多数的框架，你连底层细节都无从得知。可你真的打算做一个平庸的 ApiCaller 吗？至少我是不愿意的，因为在我看来，调用后端提供的 API，大多数情况下都是换个 URL，或者换个参数，这样的代码你写一次以后，剩下的基本就是复制和粘贴了，你可能会非常鄙视我的这种行为，可事实就是这样的，不单单我在复制，连我身边的同事都在复制。可这能怎么办啊，只要后端提供了新接口，或者是对接口进行了调整，而这些接口必须由客户端封装，我们的工作就永远不会停止，可这不过调用后端的 API 而已啊！\n有时候，我们会说工作经验和工作时间未必是正相关的，因为如果我们十年都在做一件事情，那么其实和一年是没有区别的。为了避免成为一个平庸的 ApiCaller，你必须思考那些真正重要的事情。怎么能降低后端 API 变化对客户端的影响呢？降低耦合度。怎么降低耦合度呢？依赖抽象而非依赖具体。想想 WebService，它通过 WSDL 来对服务进行描述，而通过 WSDL 就可以在客户端创建代理类，一旦 WebService 发生变更，重新生成代理类就好。再回想一下，调用后端 API 会遇到那些问题？设置 Header、设置 Cookie 、拼接 URL、拼接参数、URLEncode、SSL、JSON 序列化、FormData、上传文件、编码/解码等等，是不是每一次都在处理这些问题？看到项目里用 HttpWebRequest 去构造 Mulitpartfile 结构，我忽然间觉得绝望。既然每次都是翻来覆去这些东西，为什么要用手来写？API 文档构建工具可以帮助用户生成 curl 以及常见语言对应的代码，所以，我有理由相信，我们需要一个东西来帮助我们完成这个工作，就像 WebService 生成代理类一样。那么，有没有这样一个东西呢？这就是本文的主角——基于声明式的 RESTful 风格的客户端：WebApiClient。\n.NET 下的 Retrofit：WebApiClient WebApiClient 是.NET 平台下的 Retrofit。要理解这句话，首先要理解 Retrofit。什么是 Retrofit 呢？Retrofit 是一个 Android/Java 下的网络通信库，其本身基于 okHttp，熟悉 Android 开发的朋友，对这个库应该不会感到陌生。Retrofit 帮助我们解决了上文中提到的，在请求一个 Web API 时会遇到的问题，并通过注解这种技术，以一种声明式的方式来定义接口。简单来说，所有你想要调用 Web API 都是接口中的一个方法，你通过注解来告诉 Retrofit，该方法会请求哪一个 Web API，参数会以什么样的形式传递过去，结果会以什么样的形式返回回来，你完全不必去写那些底层 HTTP 通信相关的东西，因为 Retrofit 会帮你在运行时实现这个接口。所以，我们说 Retrofit 是一种声明式的 HTTP 客户端。声明式我们见过相当多啦，Java 里的注解，C#里的 Attribute、Python 里的装饰器、JavaScript 里的修饰器，以及如今各种各样的双向绑定框架。下面，我们来一起看看 WebApiClient 这个库。\n现在，我假设你手里已经有可供调用的 Web API，并且你真实地了解这些 Web API 是如何工作的。至此，我们需要完成的工作主要都集中在客户端，这里我们编写一个控制台应用来完成这一工作。首先，需要在项目中引入 WebApiClient 这个库，我们直接通过 Nuget 来完成安装即可(注：这里共有 Laojiu.WebApiClient、WebApiClient.JIT 和 WebApiClient.AOT 三个版本，博主使用的是 Laojiu.WebApiClient)。使用 WebApiClient 的基本流程是：首先，定义一个继承自 IHttpApiClient 的接口并在接口中声明相关方法；其次，通过 Attribute 对接口中的方法和参数进行修饰以完成和 Web API 的绑定；最后，通过 WebApiClient 生成该接口的一个实例，而通过调用相应的实例方法就可以得到结果。这是不是和代理类的感觉非常像呢？像博主这样懒惰的人，或许连接口都不愿意亲自去写，因为我相信越是严谨的规则，就越是适合应用到自动化上面去。这样说可能无法让大家形成对 WebApiClient 的直观印象，那么让我们从一个简单的例子开始吧！\nGet 请求接口 [HttpHost(\u0026#34;http://localhost:8000\u0026#34;)] public interface IValuesApiCaller : IHttpApiClient { //GET http://localhost:8000/values1 [HttpGet(\u0026#34;/values1\u0026#34;)] [OAuth2Filter] ITask\u0026lt;string\u0026gt; GetValues(); //GET http://localhost:8000/values1/{id} [HttpGet(\u0026#34;/values1/{id}\u0026#34;)] [OAuth2Filter] ITask\u0026lt;string\u0026gt; GetValue(int id); } 在这个示例中，我们展示了 WebApiClient 是如何处理带参数以及不带参数的 Get 请求的。通过 HttpGet 特性，我们分别为 GetValues()和 GetValue()两个方法指定了请求的 URL。虽然在这里我们指定一个完整的 URL，可是考虑到我们 Web API 通常都是分布在不同的域名下，所以我们可以通过 HttpHost 特性来配置一个 BaseURL。接口的返回值为 ITask，我们可以通过我们的需要指定相应的类型，在这里我们以 ITask为例，特别说明的是，如果服务器返回的是标准的 JSON 格式，那么我们可以将其映射为相应的实体结构，这就需要使用 JsonReturn 标特性对方法进行修饰。我们知道 Get 请求可以通过 QueryString 形式来进行传参，那么这一点在 WebApiClient 中如何实现呢？这就用到所谓的**\u0026ldquo;平铺参数\u0026rdquo;**，即我们在方法中声明的参数会被 WebApiClient 自动地追加到 URL 上面去，再不需要去手动地拼接这些参数；同理，这些参数可以用一个包装类封装起来，具体大家参考官方文档。\nOK，现在来看看如何调用 IValuesApiCaller 这个接口。我们在前面说过，WebApiClient 会帮助我们生成一个 IValuesApiCaller 的实例，所以我们调用一个 Web API 的时候，关注点已然从之前的过程实现转变为接口实现，这正是我们渴望看到的局面。一个非常简洁的调用示例：\n//调用Values Service using (var client = HttpApiClient.Create\u0026lt;IValuesApiCaller\u0026gt;()) { Console.WriteLine(\u0026#34;-----Invoke Values Service-----\u0026#34;); var results = await client.GetValues().InvokeAsync(); Console.WriteLine($\u0026#34;results is {results}\u0026#34;); var result = await client.GetValue(10).InvokeAsync(); Console.WriteLine($\u0026#34;result is {result}\u0026#34;); } Post 请求接口 接下来，我们再来说说 Post 请求接口。同样的，这里我们使用博主编写好的一个 Service，我们称之为 Student Service。它使用了 EF Core 来完成数据库的读写，它提供了一组和 Student 实体相关的 API，这里我们使用它来作为 Post 请求接口的示例实现。因此，我们首先定义一个接口 IStudentApiCaller：\n[HttpHost(\u0026#34;http://localhost:8000\u0026#34;)] public interface IStudentApiCaller : IHttpApiClient { //GET http://localhost:8000/student [HttpGet(\u0026#34;/student\u0026#34;)] [OAuth2Filter] [JsonReturn] ITask\u0026lt;List\u0026lt;Student\u0026gt;\u0026gt; GetAllStudents(); //POST http://localhost:8000/student [HttpPost(\u0026#34;/student\u0026#34;)] [OAuth2Filter] ITask\u0026lt;string\u0026gt; NewStudent([JsonContent] Student student); } 这里重点关注接口中的第二个方法。首先，它是一个 Post 请求；其次，它接受一个 JSON 格式的文本作为它的请求体，所以我们这里使用了 JsonContent 特性。前面我们提到过，接口返回类型 ITask，可以映射为对应的实体结构。注意到 GetAllStudtents()这个方法中绑定的 API，它负责从数据库中查询所有的 Student 信息并以 JSON 形式返回，所以这里我们将其映射为 List。与此同时，你会注意到 JsonReturn 特性，这是在告诉 WebApiClient，你希望将返回的结果映射为强类型的模型；同理，你可以使用 XmlReturn 特性来处理返回值为 Xml 的情形。除此之外，你还可以使用 FormContent 特性来修饰方法参数，其作用是将模型参数以 key1=value1\u0026amp;key2=value2……的形式写入请求体中，对应于 x-www-form-urlencode；更一般地，你可以使用 FormField 特性修饰方法参数，以 form-data 的形式写入请求体中。Mulitpart 是博主最为讨厌的一种数据格式，请大家自己去看官方文档。\n过滤器与 OAuth2 无论如何，请允许我说，这是我最喜欢的一个特性。大家会注意到，在我的示例代码中，有一个东西一直没有去说，这就是 OAuth2Filter，这其实是博主自己扩展的一个特性，这意味着在请求该 API 前，需要通过 OAuth2 授权以获得身份令牌。对于这一点，我想大家都是清楚的，因为在微服务架构中，Web API 是作为一种受保护的资源而存在的，所以鉴权和授权是非常重要的点。以博主的项目组为例，我们做到第三个项目的时候，整个后端的 OAuth2 认证服务终于实现了统一，可即使如此，每一次这种基础设施都需要联调，都要考虑到底使用哪一种授权模式。譬如，客户端是考虑把 token 存放在全局静态类里，而前端是考虑把 token 存放在 Cookie 里，甚至在此之前，我们连 refresh_token 都没有，客户端在调用 Web API 时天天担心 token 过期，于是在调用 Web API 时主动去刷新一次 token。你问我为什么不判断一下 token 有没有过期，因为后端没有提供这个接口呀。其实，我想说的只有一句话，基础设施请交给框架去处理。\nWebApiClient 提供了用于请求管道中的过滤器，可以让我们在请求前、请求后搞点事情。譬如，我们这里希望在请求前获取 token，并将其追加到当前请求的 Header 里，或者是在请求前判断下 token 是否过期(假如后端愿意开发这个接口的话)，如果过期了就自动刷新下 token，该怎么做呢？首先，我们定义一个 IAuthApiCaller 的接口，它负责从认证服务器上获取 token，这里选择客户端模式：\n[HttpHost(\u0026#34;http://localhost:28203\u0026#34;)] public interface IAuthApiCaller : IHttpApiClient { [HttpPost(\u0026#34;/oauth2/token\u0026#34;)] ITask\u0026lt;string\u0026gt; GetToken([FormField] string client_id,[FormField] string client_secret,[FormField] string grant_type = \u0026#34;client_credentials\u0026#34;); } 接下来，我们继承 ApiActionFilterAttribute 来编写 OAuth2FilterAttribute，显然，它会在请求前调用 IAuthApiCaller 接口实例，这里我们将 client_id 和 client_secret 硬编码到代码里，单单是为了演示如何去印证这个想法，实际项目中大家可以考虑通过配置或者是传参来实现：\n[AttributeUsage(AttributeTargets.Method)] public class OAuth2FilterAttribute : ApiActionFilterAttribute { public override Task OnBeginRequestAsync(ApiActionContext context) { using (var client = HttpApiClient.Create\u0026lt;IAuthApiCaller\u0026gt;()) { var client_id = \u0026#34;578c06935d7f4c9897316ed50b00c19d\u0026#34;; var client_secret = \u0026#34;d851c10e1897482eb6f476e359984b27\u0026#34;; var result = client.GetToken(client_id, client_secret).InvokeAsync().Result; var json = JObject.Parse(result); var token = json[\u0026#34;access_token\u0026#34;].Value\u0026lt;string\u0026gt;(); context.RequestMessage.Headers.Authorization = new AuthenticationHeaderValue(\u0026#34;Bearer\u0026#34;,token); return base.OnBeginRequestAsync(context); } } } 至此，我们只需要给需要需要授权的 API 添加 OAuth2Filter 特性即可，全然不需要考虑这个 token 如何储存的问题。我对静态类和静态方法没有误解，仅仅是因为它是反模式的，任何全局内可以修改的成员，不管有没有人会去修改，它始终都是不安全的。在此我要表扬一下前端的同事，他们通过扩展 ajax 方法原型，实现了和这里类似的东西。所以说，你要多尝试去看看不同领域里的东西，抓住那些相同或者相似的本质，而不是被那些“旧酒换新瓶”的概念所迷惑，技术圈子的热闹有两种，一种是发明新的技术，一种是发明新的概念，我本人更喜欢第一种，你呢？\n上传与下载 其实，上传应该是 Post 请求的一种类型，可是考虑到下载的时候，接口的返回类型应该是数据流，所以我决定将这两个内容一起来讲。这里我们就考虑单纯的上传，不考虑由文件和键值对混合组成的 MulitpartFormDataContent，因为这种结构让我觉得厌恶。这里，我们直接通过 ASP.NET Core 编写了一个文件上传/下载的 Service，同样地，我们首先定义 IFilesApiCaller 接口：\n[HttpHost(\u0026#34;http://localhost:8000\u0026#34;)] public interface IFilesApiCaller : IHttpApiClient { //Post http://localhost:8000/files/upload [HttpPost(\u0026#34;/files/upload\u0026#34;)] [OAuth2Filter] [JsonReturn] ITask\u0026lt;string\u0026gt; Upload([HttpContent]List\u0026lt;MulitpartFile\u0026gt; files); //Get http://localhost:8000/files/download/{fileId} [HttpGet(\u0026#34;/files/download/{fileId}\u0026#34;)] [OAuth2Filter] ITask\u0026lt;HttpResponseMessage\u0026gt; Download(string fileId); } 在这里，上传我使用了 ASP.NET Core 中的 IFormFile 接口，并且在 Postman 测试通过，可是在网页上用 type 为 file 的 input 标签进行测试时，发现页面一直无法正常响应，不知道具体是什么原因(后来发现它完全和 Postman 中的请求体一样，好吧😬)，我一直不太理解 ajax 上传和表单上传的区别，曾经项目上用 HttpWebRequest 去做文件的上传，里面需要大量的字符串拼接动作去构造 MulitpartFormData，只要后端上传的 API 发生变更，这段代码几乎就会变成不可维护的代码，幸运的是，在经过几次迭代以后，他们终于意识到了这个问题，在我的建议下，他们使用 HttpClient 重构了代码。在这里你会看到 Download()方法的返回值类型为 ITask，这是 HttpClient 中使用的数据结构。为什么我推荐大家使用这套 API，因为它和 ASP.NET 中的数据结构是一致的，而事实是上，WebApiClient 正是在 HttpClient 的基础上完成的，所以这里你能够想到，我将通过 HttpResponseMessage 来获取返回的数据流，进而完成文件的下载。一起来看下面的示例：\n//调用Files Service using (var client = HttpApiClient.Create\u0026lt;IFilesApiCaller\u0026gt;()) { Console.WriteLine(\u0026#34;-----Invoke File Service-----\u0026#34;); var files = new string[] { @\u0026#34;C:\\Users\\PayneQin\\Videos\\Rec 0001.mp4\u0026#34;, @\u0026#34;C:\\Users\\PayneQin\\Videos\\Rec 0002.mp4\u0026#34;, } .Select(f=\u0026gt;new MulitpartFile(f)) .ToList(); var result = await client.Upload(files).InvokeAsync(); Console.WriteLine(result); var json = JArray.Parse(result); var fileId = ((JObject)json.First)[\u0026#34;fileId\u0026#34;].Value\u0026lt;string\u0026gt;(); var fileName = Path.Combine(Environment.CurrentDirectory, \u0026#34;Output/Video001.mp4\u0026#34;); var filePath = Path.GetDirectoryName(fileName); if (!Directory.Exists(filePath)) Directory.CreateDirectory(filePath); using (var fileStram = new FileStream(fileName, FileMode.Create)) { var stream = await client.Download(fileId).InvokeAsync(); stream.Content.ReadAsStreamAsync().Result.CopyToAsync(fileStram); } } 这里说明的是，非常遗憾，这里的上传接口并没有被成功调用，可能我还是被 MulitpartFormDataContent 这种东西所困惑着，尽管我使用了 WebApiClient 中提供的 MulitpartFile 类，并且使用 HttpContent 特性对参数进行了修饰。(后来发现是因为我使用 JsonReturn 特性，可我的 Action 的确是返回了 JSON 啊，所以，我不暂时理解不了这一点😬)。我了解到的一点信息是，Spring Cloud 中的 Feign，一个和 Retrofit 极其相似的 HTTP 客户端，其本身并没有实现文件上传的功能，需要借助插件来实现相关功能，所以，这是否说明 HTTP 协议中的上传实现本身就是一个错误，因为它和 form-data 搅和在一起，试图用键值对的形式去描述一个文件，我们的业务中需要给文件增加备注关联相关信息，坦白讲，这种数据结构令人非常痛苦，所以，上传这块会有三个不同的版本，我一直希望上传可以和具体的业务解耦，即使需要给文件增加备注或者是关联相关信息，应该交给新的 Service 去做这件事情啊，这简直教人头疼啊。\n可配置与动态化 我知道许多人对特性这种**”配置“方式并不感冒，因为他们觉得通过配置文件就可以做到不修改代码。我曾经帮助组里写了一个非常简洁的配置方案，后来这个方案在 Code Review 的时候被拒绝，因为我和别人写得不一样。直到前几天我看到 ASP.NET Core 里全新的配置方式，我瞬间意识到这种配置方式和我之前的想法不谋而合，这个世界上聪明的人的想法总是如此一致。我相信人们看到这篇文章里出现的各种特性，都会认为像 Host、URL 等等这些东西都被硬编码了，说得好像你们的代码不需要随着配置文件变化而变化似的，说得好像你们的代码每次都不需要重新编译似的。我曾经考虑到这一点，在开发一个库的时候，充分考虑到了可配置化，事实是大家都不喜欢写配置文件，从那以后，我就变成了坚定的“约定大于配置“**主义。\n回到 WebApiClient 这个话题，如果你不喜欢这种基于特性的配置方式，那么你可以通过 HttpApiConfig 这个类，动态地对诸如 Host、URL 等参数进行配置，并在 WebApiClient 创建接口实例的时候传入这些配置。下面是一个简单的示例：\n//手动创建配置 var config = new HttpApiConfig() { HttpHost = new Uri(\u0026#34;http://www.yourdomain.com\u0026#34;), }; //调用Values Service using (var client = HttpApiClient.Create\u0026lt;IValuesApiCaller\u0026gt;(config)) { Console.WriteLine(\u0026#34;-----Invoke Values Service-----\u0026#34;); var results = await client.GetValues().InvokeAsync(); Console.WriteLine($\u0026#34;results is {results}\u0026#34;); var result = await client.GetValue(10).InvokeAsync(); Console.WriteLine($\u0026#34;result is {result}\u0026#34;); } 我知道杠精们绝对还有话要说，如果我连请求的 URL 都是动态地该怎么办呢？此时，你总不能让我再让我去配置 URL 了吧！对于这个问题，WebApiClient 提供了 Url 特性，该特性可以修饰参数，表明这是一个 URL，需要注意的是，该参数必须放在第一位，具体可以参考官方文档。\n[HttpGet] ITask\u0026lt;string\u0026gt; Login([Url] string url, string username, string password); 本文小结 有时候，我会一直在想，前后端分离到底分离的是什么？在我看来，找出这种界限是最重要的，即前端与后端各自的职责是什么。我们想分离的其实是职责，可惜这种想法极其容易演变为前后端人员的分离。而这种人员上的分离，则让接口的设计和沟通充满了坎坷。前后端分离不在于项目是否由两个或者更多的人完成，而在于你是否可以意识到前后端代码里的界限。在这种前提下，博主通过项目上前后端分离的实践经验，配合产品本身的技术架构体系，引申出一个话题，即前端/客户端如何应对后端 API 快速扩增带来的影响，并由此提出，通过代理类来调用后端 API 的想法，这一想法借鉴了 WebService。接下来，我们介绍了.NET 平台下的 Retrofit：WebApiClient，它可以让我们以一种“契约式”思想来声明接口，而不必关心这个接口该如何去实现，因为 WebApiClient 会帮助你实现具体功能。更改接口的代价永远比实现接口要小，所以，我相信这种声明式的 HTTP 客户端，可以让你更快速地应对来自后端的影响。在 Java 的世界里有 Retrofit、有 Feign，为了不被超越 太多，我们只能迎头赶上。谢谢大家，本篇到此结束，周末愉快！😬\n","date":"2018-07-16T09:02:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/380519286/","slug":"380519286","tags":["RESTful","WebApi","HttpClient"],"title":"声明式 RESTful 客户端 WebApiClient 在项目中的应用"},{"categories":["生活感悟"],"content":"\r一如往常地坐公交回家，下过一场雨以后，天空被洗刷得干干净净，洗去了夏天的骄阳似火，洗去了归途的月明星稀。抬眼瞥见对面车窗，一个被夕阳裁剪得整齐的轮廓，就这样静静地映在玻璃上，一瞬间散发某种神圣的气息。突然间，我想到了被嘲笑不会撩妹的 X 君，想到了自嘲不会拒绝别人的 Y 君，想到了喜欢一个人而爱不得的 Z 君……大概这个模糊的轮廓，可以是这个世界上任何一个人。\n我想说什么呢？你的生命也许从来都平淡无奇，可因为这一场秋雨的到来，在别人的眼睛里，就突然平添无数的神圣感。原来温暖从来都要自己去寻找，即使太阳的寿命意外的漫长，不至于像星辰一般昙花一现，你肉眼可以看到的星星，可能下一秒钟就消逝不见，就像这每天都见到的夕阳，有一天意外地点缀些金色或者黄色，你就会觉得它浑身都是温暖的力量。其实，那是极其平淡的一天，就像米花的味道一样，没有什么太特殊的含义，可你总愿意相信，当一个人的心足够虔诚的时候，神灵就可以听到你的心声，让那些奇迹发生。\n米花之味，这部小清新得不像国产电影的电影，从一开始，就表现出了它不同于以往国产电影的气质，比如电影中卖鸡蛋的小女孩，在电影中前后共出现 2 次，第一次是女主辞掉城里工作回来的路上，第二次是女主独自驱车前往机场的路上。如果说第一次是女主出于善良而买小女孩的鸡蛋，那么第二次则是对家乡现状的一种无力感。这基本上奠定了这部电影整体的基调，现实与传统的始终形成鲜明的对比，甚至是作为矛盾冲突穿插在母女两人中间，可导演似乎并不想通过这些来表达什么观点，所以这就造成这部电影单看画面质感是非常美的，可整部电影的立意实在不算太高。\n那么，我们在电影里看到了什么呢？跳广场舞大妈的一脸生无可恋、村里人夸夸其谈的致富梦想、表面奉承背地里说人长短、学校老师会接受学生“贿赂”、油腻感十足的新郎、“瓜分”募捐来的善款、搞封建迷信“请神”……这些非常真实的人物，像一张巨大的网将母女俩裹在其中，看起来两个人的矛盾，是留守儿童这样一个社会问题，可在我的理解中，这是现实与传统的一种碰撞，留守儿童不再是印象中内向闭塞的孩子，而或许是跟我们一样，知道什么是“吃鸡”，知道什么是“王者农药”，小镇村民不再是印象中善良淳朴的人们，而或许是知道生病了应该去医院，但“喊魂”这种事情同样需要，而对于募捐来的钱，无论是个人还是集体，都希望能分一点儿。\n影片中喃杭的小伙伴喃湘露，是因为错过最佳治疗时机而死，而第一个送孩子去医院的人，恰恰是她们不大喜欢的老师，大人们说要等机场修好，就可以坐飞机去外面治病，接近尾声时，人们看到头顶呼啸而过的飞机，不知道会不会想起喃湘露这个孩子。母女俩完成和解是因为喃湘露的死亡，借喃杭的话说，“她不相信喃湘露已经死了，甚至都感觉不到悲伤”，可在一开始，女主就告诉女儿，以后不要和喃湘露一起玩儿。“请神”的时候，人们说已经有 5 年没有去祭拜过石佛啦。为什么要祭拜石佛呢？因为人们相信如果不这样做，以后会有更多的麻烦出现，可当一个地方被开发为旅游景点以后，我们以往所珍视的那些传统，究竟是否能在现代文明的洗礼下保存完整？\n村民穿戴着传统的民族服饰，携带着供奉神灵的物品，一起到山里祭拜石佛，可门口悬挂着的“Closed”的木牌，连同将村民隔绝在外的那把铁锁，又仿佛将故事带入了后现代主义的胡同。如果石佛真的可以庇佑一方黎民，为何会被旅游开发者的一道铁门拦截？如果石佛真的可以感受到人们的虔诚，为何一定要到山林深处去朝圣祭拜？就像喃杭问她母亲，“我们给神跳舞，神就一定会知道吗？”，女主回答说，“只要你的心足够虔诚，神就可以感受得到”。这恰恰印证了老贺的举动，一行人被景区前的一道铁门给拦了下来，正暗自沮丧的时候，老贺说，“既然来都来了，无论在哪里跳舞，佛都会看见的”。\n对于生活本身而言，鸡汤固然没有什么实际的用途，可人们往往又需要鸡汤，因为心里缺少了一样东西，就会很容易地被其它的东西填满，而这种东西我们都叫做它信仰。你总要试着去相信点什么，不管是唯物的还是唯心的。有时候我们之所以会焦虑，是因为我们想要索取的东西太多。其实生命里少了某些东西又能怎么样呢？你羡慕别人做什么事情都有人陪伴，可当你尝试去和别人一起做一件事情的时候，你就会发现，即使看电影这样一件小事，都会存在千差万别，比如你喜欢看好莱坞视觉大片，而我喜欢看日式田园小清新，真要找一部两个人都喜欢看的电影，难免会引发我的选择困难症。\n有时候，你分不清喜欢一个人，到底是喜欢 Ta 还是喜欢 Ta 的习惯，分开以后的情侣，某一天意外地重逢，你说着对方那时这样或那样的习惯，而对方苦笑着说早就不喜欢那样子啦，那么，你开始怀疑，对方是不是真的喜欢这些，无论你是不是存在……喃杭打伤了大嘴，就在老师陪着大嘴在医院接受治疗的间隙，她对母亲撒谎说，“老师已经和大嘴回去了”……然后就是母女两人的冲突爆发，周围人的风言风语，母亲对女儿学习、生活上的种种不满，女儿对母亲的那种疏离感，相互纠缠在一起。喃湘露平时都见不到父母，甚至开玩笑地说，等到生一场大病看他们怎么办，可她依然相信，没有母亲会不爱自己的孩子。\n最令我动容的是，喃杭说要给她变一个魔术，然后喃湘露就见到了自己的父母，三个人，六只眼睛，有惊异、有辛酸，霎时之间全部涌上心头。“神婆”说米花米酒都变味了不好吃，大概是因为我们缺少了那种简单和纯粹，母女俩一起炸米花的时候，中间女主被叫去一段时间，喃杭炸的米花在翻动的时候，从中间破碎成两半，或许人的心原本如此，当有了隔阂的时候，即便是再简单的事情，都会做不好。老人说山里不许女人进去，母女俩终于决定亲自走进洞里去，忽然发现，神圣无比的石佛，不过是在一个寻常无比的钟乳石洞里，听起来清脆无比的声音，不过是游客随手丢弃在地上的易拉罐……\n心中去敬畏一样东西，不是永远被表象迷惑而且不敢有所怀疑，而是相信科学的解释，同样敬畏一切超越人力的力量，我怀疑云南的女孩子都会跳舞，比如曾经表演过千手观音的杨丽萍老师就来自云南，张大胡子甚至为了找一双好看的手，而让她出演了史上最美的梅超风，于是佛像前的一段舞蹈，成为了不亚于何小萍操场独舞的惊鸿一瞥，假如真的有来生，就祈祷喃湘露出生在一个富足的家庭里吧！你问神真的会灵验吗？不，不要去问神，而是去问你自己，所谓“心诚则灵”，相信一切美好的事情，All is well。\n","date":"2018-07-02T09:50:17Z","image":"/posts/2941880815/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/2941880815/","slug":"2941880815","tags":["影评","米花之味","电影"],"title":"米花之味：永远相信美好的事情"},{"categories":["开发工具"],"content":"最近在考虑将整个项目组的产品，努力向着持续集成(CI)/持续部署(CD)的方向靠拢，因为目前我们仅仅实现了基于 Docker 的自动化部署，而部署包的构建依然依赖于人工打包，而每个版本的测试和部署，基本上都要给所有相关人员发一遍邮件，而写邮件无非是填写版本号和变更历史。身处在这样一个社会化分工逐渐加剧的『摩登时代』，我们唯一的希望就追求技能的多元化，你越是担心有一天会被 AI 所替代，就越是应该去追求灵动与美。这个世界何尝不是一个运行中的大型机器，可恰恰就是这种掺杂了情感的冰冷法则，让我们意识到需要更多的理解和宽容。管理者常常迷信敏捷开发的人月神话，希望人可以像零件一样按部就班，在这场噩梦到来以前，为何不去做一点更有用的事情，让云计算帮我们解放双手。\n背景说明 我们的产品，从结构上来讲，分为后端、前端和客户端三个部分，其中，后端提供了从认证到上传、查询和下载等主要的 AP 接口；前端提供了基于后端 API 接口的页面，主要功能是监控和管理；客户端承担了主要的业务交互能力，主要功能是整合常用的硬件资源。从技术上来讲，后端是基于 Spring Cloud 的微服务架构，前端是基于 node.js 的典型前端工具链，而客户端是基于 .NET / Win32 的技术体系。所以，即使我们的客户端是运行在 Window 平台上，我们依然有大量的服务是运行在 Linux 环境下。负责部署的同事不愿意单独再构建一套持续集成(CI)环境，所以我们决定借助 Docker 完成整个持续集成(CI)环境的构建。\n构建过程 完成整个项目的构建，需要覆盖到代码编译、单元测试、静态检查、版本发布这四个基本环节，我们整体上使用 Jenkins 作为内部持续集成的平台，这意味着我们只需要在提交代码或者合并代码的时候，触发一个构建指令即可。这里我们考虑通过 Docker 来完成这些工作，一个整体上的设计思路如下图所示：\n构建思路\rMSBuild 首先是 MSBuild，它是我们整个构建流程中最重要的环节，我们平时通过 Visual. Studio 编译一个项目，背后其实就是由 MSBuild 这个构建工具来驱动，而通过 MSBuild 我们定义更多的构建流程，例如执行单元测试、实现 Zip 打包等等的流程。在 Window 平台下我们安装 Visual Studio 后就可以使用 MSBuild ，那么在 Linux 平台下呢？目前， MSBuild 已经被微软开源并托管在 Github 上，大家可以通过这个地址：https://github.com/Microsoft/msbuild来访问。通过阅读 MSBuild 的文档，我们了解到，目前 MSBuild 实际上有三个流向，分别是目前官方主推的 .Net Core 、传统的 .Net Framework以及由 Mono 托管的部分。\n.Net Core 中 MSBuild 实际上被集成在 .Net CLI 中，熟悉 .NET Core 的朋友一定都知道， .NET Core 类型的项目，是可以直接通过 dotnet 命令来创建项目、还原 Nuget 包、运行项目、构建项目和发布项目的，可以想象的到这些功能是依赖 MSBuild 和 Nuget 的，可惜这种目前对我们来说不太适合。接下来，我们有两个选择，一个是 Full Framework，一个是 Mono，因为我们的服务器是一台 Linux 服务器，所以 Full Framework 对我们来说不适合，我们在无奈的情况下选择了 Mono，按照官方文档，从源代码安装过程如下：\ngit clone -b xplat-master https://github.com/mono/msbuild/ cd msbuild make 果不其然，这个无论是在 Linux 主机还是 D ocker 中都失败了，官方的源代码我们编译不过去，那就只能考虑非源代码安装啦！按照官方的说法，我们需要 Mono，所以兴奋地跑到 Mono 官方去安装，根据以前使用 Mono 的 经验，飞快地在终端里输入下面两行代码：\nsudo apt-get install mono-runtime sudo apt-get install mono-xbuild 装完以后，发现可以使用 Mono 和 XBuild，可无奈是 XBuild 版本实在太低，换句话说我们从 Ubuntu 官方源里安装完的 Mono 相当于 .NET Framework 2.0 的版本，这怎么可以呢？果断从 Mono 官方下载最新版本的 Mono，这是一个经过反复试验的安装方法：\nsudo apt-get update sudo apt-get upgrade -y sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF sudo apt install apt-transport-https -y sudo apt-get install wget -y echo \u0026#34;deb https://download.mono-project.com/repo/ubuntu stable-trusty main\u0026#34; | sudo tee /etc/apt/sources.list.d/mono-official-stable.list sudo apt-get update sudo apt-get install aptitude -y sudo apt-get install -f sudo apt-get install -y git sudo aptitude install -y mono-complete 这里顺带安装了 git 和 wget，因为下面我们会用到这两个软件。 aptitude 实在是修复 Linux 依赖问题的神器，我准备找时间用它修复下我的 Linux 环境。 apt-transport-https 这个是为了支持 https 协议，这个不用说太多，我们选择了最全的一个 Mono 版本 mono-complete，它包含了我们在 Linux 下可以使用的所有程序集，换句话说，这些程序集以外的程序集，或者是和 Windows 联系紧密的 COM 组件、 OCX 等等，想都不要想啦，只有一件事情是对的，对平台的依赖越少，跨平台的可能性越高。\nNuget Nuget是 .NET 下使用最多的包管理器，虽然目前 .NET Core 里的依赖管理越来越像 Maven，可我觉得作为整个构建工具里的一环，还是应该考虑进来，虽然我们的项目中的第三方库基本都靠拷。Nuget 只有单独的命令行版本和 Visual Studo 扩展两个版本，这里我们使用 wget 下载命令行版本，然后再通过 Mono 来调用 nuget.exe :\nsudo wget https://dist.nuget.org/win-x86-commandline/v4.6.2/nuget.exe /usr/local/bin/nuget.exe alias nuget=\u0026#34;mono /usr/local/bin/nuget.exe\u0026#34; Sonar 对于 Sonar 的话，这里我推荐用 SonarCloud，因为我们只需要通过 wget 下载 SonarScanner，然后通过 Mono 调用并提供 SonarCloud 提供的 token 即可。曾经博主写过一篇关于使用 SonarCloud 为.NET/.NET Core 项目提供静态检查的文章，在这篇文章中我们提到，SonarCloud 支持 .NET Framework 4.6+ 以上的版本以及 .NET Core 版本，所以，这里我们沿用当时的脚本即可，想了解 SonarCloud 的朋友，可以找到这篇文章进行深入了解。下面给出脚本：\nsudo wget https://github.com/SonarSource/sonar-scanner-msbuild/releases/download/4.3.0.1333/sonar-scanner-msbuild-4.3.0.1333-net46.zip sonar-scanner.zip sudo unzip sonar-scanner.zip sudo alias sonar-scanner=\u0026#34;mono ./sonar-scanner/SonarQube.Scanner.MSBuild.exe\u0026#34; sonar-scanner begin /k:\u0026#34;Sonar-HttpServer\u0026#34; /d:sonar.organization=\u0026lt;Your-Org\u0026gt; /d:sonar.host.url=\u0026#34;https://sonarcloud.io\u0026#34; /d:sonar.login=\u0026lt;Your-Token\u0026gt; msbuild /t:Rebuild sonar-scanner end /d:sonar.login=\u0026lt;Your-Token\u0026gt; NUnit 既然我们有了 Nuget，那么自然要用 Nuget 来做点事情。对于单元测试，微软提供的 MSTest 功能相对薄弱，关键是严重依赖 Visual Studio，一旦我们想要移植到 Linux 平台下，就会发现阻力重重，所以在平时开发中，我更建议大家去使用 NUnit 或者 XUnit，它们比 MSTest 功能强大，可以直接通过 Nuget 安装，同时自带 TestRunner，这是一个控制台程序，我们直接通过 Mono 调用它，并把单元测试项目生成的动态链接库作为参数传递给它即可。 下面给出基本的脚本：\nnuget install NUnit.Runners -Version 3.8.0 -OutputDirectory ./TestRunner alias nunit=\u0026#34;mono ./TestRunner/NUnit.ConsoleRunner.3.8.0/tools/nunit3-console.exe\u0026#34; nunit \u0026lt;Your-UnitTest-Project\u0026gt; 牛刀小试 下面我们来试试在 Docker 里完成镜像的构建，其实这里更推荐在 Linux 下安装 Docker，博主在 Window 平台下安装了 Docker for Windows，需要系统支持虚拟化技术。因为博主在构建镜像的时候，一直提示磁盘空间不足，所以，这里我们把 Dockerfile 放到 DaoCloud 上去跑，关于 Docker 的安装以后有机会在同大家分享。这里， DaoCloud 你可以理解为一个帮我们装好了 Docker 的云主机，事实上用 DaoCloud 以后，感觉测试 Dockerfile 可以更省时间啦，效率上相差十倍啊！ Dockerfile 其实就是本文中这些脚本的集合，这里我们给出完整的 Dockerfile，这个文件可以从这里获取：\nFROM ubuntu:14.04 LABEL vendor=\u0026#34;qinyuanpei@163.com\u0026#34; # Install Mono \u0026amp;\u0026amp; XBuild RUN sudo apt-get update RUN sudo apt-get upgrade -y RUN sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF RUN sudo apt install apt-transport-https -y RUN sudo apt-get install wget -y RUN echo \u0026#34;deb https://download.mono-project.com/repo/ubuntu stable-trusty main\u0026#34; | sudo tee /etc/apt/sources.list.d/mono-official-stable.list RUN sudo apt-get update RUN sudo apt-get install aptitude -y RUN sudo apt-get install -f RUN sudo apt-get install -y git RUN sudo apt-get install -y zip RUN sudo apt-get install -y unzip RUN sudo aptitude install -y mono-complete # Intall Nuget RUN sudo wget -O nuget.exe https://dist.nuget.org/win-x86-commandline/v4.6.2/nuget.exe #RUN alias nuget=\u0026#34;mono nuget.exe\u0026#34; # Install Sonar-Scanner RUN sudo wget -O sonar-scanner.zip https://github.com/SonarSource/sonar-scanner-msbuild/releases/download/4.3.0.1333/sonar-scanner-msbuild-4.3.0.1333-net46.zip RUN sudo unzip sonar-scanner.zip -d ./sonar-scanner #RUN alias sonar-scanner=\u0026#34;mono .SonarQube.Scanner.MSBuild.exe\u0026#34; # Install NUnit RUN mono nuget.exe install NUnit.Runners -Version 3.8.0 -OutputDirectory ./TestRunner #RUN alias nunit=\u0026#34;mono ./TestRunner/NUnit.ConsoleRunner.3.8.0/tools/nunit3-console.exe\u0026#34; # Build Project \u0026amp;\u0026amp; Sonar Analyse \u0026amp;\u0026amp; UnitTest RUN git clone https://github.com/qinyuanpei/HttpServer.git RUN sudo mono ./sonar-scanner/SonarQube.Scanner.MSBuild.exe begin /k:\u0026#34;Sonar-HttpServer\u0026#34; /d:sonar.organization=\u0026#34;qinyuanpei-github\u0026#34; /d:sonar.host.url=\u0026#34;https://sonarcloud.io\u0026#34; /d:sonar.login=\u0026#34;db795a28468dc7c12805b330afed53d362fdd2d9\u0026#34; RUN msbuild /p:Configuration=Release ./HttpServer/HTTPServer/HTTPServer.sln RUN sudo mono ./sonar-scanner/SonarQube.Scanner.MSBuild.exe end /d:sonar.login=\u0026#34;db795a28468dc7c12805b330afed53d362fdd2d9\u0026#34; # RUN mono ./TestRunner/NUnit.ConsoleRunner.3.8.0/tools/nunit3-console.exe ./HttpServer/HTTPServer/HTTPServerLib.UnitTest/bin/Release/HttpServerLib.UnitTest.dll EXPOSE 2048 好了，下面我们通过 Dockerfile 来构建镜像，这里不需要考虑部署，我们就是在 Docker 这个环境里跑跑结果(PS：不知道为什么 alias 在 Docker 里不起作用)：\ndocker build -t httpserver:v1 . 可以看到，我们整个过程除了单元测试没有通过以外，其它的环节都非常顺利，这其中一个重要的原因是，博主这个项目对 Window 依赖较少，它是一个 C# 开发的简易 Web 服务器，主要是类库和控制台程序，可以完美地运行在 Linux 平台下，所以，跨平台最终考验的还是开发人员。 Docker中构建的结果\rOne More Thing 这里我们主要针对的是 .NET Framework，那么针对传统的 ASP.NET 以及最新的 .NET Core 又该如何做持续集成呢？这里简单说一下思路，具体的 Dockerfile 大家可以去 DockerHub 去找(抄)，这里我就不帮大家写了。对于传统的 ASP.NET，在本文的基础上增加 Jexus 就可以做 Linux 下的部署，当然，前提是要避免和 Window 太过紧密的耦合，否则即便是大罗神仙亲临，这持续集成永远都是个梦。对于 .NET Core ，只要安装了它的 SDK，编译、依赖管理、发布、部署都不再是问题，只要完善下单元测试和静态检查就可以，因为它是可以自部署的，并且天生就是为了跨平台而生，如果有可能，还是考虑用 .NET Core 吧，Windows 最适合的还是吃鸡打游戏(逃……\n本文小结 读过我之前博客的朋友，一定会发现，我今天这篇博客里所做的事情，同我曾经在 .NET 项目上使用 TravisCI 是完全一样的，所不同的是， TravisCI 里的构建环境是别人提供好的，而这里的构建环境是我们自己搭建的。这并不是在做无用功，如果你需要搭建私有的 Linux 下的构建环境，我相信这篇文章会带给你一点启示。项目组最后还是放弃了这个方案，因为产品里集成了太多和 Window 关联的东西。而负责部署的同事最终如释重托，因为他们不必去踩这些无聊的坑，可对我来说，这像一道屈辱的烙印刻在我的心上，我甚至试过在 Docker 环境里搭建 Window 的环境，哪怕最终我发现我不能把 Docker 当一个虚拟机来用，我越来越害怕自己对那些变化一无所知，还庆幸自己可以在时光的影子里偷懒。\n有时候，人们假装配合持续集成的流程，因为它听上去非常美好，可对环境的依赖不愿意削弱，对单元测试不是那么重视，对代码质量不是那么在意，这一切又永远都只是听上去美好而已。我听到有面试官在面试的时候，批评面试者所做的运维工作不是那么的高大上，毕竟我们只是写了点脚本而已，离面试官心中的 DevOps 相去甚远。可 MSBuild是 XML 写成的脚本， make 不过是个纯文本的脚本，到底哪一种更高大上？我在这篇文章里使用了 Docker，能否让我的工作显得高大上？我们的工作到底有多少能适应 DevOps？我觉得想清楚这个再谈高大上，不是不可以啊？对吧？好了，这就是这篇文章的全部内容啦，谢谢大家！\n","date":"2018-06-12T17:53:59Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3995512051/","slug":"3995512051","tags":[".NET","Docker","MSBuild"],"title":"基于 Docker 构建 .NET 持续集成环境"},{"categories":["编程语言"],"content":"太阳照常升起，在每个需要挤公交车上班的日子里，即使窗外早已大雨如注。想来只有在周末，太阳会陪着我一起起床，所谓睡觉睡到自然醒，在雨天里保持晴天的心情，相当大的程度上，是因为今天不必上班。因此，一周里的心情晴雨表，简直就是活生生的天气预报，可惜我并不能预测我的心情，因为 Bug 会在某一瞬间发动突然袭击。一周前测试同事小 J 得到用户的反馈，我们某一笔订单突然无法从系统中查到，可就在数分钟前用户创建了这笔订单。前端同事小 Q 立刻追踪了这个问题，发现查询交易的接口调用正常，而后端同事小 L 确认数据库中是有这条交易记录的。于是，为了解决这样一个诡异的问题，几乎花费了大家大半天的时间。而最后的问题根源，居然充满了无厘头的意味，如本文主题所言，这是一个由服务器时区引发的 Bug。在这篇文章中，我想和大家聊一聊，关于时区以及日期/时间格式化的相关问题，希望大家会喜欢这个话题，就如同我希望大家会喜欢我一样。 可能大家都不会意识到时区会成为一个问题，因为对大多数中国人而言，我们唯一的时间概念就是北京时间。我们不得不承认，互联网在弱化了空间地域性的同时，无形中疏远了人与人之间的距离，尤其当我们处在一个分布式架构的时代，云的存在让我们的 Service 分布在无数个服务器节点上去，我们甚至意识不到它们的存在。比如我们在阿里云上选购主机的时候，阿里云会让我们去选择主机所在的地域，因为选择离自己更近的地域，意味着可以更快的访问速度。再比如像亚马逊这样的云计算服务商，会在国内(宁夏·中卫)部署自己的资源，这显然是为了服务国内用户。那么，我们不得不去思考一个问题，假如我们要同时服务国内、外的用户，那么这些 Service 可能会被同时部署到国内和国外的服务器上面。因此，我们就可能会遇到国内、外服务器时区不一致的问题，通常我们会以服务器时间为准并将其储到数据库中。此时，因为时区不一致，难免会产生本文中遇到的这个问题。\n时区为什么会不同 既然时区是本文里的**\u0026ldquo;罪魁祸首\u0026rdquo;**，那么我们就会不由得思考这样一个问题，即为社么时区会不同。我们知道，地球是自西向东自转的，因此东边会比西边先看到太阳。相应地，东边的时间会比西边的早。这意味着时间并不是一个绝对的概念，即东边的时间与西边的事件存在时差。现实中的时差不单单要以小时计，而且还要以分和秒计，这给人们带来了不便和困扰。因此，1884 年在华盛顿召开的国际子午线会议上，规定将全球划分为 24 个时区(东、西各十二个时区)，其中以英国格林尼治天文台旧址作为零时区，每个时区横跨经度 15 度，时间恰好为 1 小时，而东、西第 12 时区各跨经度 7.5 度，以东、西经 180 度为界。每个时区内时间，统一以该时区的中央经线的时间为主，相邻的两个时区间总是相差一个小时，这就是时区的由来，时区的出现解决了人们换算时间的问题。 世界时区分布\r事实上，时区的划分并不是一个严谨的事情，因为常常会出现一种情况，一个国家或者一个省份同时跨着 2 个或者更多的时区。以中国为例，中国幅员辽阔，差不多横跨 5 个时区，理论上在国内应该有 5 个时间，但为了使用起来方便，我们统一使用的是北京时间，即东八区时间。什么叫做东八区呢？即东半球第八个时区，其中央经度为东经 120 度。时区的计算非常简单，当你往西走时，每经过一个时区，时间会慢一个小时；当你往东走时，每经过一个时区，时间会快一个小时。例如，日本的东京位于东九区，因此，北京时间 2018 年 6 月 9 日 8 点整，对应的东京时间应该是 2018 年 6 月 9 日 9 点。这样，我们就会遇到一个非常有趣的问题，如果一个人到世界各地去旅行，它就需要不停地去将手表拨快或者拨慢，即使我们现在有了智能手机，它一样会提供不同时区的时间选择，假如我们偷懒选择了网络时间，那么它将永远和当地时间保持一致，因为我十分地确信，东京的运营商绝对不会选择使用北京时间。\n数据库里如何存储时间 截至到目前为止，我们可以搞清楚的一件事情是，在不同的地域使用的时间是不同的，因为我们所使用的时间，本质上都是相对于格林尼治时间的相对时间，即使这些时间会因为地域存在差异，可从整个宇宙的角度来看，时间分明又是在绝对地流逝着，它对我们每一个人而言都是客观而公正的，当你发现时间越来越不够用的时候，你需要思考时间到底被浪费到什么地方去。我无意像霍金先生一样，去追溯时间的起源以及它的未来，在这篇文章里，我更关心的是，数据库里究竟是怎么样存储时间的，因为最根本的问题是，用户作为查询条件的时间，服务器上存储记录的时间，这两个时间的上下文发生了混乱。人类更喜欢在工作中不停地切换上下文，尤其是在面对无休止的会议、需求分析、Review 等等诸如此类的中断的时候，你是否会想到频繁地切换上下文，本质上是需要付出代价的呢？ Time is All\r回到这个问题本身，我们现在来看看数据库中是如何存储时间的，这里我们选择三种最为常见的数据库来分析，它们分别是 MySQL、Oracle 和 SQL Server。\nMySQL 对 MySQL 来说，它支持 YEAR、DATE、TIME、DATETIME 和 TIMPSTAMP 共 5 种数据类型。其中，\nYEAR 类型占 1 个字节，取值范围为 1901~2155，可以采用 4 位字符串或者 4 位数字赋值，不建议使用 2 位数字或者 2 为字符串赋值，因为容易混淆 0 和‘0’。 DATE 类型占 4 个字节，取值范围为 1000-01-01 ~ 9999-12-31，采用 YYYY-MM-DD 的格式赋值，不建议使用@或.这样的分隔符，不建议将年份表示为 YY，理由同上。 TIME 类型占 3 个字节，取值范围为-838:59:59 ~ 838:59:59，采用 HH:MM:SS 的格式赋值，不建议使用 HH:MM 或者 SS 的简写格式，以及混合使用 D 的格式。 DATETIME 类型占 8 个字节，取值范围为 1000-01-01 00:00:00 ~ 9999-12-31 23:59:59，标准格式为 YYYY-MM-DD HH:MM:SS，规则同上 TIMESTAMP 类型占 4 个字节，取值范围为 19700101080001 ~ 20380119111407，系统可以使用 CURRENT_TIMESTAMP 或者自动输入当前的 TIMESTAMP，需要注意的是，该数值与时区有关。 Oracle 对 Oracle 来说，它支持 DATE、TIMPSTAMP 和 INTERVAL 共 3 种数据类型。其中，\nDATE 类型占 7 个字节，是一种表示日期/时间的数据类型，本身包含世纪、世纪中的哪一天、月份、月份中的哪一天、小时、分钟和秒 7 个属性，例如 2005-12-05 12:30:43 对应的表示是 120、105、12、5、12、31、44，我们注意到这里世纪和世纪中的年份，都被相应地增加了 100，而分钟数和秒数分别增加了 1，这里增加的 100 是为了区分公元前和公元后，一般在写入该类型的数据时，最好能显式地指定日期或者时间的格式。 TIMPSTAMP 类型，同 DATE 类型类似，不同的是，TIMESTAMP 类型可以支持秒分量的小数位数以及时区。秒分量的小数部分最多可以支持 9 位，当秒分量的小数部分为 0 时，它和 DATE 类型在功能上完全一致。 INTERVAL 类型，顾名思义，这是一个表示时间间隔的数据类型，同.NET 中的 TimeSpan 类型相似，它可以用来存储一个时间间隔，比如 8 个小时或者是 30 天，两个 DATE 或者 TIMESTAMP 相减可以得到 INTERVAL，而 DATE 或者 TIMESTAMP 增加一个 INTERVAL 就可以得到相应的 DATE 或者 TIMESTAMP。 SQL Server 对 SQL Server 来说，它支持 Date、Time、DateTime 和 DateTime2 共 4 种数据类型。其中，\nDate 类型仅存储日期，不存储时间，需要 3 个字节的存储空间，默认格式为 yyyy-MM-dd(PS:为什么没有一个标准来统一这些占位符的大小写)，取值范围为 0001-01-01 ~ 9999-12-31，可以采用字符串、GetDate()、SysDateTime()三种方式赋值。 Time 类型仅存储时间，不存储日期，需要 7 个字节的存储空间，默认格式为 hh:mm:ss.nnnnnnn，可以注意到默认的秒分量小数部分为 7 位，建议使用字符串或者 SysDateTime()这两种方式赋值，不建议使用 GetDate()，因为该方法返回值为 DateTime 类型，其时间部分的精度没有 Time 类型的经度高。 DateTime/DateTime2，这个命名好 COM+啊，其中 DateTime 类型存储日期和时间，需要 8 个字节的固定存储空间，相对应地，DateTime2 的存储空间则是不固定的，因为它可以指定秒分量的小数位。DateTime 的默认格式为 yyyy-MM-dd hh:mm:ss.xxx，取值范围为 1753-01-01 00:00:00.000 ~ 9999-1-3123:59:59.997，精确度为 3.33 毫秒，相应地，DateTime2 的秒分量小数位默认可达到 7 位。通过 GetDate()和 GetUTCDate()两个函数，可以为 DateTime 类型赋值；通过 SysDateTime()和 SysUTCDateTime()函数，可以为 DateTime2 类型赋值。通常来说，DateTime2 相比 DateTime，具有更好的性能表现。 此时此刻，我们不得不面对这样一个现实，那就是：不同的数据库中对日期/时间的存储处理是不同的。看起来这像是一个显而易见的结论，因为这就像 SQL 这门语言一样，即使我们有着相同的标准，可最终我们面对的还是各种“方言”版本的 SQL，甚至连这些难以统一的内置函数，都会成为某次面试中的题目。我们注意到，这些和日期/事件相关的数据类型，在对时区的支持上差异明显，MySQL 中的 DATESTAMP 是标准的 UNIX 时间戳，存储的是自 1970-01-01 至今经过的秒数，这个数据的存取都是相对简单的，因为 MySQL 内部帮你做了大量的转换的工作， 可它的缺点是什么呢？由于 4 个字节长度的限制，它最多到 2038 年，可现在都 2018 年了啊！DATETIME 类型的数据范围好像可以解决这个问题，遗憾的是它没有办法包含时区信息，这就尴尬了啊！或许有人会想到能不能用 int 类型来存储日期，这理论上是没有问题啊，可你愿意每次存取都要做一遍转换吗？这意味着我们需要一种同时支持日期、时间和时区的表示方法，所以，下面我们来说一说 DateTime 相关的格式化，这里特指 UTC 时间、GMT 时间、本地时间和 Unix 时间。 繁杂的日期格式 我对日期/时间的格式化的厌恶，最早来自为 Excel 编写读写库，人们发明了各种各样的样式，虽然常用的无非那么多种，可对于一个编写 Excel 读写库的人来说，你不得不去在读写过程中面对各种各样的格式，或者是从字符串变为 DateTime 类型，或者是从 DateTime 变为字符串。我本人非常喜欢 OADate 这种方式，因为它真正地做到了样式与数据分离，我们大多数时候面对的时间，它到底是一种什么数据类型，为什么你在 Excel 里输入日期/时间字符串会被当作是日期/时间，而通过快捷键插入的系统时间同样会被当作是日期/时间，有没有一种统一的可以描述时间的方式呢？这里需要介绍 UTC 时间、GMT 时间、本地时间和 Unix 时间 4 个概念。\nUTC 时间 UTC 时间，即 Coordinate Universal Time。它是一种通用的时间表示方法，UTC 是根据原子钟来计算时间，它是经过平均太阳时、地轴运动综合修正计算后的一个结果，使用秒作为计量单位，由于原子钟计量的时间精度非常高，因此，可以认为 UTC 一个世界标准时间。\nGMT 时间 GMT 时间，即 Greenwhich Mean Time。如果大家对这个名词不熟悉，那么我相信，对于格林威治天文台，大家一定非常熟悉啦！ 十七世纪，为了满足英国海上霸权的扩张，格林威治皇家天文台开始对天文进行观测。历史上每一次霸权主义的扩张，其初衷必然是非正义的，可伴随着这个过程中而产生的文明，可谓是是泽被后世，前文中提到的时区划分，就是以格林尼治天文台旧址作为零时区，所以 GMT 时间和 UTC 时间等价，前者提出较早，基于天文观测；后者提出较晚，基于现代物理。\n本地时间 本地时间，即 LocalTime。从定以上来讲，本地时间=UTC 时间+时差，其中东半球时区记为正，西半球时区记为负。以东八区为例，UTC+0800，即为本地时间，这就是我们所熟悉的北京时间。因为同时存在 UTC 和 GMT 两种标准，所以我们在某些场合下会看到 GMT+0800，这两者表示的实际上是同一个时间，都以秒作为单位。\nUnix 时间 Unix 时间，又称 Unix 时间戳，顾名思义，这是一种在 Unix 及类 Unix 操作系统中表示时间的方法。Unix 时间戳其实就是 UTC 时间在计算机领域的一个应用，我们所看到的计算时间，其实都是从 1970-01-01 00:00:00 开始，截止到此时此刻的总秒数，这个方案被 Unix 及类 Unix 操作系统继承下来，甚至影响到了大量非 Unix 操作系统，这个方案后来被称为 POSIX 标准，因为该时间又被称为 POSIX 时间。或许有朋友会感到疑惑，计算机是会关机和断电的啊，那么这个时间不就会丢失吗？事实上计算机内部有一个称为 RCT 的硬件模块，该模块内部独立供电，所以可以准确记录下这个时间。一个有趣的事情是，计算机内部使用 32 位整型来表示时间，而 32 位整型最大能表示为 2147483647 秒，我们做个简单计算:2147483647/365/24/60/60，就可以知道这个数值为 68.1 年，这意味着计算机内部能表示最大年份为 1970+68=2038。想想看今天已经是 2018 年啦，难道在我们有生之年会有幸见到这个 Bug 吗？这对整个数字时代来说算不算一次世界末日呢？哈哈，实际上我们有了 64 位以后这个问题就可以解决了，至于 64 位出现类似问题，这个只能交给时间来解决啦，因为那时你和我都早已不复存在。\nISO8601 OK，现在我们来一起看一个实际的格式化问题，我们在调用后端提供的 API 接口时，前端同事使用日期格式是：2018-06-05T03:03:57.000Z，而后端同事使用的日期格式是：2018-03-16T19:14:22.077+0800。这两种不同的日期格式到底是什么呢？和我们这篇文章中提到的内容又有什么关联呢？因为博主曾经在写一个小工具的时候，遇到无法解析这种格式日期的问题，所以对这两种日期格式可谓是记忆犹新。这两种日期格式实际源于一个国际标准ISO8601。 根据该格式的定义，当日期和时间组合使用时，需要在时间面前增加一个大写字母 T，而 Z 表示时区，且默认表示 0 时区，因此字母 Z 可以省略，以我国为例，我国是东八区，所以正确的写法是+08:00，由此可以得知，第二种写法实际上就是一个表示东八区时间的表示方法，虽然这个写法是错误的。第一种写法有什么问题呢？它表示的是 0 时区的时间，因此对中国用户而言，他们需要在这个时间上增加 8 个小时的时差，可如果这个时间是经过时区修正后的时间会怎么办呢？时间对每一个人都很重要，可看到它的稀奇古怪的表示方法，难免会让人感到风中凌乱啊…… 我们知道，Json.Net是.NET 中一个非常流行的 JSON 解析和生成库，而我们在对一个实体进行序列化的时候，如果实体中属性的数据类型为 DateTime，那么在序列化的时候就会出现一个非常有趣的现象。假如我们在数据库中有一个字段 dateCreated，那么通过这个库转换出来的结果可能会是\u0026quot;/Date(1269582661683+0800)/\u0026ldquo;这样的结果，例如下面这段 JSON：\n{ \u0026#34;DateCreated\u0026#34;:\u0026#34;\\/Date(1528687303302)\\/\u0026#34;, \u0026#34;UserName\u0026#34;:\u0026#34;Payne Qin\u0026#34; } 出现这个结果的原因，是因为我们使用微软提供的 JavaScriptSerializer，而这个序列化器遵循的实际上是 Unix 时间标准，换句话说，这里展示的这个数值是 1970-01-01 00:00:00 至今的毫秒数， 这一点我们通过一个简单的计算就可以得到验证。Json.Net 中默认使用 ISO8601 风格的序列化器，我们一起来看下面的例子，这里我们定义一个简单的数据结构，按照惯例，这个数据结构用 Foo 类表示：\nclass Foo { [JsonConverter(typeof(IsoDateTimeConverter))] public DateTime IsoDateTime { get; set; } [JsonConverter(typeof(JavaScriptDateTimeConverter))] public DateTime JSDateTime { get; set; } public string UserName { get; set; } } 此时，我们可以注意到序列化后的结果如下：\n{ \u0026#34;IsoDateTime\u0026#34;:\u0026#34;2018-06-11T11:35:45.898768+08:00\u0026#34;, \u0026#34;JSDateTime\u0026#34;:new Date(1528688145898), \u0026#34;UserName\u0026#34;:\u0026#34;Payne Qin\u0026#34; } 为了将这两种统一起来，建议通过 JsonSerializerSettings，因为我们可以定制日期的样式：\nvar settings = new JsonSerializerSettings(); settings.DateFormatHandling = DateFormatHandling.IsoDateFormat; settings.DateFormatString = \u0026#34;yyyy-MM-ddTHH:mm:ss.fffzzz\u0026#34;; var json = JsonConvert.SerializeObject(entity,settings); 此时，可以注意到结果为：\n{ \u0026#34;IsoDateTime\u0026#34;:\u0026#34;2018-06-11T11:45:46.981+08:00\u0026#34;, \u0026#34;JSDateTime\u0026#34;:\u0026#34;2018-06-11T11:45:46.981+08:00\u0026#34;, \u0026#34;UserName\u0026#34;:\u0026#34;Payne Qin\u0026#34; } JavaScriptDateTimeConverter 和 IsoDateTimeConverter，均是 DateTimeConverter 的子类，因此我们可以定义更多的转换器，毕竟喜欢折腾的人类永远不会满足，除了本文中介绍到的时间表示方法以外，我们还有 CST 和 DST 等不同的表示方法，对了，关于格式化参数 fff/zzz 等请参考这里，你就会知道人类是多么的无聊啊。\n不同语言中对时区的处理 好了，这篇文章基本上通篇都在讲时间，我们最初的问题是，服务器上的时区和当前时区不一致，导致在查询的时候时间无法对应起来。现在，我们应该可以达到一个共识，不管什么时候，我们都应该使用 UTC 时间或者 GMT 时间，而在拿到这样一个时间后，如果有必要请转换为本地时间，而当相关流程结束以后，最好将这个时间转换为 UTC 时间或者是 GTM 时间。现在，我们来看看不同的语言中是如何处理时区问题的，按照博主对语言的熟悉程度，博主选择了 C#和 Python 两门语言来说明问题。\nCSharp C#中关于日期/时间的 API 都集中在 DateTime 类中，而关于时区的 API 则集中在 TimeZone 和 TimeZoneInfo 类中，我们一起来看下面的代码：\n//当前时区：中国夏令时 var timezone = TimeZone.CurrentTimeZone; //获取所有时区 var timezones = TimeZoneInfo.GetSystemTimeZones(); //获取时区ID：北京时间+08:00/China Standard Time var timezoneId = TimeZoneInfo.GetSystemTimeZones()[102].Id; //当前系统时区：(UTC+08:00) 北京，重庆，香港特别行政区，乌鲁木齐 var currentTimeZone = TimeZoneInfo.Local; //本地时间转换为UTC时间：2018/6/11 5:13:49 var dtUTC2 = TimeZoneInfo.ConvertTimeToUtc(DateTime.Now); //将本地时间转换为指定时区的UTC时间：2018/6/11 5:13:49 var dt = DateTime.SpecifyKind(DateTime.Now, DateTimeKind.Local); var dtUTC1 = TimeZoneInfo.ConvertTimeToUtc(dt, TimeZoneInfo.Local); //将指定时间从指定时区转换至目标时区的时间：2018/6/11 1:13:49 var dtUTC3 = TimeZoneInfo.ConvertTime(dt, TimeZoneInfo.Local, TimeZoneInfo.GetSystemTimeZones()[30]); //当前UTC时间：2018/6/11 5:13:49 var dtUTC = DateTime.UtcNow; //当前Unix时间：1528694152 var startTime = TimeZone.CurrentTimeZone.ToLocalTime(new System.DateTime(1970, 1, 1)); var dtUnix = (int)(DateTime.Now - startTime).TotalSeconds; Python Python 中针对时区的处理，发扬了 Python 一贯主张简单的传统，有多传统呢，大概只需要两行代码，是的，你没有听错，只需要两行代码：\ntz = pytz.timezone(\u0026#39;Asia/Shanghai\u0026#39;) dt = datetime.datetime.now(tz) 简单来说，在 Python 中我们只需要给定时区，即可将本地时间转化为指定时区对应的 UTC 时间，这里我们使用的是 Python 中的 pytz 这个库，如果你打开这个库的安装目录，就会发现其实它有大量时区相关的数据组成，如果我们直接调用 pytz.timezone()就可以获得所有的时区信息。博主有一个 Python脚本运行在 TravisCI 的服务器上，而 TravisCI 来自一家法国的技术公司，因此在不指定时区的情况下，会默认使用 TravisCI 服务器上的时间，这并不是我想要的结果，所以，我们需要 pytz 来解决这个问题，至于这里为什么我们使用的是上海而不是北京，这是因为中国横跨 5 个时区，在国内大家习惯使用北京时间，而在国外这些时区数据没有做及时更新，所以这算是一个关于时区的历史遗留问题吧！\n本文小结 本文从实际生活中一个案例入手，首先，向大家解释了为什么我们需要时区，以及为什么地球上不同地域拥有不同的时间。接下来，我们以 MySQL、Oracle 和 SQL Server 三种数据库为例，了解了在数据库中是如何存储时间的，可以注意到大多数数据库都使用了时间戳来存储时间。由此，我们引出了 UTC 时间、GMT 时间、本地时间以及 Unix 时间，并讲述了它们之间的区别。其中，UTC 和 GMT 可以看作是等价的时间表示方法，两者仅仅是计量工具不同，在历史上提出的先后顺序不同，并且 GMT 时间是以 UTC 时间为基准的。而 Unix 时间是计算中表示时间的方法，其含义是自 1970-01-01 00:00:00 至今经过的总秒数，在此基础上我们引出了为什么 32 位计算机下能表示的最大年份是 2037。在文章的最后，博主选择了最熟悉的 C#和 Python，向大家展示了和时区相关的操作。 我承认，这篇文章相当地细碎，可能因此牵扯了太多的概念，我一直在犹豫要不要发到博客上来。其实，有太多的时候，越来越发觉自己写不出来一篇好的文章，大概我需要去读更多的书，或者去解决更多的问题，可坚持写博客的一个重要原因，无非是我觉得我需要花点时间区整理这些东西，因为别人没有去关注的一个问题，而我去尝试关注或者解决了，这就是我的收获啊，总而言之，在这篇相当细碎的文章背后，我收获的可能并不比这篇文章里写出来的少，原谅我这些唠叨的碎碎念吧，这篇文章就是这样啦，谢谢大家！\n","date":"2018-06-05T11:03:57Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/172426938/","slug":"172426938","tags":["时区","时间","格式化"],"title":"一个由服务器时区引发的 Bug"},{"categories":["生活感悟"],"content":" 最近看过了由全智贤主演的电影《暗杀》，虽然说这是一部我们早已熟稔的抗战题材电影，可是在全女神颜值和演技的诱惑下，我终于还是花了点时间来看这部电影。或许是因为我们见识过了太多的**“抗日神剧”**，所以在面对这样一部电影的时候，我们难免带着某种不屑的眼光去审视它。可是当你看完了这部电影，突然间兴奋到难以自制，不由地惊呼一声：想不到韩国拍这种主旋律电影都能这么好看。我想，这是一种由视角转换所引起的代入感，我曾经看过日本拍摄的甲午海战，日本在明治维新以后，积极地向西方学习先进技术，从天皇到官员，都能从俸禄中省出钱来发展海军事业，相比之下，以天朝上国自诩的大清帝国则是李鸿章一人在支撑着岌岌可危的朝廷。同样地，通过《浪客剑心》同名电影，你会意识到，在明治维新这场变革背后，可能会有无数个像志志雄这样的政治牺牲品。这些都是因为视角发生变化而引起的变化，同样地，在这部电影中，它讲述了韩、朝两国人记忆中的抗日战争，这场我们曾经经历过的抗日战争，在韩国人眼中到底是什么样子的，这或许对我们看待历史会有所启示。\n一明一灭，两条暗杀线 电影讲述了 19 世纪 30 年代，以安沃允(全智贤饰)为首的暗杀三人组，奉命刺杀日军驻朝鲜司令官及本国卖国贼的故事。故事发生在京城(即韩国首尔)和上海，时任韩国临时政府局务局局长的廉锡镇(李政宰饰)，在早年刺杀日军大将失败以后，暗地里早已投靠日本人，此时更将暗杀三人组的消息泄露给日方。一时间，暗杀三人组的刺杀行动和处决叛徒廉锡镇刺杀行动，构成了一明一暗两条线索，呼应了本片片名**“暗杀”，而廉锡镇更是找来赏金猎人“夏威夷手枪\u0026quot;(河内宇饰)，意图阻挠暗杀三人组的刺杀行动，可以说，整个故事脉络就像这个刺杀行动一样简单直接，因此其故事悬念就落在了刺客的刺杀计划如何落实以及反派的阻挠方案将如何开展**。所以，这个电影吸引人的地方就在于，即使你知道反派最终一定没有好下场，可不到全女神放下枪的那一刻，你总是不肯放下心来。因为即使是看双方斗智斗勇，在颜值与演技都在线的情况下，这一切依然显得赏心悦目。全女神在片中几乎承担了所有的动作戏份，可即便如此，李政宰在片中饰演的反派廉锡镇，风头一度不亚于全女神，这些我们后面再详细说。\n电影中前期出现的场景都是在上海，可能有朋友会疑惑：为什么一部韩国拍的电影里会出现上海。这就要首先说说这段历史，当时韩国的临时政府是设在上海的，目的是方便金九、金元凤这样的革命志士开展救亡图存活动，因为当时的韩国(在南、北朝鲜没有分裂以前，指整个朝鲜半岛）早已笼罩在日军的军事统治阴影下。历史上，这段时期长达 35 年之久。日本是什么时候占领朝鲜的呢？没错，就是我们熟悉的中日甲午战争。当时北洋水师正是在运送陆军抵达朝鲜后的返航途中，与日本海军发生近代历史上第一次大规模铁甲舰海战。这场战争的结果我们都知道，北洋水师几乎全军覆没，清政府更是同日本签订了丧权辱国的马关条约。日本占领朝鲜半岛后，曾对当地人进行了惨无人道的屠杀，正是从那个时候起，朝鲜开始笼罩在日军的统治阴影之下，而流亡国外的临时政府，不得不寄居在上海的法租界，继续开展抗日活动。影片中的金九和金元凤，在历史上都可以找到记录。上海虹口爆炸事件，其实就是这部电影的历史原型，影片中二金的合作促成了三人暗杀组的成立，故事由此开始。\n这个反派有点帅哦 李政宰饰演的廉锡镇，在电影一开始是以革命志士的形象出现的，他刺杀日军大将的任务失败，直接导致他在被捕后遭受严酷的刑罚。与此同时，间接导致了女主安沃允的母亲被亲日派父亲康寅国派人杀死，安沃允与双胞胎姐姐美津子分离，直至多年后，来自东北抗日武装的安沃允，和自幼在日占区长大的美津子，终于在一个屋檐下相认，可转眼间，姐姐就被卖国贼康寅国给杀死了，电影中全女神亲眼目睹姐姐死亡的那一幕真的是令人心碎。可偏偏是这样一个人，亲手挑选了这三名暗杀组的成员，亲手将刺杀行动的情报泄露给日方人员，尤其是他从衣兜里取出假信件投入火堆，伪造出信件被毁的假象这一幕。面对金九的怀疑，在明知手枪里没有子弹的情况下“以死明志\u0026quot;。对昔日的同志毫不手软，两个被金九派去刺杀他的同志，均被他重伤甚至杀死。面对曾经刺杀过的日军大将，他可以厚颜无耻地邀功请赏，并接受日军授予的爵位。可恰恰是这样一个人，在喝醉酒以后，诉说朝鲜各种武装力量各自为政的现实，忏悔把暗杀三人组送去送死。在严刑拷打面前，他做了叛徒，一如暗杀组成员干革命要给钱，这些或许没有那么伟光正，可它是那么的真实。\n演技与颜值同时在线的全女神 全女神饰演的安沃允，是一个来自东北抗日武装的狙击手，一出场就瞬间狙杀四名敌人，身手当真是是不凡啊，更不必说端着汤姆生冲锋枪窜房顶跨屋脊，在负伤的情况下趴在疾驶的卡车引擎盖上。据说全女神电影中的动作戏都没有使用替身，一个明明可以靠颜值的人，尚且可以如此努力地去拼搏，那么身为普通人的你我，又有什么理由不努力呢？野蛮女友时期的全女神，或许看起来只是漂亮而已，而现在看来则是实力派。可她同样是一个憧憬着喝咖啡谈恋爱的少女，是一个看到姐姐洁白的嫁会衣泣不成声的妹妹，是一个面对亲生父亲无论如何都下不去手的狙击手。在假扮姐姐美津子参加婚礼以前，她做好了最坏的打算，就像她脑海中浮现过的画面一样，在敌人乱枪扫射下，献血染红了她洁白的婚纱……这或许是“夏威夷手枪”脑补的画面？这里有一个细节，“夏威夷手枪”将全女神送到医院以后，“夏威夷手枪”讨论起他对于暗杀行动的看法，全女神说了这样一段话，大意是“杀掉日军司令和汉奸康寅国，到底能不能让国家独立，这一点没有人会知道，但她必须要让人们知道，她们一直在战斗”……\n这一刻，这个娇弱而坚强的女性形象就立起来了。全女神在本片中分饰两角，即姐姐美津子和妹妹安沃允，不过这种差异基本都是通过眼神表现出来的，姐姐身上有那种从小生活安逸的娇气，而妹妹身上有那种内敛冷静的帅气。战争从来都是残酷的，康寅国为了依附日本人，将美津子误认为安若允并杀死。在我看来，即便没有认错，以康寅国的为人，知道女儿和独立军有关联，他还是会这样做，因为女儿的幸福他完全不在乎，和日本人联姻无非是为了拉拢日本人。正如全女神所言，他用那双杀死了母亲的手，杀死了自己的女儿。人类的情感有时候就是这样诡异，一个对自己从来没有养育之恩的父亲，对方叛国投敌助纣为虐，即使两者间唯一的联系，是那可有可无的血缘关系，可最终还是需要“夏威夷手枪”，这个曾经是“杀父联盟”一员的人，替她开出这一枪。\n一个超有力量感的故事结尾 故事从挟持人质这里开始，就突然变得敷衍起来，可能这里就需要感情戏来作为某种过渡，假如两个人真的去了米拉波，这就真的变成了爱情电影，可这部电影不就是，一部打着主旋律幌子的动作电影吗？这种类型电影为了增加娱乐性，是需要幽默和爱情的。真正将影片推向高潮的是结尾出的审判，证人在开庭前就被廉锡镇派人杀死，于是没有可以再证明，廉锡镇曾经投敌叛国、出卖同志的罪行。这个世界上永远有大量的无知的人，他们选择用暴力来面对一名“韩奸”，可当廉锡镇脱下衣服，义正言辞地讲述自己“支持”独立运动的事迹时，这些人突然开始宣布这名“韩奸”无罪，这是否说明大众都是愚蠢的，可正是这些人的想法，在左右着我们每一个人，这和那些努力制造“焦虑”的人没有区别，我们不愿意相信真相，宁愿相信自己早已固化地思维，或者是人云亦云，没有自己独立的判断，这实在是件可怕的事情。法官失落地宣布证据不足、廉锡镇无罪释放的时候，大概内心会有某种无可奈何或者是不甘心。\n这让我想起 Unnatural 里高濑这个案件，因为没有办法证明对方杀人，而关键的信息又被久部泄露出去，所以，这个案件一度到了要修改鉴定报告的程度，这和身为法医的三橙心中的使命感不相符合，关键时候，是神仓所长坚持递交了原始的鉴定报告。当我们想要制裁一个人的时候，能不能依然客观地去证明对方有罪，不冤枉任何一个人固然值得赞赏，可为了让对方伏法而采用非正义的手段是否是正确的呢？如果身为法医的三橙，用修改鉴定报告的方式，给高濑这个罪犯定刑的话，我相信，我们所有人都会失望，因为她不愿意输给非正常死亡，不愿意正常的人被乱入非正常的事件，采用非正常的方法去伤害别人或者是自己。相比中堂使用逼供的方式查找真相，她更希望中堂医生以一名法医学者的身份去战斗。自然，故事的结尾，所谓善恶有报，16 年前的暗杀任务，终于在韩国光复以后，有安沃允和明宇重新执行，结尾处被乱枪打死的廉锡镇，在被问到为什么要出卖同志时，说了一句“我没想到会解放啊”，一句听起来像开玩笑的话，其实说出了战争年代人们的无奈，如果没有战争，或许这些事情就真的不会发生，可当战争机器被发动时，又有谁会想到这些呢？被卷入战争里人们没有选择，而发动战争的人从来不考虑以后。\n写在战争结束以后 旷日持久的战争终于结束了，当画面定格到全女神那张近乎素颜的脸上时，她突然想起那些曾经最为亲切的面孔，想起“炸弹专家”黄德三，想起“速射炮”邱尚沃，想起酒馆老板娘……战争带给我们的是永远的伤痛，今天我们对于日本这个国家，可能有时候还会充满抵触情绪，但我想说的是，这场战争并没有结束，金九认为日本人已经投降，不再需要可依靠捐助维持，以光明正大地回到国内搞建设，可事实上像廉锡镇这样投日派，并没有完全得到清算，所以，金九在回国后不久就被韩国激进分子刺杀，廉锡镇所说的独立运动派系之争，在历史上是真实存在着的，金九就是被卷入到这场政治斗争中的牺牲品，所以，金元凤最终选择了朝鲜，而这种派系之争，更是加剧了整个朝鲜半岛的分裂，在这片土地上，曾经一起战斗过的兄弟、朋友，最终变成兵戎相向的敌人。\n可这真的是和平吗？战争真的结束了吗？被 38 线分割开的这两个国家，一个通过韩剧、料理和科技为世界所知，一个更像是改革开放初期的中国，不知道还说神秘还是落后。何况，这条 38 线是停战线，并非某种和平的象征，而直至今天，这种刺杀的阴影一直笼罩在韩国政坛上，韩国现任总统朴槿惠的父亲和母亲先后都死于刺杀，所以，即使战争结束了，就能换回和平吗？就能抚平人们心中的伤痛吗？朝鲜与韩国，也许在我们有生之年里，都难以看到他们真正地握手言和，就像苏联解体以后不会再联合在一起，欧盟并非想象中的牢不可破，爱尔兰和北爱尔兰原本就是一家，印度和巴基斯坦是殖民战争的遗留问题……战争，带来的坏处，永远比好处要多。我们向往铸剑为犁的和平生活，可战争结束以后，是否真的能带来和平，人心中的伤痛需要多久可以愈合，人与人的相争逐利之心需要多久可以平息。\n2018 年的儿童节，同往年不同，因为许嵩为炮火中的叙利亚孩子们，创作了一首新歌《大千世界》，这首歌以 2017 年 4 月 15 日叙利亚炸弹袭击事件为背景，呼唤爱与和平，控诉那些肆意发动战争，而将无辜孩童卷入战火的人们。大千世界里的大人们，不要忘了你们曾经都是孩子，当人们都在通过晒娃这种方式度过儿童节时，你是否会想到在世界的某个地方，有人在穿着捐助的衣服和玩具的同时，更是被迫享受着温柔的暴力，我们从小给小孩子的玩具枪，是否有一天会真的变成荷枪实弹呢？我们在盛世之年，我们在贫富之间，我们在虚实交错路口，不断找寻，任何形式的相遇。愿大千世界，再无战争，再无暴力，愿每个深爱的人，都能被温柔对待。\n","date":"2018-06-01T09:33:25Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2462008667/","slug":"2462008667","tags":["影评","和平","全智贤"],"title":"关于电影《暗杀》背后的故事和想法"},{"categories":["读书笔记"],"content":" 距离读完马尔克斯的《霍乱时期的爱情》这本书，差不多已经有一个月左右的时间啦。相比小说中错综复杂的人物关系，更加让人印象深刻的或许是“百年孤独”式的开头。不论是多年后面对行刑队的布恩迪亚上校，还是拍打鹦鹉结果从梯子上摔下来的乌尔比诺医生。在这一刻，因为人物的过去与现在层叠出的这种时空感，或许就是马尔克斯想要去描绘的魔幻现实主义。最近看了由小说改编的同名电影，感觉对这部小说的印象更为具体化，小说的时间跨度将近半个世纪，是选择乌尔比诺和费尔米纳这样稳定的婚姻关系，还是选择阿里萨和费尔米纳这样偏执的爱情故事。我想，这是一个值得去思考的问题吧。\n当乌尔比诺从梯子上摔下来即将离开人世的时候，他拼尽最后一口气对费尔米纳说：“只有上帝才知道我有多爱你”。单单从这句话来看，他们两个人或许是相爱的。可明明不久前，两个人还在为了一块肥皂的事情而争吵。现在大家对出轨这个问题看得特别重，重要到不要说是肉体出轨，连精神出轨都是不能被原谅的。从去年至今，网络上各种出轨的舆论消息层出不穷，好像爱情越来越不值得期待。可你看乌尔比诺和费尔米纳的婚姻是什么样的呢？乌尔比诺在妻子外出期间出轨了一名黑人女子，虽然他选择主动向妻子承认出轨，及时回归家庭，可在我们这些外人看来，这样的婚姻是含有杂质的婚姻。从妻子费尔米纳的角度，她结婚以前是不吃茄子的，而结婚以后则适应了茄子，你可以说两个人在一起，一定会有一方选择妥协。可在一个动辄讲三观、讲兴趣、讲地位的年代，你是否会觉得两个人合适呢？\n合适，是一个特别巧妙的词汇，巧妙之处在于它真正可以做到“以不变应万变”。乌尔比诺夫妇的婚姻，或许是大多数人的真实写照。两个人第一次见面，源于费尔米纳的一场疾病。当时外面正流行着霍乱疫病，费尔米纳因为怀疑被感染了霍乱不得不寻找医生治疗，恰好乌尔比诺正从巴黎旅行回来。在医学技术不发达的年代，医生是没有听诊器的，所以乌尔比诺必须贴着费尔米纳裸露的胸部听心跳。书作和电影中都详细地描绘了这个过程，两个青年男女在这种情况下发生了身体上的接触，费尔米纳更是被对方身上的男性气概所吸引。可这算是爱情吗？我更愿意相信，这是一种原始的欲望冲动，可你说这两个人间没有爱情，估计所有人都会反对，谁让我们都喜欢用海枯石烂表示爱情的忠贞，这两个人在一起生活 50 年，甚至作者都表示：一对恩爱的夫妻最重要的不是幸福，而是稳定的关系。\n所以，不管你愿不愿意承认，爱情最终都会部分地转化为亲情，爱情本身是有瑕疵的、有缺陷的，争吵不可避免，犯错不可避免。诚然，我们都希望对方忠诚的对待自己，可人归根到底是一种对自我忠诚的动物，你说你不能接受对方变心，可人、时间和空间无时无刻不在发生着变化，喜欢或者不喜欢，不过是某一瞬间的状态，你必须相信，爱情本来就不完美、充满瑕疵，可这就是真实的爱情的样子啊，人们会记得你婚礼上的海誓山盟，唯独不会记得你每天柴米油盐的平平淡淡；人们会给他们愿意看到的表面现象去点赞，唯独不会关注你是不是真正的快乐。爱情里有人不厌其烦地寻找真爱，有人沉溺在回忆里不敢再触碰爱情，对我来说，这两种选择我都表示尊重，因为爱情本来就有它真实的样子。\n我不知道，还会不会有人为了别人而苦等 51 年 9 个月零 4 天，一个人究竟有多大的勇气和执念，才能从一个朝气蓬勃的青年变成一个白发苍苍的老人。金庸先生的名篇《射雕英雄传》里，神算子瑛姑因为失去爱子而一夜白头，我想，这其中有对段皇爷见死不救的怨恨，有对周伯通求而不得的执念。可对阿里萨而言，从他遇见费尔米纳那天开始，他的生命就仿佛注定是属于她的，他坚持给她写信，在楼下为她拉小提琴，在长椅上刻下她的名字，甚至是喝花露水、吃玫瑰花。如果说爱情像一场霍乱，应该会没有怀疑，因为阿里萨的确像是生了一场霍乱，不然怎么会疯狂地爱直至偏执甚至有些荒唐。我完全可以理解阿里萨的举动，因为年轻时的我们都曾做出过类似的举动。我并不反对这样的爱情，可当你回头来看这两个人的爱情的时候，费尔米纳对阿里萨这个人几乎一无所知，除了知道对方的职业是报务员。\n电影中费尔米纳甚至给阿里萨回了信，可就如同费尔米纳所言，“他们两个人之间只有虚幻，爱情蒙蔽了彼此的双眼”，大概所有一见钟情的人都没能考虑一个问题，那就是你真的了解对方这个人吗？非常不幸的是，即使亲近如父母、妻子和丈夫这样的关系，一个人也永远不可能了解另外一个人。一个人究竟要爱得多卑微，才会心心念念地等着对方的丈夫死掉，甚至怕对方比丈夫先死掉。假如阿里萨只是这样痴痴等待 50 年的话，我们最多只是替他感到惋惜而已，可偏偏阿里萨为了“报复”费尔米纳，缓解被她伤害的心，开始一次又一次地疯狂纵欲，在肉体的狂欢中不断强化精神层面上对费尔米纳的爱，据他自己记载，他和寡妇、少妇甚至少女都发生过关系，可当他终于等来费尔米纳的时候，他声称自己是一个处男。\n人常常复杂到让你我怀疑人生，而阿里萨则是一个复杂到，让你觉得他还有点可怜的人。他视其它女性的肉体如无物，唯独将费尔米纳推上女神的圣坛。更微妙的是，费尔米纳居然是喜欢过阿里萨这个人的。她不过是在乌尔比诺和阿里萨间选择了更好的一个而已，可阿里萨这种病态的爱在我看来是极为自私的，因为无论两个人多么地爱彼此，一旦出现这种肉体的出轨，就意味着永远无法挽回。虽然费尔米纳选择嫁给了乌尔比诺，可假如有一个人在别人的身体上出轨无数次，在你的丈夫逝世以后告诉你，他等这一天已经等了 51 年 9 个月零 4 天，我不知道你会作何感想。我没有任何的封建思想残留，我尊重女性在丈夫死后改嫁的自由，可选择这样一个充满“缺点”的人，我觉得还是需要去认真想一想的。\n两个人如果真心相爱，即便是满头白发的蹒跚老者，我认为结婚都是没有问题的，可当两个 70 多岁的老人坦诚相见时，当阿里萨看到费尔米纳干瘪下垂的胸部时，当各自看到对方充满皱纹和赘肉的身体时，我真的想知道，这 50 多年的等待真的值吗？或许是值的的，就像这两个人喜欢的都是有点幻想成分的对方一样，我向往永远靠精神慰藉彼此的帕拉图之恋，也不排斥男欢女爱的肉体之欢，可无论哪一种都必须建立在真实的现实中，一个虚幻的爱慕者，一个你并不真正了解的人，当幻想被打破的一瞬间，或许就是爱情破碎的时候，所以，我希望我们对待感情更慎重些，即使没有人爱你，学会自爱未尝不可。我们的生命原本就短暂，何苦要将这生命浪费在别人身上，况且我们有时候我们就像费尔米纳一样，分不清到底是爱还是孤独，人在经历枯燥和乏味以后是会变的，会变得对事物充满新鲜感，即使是曾经不喜欢的东西。\n从某种角度而言，阿里萨是成功的，因为他用一生的时间得到了喜欢的女人。可我时常觉得人生有比这更重要的事情，就像你小时候看到喜欢的东西，却发现自己买不起的时候，你会怎么样做呢？我想大多数人都会选择不要了或者是等以后有机会再买。可人就是这样奇怪的动物，明明以前非常非常喜欢，可突然有一天发现咋再喜欢不起来。为什么我们对这件事情可以坦然接受，唯独在面对感情的时候常常无法自拔呢？你当初有没有得到这样一件喜欢的东西，或许会影响你在未来的人生轨迹，可在大多数情况下，我们的生命实在泛不起多少涟漪。有人说，人生下来的时候，结局就早已注定，我们唯一能做的事情，就是努力去填补和丰富这五六十年的时间。这样说来，人生实在是没有什么事情非做不可的，如果有，那只有一件事情，那就是努力地活下去。\n《Unnatural》里中堂系一直对恋人的死无法释怀，整整八年时间一直都在调查恋人的死亡原因，直到真相被查明，得到恋人父亲的原谅。我不是说，人生不可以有执念，我只是希望大家明白，执念只会让你太关注结果而忽略过程，而我们的生命是需要一天天去度过的。就像阿里萨终于得到了费尔米纳，可两个 70 多岁的老人，在这个世界上还有多少时间可以挥霍呢？我倒情愿日子过得稍微慢一些，用一辈子的时间去了解对方，我们一直所希望看到的，不就是被人理解和认同吗？如果永远一个可以同你交流灵魂的人，那么就努力学习一个人去生活，人生没有那么多必须做的事情，只有你愿不愿意去做的事情。起风了，就当努力生存。活着不好吗？我实在不愿意再看到罗密欧与朱丽叶这样的悲剧，虽然我们都曾歌颂过这样的故事，可只要活着就会有新的机会啊。\n有时候想想我们父母这一代人，几乎在毫无准备的情况下，被动地步入了婚姻的殿堂。时隔多年以后，或消融在柴米油盐的平淡里，或交织在子女亲情的羁绊中，或穿行在流水光阴的得失间……直至爱情彻底消亡最终变成亲情，像一滴松胶油慢慢变成一颗精美的琥珀。有人说，婚姻是爱情的坟墓，甚至结过婚的人会觉得婚姻非常无趣。那么，罗曼蒂克是否一定会消亡？如果是，是不是婚姻里有没有爱情都可以，因为总有一天它会枯竭，人生里充满太多无可奈何的事情，单单是爱情这一件事情就可以写满整个历史，爱情里有像童话一般美好的故事，同样有像悲剧一般哀伤的故事。\n或许是我们这一代独生子女们，在接触到更广阔的网络世界后，极大地影响了我们对这个世界的认知，以至于我们觉得自己就是活得太明白了。可如果要这样稀里糊涂地度过余生中的五六十年，每个人突然间又不甘心接受这残酷的命运。当我发现，我要远离父母生活，甚至完全能力能力和精力照顾他们的时候，我很容易地想到我的未来，是不是会和他们一样。有人说，婚姻是为了找到一个人陪你一起往前走，可我们这些独生子女们，早就习惯了一个人去生活，生命里总是充满着无尽的变故，或许她曾经特别特别喜欢你，可突然有一天她就不再喜欢你了；或许你们曾经是特别特别友好的朋友，可突然有一天对方就突然离你远去；或许你们朝夕相处亲密无间，可到最后突然发现根本不了解彼此……\n人明明都是会变的，可偏偏总爱把希望寄托在变化的事物上面。在一个周围一切都在变化的世界里，追求一成不变毫无疑问是贪心的，我们能追求的只有稳定，可难免会问一个不理智的问题：稳定可以理解为爱吗？这正是乌尔比诺和费尔米纳两个人的感情生活留给我们的谜题。年少时或许会憧憬阿里萨这样因爱成痴的故事，可正如村上春树所说，“哪里会有人喜欢孤独，不过是不喜欢失望罢了”。如果爱情是一场霍乱，我希望每个生病的人，都能尽早地从这场疾病中治愈。“起风了，当努力生存”，就像石原里美饰演的三橙说过的，“有时间绝望还不如去吃点好吃的呢”，比起找到心爱的人，学会如何爱自己不是更重要吗？\n","date":"2018-05-22T09:05:34Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3782208845/","slug":"3782208845","tags":["阅读","影评","马尔克斯"],"title":"爱情像一场霍乱"},{"categories":["开发工具"],"content":"Hello，大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是：https://qinyuanpei.github.io。今天想写一点关于 Linux 部署 ASP.NET 相关的话题，为什么突然想写这个话题呢？因为就在几天前，我被我所认识的一位前辈深深地鄙视了一番，原因是我依然在使用一个落后的 IoC 框架——Unity，在如今已然是公元 2018 年的今天。我突然想到，距离.NET Core 2.0 发布已经有一段时间，而.NET Core 3.0 的 roadmap 已经开始提上日程，可我好像还没来得及认真地去对待这个现状。我一直在关注跨平台和跨语言的技术，就像我在大学里的时候就开始接触 Linux 一样，未来我们要面对的是种类繁多的终端平台，从 PC 时代到移动互联网，再到 VR、AR、IoT 和 AI，有太多太多的事情在悄然发生着变化。偶尔我的内心会泛起焦虑和迷茫，可在时光蹉跎直至褪色以前，我或许只是变回了曾经的自己。既然要如同涅槃一般重新开始，为什么不首先重新拾起曾经关注的领域呢？所以，在这今天这篇文章里，你将看到：如何使用 Jexus 实现 ASP.NET 在 Linux 平台下的部署。\n故事背景 我们项目组在开发这样一种服务，它可以通过收集招聘网站的简历来提取相关信息，而这些信息将作为训练集供 AI 算法使用。考虑到 Python 在 AI 领域的优势，我们决定采用 Python 来开发自然语言处理相关的业务，而简历的收集则是通过.NET 中的 Web Service 暴露给前端。整个开发相对顺利，可是在部署环节出现了问题。因为项目组以往的的项目都是部署在 Linux Server 上，所以在部署 Web Service 的问题上产生了分歧，负责运维的同事不愿意为这一个项目而单独配置一台 Windows Server。这里需要说明的是，采用.NET 来开发 Web Service 的一个重要原因是，这些简历中存在大量 Word 文档(.doc/.docx)，因此不得不采用 Office 提供的 COM 组件来支持文档的解析，虽然后来证明的确是这些 COM 组件拖了跨平台的后腿。所以，在这个时候，我们面临着两种选择，第一种方案是采用 Windows Server 来部署，我们的运维同事表示不开心；第二种方案是采用 Linux Server 来部署。我们知道.NET 跨平台的一个关键技术是 Mono，可 Mono 的问题是它的基础类库不大健全，相信微软收购 Mono 以后这个问题能够得到解决。目前官方主推的跨平台技术是.NET Core，考虑到迁移到.NET Core 版本的成本，我们最终没有选择这个方案。事实上，即使采用.NET Core 进行开发，最终我们的部署依然需要依赖Jexus。综合考虑这些因素，我们决定采用Jexus来将 ASP.NET 项目部署到 Linux 平台。\n关于 Jexus Jexus 是由宇内流云开发的一款 Linux 平台上的高性能 Web 服务器，它是一个可以免费使用、不开源的项目，最大的特色是可以支持 ASP.NET、ASP.NET Core、PHP。通俗地来讲，我们可以认为它是 Linux 平台上的 IIS，这并不为过，因为你可以注意到Jexus Manager这个项目，它可以同时支持 Jexus，IIS 和 IIS Express 三种服务器的管理，并提供了各个平台下一致的使用体验，而 Linux 平台则主要是针对 Jexus。Jexus 提供了不亚于商用服务器的众多特性，比如多站点支持、使用应用程序池来调度管理工作进程、具有良好的稳定性和容错能力、支持 HTTPS 和 WebSockets、支持 FastCGI 协议和 OWIN 标准。除此以外，它同时支持 URL 重写、反向代理、压缩传输、入侵检测等重要功能。Jexus 底层采用 Linux 中的 epoll 机制来处理网站请求，所以会比通常使用 libuv 实现的技术拥有更高的性能。作为一款跨平台软件，Jexus 支持主流的 Linux 发行版本。目前，国内外已经有大量的网站采用 Jexus 作为它的服务器，我们可以在 Jexus 的官网上找到这些案例。虽然微软官方正在全力推广.NET Core，但对于那些需要维护的旧项目而言，迁移到全新的.NET Core 平台着实是个不小的挑战，而且目前支持.NET Core 版本的类库并不丰富，虽然最终的趋势一定是.NET Core 替代 Mono，但对于 Mono 而言，在.NET 宣布开源以后，从.NET Framework 中吸收的基础类库，极大的改善了 Mono 基础类库不完善的状况，而 Mono 针对 CLR 的实现、C#编译器的实现、AOT 环境等等特性，或许可以为.NET 跨平台提供借鉴，这是一个相互促进的过程。在新时代到来以前，我们暂时需要使用 Jexus 来过渡。\nHello Linux OK，下面我们来体验一下 Jexus 在 Linux 平台上的效果，这里我们以 ASP.NET MVC4 为例，我们直接通过 Visual Studio 创建一个项目即可，这里我们需要的是这个项目发布以后的所有文件。总之，这些文件需要通过某种方式放到 Linux 平台上，大家自己去想办法就好啦，这个不再说多余的话。\n安装 Jexus Jexus 安装起来是非常简单的，这里博主使用的是 Elementary OS，基于 Ubuntu14.0 的衍生版本。在终端下执行如下命令：\ncurl https://jexus.org/release/x64/install.sh|sudo sh 你没有看错，真的只需要一行命令。事实上，Jexus 分为两个版本，即通用版和独立版。其差别是通用版不含 Mono 运行时，独立版含有 Mono 运行时。官方建议使用独立版，如果有朋友想尝试安装通用版，请在终端下执行如下命令：\ncurl https://jexus.org/release/install|sudo sh 无论采用哪一种方式安装，当你看到终端中显示：Jexus 已经被成功安装到系统，就表示 Jexus 安装成功了。\n配置 Jexus Jexus 部署到网站，需要两个东西，一个是网站内容(废话)，一个是网站配置。假定我们这里将这两个东西打包在一起，压缩包的名字为 app.tar。为什么这里选择了.tar 格式的压缩文件呢？因为在 Linux 平台下这个格式更好用些，我们熟悉的.zip 格式，可能会需要我们安装相应的扩展。此时，我们可以使用如下脚本来部署网站：\ntar -xf app.tar sudo mv -f .aspnetconf usr/jexus/siteconf/aspnetconf sudo mv ./aspnet /var/www OK，现在来解释下这个脚本，这里我们需要部署一个名为“aspnet”的网站，所以，网站的内容被放置在“aspnet”这个目录里。该网站对应一个作用于 Jexus 的配置文件，配置文件的名字为 aspnetconf。首先，我们将“aspnetconf”这个配置文件移动到了“usr/jexus/siteconf/”目录下，这是 Jexus 指定的配置路径，即每一个站点都有一个配置文件，且该配置文件被放置在“usr/jexus/siteconf/”目录下。然后，我们将“aspnet”这个文件夹移动到了“/var/www”目录下，这是 Jexus 指定的网站目录，即每一个站点都有一个文件夹，文件夹的名字可以理解为网站的名字。默认情况下，Jexus 会在 www 目录里创建一个名为 default 的文件夹，即默认有一个名为 default 的站点，不过经过博主核实，最新版(v5.8.3)中是没有 default 站点。同理，Jexus 会 siteconf 目录里创建一个名为 default 的配置文件。我们通常以这个配置文件为参照来编写我们自己的配置文件，例如下面是 aspnetconf 中的内容：\nport=4000 root=/ /var/www/aspnet hosts= indexs= aspnet_exts= 其中，\nport 表示 Jexus Web 服务器监听的端口(必填） root 表示网站虚拟目录与其对应的物理目录，中间使用空格分开(必填） hosts 表示网站域名(建议填写)，可以使用泛域名如*.yourdomain.com 或者填写*表示默认网站，一个端口有且只有一个默认网站，选填 indexs 表示网站首页文件名，如 index.html、index.aspx 等，多个文件名使用英文逗号分开，选填 aspnet_exts 表示 ASP.NET 扩展名，不建议填写。如要填写，多个扩展名(不含.)使用英文逗号分开。 最简单的配置只需要 port 和 root 即可，更多的配置项可以参考官方文档。\n基本使用 Jexus 的常用命令简单到只有 3 个，start、restart、stop。命令的基本格式为：\nsudo /usr/jexus/jws start [站点名(可选，不指定时表示所有)] sudo /usr/jexus/jws restart [站点名(可选，不指定时表示所有)] sudo /usr/jexus/jws stop [站点名(可选，不指定时表示所有)] 在这个例子里，我们执行如下命令来启动 aspnet 这个站点：\nsudo /usr/jexus/jws start aspnet 当终端中返回 OK 时，就表示启动成功啦，此时，我们打开浏览器，输入http://localhost:4000 就可以看到如下画面(这里的端口号为 4000)： 运行在Linux上的ASP.NET\r你就说，这算不算惊喜。我们还可以输入http://localhost:4000/info来验证 Jexus 是否配置正确，当 Jexus 被正确配置以后，你就会看到一个显示着“Welcome to Jexus”的页面。嗯嗯，好像是和 Nginx 挺像的哈！\nDocker+ 接下来，让我们考虑将这些 Linux 上的工作转移到 Docker 中来做，因为借助 Docker 的容器技术，它可以为我们提供一个足以自给自足的环境。通过这个环境编译测试通过的镜像可以批量地部署到生产环境中。如果你不想在每一台 Linux Server 上都覆盖本文的流程，那么 Docker 将是提高你部署效率的不二选择，而且从认知完整性的角度来看待 Docker，你就会发现它和 Jekins、TravisCI、VSTS 工具一样，都可以非常完美地被接入到持续集成(CI)的流程里去，譬如我们项目组采用的是 Jekins + Gitlib + Docker 的方案，所以，如果你想要选择一个最适合你的持续集成(CI)方案，无论如何，Docker 都是需要去了解的一个知识。关于 Docker 的背景知识大家可以自己去了解，这里我们通过编写 Dockerfile 来完成网站镜像的构建：\nFROM ubuntu:14.04 LABEL vendor=\u0026#34;qinyuanpei@163.com\u0026#34; # Prepare Environment RUN sudo apt-get update RUN sudo apt-get install -y RUN sudo apt-get install -y curl RUN sudo apt-get install -y wget RUN sudo curl -sSL https://jexus.org/release/x64/install.sh|sudo sh # Deploy Website ADD dest/ / RUN sudo mv -f aspnetconf /usr/jexus/siteconf/aspnetconf RUN sudo mkdir -p /var/www RUN sudo mv ./aspnet /var/www # Start Jexus EXPOSE 4000 WORKDIR /usr/jexus CMD sudo ./jws start aspnet 如果你熟悉 Linux 下的命令的话，你就会知道 apt-get、curl、wget 这些命令的含义，真正需要的解释的是 ADD，它表示的是，将 Dockerfile 同级目录下的 dest 目录添加到 Docker 环境中，接下来的命令我们同样非常熟悉，因为这和 Linux 下操作是完全一样的。不过，这里的确有些坑需要踩，在博主构建镜像的过程中，发现容器环境和虚拟机环境还是有本质不同的，这里的 mv 命令在 Docker 下有时候会引发“hard link”的问题，从 Stackoverflow 上好像并没有找到太有价值的答案，总之，这个问题非常的玄学。接下来，我们会将 Docker 容器的 4000 端口暴露出来，为什么是 4000 端口呢？因为这个网站的配置中指向了 4000 端口，这一点在上文中我们已经提及。而入口处的命令，显然是启动 Jexus 服务，这个不再解释。\n这里，我们通过如下命令来构建一个镜像版本：\ndocker build -t jexus-aspnet:v1.0 . 假如这个镜像被成功构建出来，我们就可以使用这个镜像来启动网站啦。如下图所示： 使用Docker创建网站镜像\r具体地，我们可以使用 docke image 命令来管理所有的 docker 镜像。这里我们启动网站：\ndocker run -p 4050:4000 -t jexus-aspnet:v1.0 这里，我们将 Docker 容器的 4000 端口映射到主机的 4050 端口，当我们在浏览器中输入：http://localhost:4050，就可以得到和 Linux 下一样的结果。不过，在写作这篇博客时，博主使用的是 Windows 下的 Docker，如果大家遇到相关问题，欢迎在博客评论区留言。\n本文小结 本文从一个实际工作的场景切入，分析和阐述了如何使用 Jexus 实现 ASP.NET 项目在 Linux 下的部署。为了简化这篇文章的写作，我们使用了一个 ASP.NET MVC4 的示例项目，真实的项目通常会有数据库，所以情况会比本文所介绍的流程更为复杂，可这让我们看到了一种可能性不是吗？通过查阅相关资料，博主发现 ASP.NET Core 的部署不需要 Jexus，它只需要一个 dotnet run 命令即可。然后，作为一次体验 Docker 的过程，我们通过编写 Dockerfile 的方式让 Jexus 和 Docker 发生了某种奇妙的关联。作为本文的一个延伸，我们需要考虑网站服务停止后可以自动重启，这就是所谓的守护进程机制啦，感兴趣的朋友可以继续深入研究，Jexus 提供了大量的优秀特性，这篇文章中所看到的不过是冰山一角。最终，我们的项目还是没有使用 Jexus，这其中有对 Jexus 性能的不信任，有因为 COM 组件而做出的妥协，有对 Mono 非官方方案的鄙夷……可以说，技术选型是一个受到多种因素制约的问题，谁拥有了话语权，就可以左右技术选型的走向，这是否又印证了，人类并非如自己所标榜的那般理性和正义？好了，以上就是这篇文章的全部内容啦，今天是 5 月 20 日，如果没有人对你说“我爱你”，请记得对自己说“我爱你”，谢谢大家！\n","date":"2018-05-20T14:00:03Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/815861661/","slug":"815861661","tags":["Jexus","Docker","Linux"],"title":"使用 Jexus 实现 ASP.NET 在 Linux 平台下的部署"},{"categories":["开发工具"],"content":"Hi，朋友们，大家好，欢迎大家关注我的博客，我是 Payne，我的博客地址是http://qinyuanpei.github.io。在不知不觉间，5 月份已然度过大半，最近无论是读书还是写作均停滞不前，被拖延症支配的我深感有虚度时光之嫌。今天这篇文章，我将为大家介绍如何使用SonarCloud，来为.NET/.NET Core 项目集成静态检查。如果大家使用过SonarCube的话，对接下来我要讲的内容一定不会感到陌生，因为SonarCloud实际上就是SonarCube的“云”版本。在云计算概念深入人心的今天，我们可以通过互联网来访问各种各样的服务。譬如，我曾经为大家介绍过的 TravisCI 就是一个在线的持续集成(CI)服务。这些云服务可以让我们不再关心基础设施如何去搭建，进而集中精力去解决最核心、最关键的问题。和持续集成关注“持续”不同，静态检查关注的是代码质量。目前，SonarCloud 支持**.NET Framework 4.6以上及.NET Core版本。通过这篇文章，你将了解到SonarCloud 的基本使用**、SonarCloud 与 TravisCI 的服务集成这两方面的内容。\nSonarCloud 静态检查，顾名思义就是通过扫描源代码来发现代码中隐藏的缺陷，譬如潜在的 Bug、重复/复杂的代码等等，这些通常被称为代码中的“坏味道”，静态检查就是通过工具去扫描这些“坏味道”。Sonar 是一个基于 Java 的代码质量管理工具，由 Sonar 和 SonarScanner 两个主要部分组成，前者是一个 Web 系统用以展示代码扫描结果，而后者是真正用以扫描代码的工具。Sonar 具备良好的扩展性，众多的插件使得它可以和 Jenkins 等集成工具结合使用，同时可支持不同语言项目的扫描分析。在.NET 中我们可以使用Stylecop来进行静态检查，无独有偶，ReShaper中同样提供了静态检查的特性。在这篇文章中我们主要使用 Sonar 来作为.NET 项目的静态检查工具。\n通常使用 Sonar 来构建静态检查工具时，需要我们在本地搭建一套运行环境，而 SonarCloud 是针对 Sonar 推出的一个“云”版本。我们只需要执行脚本就可以完成代码分析，而分析的结果则可以直接在 SonarCloud 网站中看到。这就是“云计算”的魅力所在，我们无需关心 Sonar 是如何安装以及配置的，当我们需要使用这种服务的时候直接使用就好了。目前，SonarCloud 对开源项目是免费提供的。因此，如果你不想亲自去搭建一个静态分析的环境，那么你可以选择使用 SonarCloud 来对代码进行静态分析。SonarCloud 支持 17 种语言的扫描分析，支持和 Travis、VSTS、AppVeyor 等 CI 工具集成，甚至你可以在 SonarCloud 上找到大量实际的项目。\n我对 SonarCloud 感兴趣的一个重要原因是，它可以和 TravisCI 完美地集成在一起，而且在此之前，我曾经使用过一段时间的 Sonar。在使用 SonarCloud 前，我们需要注册一个账号，这里建议使用 Github 账号授权登录，因为我们需要授权给 SonarCloud 来拉取代码，尤其当你使用 TravisCI 来集成 SonarCloud 的时候。除此之外，我们需要准备好以下工具：\nJDK，即 Java SE Development Kit，运行 SonarScanner 时依赖 Java 环境。 Git，版本控制工具，如果身为一名程序员而没有安装 Git，请面壁思过并自我检讨。 MSBuild，.NET 平台项目构建工具，推荐一个无脑安装的方法，安装全宇宙无敌的 IDE：Visual Studio。 SonarScanner，即 Sonar 的代码扫描器，注意这里有两个版本：.NET Framework 4.6 + 和 .NET Core，本文以.NET Framework 4.6 +为例。 第一个.NET 项目 好了，下面我们来使用 SonarCloud 对博主的一个项目HttpServer进行分析。首先，我们需要在 SonarCloud 中创建一个项目。如下图所示，我们首先选择 Organization，默认情况下，通过 Github 授权登录以后，会生成一个格式为：${UserName}-github 的组织名称，例如我这里是：qinyuanpei-github。这里我们选择默认组织，然后点击：Continue。\n设置组织名称\r接下来，我们需要设置一个 Token，其目的是通过这个 Token 登录 SonarCloud，然后把 SonarScanner 在本地扫描的结果发送到 SonarCloud。这里我们可以选择生成一个新的 Token 或者是使用一个已经存在的 Token。建议使用一个 Token 来管理所有的项目，因为这个 Token 显示一次后就不再显示，同时维护多个 Token 实在是太痛苦啦，当然，如果你能管理好所有 Token 的 Key 的话。设置完 Token 点击下一步：\n设置Token\r设置完 Token 以后需要选择项目类型以及设置项目名称，在这个例子中，博主的项目名称是 HttpServer，建议使用 Sonar-${Project Name}的形式来为项目命名，而项目类型显然应该选择“C# or VB.NET”。\n设置项目名称\r接下来我们就得到最关键的信息，如图所示，这里有三条命令，我们将其复制下来，然后将其写到批处理(.bat)或者 PowerShll 脚本里。以后运行这三条命令，就可以对当前项目进行静态检查，是不是很简单啊？简单分析下，这三条命令，第一条命令根据我们设置的 Token、项目名称、组织等信息“开始”对项目进行分析，注意到这里有一个“begin”；第二条命令是一个 MSBuild 命令，其目的是对整个项目重新构建；第三条命令是将静态分析的提交到 SonarCloud，注意到这里有一个“end”。具体文档可以参考 这里 哦！\n复制3条命令\r好了，现在我们在 SonarCloud 中就可以看到扫描结果啦，开心！如果执行命令出现问题，请确保正确安装了相关工具，并检查这些工具是否被添加到系统变量中，特别是 Java 需要设置 JAVA_HOME。\n扫描结果\rTravisCI 与 SonarCloud 的集成 现在我们来回顾下整个过程，我们需要在本地安装 SonarScanner，这是一个 Java 编写的应用程序，因此我们需要一个 Java 运行环境。每次都需要通过 SonarCloud 来创建项目，获得项目相关的信息以后，在命令中携带这些参数并执行命令，就可以在 SonarCloud 中获得本地的扫描结果。在整个过程中，我们依然需要一个本地的环境，这一点都不灵活。现实世界的复杂性，就在于我们无法为还原出完全一致的处境。\n所以，托尔斯泰开宗明义地说道：“幸福的家庭都是相似的，不幸的家庭各有各的不幸”，况且作为一个执着于让重复的事情自动化的人，如果让我做这件事情，我保证第一次会意外地觉得好奇，而等到第二次、第三次的时候我就会感到厌烦，这就是人们所说的三分钟热度。诚然，我的确是一个花心的双子座。我们提到，SonarCloud 支持 TravisCI，所以，接下来我们来考虑如何让 TravisCI 帮助我们运行 Sonar。\n常规的思路是，下载 SonarScanner 并执行脚本。这种思路的问题在于 TravisCI 运行在 Linux 下，我们确定 SonnarScanner 是否可以支持 Linux 平台，尽管 SonarScanner 使用 Java 开发。通过阅读 TravisCI 的文档，我们发现 TravisCI 本身是支持 SonarCloud 的插件的，由此我们就可以着手将这一切交给 TravisCI 来做啦！\n关于如何使用 TravisCI，这里不再赘述啦！大家可以参考我的这两篇博客，这两篇博客分别是：持续集成在 Hexo 自动化部署上的实践、基于 Travis CI 实现 Hexo 在 Github 和 Coding 的同步部署。当然第一手的资料必然是官方文档，我是不好意思随便对别人说 RTFM 的。按照文档说明，我们首先需要一个名为 sonar-project.properties 的配置文件，在该配置文件中配置了诸如项目名称、组织名称等关键信息，Sonar 会自动读取这个配置文件里的信息并携带到命令中去，这个配置文件是在是太熟悉啦，假如你认真地读了这篇文章，并注意到了 SonarCloud 生成的三条命令。这个配置文件内容如下：\n# must be unique in a given SonarQube instance\rsonar.projectKey=Sonar-HttpServer\r# this is the name and version displayed in the SonarQube UI. Was mandatory prior to SonarQube 6.1.\rsonar.projectName=HttpServer\rsonar.projectVersion=1.0\r# Path is relative to the sonar-project.properties file. Replace \u0026#34;\\\u0026#34; by \u0026#34;/\u0026#34; on Windows.\r# This property is optional if sonar.modules is set.\rsonar.sources=.\r# Encoding of the source code. Default is default system encoding\r#sonar.sourceEncoding=UTF-8 配置文件中有来自官方的注释，我就不再狗尾续貂的去做相应的解释了。我们发现，这个里面是没有 token 的，按照官方文档中的说明，token 应该配置在.travis.yml 这个文件中，熟悉 TravisCI 的朋友就会知道，这个文件通常用来配置持续集成的流程。按照约定，SonarCloud 属于 TravisCI 的一个插件，应该配置在 addons 节点下，我们注意到，在这里可以配置组织名称和 token 两个节点的信息。组织信息这个简单，直接按照前面的流程填写即可，需要注意的是这里的 token。\n因为 token 采用明文配置的话，难免会存在安全风险，所以官方的建议是：使用 TravisCI 的终端工具进行加密。这是一个基于 Ruby 的命令行工具，直接在命令行中对 token 进行加密即可。不过想起很多年前，第一次接触 Jekyll 时被 Ruby 支配的恐惧感，我决定寻找新的出路。官方文档说可以在 TravisCI 中配置全局变量，这种方式我们接入 Coding Page 时曾使用过，不过经过博主尝试，这种方式一直无法获得权限，所以，我不得不在配置文件中写明文，大家不要学我啊：\naddons: sonarcloud: organization: \u0026#34;在这里输入你的组织名称\u0026#34; token: \u0026#34;在这里输入你的token\u0026#34; 原本走到这一步时，我就该和大家对本文进行小结啦！可偏偏我注意到了 SonarCloud 生成命令中有 MSBuild 的身影，于是我开始尝试在 TravisCI 脚本中编写.NET 相关的命令，因为我从未在 TravisCI 中对.NET 项目进行持续集成，所以我很好奇它如果跑起来会是什么样子的。同样参照官方文档，发现目前 TravisCI 支持 Mono 和.NET Core 的两个版本的构建工具，Mono 我可以理解，因为 TravisCI 运行在 Linux 环境下，这和我们以前运行在 Windows 环境下是不一样的。而.NET Core 原本就支持跨平台，目前官方释放出了 2.0 预览版，同时 3.0 的计划开始提上日程。无论或早或晚，我们面对的都将是一个多平台化的未来，永远不要固执地封闭在一个生态系统里，技术是如此，人生何尝不是如此呢？\n好啦，言归正传，了解到这种可能性以后，我开始尝试编写 TravisCI 脚本，官方默认的构建系统是 XBuild，实际使用中遇到些问题，开始考虑能不能替换成 MSBuild，事实上 MSBuild 目前已经是跨平台的，Nuget 同样跨平台。微软收购 Mono 以后，Visual Studio 基本上算是跨平台了，况且我们还有一个编辑器中的黑马 Visual Studio Code。IIS 目前可以考虑用 Jexus 替换，而有了 OWIN 这个服务器接口以后，我们有更多的 Host 可以去选择，现在剩下的只有 SQL Server 啦，可想而知，除了 WinForm/WPF/COM 等这种系统依赖性强的东西，大多数的服务其实都可以跑在 Linux 上。经过反复尝试，最终我们实现了：在 TravisCI 下使用 MSBuild 构建项目、使用 Nuget 在线安装 NUnit 并运行单元测试、使用 SonarCloud 对代码进行静态检查。一起来看脚本怎么写：\njdk: - oraclejdk8 mono: - latest language: csharp solution: ./HTTPServer/HTTPServer.sln notifications: email: recipients: - 875974254@qq.com #请替换成你的邮箱，谢谢 - qinyuanpei@163.com #请替换成你的邮箱，谢谢 on_success: change # default: change on_failure: always # default: always install: - cd ./HTTPServer - nuget restore ./HTTPServer.sln # restore nuget - nuget install NUnit.Runners -Version 3.8.0 -OutputDirectory ./TestRunner # install nunit script: - msbuild /p:Configuration=Release HTTPServer.sln - mono ./TestRunner/NUnit.ConsoleRunner.3.8.0/tools/nunit3-console.exe ./HTTPServerLib.UnitTest/bin/Release/HttpServerLib.UnitTest.dll - sonar-scanner -Dsonar.verbose=true -X branches: only: - master addons: sonarcloud: organization: \u0026#34;在这里输入你的组织名称\u0026#34; token: \u0026#34;在这里输入你的token\u0026#34; cache: directories: - \u0026#39;$HOME/.sonar/cache\u0026#39; 好啦，感受技术的魅力吧！可以注意到，我这里有 4 个单元测试，其中 2 个通过、2 个失败。虽然单元测试没有通过，可我代码没有 Bug 呀！ NUnit运行结果\r本文小结 本文介绍了一个“云”服务：SonarCloud。SonarCloud 是一个基于 SonarCube 的静态分析工具，通过 SonarCloud 我们无需搭建 Sonar 环境就可以对项目进行静态分析。为了验证和实现这个诉求，我们首先提供了通过 SonarScanner 来扫描代码的示例，其原理是在命令行参数中携带相关信息，通过 token 来验证和登录 SonarCloud，在完成对代码的扫描以后，就可以在 SonarCloud 中查看整个项目的分析结果。接下来，为了验证 SonarCloud 和 TravisCI 进行集成的可行性，我们尝试通过 travisCI 脚本的方式来调用 SonarCloud，其原理是通过配置文件获得相关信息由 TravisCI 完成所有的分析工作，这里需要注意的是要对 token 进行加密。在编写 TravisCI 脚本的过程中，我们一同验证了 MSBuild、Nuget、NUnit 等.NET 常规工具或者类库在 Linux 平台下使用的可能性，最终在 TravisCI 的帮助下完成了从项目构建、单元测试再到代码的分析的整个流程。虽然静态分析并不能完全保证代码没有问题，可人类总是不情愿承认自己仅仅是一种高等动物而已，这个世界上有好多东西人们不一定会喜欢，因为它们要么是正确的要么是有益的。本文这个方案需要把代码暴露在 Github，对于一般的服务集成，我们更推荐 Jenkins + Sonar 这样的组合，前者可以替换 TravisCI 提供持续集成服务，同 Github、Gitlib 等代码托管服务进行集成、同 Stylecop、Sonar 等静态检查工具进行集成，这方面的资料非常丰富，我们这里就不再多说啦，总而言之，让一切更好就是我们的目的，晚安！\n","date":"2018-05-12T01:16:52Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/4891372/","slug":"4891372","tags":["Mono","Sonar","Travis"],"title":"使用 SonarCloud 为.NET/.NET Core 项目集成静态检查"},{"categories":["编程语言"],"content":"最近遇到一道非常有趣的题目，题目大意如下：有一个富翁在银河系里做生意，而银河系使用的是罗马数字，所以他需要一个精明能干的助手，帮助他完成罗马数字与阿拉伯数字的相互转换，题目在这个背景下衍生出交易场景，我们需要帮助他计算出相关商品的价格。对于这道题目，如果剥离开这个题目本身的交易场景，这道题目本质上就是一个纯粹的算法问题。说来惭愧，博主当时并未能快速地解决这个问题，事后通过研读别人的文章始能有所领悟。所以，今天想在这篇文章里，同大家一起来讨论下这个问题。今天，全世界都在使用 0 到 9 这 10 个阿拉伯数字，比阿拉伯数字早 2000 年的罗马数字。为什么没有流传下来为后世所用呢？我觉得这是一个非常有意思的问题，数学同计算机学科间那种千丝万缕的联系、技术演进过程中若有若无的某种必然性……这些都是令我觉得非常有意思的地方。那么，一起来看看这个问题可好？\n罗马数字起源 罗马数字，顾名思义，就是古罗马人使用的数字系统。在罗马数字中，共有 7 个基本数字，即 I、V、X、L、C、D、M，它们分别表示 1、5、10、50、100、500、1000。可以注意到，在这套数字系统中，0 不被视作是一个整数。据说，曾经有一位罗马学者不顾教皇的反对，执意将与 0 相关的知识以及 0 在运算中的作用向民众传播，因此被教皇囚禁并投入监狱，理由是 0 是一个邪物，破坏了神圣的数。同样罗马数字无法表示小数(注：罗马数字有分数的表示方法，可仅仅能表示 1/12 的整数倍)，因此罗马数字常常用来表示纪年，在欧洲国家的古书籍、建筑和钟表中，我们都可以见到罗马数字的身影。我们熟悉的元素周期表，同样采用了罗马数字来表示元素所在的\u0026quot;族\u0026quot;。需要说明的是，罗马数字是一种计数规则，而非计算规则，这意味者罗马数字是没有进位和权重的概念的，所以一般罗马数字只用以计数而不用以演算。\n既然罗马数字是一种计数规则，那么我们就不得不说一说它的组合规则，因为 4000 以内的数字，都可以用这 7 个基本数字组合表示。具体来讲，罗马数字的基本规则有以下 4 条：\n重复次数：**一个数字重复多少次，所表示的数字就是这个罗马数字的多少倍；一个罗马数字最多重复三次。**这条规则该怎么理解呢？第一点，I、II、III 分别表示 1、2、3；第二点，4 必须被表示为 IV，而不是 IIII。关于 4 的表示方法，在历史上一直存在争议，一种观点认为 IIII 这种写法占用书写空间，IV 可以达到简化书写的作用；而一种观点则认为 IV 有亵渎神灵朱庇特、含不敬侮辱之意。 左减原则：当一个较小的数字被放在一个较大数字的左边时，所表示的数字等于这个大数减去这个小数，且左边最多只能放一个较小的数字。联系第一条原则，IV 表示的实际上是 V-I，所以这个数值表示 4；同理，9 为了满足第一条原则，必须被表示成 IX。 右加原则：当一个较小的数字被放在一个较大数字的右边时，所表示的数字等于这个大数加上这个小数，且右边最多只能放一个较小的数字。这一条原则和第二条原则相对应，例如 11 会被表示成 XI、21 会被表示为 XXI，以此类推。 搭配原则：I 只能被放在 V 和 X 的左边；X 只能被放在 L 和 C 的左边；C 只能被放在 D 和 M 的左边；V、L、D 不能被放在左边。这一条可以看作对是第二条的总结，所以没有什么可说的。 好了，通过这个这些规则我们就可以组合出不同的数字，我们可以注意到这些数字呈现出 1、4、5、9 的规律。什么是 1、4、5、9 的规律呢？我们可以注意到 4 和 9 是两个特殊的数字，4 必须通过 5 左减来得到，9 必须通过 10 左减来得到，这是因为罗马数字要满足最多重复三次的原则，而 4 和 9 相对 1 和 5 的偏移量恰好是 4，所以它们的表示方法和其他数字不同。因为罗马数字没有进位和权重的概念，所以除了左减和右增这两种特殊情况以外，它的基本数字应该从左至右依次递减，即使在左减的情况下，左边的数字应该和右边的数字处在同一序列。这句话怎么理解呢？例如，90 必须用 100-10 来表示；而 99 必须拆解为 90 和 9，然后分别用 100-10 和 10-1 来表示，唯独不能通过 100-1 来表示，因为 100 和 1 分属两个不同的序列。\n数字转换实现 了解完罗马数字的历史渊源，我们就对罗马数字有了一定的了解。现在来考虑一个问题，即罗马数字和阿拉伯数字间的相互转换。罗马数字的确是古罗马人发明的，可阿拉伯数字实际上却是古印度人发明的。今天全世界人都在使用阿拉伯数字，因此这两者间需要一个转换器，这正是我们一开始所讨论的问题：假如银河系里的人们都使用罗马数字来计数，当一个地球上的富翁来到银河系以后，他要如何去和这里的人们进行交易。显然，这种转换应该是双向的，我们下面分别来看如何实现相应的转换。\n阿拉伯转罗马 首先来考虑阿拉伯数字转罗马数字，因为一个罗马数字必然是从左到右依次递减，所以我们只需要将这 7 个基本数字从大到小排列，找到第一个不小于指定数字的数位即可。例如 1024 显然超过了 1000，而罗马数字中的 1000 对应 M，因此 1024 的第一位应该是 M。接下来 24，显然超过 10，因此 1024 的第二位数字应该是 X。接下来 14，显然超过 10，因此 1024 的第三位数字同样是 X。接下来 4，这是一个特殊的数字，需要被表示为 IV，这是 1024 的第四位数字。我们将整个过程串联起来，就可以得到 1024 的罗马数字形式 MXXIV。我们注意的一点是，这里需要 4 和 9 这两个数字作为辅助数字，因为 1 到 3、6 到 8 的数字，我们总可以通过不断地重复 1 来得到，就像辗转相除法一样。如果没有这两个辅助数字会怎样呢？4 会变成 IIII，而 9 会变成 VIIII，显然这是不符合我们预期的。整理下我们的思路，这段代码实现如下：\npublic static string ConvertToRoman(int number) { var output = new StringBuilder(); var digitMap = new Dictionary\u0026lt;int,string\u0026gt;() { {1,\u0026#34;I\u0026#34;},{4,\u0026#34;IV\u0026#34;},{5,\u0026#34;V\u0026#34;},{9,\u0026#34;IX\u0026#34;}, {10,\u0026#34;X\u0026#34;},{40,\u0026#34;XL\u0026#34;},{50,\u0026#34;L\u0026#34;},{90,\u0026#34;XC\u0026#34;}, {100,\u0026#34;C\u0026#34;},{400,\u0026#34;CD\u0026#34;},{500,\u0026#34;D\u0026#34;},{900,\u0026#34;CM\u0026#34;}, {1000,\u0026#34;M\u0026#34;} }; var digits = digitMap.OrderByDescending(e =\u0026gt; e.Key).ToList(); for (int i = 0; i \u0026lt; digits.Count \u0026amp;\u0026amp; number \u0026gt; 0; i++) { if (number \u0026lt; digits[i].Key) continue; while (number \u0026gt;= digits[i].Key) { number -= digits[i].Key; output.Append(digits[i].Value); } } return output.ToString(); } 罗马转阿拉伯 接下来考虑罗马数字如何转换为阿拉伯数字，我们可以明确的一点是，罗马数字基本上是从左到右依次递减排列的，每一个数字的左侧和右侧出现的数字一定处于当前数字的同一序列。比如，I 只能被放在 V 和 X 的左边；X 只能被放在 L 和 C 的左边；C 只能被放在 D 和 M 的左边。因此，我们从左到右依次遍历整个字符串，将每个字符转化为对应的阿拉伯数字然后累加即可，需要注意的是，当当前元素小于下一元素时，表示当前元素为负数；当当前元素大于下一元素时，表示当前元素为正数。显然，这里最后一位应该是正数，因为它没有下一个元素可以比较。至此，我们梳理出整个思路：从第一位到第 n-1 位依次循环，判断当前元素的正负然后累加，再加上最后一位元素的值即可。下面是代码实现：\npublic static int ConvertToNumber(string romanNumber) { var number = 0; var length = romanNumber.Length; var digits = new Dictionary\u0026lt;string,int\u0026gt;() { {\u0026#34;I\u0026#34;,1},{\u0026#34;V\u0026#34;,5},{\u0026#34;X\u0026#34;,10},{\u0026#34;L\u0026#34;,50},{\u0026#34;C\u0026#34;,100},{\u0026#34;D\u0026#34;,500},{\u0026#34;M\u0026#34;,1000} }; for (int i = 0; i \u0026lt; length - 1; i++) { //前面 n-1 位数字通过左右比较决定正负 \u0026amp; 第 n 位数字必然为正 if ((digits[romanNumber[i].ToString()] \u0026gt;= digits[romanNumber[i + 1].ToString()]) || i + 1 \u0026gt;= length) { number += digits[romanNumber[i].ToString()]; } else { number -= digits[romanNumber[i].ToString()]; } } return number; } 为什么会溢出 相信上面这两段代码，大家都已然把玩过了。可我们仔细想想，就会觉得这事儿不靠谱。前段时间网络上一直流传着，我们这些佛系青年正在被同龄人抛弃。这个题目里我们所面对的，可是一个来自地球的的富翁啊！富翁的钱不都是按亿来计数的吗？我们没有一个亿这样的小目标，我们的目标是月入 5 万啊，这是一个社会上流行的说法。好了，回到这个题目中来，如果我们输入 50000 这个阿拉伯数字，它会输出什么呢？答案是 50 个 M，这很罗马数字啊，当然更神奇的事情是什么呢？当我们尝试把这由 50 个 M 组成的罗马数字转换为阿拉伯数字时，会发现它不能像我们期望地输出 50000，而会变成是一个负数。为什么这里是负数呢？答案是溢出啦！\n过去，我们常常听到”溢出“这个词儿，最常见的是数据溢出。为什么会发生数据溢出呢？因为我们定义的数据超过了计算机所使用的数据的表示范围。这一点我们可能无法理解，一个相对粗浅的认识是，现代计算机的内存已经大到非常客观，甚至我们的硬盘都已经使用 TB 这样的容量单位，为什么还是会发生数据溢出呢？回到罗马数字这个问题，我们发现一个残酷的事实是，古罗马人并没有定义 1000 以上的数字表示，这或许和古罗马人发明数字的过程有关。古人最早都是使用手指、绳结、竹筹这样的工具来计数，在人们没有接触到相当大的数字以前，人们认为这些数的表示是足够的。同样的，我们的计算机经历了从 8 位、16 位、32 位到 64 位的发展。所以，这个世界上没有任何东西是一成不变的，一个技术方案势必要随着业务演化而扩展。\n我们前面曾提到，这 7 个基本数字可以表示 4000 以内的数字，为什么是 4000 以内呢？因为根据罗马数字最多重复三次的规则，我们应该用 5000-1000 来表示 4000，可问题是这 7 个基本数字中并没有 5000 的定义，这和计算机中的数据溢出是非常相似的，因为我们都无法通过现有的构造去描述一个新的东西。这和数学上的那些”扩充“有着极其相似的地方，当我们意识到所有的数不都是整数的时候，我们引入了分数/小数；当我们意识到所有的数不都是有理数的时候，我们引入了无理数； 当我们意识到所有的数不都是实数的时候，我们引入了虚数。在数学上，这叫做数的扩充；在计算机里，这叫做数据溢出。数学作为一本学科，可以通过完善理论来自圆其说；而编程语言里数据结构，是在一开始就定义好的一套规范，它无法更不应该经常去修改，关于如何去解决程序中数据溢出的问题，这已然是一个新的问题了，不过我们可以看看古罗马人是怎么做的。\n聪明的罗马人自然想到了这个问题，他们提出的解决方案是这样的：在一个数字的上面加一条横线，表示这个数增值 1000 倍。所以，按照这个定义，4000 应该由 IV 变化而来，9000 应该由 10000 变化而来，而 10000 则可以看作是 10 的 1000 倍，即 10000 应该由 X 变化而来。我们在最初的规则中为什么没有说这一条呢？因为在数字上面增加一条横线，这更接近一个书写的行为，它增加了我们程序解析的难度，当一个数字的上面出现横线以后，我们就不能再按照原来的方式去转换。所以，考虑这个因素，实际上还是为了简化问题本身，这道题目中同样回避了这个问题。罗马人这个想法的确很好，可以解决眼下我们所面临的问题，可时间久了以后，罗马人发现这套计数规则书写了繁琐复杂，因而这套规则渐渐地就被人们放弃了。在 2015 年意大利官方宣布，国内街道编码、文件编码等全部废弃原有的罗马数字，改为使用阿拉伯数字。\n选择阿拉伯数字 历史最终选择了阿拉伯数字，而不是罗马数字，这并不是一个巧合，尽管罗马数字要比阿拉伯数字早 2000 年。罗马数字的缺陷不仅仅在于其书写的繁杂，一个更重要的原因是，它不能更好地推动数学学科的发展。罗马人发明罗马数字的目的是为了计数，可一旦产生了数，就势必会产生计算。可我们发现罗马数字并不适合计算，因为它对数字的构造并不是正交的。一个最为直观的例子是，数字可能会用一个字母、两个字母或者三个字母来表示，如果两个数字要进行加减法，我们会发现它的数字是无法”对齐“的，你必须非常小心地分清楚不同的数位，而罗马数字恰好是没有数位的概念的。同样，当数字加减时会产生进位或者借位，罗马数字的构造会导致牵一发而动全身，因为任何一个中间步骤，我们都必须将其记录下来，记录的代价是将整个结果重写。反观阿拉伯数字，0 到 9 共 10 个数字可以表示一切，形式上的统一让计算更加便捷，书写更为简洁，这套定义可以扩展到无限大的数上面去，可以扩展到小数、分数甚至无理数、虚数。这是否意味着，一个统一化的定义或者构造，更适合去做相关的运算流程或者逻辑流程呢？\n本文小结 本文从一道有趣的题目作为引子，引出这篇文章的主题：罗马数字。我们首先为大家回顾了罗马数字的历史渊源。罗马数字是一种由古罗马人创造的数字系统，这套数字系统主要的用途是进行计数。罗马数字由 I、V、X、L、C、D、M 共 7 个基本数字组成，其基本规则是最多重复三次、左减右增。接下来，我们分析了罗马数字与阿拉伯数字相互转换的规律，并提供相关代码实现。在当前方案的基础上，我们引出了罗马数字中的”4000“问题，联系计算机中的数据溢出的相关概念，我们分析了为什么当罗马数字超过 4000 时会发生”溢出“，以及罗马人是如何解决这个问题的。虽然罗马数字比阿拉伯数字早 2000 年，可历史最终选择了阿拉伯数字，这里我们简要地分析了原因，因为罗马数字并不适合计算，而数字作为数学的基本要素，一个不能被运用到计算出的数字系统，最终免除不了被人们抛弃的命运。好了，这篇五一节前的文章 就是这样啦，4 月再见！\n","date":"2018-04-30T10:59:46Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/4158690468/","slug":"4158690468","tags":["数学","算法","数字"],"title":"罗马数字与阿拉伯数字的相互转换"},{"categories":["编程语言"],"content":"Hi，各位朋友，大家好，欢迎大家关注我的博客，我是 Payne，我的博客地址是https://qinyuanpei.github.io。这个月基本上没怎么更新博客和公众号，所以今天想写一篇科普性质的文章，主题是 JavaScript 中的修饰器。 为什么使用了\u0026quot;邂逅\u0026quot;这样一个词汇呢？因为当你知道无法再邂逅爱情的时候，你只能去期待邂逅爱情以外的事物；当你意识到爱情不过是生命里的小插曲，你只能去努力弥补生命的完整性。在过往的博客中，我曾向大家介绍过譬如 Spring.NET、Unity、AspectCore 等 AOP 相关的框架，亦曾向大家介绍过譬如 Python 中的装饰器、.NET 中的 Attribute、Java 中的注解等等。再我看来，这些都是非常相近的概念，所以今天这篇文章我们又双叒叕要说 AOP 啦！什么？你说 JavaScript 里居然 AOP！这简直比任何特性都要开心好吗？而这就要从本文的主角——JavaScript 中的修饰器说起。\n什么是修饰器 JavaScript 中的修饰器(Decorator)，是 ES7 的一个提案。目前的浏览器版本均不支持这一特性，所以主流的技术方案是采用 Babel 进行转译，事实上前端的工具链有相当多的工具都是这样，当然这些都是我们以后的话题啦！修饰器的出现，主要解决了下面这两个问题：\n不同类间共享方法 在编译时期间对类及其方法进行修改 这里第一点看起来意义并不显著啊，因为 JavaScript 里有了模块化以后，在不同间共享方法只需要将其按模块导出即可。当然，在模块化这个问题上，JavaScript 社区发扬了一贯的混乱传统，CommonJS、AMD、CMD 等等不同的规范层出不穷，幸运的是 ES6 中使用了 import 和 export 实现了模块功能，这是目前事实上的模块化标准。这里需要关注的第二点，在编译时期间对类及其方法进行修改，这可以对类及其方法进行修改，这就非常有趣了呀！再注意到这里的修饰器即Decorator，我们立刻想 Python 中的装饰器，想到装饰器模式，想到代理模式，所以相信到这里大家不难理解我所说的，我们又双叒叕要说 AOP 啦！\n那么说了这么多，JavaScript 中的修饰器到底长什么样子呢？其实，它没有什么好神秘的，我们在 Python 和 Java 中都曾见过它，前者称为装饰器，后者称为注解，即在类或者方法的上面增加一个@符号，联想一下 Spring 中的 Controller，我们大概知道它长下面这样：\n/* 修饰类 */ @bar class foo {} /* 修饰方法 */ @bar foo(){} OK，现在大家一定觉得，这 TM 简直就是抄袭了 Python 好吗？为了避免大家变成一个肤浅的人，我们一起来看看下面具体的例子：\n修饰类 @setProp class User {} function setProp(target) { target.age = 30 } console.log(User.age) 这个例子展示的是，我们如何通过修饰器函数 setProp()来为 User 对象赋值，为什么叫做修饰器函数呢？因为这就是个函数啊，而且 JavaScript 和 Python 一样都是支持函数式编程的编程语言，所以大家看到这个大可不必感到吃惊，因为大道至简殊途同归。好了，注意到 SetProp()方法有一个参数 target，因为该方法修饰 User 类，所以它的参数就是 User 类，显然它为 User 类扩展了一个属性 age，并给它赋值为 30。相信有朋友一定会奇怪这个 age 是哪里定义的，我只能说 JavaScript 是个神奇的语言，一切都是对象，一切都是函数。现在，当我们执行到最后一句时，会输出 30，这是因为修饰器对类进行修改。\n现在我们尝试修改下这个方法，我们希望可以通过修饰器修改 age 属性的值，而不是让它成为一个固定数值 30，这样就涉及到带参数的修饰器函数。修饰器函数本身会接收三个参数，第一个参数是被修饰的对象，因此为了增加一个新的参数，我们需要对原来的函数进行一层包装，你知道吗？此时我感到非常兴奋，因为这 TM 真的和 Python 一模一样啊。好了，遵从这个策略，我们修改原来的代码，并将其调整如下：\n@setProp(20) class User {} function setProp(value) { return function (target) { target.age = value } } console.log(User.age) 此种差别，大家可以非常明显地看出来，我们在使用修饰器函数 setProp()的时候，现在允许传入一个参数 20，此时的结果是非常地显而易见的，这段代码将如你所愿地输出 20。\n修饰方法 既然修饰器可以修饰类，那么可不可以修饰方法呢？答案自然是可以的。因为当修饰器修饰类的时候，修饰器函数的参数是一个对象，即 target，而当修饰器修饰方法的时候，修饰器函数的参数是一个函数。可函数难道就不是对象吗？.NET 里的委托最终不是同样会生成一个类吗？Python 中不是有函数对象这一概念吗？那么，我们继续看一个例子 ：\nclass User { @readonly getName() { return \u0026#39;Hello World\u0026#39; } } // readonly修饰函数，对方法进行只读操作 function readonly(target, name, descriptor) { descriptor.writable = false return descriptor } let u = new User() // 尝试修改函数，在控制台会报错 u.getName = () =\u0026gt; { return \u0026#39;I will override\u0026#39; } 在这个例子中，我们通过修饰器函数 readonly()对 getName()方法进行修饰，使其变成一个 readonly 的方法。我们提到修饰器函数有三个参数，target 指被修饰的对象，name 指被修饰器对象的名称，descriptor 指被修饰对象的 defineProperty。因为设置 descriptor 的 writable 属性为 false 以后，这个函数就无法被覆盖重写，所以代码中尝试重写该方法时就会报错；同理，如果我们对 descriptor 的 value 属性进行修改，则可以对该函数进行重写。\n总结 相信熟悉 Python 中的朋友，应该会知道在 Python 中内置了大量的装饰器，譬如@property 可以让一个方法像属性一样被调用、@staticmethod 可以让一个方法变成静态方法、@classmethod 可以让一个方法变成类方法等。那么，作为 Python 的追随者，JavaSript 中是否存在相类似的概念呢？答案还是肯定的啊！哈哈。具体大家可以参考这里：ES6 Decorator\nAOP 与修饰器 熟悉我写作风格的朋友，应该可以猜到我接下来要做什么了。的确，作为一个在某些方面有强迫症的人，我一直在不遗余力地向大家推广 AOP，因为我相信 AOP 真的可以帮大家去做很多事情。比如最简单的记录日志，或许在前端项目中大家更习惯用 console.log()来记录日志，甚至是使用 alert()，毕竟这些东西不会在界面上展示出来，所以写一写这些东西好像无可厚非。可当你有了 AOP 以后，为什么还要做如此出力不讨好的事情呢？我写这篇文章的一个重要原因，正是我看到在前端同事的代码中，使用修饰器做了一个简单的 AOP，这非常符合我的品味。具体怎么样去做呢？我们一起来看这段代码：\nclass Bussiness { @log step1() {} @log step2() {} } function log(target,name,decriptor){ var origin = descriptor.value; descriptor.value = function(){ console.log(\u0026#39;Calling function \u0026#34;${name}\u0026#34; with \u0026#39;, argumants); return origin.apply(null, arguments); }; return descriptor; } 我们刚刚提到通过修改 descriptor 的 value 属性可以达到重写方法的目的，那么这里就是利用这种方式对原来的方法进行了修改，在调用原来的方法前调用 console.log()写了一行日志。的确，就是这样一行平淡无奇的代码，将我们从泥潭中解救出来。试想看到一段日志记录和业务流程掺杂的代码，谁会有心情去解读代码背后真实的含义，更不必说将来有一天要去删除这些日志有多么艰难啦。AOP 的基本思想是在代码执行前后插入代码片段，因为根据 JavaScript 中的原型继承，我们可以非常容易地为 Function 类型扩展出 before 和 after 两个函数：\nFunction.prototype.before = function(beforefunc){ var self = this; var outerArgs = Array.prototype.slice.call(arguments,1); return function{ var innerArgs = Array.prototype.slice.call(arguments); beforefunc.apply(this,innerArgs); self.apply(this,outerArgs) }; }; Function.prototype.after = function(afterfunc){ var self = this; var outerArgs = Array.prototype.slice.call(arguments,1); return function{ var innerArgs = Array.prototype.slice.call(arguments); self.apply(this,outerArgs) afterfunc.apply(this,innerArgs); }; }; 想象一下，现在我们在重写 descriptor 的 value 属性的时候，可以同时指定它的 before()方法和 after()方法，所以最初的这段代码可以继续被改写为：\nvar func = function(){ console.log(\u0026#39;Calling function \u0026#34;${name}\u0026#34; with \u0026#39;, argumants); return origin.apply(null, arguments); }; func.before(function(){ console.log(\u0026#39;Start calling function ${name}\u0026#39;); })(); func.after(function(){ console.log(\u0026#39;End calling function ${name}\u0026#39;); })(); 所以，所有让你觉得会增加风险的东西，都是源于你内心的恐惧，因为你不愿意去尝试改变，这是真正的复用，如果 Ctrl + C 和 Ctrl + V 可以被称为复用的话，我觉得每一个人都可以说自己是网红啦！这并不是一个笑话，还有什么比写一个@log 更简单的吗？同样，我们可以使用修饰器去统计代码运行的时间，而不是在所有地方用两个 Date()对象去相减。遵从简洁，从心开始：\nfunction time(){ return function log(target,name,decriptor){ var origin = descriptor.value; descriptor.value = function(){ let beginTime = new Date(); let result = origin.apply(null, arguments); let endTime = new Date(); let time = endTime.getTime() - beginTime.getTime(); console.log(\u0026#34;Calling function \u0026#39;${name}\u0026#39; used \u0026#39;${time}\u0026#39; ms\u0026#34;); return result; }; return descriptor; }; } @time foo() 再比如，我们的业务中要求：用户在访问相关资源或者是执行相关操作时，需要确保用户的状态是登录着的，因此，我们不可避免地在代码中，使用 if 语句去判断用户是否登录，试想如果所有的业务代码都这样写，两个模块间就存在了直接耦合，当然我们可以说这是最简单的做法，因为它照顾了大部分人的思维和情绪，可你看 Angular/Redux/TypeScript 等项目中无一不遍布着修饰器的身影，当一种框架逐渐流行并成为一种趋势的时候，好像大家立刻就忘记了一件事情：原本我们都是非常排斥这些奇技淫巧的，可因为框架的流行你就默认接受了这种设定。那么，这个逻辑如何使用修饰器来编写会怎么样呢？\nclass User { @checkLogin getUserInfo() { console.log(\u0026#39;获取已登录用户的用户信息\u0026#39;) } @checkLogin sendMsg() { console.log(\u0026#39;发送消息\u0026#39;) } } // 检查用户是否登录，如果没有登录，就跳转到登录页面 function checkLogin(target, name, descriptor) { let method = descriptor.value descriptor.value = function (...args) { //假想的校验方法，假设这里可以获取到用户名/密码 if (validate(args)) { method.apply(this, args) } else { console.log(\u0026#39;没有登录，即将跳转到登录页面...\u0026#39;) } } } let u = new User() u.getUserInfo() u.sendMsg() 显然，现在我们可以避免模块间的直接耦合，无需在每个业务方法中重复去写 if 语句，更重要的是通过 JavaScript 中的模块化规范，我们可以把 checkLogin 这个方法，扩展到更多的业务类及其方法中去，而唯一的代价就是在方法上增加@checkLogin 修饰，你说，有这样优雅的策略，你为什么就不愿意去使用呢？在 ASP.NET 中我们通过 Authorize 特性就可以为 API 和页面授权，现在看来这是不是有点异曲同工之妙呢？你现在还觉得这样麻烦吗？\n本文小结 这篇文章从一个前端项目中的日志拦截器(InterceptLog)为引子，引出了 ES7 提案中的一个特性：修饰器。修饰器的出现，解决了两个问题：第一、不同类间共享方法；第二、在编译时期间对类及其方法进行修改。虽然目前修饰器不能直接在浏览器中使用，可是通过 Babel 这样的转译工具，我们已经可以在项目中提前感受这一特性，这里表扬下前端组的同事们。JavaScript 中的修饰器同 Python 中的修饰器类似，可以修饰类及其方法。JavaScript 中的修饰器不建议修饰函数，因为存在一个函数提升的问题，如果一定要修饰函数，按照高阶函数的概念直接包装函数即可。通过修饰器可以简化我们的代码，在本文中我们例举了日志记录、运行时间记录、登录检查三个 AOP 相关的实例，希望大家可以从这篇文章中有所收获。\n最后，请允许博主爆一个料，因为要写一个简单的修饰器，需要安装若干 Babel 甚至是 Webpack 插件，我这篇文章中的代码，截止到写这篇文章时都没能在实际环境中运行，这不能怪我啊，因为前端的工具链实在是太长太多啦，这当然不能和直接内置装饰器的 Python 相比啊，这真的不是吐槽诶，我需要一个开箱即用的特性就这么难吗？人生苦短，我用 Python！(逃\n参考文章 读懂 ES7 中 JavaScript 修饰器 ES7 Decorator 入门解析 ES7 Decorator 装饰者模式 ECMAScript 6 入门 ","date":"2018-04-15T21:20:03Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3668933172/","slug":"3668933172","tags":["AOP","ES6","JS"],"title":"邂逅 AOP：说说 JavaScript 中的修饰器"},{"categories":["生活感悟"],"content":"\r上周看了部印度电影《小萝莉的猴神大叔》，以一言敝之，这是一部被名字耽误的好电影，就像我们所熟知的《三傻大闹宝莱坞》、《偶滴个神呐》、《外星醉汉 PK 地球神》等等电影一样。不过作为一部由“印度三汗”之一萨尔曼·汗主演的电影，可能因为其在国内的知名度不及阿米尔·汗，所以早在这部 2015 年就上映的电影，并未在国内产生太显著的影响力。相反，同档电影《环太平洋 2》票房热度居高不下，大概是因为景甜姐姐终于不负国人期望去拯救全人类啦。即使当时电影院里看这部电影的人寥寥无几，可我觉得还是有必要给大家说一说这部电影，一个即使你能猜对所有情节依然会喜欢的电影。\n小萝莉与猴神大叔\r影片一开始，讲述的是来自巴基斯坦穆斯林家庭的小女孩沙希达的故事。沙希达到 6 岁还不会说话，她的父母对此感到焦虑不已。祖父建议带她到神庙里去向神祈祷，可这座神庙在印度境内，并且两国间互相仇视达半个世纪之久。因为沙希达的父亲曾在巴基斯坦军方服过 5 年兵役，因此他没有办法申请前往印度的签证。无奈之下，沙希达的母亲独自带领女儿前往印度，不想在返回巴基斯坦的途中，沙希达在列车停靠期间，为下车救一只小羊羔而和母亲走散，并被一节载满粮食的火车带到印度。沙希达在那里遇到了“猴神”帕万，面对印度警方的无动于衷，大使馆因为印巴冲突而关门，黑中介拐卖幼童等等一系列的变故，帕万不得不走上亲自送沙希达回家的旅程……\n沙希达与小羊羔\r电影的主线是非常清晰的，不同的是，整个电影被放在一个充斥着宗教矛盾、种姓歧视、印巴冲突的大背景里，所以导演试图去表达的内涵，就不再单纯地是为了讲这样一个故事，一如“三傻”抨击印度教育制度、OMG/偶滴个神/PK 探讨宗教和神一样，这部电影里有许多值得去探讨的东西。电影两条相互交叉的线索构成，男主帕夫是一个虔诚的宗教信徒，他信奉的是印度猴神哈奴曼，并努力将宗教的教义落实到言行中去，甚至在生活中显得憨厚而木讷，导演在刻画这个人物时明显夸大了这一点，印象最深的是男主考高中就考了 10 次，最终父亲居然因为这个“惊喜”而去世。男主寄身在父亲身前的好友家中，并邂逅了一份“坎坷”的爱情。男主的确是一个普世价值中的失败者，没有稳定的工作，岳父要求他在六个月内买一套婚房，甚至从被黑中介欺骗这里可以看出，他并不是一个社会经验丰富的人。善良的男主在猴神节上给沙希达买了薄饼和饮料，沙希达就认定他是一个值得信赖的人，两个人的故事就此展开。\n小孩子都懂得说谎\r帕夫最初希望向印度警方寻求帮助，可警方认为警察局并不是孤儿院，拒绝为沙希达提供相关帮助。无奈之下，帕夫将小女孩带回岳父家中，起初人们见到这个可爱的小女孩，一度认为这是这个小女孩来自印度的某个高贵种姓。种姓制度是一种以血统论为基础的等级制度，广泛存在于印度社会运作与生活中，虽然印度早在 1947 年就脱离了英国殖民者的统治，可这种在殖民时期被固定和僵化的制度，在今天依旧影响深远。无独有偶，印巴冲突正是英国殖民统治者将印度划分为两个地区统治的结果，电影中印度的印地语和巴基斯坦的乌尔都语，其实是非常相似的两门语言。曾经有两个自命不凡的民族，一个是德意志雅丽安族，一个是扶桑国大和民族，一起发动第二次世界大战，为了所谓的种族优越性，大肆迫害犹太人和中国人，一个 6 岁的孩子因为可爱就被迫贴上这种种族的标签，人类繁衍至今，这种病态的虚荣心不觉得可悲吗？\n有什么区别？\r如果说这种自带歧视的贴标签行为，仅仅是人类的一厢情愿的话，那么接下来沙希达所到的一切事情更像是一种本能。男主的岳父家是一个典型的印度佛教徒家庭，沙希达因为受到肉食的诱惑而到领居家吃鸡肉，当她遇到清真寺就会像母亲一样裹上头巾上前参拜……男主起初不愿意或者说不敢去清真寺内，或许是因为两种截然不同的宗教信仰，让他习惯性地去排斥一种新的文化。这何尝不是我们呢？一旦长大三观就特别难更改，面对三观不合这样的问题，大家都出奇地相信分手就能解决问题。可人类不曾见过一颗恒星的诞生和消亡，可我们就固执地相信这短短几十年里的所见所闻。其实我们大可不必去接受什么，就像这世间有千万种书千万句话，渐渐地丰富了我们原本枯竭的生命。男主纠结于该不该到清真寺里招人，他的女朋友则告诉他，沙希达只是一个 6 岁的孩子，宗教对她来说有什么意义呢？是啊，我们简简单单地来到这个世界上，等离开时突然发现平添了无数个毫无意义的身份，我承认，沙希达从背后抱住男主的时候，我一个大男人心里像是被温暖到了。\n沙希达像母亲一样参拜\r真正揭开小女孩身份的是一场球赛，一场特殊的球赛，一场发生在印度和巴基斯坦间的球赛。这里有一个细节，小女孩的母亲怀孕期间就是在看一场板球比赛。在这场比赛中，印度输掉了比赛，当所有人都一脸失落时，唯独沙希达高兴地手舞足蹈，甚至冲到电视机面前，亲吻巴基斯坦国旗。男主小心翼翼地问小女孩，“巴基斯坦？”。小女孩的确认让男主悲喜交加，喜的是终于知道小女孩的身世，悲的是两国积怨久矣为岳父所不容。事实上两国曾因为流民而引发流血事件，电影中男主岳父仇视巴基斯坦人的原因恰在于此。这让我想到，阿米尔·汗主演的电影 PK 中，女主嘉谷爱上了一个穆斯林男孩，父母对穆斯林的偏见以及宗教对这段感情的干涉，差点让两个真心相爱的人分开。男主再次遭遇无奈，他前往领事馆希望寻求领事馆的帮助，领事馆以沙希达没有护照为由，拒绝为男主提供帮助，这里导演让我们了解了两国间的冲突到底有多严重，一场动乱导致领事馆暂停营业一个月。男主通过岳父介绍，找到一家旅游中介公司，对方声称需要 12 万卢比(约合人民币 10000 多元)，男主的女朋友甚至拿出了准备买房子的钱，可黑心中介转手就把沙希达卖到了妓院，一个 6 岁的小女孩啊，妓院是什么地方？\n安全感十足的大叔\r电影开始介绍男主时说他学习不好，更不擅长体育运动(摔跤)。妓院里男主一个房间一个房间的找小女孩，当找到小女孩的时候，这个电影突然燃了起来，虽然按照角色设定，男主不应该有这样的身手，可事实上这名印度演员萨尔曼·汗，就是以健身达人的称号闻名于世的，据说他参与演出的电影都会安排裸露上身的戏份，这一次我们就当做主角光环好啦，甚至当看到黑中介在电线上荡秋千时，小女孩那纯真的笑脸，我能想到的只有一句话，“愿你被世界温柔对待”，这样一个可爱的小女孩，我愿意她一直都长不大，因为这个世界并没有那么好，幸运的是，她遇到了善良的猴神大叔。我很想知道，为什么人长大以后，反而更加不惮于勇险恶的用心去猜度别人，这到底是一种成熟，还是一种倒退。你大概不会想到，一个印度人，为了帮助一个萍水相逢的巴基斯坦人，在没有签证的情况下，冒着被当做间谍的风险，偷偷穿过两国边界的防线，只为了送这个小女孩回家，你说他不为博眼球出名，不为做好事谋利，他到底是为了什么？原谅我说句俗气的话，这就是爱啊！\n公交车司机的神助攻\r在穿越巴基斯坦边防线的时候，男主坚持要得到边防战士的批准后再入境，虽然这是一个讲述诚实和信念的励志故事，可我还是想说，遵从某种信念，并不是教条地照本宣科，而是在道德和法律允许的条件下适当变通。为什么要强调道德和法律呢？因为万物皆虚，万事皆座啊，《刺客信条》里刺客的信仰是每一代刺客都在追求的东西，以阿泰尔为例，他年轻时因为所罗门神殿任务的失败，而被降级为新手刺客，在完成刺杀 9 个圣殿骑士的任务中，他开始变得冷静而沉着，“万物皆虚，万事皆允”的理念开始变得越来越清晰。帕夫常常说他向哈奴曼神保证永远不偷偷摸摸，可他先后躲在清真寺里、车顶逃避警察搜捕，反而是那位伊斯兰教阿訇令人印象深刻，男主说自己不能进清真寺，阿訇说清真寺从来不锁门，所以任何人都可以进去。阿訇送男主一行三人离开后，以伊斯兰教礼仪祝福男主，并询问在印度教中如何表达祝福，随后以印度教礼仪向男主行礼，并煞有介事地问，“哈奴曼神在这里还管用？”。是啊，两个信仰不同的国家，素来相互仇视，可到了人家的地方，居然还要祈求本国的神灵护佑，这是不是有点讽刺呢？其实，宗教并不是要教会我们去排斥什么，而是要学会拥有一颗包容的心，只要一件事情的出发点是善良的，我相信这不会违背任何宗教的教义，佛家讲因果，道家讲无为，儒家讲修身，本质上都是劝人向善，大概爱是人类共同的语言，同样是神共同的语言。\n阿訇以印度教礼仪祝福众人\r一旦明白了这一点，你就会理解男主接下来所做的事情，比如在电话里向警察撒谎以躲避警察追捕、身为一个印度教徒参拜伊斯兰教神殿、在身上蒙上黑色衣袍伪装成伊斯兰教女子、以伊斯兰教礼仪向帮助他的人表示感谢……所谓“大道至简“，我希望我们相互去借鉴和学习不同领域里的东西，而不是相互对立和排斥彼此。《笑傲江湖》里的五岳剑派，一心想成为能和少林武当抗衡的江湖势力，其实像左冷禅这样的“宗师”级人物，如果可以以华山石壁上魔教长老破除各派剑法的图形为基础，集结各派所长，创造一门新的武功并不是不可能，奈何江湖人物一样逃不过权势的诱惑，想要扫除魔教势力，先破武当，再灭少林，再加上狭隘的门户之见，五岳剑派的辉煌不过是转眼云烟。想想《碧血剑》里大反派玉真子，一人一剑就敢上华山挑战，华山派的衰落可见一斑。如今全世界都被互联网连接在一起，文化间的相互影响越来越深，宗教如是，文化如是，不要以狭隘的民族观去审视我们不懂的文化，就如同不要觉得朋友圈里你不懂的东西 Low 一样，心态要开放，眼光要长远。\n突然觉得生活美好起来\r这部电影有太多的意料之中，同样有太多的意料之外。我知道小女孩会和母亲重逢，可我不知道真正打动我的是那一声叔叔；我知道男主会被警方逮捕，可我不知道巴方会想到屈打成招，理由仅仅是因为对方是一个印度人；我知道小女孩并不是哑巴，可我不知道原来说出一句话需要莫大的勇气……这里感谢那个电视台记者，这是一个媒体人的自我修养；感谢那个正直的警官，他没有给他的国家丢脸；感谢那些冲开边防线的巴基斯坦人民，有一种力量叫做民心所向。我希望两个国家可以像电影一样美好，不再有冲突，不再有流血，即使我知道现实中的困难远比电影中的多……可这的确是一群成年人的童话，就像电影最终定格在沙希达和她的猴神大叔拥抱在一起一样，有些美好值得我们去期待，有些东西值得我们去追寻，它们或许是爱，或许是正义，或许是善良，或许是信念，或许是和平，或许是勇敢……我猜中了故事的结尾，依然为这个故事而热泪盈眶，这一切并非是因为我矫情，而是因为我简单，简单到有些东西不必理解得那么深刻，就像我单纯地喜欢着你却不知道为什么一样。好了，这篇影评终于写完啦，晚安！^_^\n永恒的瞬间\r","date":"2018-04-03T09:08:04Z","image":"/posts/2613006280/P2561540696.webp","permalink":"https://qinyuanpei.github.io/posts/2613006280/","slug":"2613006280","tags":["影评","印度","电影"],"title":"一念执着，千山无阻"},{"categories":["编程语言"],"content":"最近随项目组对整个项目进行联调，在联调过程中暴露出各种问题，让我不得不开始反思，怎么样更好地去做好一件事情，譬如说在开发过程中如何保证 Web 服务的稳定性，在敏捷开发中如何降低文档维护的成本，以及如何提高多环境服务部署的效率等等。我为什么会考虑这些问题呢？通常我们都是在约定好接口后并行开发的，因此在全部接口完成以前，所有的服务都是以渐进的形式进行集成的，那么如何保证服务在集成过程中的稳定性呢？尤其当我们面对开发/测试/生产三套环境时，如何提高服务部署的效率呢？当接口发生变更的时候，如何让每一个人都知悉变化的细节，同时降低人员维护文档的成本呢？这些问题或许和你我无关，甚至这不是一个技术问题，可恰恰这是我们时常忽视的问题，我是我想要写这篇文章的一个重要原因。\n代码越来越复杂 面对这种问题，尤其是当你发现，它并不是一个纯粹的技术问题的时候。选择一件你喜欢的事情的去做，固然可以令你开心；而选择一件你不喜欢的事情去做，则可以令你成长。我们每一个人都不是人类学家，可生命中 80%的时间都在研究人类。当你接收到一条别人的讯息时，不管这个讯息本身或对或错，在生而为人的角色预设中，你都必须去提供一个响应，甚至是比对方期望更高的一个响应。可是服务器会返回 403、404 或者 500 甚至更多的状态码，人生有时候并没有机会去选择 Plan B 或者 Plan C。所以，即使所面临境地再艰难，能不能勇敢地再去尝试一次，说服对方或者选择妥协，就像一段代码被修改得面目全非，可人类本来就是喜欢皆大欢喜的动物，总希望别人都认认真真，而自己则马马虎虎，因为“认真你就输了”，有谁喜欢输呢？\n好了，现在假设我们有这样一个业务场景，我们需要调用一个 WebAPI 来获取数据，然后对这些数据做相关处理。这个 API 接口被设计为返回 JSON 数据，因此，这个“简单”的业务场景通过以下代码来实现：\ndef extract(url): text = requests.get(url).content.decode(\u0026#39;utf-8\u0026#39;) json_data = json.loads(text) data = json_data[\u0026#39;raw_data\u0026#39;] return data 这个代码非常简单吧！可是过了十天半个月，每次解析 JSON 数据的时候随机出现异常，经验丰富的同事建议增加 try\u0026hellip;except，并在捕获到异常以后返回 None。于是，extract()方法被修改为：\ndef extract(url): text = requests.get(url).content.decode(\u0026#39;utf-8\u0026#39;) try: json_data = json.loads(text) data = json_data[\u0026#39;raw_data\u0026#39;] return data except Exception: print(\u0026#34;JSON数据无效，重试！\u0026#34;) return None 修改后的代码，果然比修改前稳定啦，可是负责后续流程的同事开始抱怨，现在代码中出现大量判断返回值是否为 None 的代码片段，甚至在 Web API 返回正确结果的情况下，依然会返回 None，为此，机智的同事再次修改代码如下：\ndef extract(url): text = requests.get(url).content.decode(\u0026#39;utf-8\u0026#39;) try: json_data = json.loads(text) data = json_data[\u0026#39;raw_data\u0026#39;] return data except Exception: print(\u0026#34;JSON数据无效，重试！\u0026#34;) return extract(url) 可以预见的是，使用递归可能会导致递归的深度问题，假如调用者传入一个错误的 URL，将导致这里无限递归下去，于是考虑限制重试的次数；增加重试次数的限制以后，发现每次重试需要有一个时间间隔……更不必说要在这里增加日志记录，以及在特定场景下需要将异常抛出，由此可见这段简单的代码会变得越来越复杂，如下所示：\ndef extract(url): text = requests.get(url).content.decode(\u0026#39;utf-8\u0026#39;) try: json_data = json.loads(text) data = json_data[\u0026#39;raw_data\u0026#39;] return data except Exception: for i in range(3): print(\u0026#39;正在进行第{0}次重试\u0026#39;.format(str(i)) result = extract(url) if(result!=None): return result 可以注意到，这是一个非常合理的代码演进过程。在这个演进过程中，一段非常简单的代码变得越来越复杂。在我写下这篇文章前，我亲眼目睹了这种复杂的代码，是如何难以复用以及集成的，日志记录、异常处理等流程和正常流程“混合”在一起，甚至你不得不通过函数的返回值来判断是否异常，我一直在想怎么样去解决这些“corner”问题，就像人们一致认为：王垠博士擅长解决的是理想状态下的纯问题。而现实世界中存在着各种各样的“corner”问题，这或许就是学术界与工业界的区别，那么怎么样去更好地解决这一切问题呢？\n应用程序重试策略 既然我们可以预见到这些问题的存在，那么，现在让我们正式切入今天这篇博客的主题，即应用程序重试策略。我们在这里以一种渐进式的方式，向大家展示了一个简单的应用程序，是如何因为异常处理变得越来越复杂的，这里我们选择重试，是因为现实世界本身存在不稳定性，即使我们现在有各种自动化工具来替代传统运维。就像有时候你怀疑是代码出现 Bug，实际上则是服务器在某一段时间内宕机，当这种事情就发生在你身边的时候，你不得不去着手解决这些“corner”问题，而这恰好是人生的无奈之处。\nTry-Catch-Redo 策略 这应该是我们最容易想到的一种重试策略了，其思路是对函数的返回值和异常进行处理，其缺点是无法解决重试无效的问题。这里我们将问题简化为对异常进行处理，其基本的代码实现如下：\nprivate void Retry() { try { var result = DoWork(); if(!result){ //重试一次 Thread.Sleep(1000); DoWork(); } } catch(Exception e) { //重试一次 Thread.Sleep(1000); DoWork(); } } 可以注意到，这种策略最多可以重试一次，因此如果重试后无效，这个策略就变得毫无意义起来，我们需要寻找一种更好的方式。\nTry-Catch-Redo-Retry 策略 考虑到第一种策略无法解决重试无效的问题，我们在此基础上增加对重试次数以及重试间隔的控制，这就是 Try-Catch-Redo-Retry 策略，其基本实现如下：\nprivate void Retry() { //最大重试次数为5次 int times = 5; //重试间隔为10秒 int interval = 10; //存储异常的列表 var exceptions = new List\u0026lt;Exception\u0026gt;(); while(true) { try { var result = DoWrok(); if(result) return result; } catch(Exception ex) { if(--times\u0026lt;=0) throw new AggregateException(exceptions); exceptions.Add(ex); Thread.Sleep(TimeSpan.FromSeconds(interval)); } } } 可以注意到，通过 while(true)结构的确可以增加重试的次数。问题在于：如果不设置合理的循环跳出条件，就有可能造成逻辑上的死循环。尤其当循环体内的逻辑执行时间较长时，会增加用户的等待时间，这看起来亦非良策啊！\nRetry-Builder 策略 Try-Catch-Redo 和 Try-Catch-Redo-Retry 这两种策略理解起来非常容易，可这两种策略都有一个致命的缺陷，即正常逻辑和重试逻辑重度耦合。我们希望采用一种更优雅的方法，以一种非侵入式的方式给正常逻辑增加重试重试逻辑。需要考虑的是，在确保重试次数和重试间隔可配置的前提下，支持自定义重试源，即可以捕捉一个或多个异常以及对返回值进行处理。在这里推荐三个框架，分别是 Java 中的Spring-Retry和Guava-Retrying、.NET 中的Polly。其中 Spring-Retry 是基于 Throwable 类型的重试机制，即针对可捕获异常执行重试策略，并提供相应的回滚策略；而 Guava-Retrying 提供了更为丰富的重试源定义，譬如多个异常或者多个返回值；而 Polly 则提供了除重试以外的断路器、超时、隔板隔离、缓存、回退等多种策略。这三者的相似之处在于，通过一个 Factory 来创建满足不同重试策略的 Retryer，然后由 Retryer 来通过回调来执行重试逻辑，我不喜欢 Java 中回调函数写法，所以这里以 Polly 为例：\ntry { var retryTwoTimesPolicy = Policy .Handle\u0026lt;DivideByZeroException\u0026gt;() .Retry(3, (ex, count) =\u0026gt; { Console.WriteLine(\u0026#34;执行失败! 重试次数 {0}\u0026#34;, count); Console.WriteLine(\u0026#34;异常来自 {0}\u0026#34;, ex.GetType().Name); }); retryTwoTimesPolicy.Execute(() =\u0026gt; { var a = 0; var b = 1/a; }); } catch (DivideByZeroException e) { Console.WriteLine($\u0026#34;Excuted Failed,Message: ({e.Message})\u0026#34;); } 可以看到，写出一段语义化的代码是多么的重要，因为我相信大家都看懂了。这里的 Policy 承担了 RetryBuilder 的角色，它定义了这样一种策略：当程序引发 DivideByZeroException 时进行重试，重试次数为 3 次，并且以匿名函数的方式指定了重试时的回调函数；而创建的 retryTowTimesPolicy 承担了 Retryer 的角色，它通过 Execute()方法来定义要执行的重试逻辑。当 3 次都重试失败时就会引发 DivideByZeroException 并在最外层函数中被捕捉到。我经常听到有人说设计模式没有用，我想说因为你从来都不知道什么叫做大道至简，引入无数个中间层是无法让你直接看到代码定义，可计算机领域里有一句名言，“任何一个问题都可以通过引入一个中间层来得到解决”。\n装饰器/AOP 策略 我从来不惮于将各种重复的工作自动化，这并不是我喜欢在别人面前炫技，而是因为在现实生活中我是一个懒惰的人，甚至是每天早上 10 点开站会这样的事情，我都愿意让计算机程序去提前通知我做好准备。我并不是一个不懂得自律的人，仅仅是因为我觉得我们可以用这个时间去做些别的事情。AOP 是一种可以在运行时期间动态修改代码的技术，我们自然可以想到给所有的函数都加上异常处理和重试的特性，幸运的是 Python 中的有这样一个第三方库：Tenacity，它可以帮助我们优雅地实现重试：\nfrom tenacity import retry from json.decoder import JSONDecodeError @retry(retry=retry_if_exception_type(JSONDecodeError), wait=wait_fixed(5), stop=stop_after_attempt(3)) def extract(url): text = requests.get(url).content.decode(\u0026#39;utf-8\u0026#39;) json_data = json.loads(text) data = json_data[\u0026#39;raw_data\u0026#39;] return data 通过@retry 这个装饰器函数，我们就可以知道，这里设计的重试策略是：当引发 JSONDecodeError 这个异常时，每隔 5 秒中重试一次，最大重试次数为 3 次。Python 中的装饰器，本质上就是高阶函数的概念，修饰器函数对被修饰函数进行“操作”后返回一个新的函数，这个特性在.NET 中可以通过委托/匿名方法/lambda 来实现，结合 Unity、AspectCore 等 AOP 框架，相信大家完全可以将这个特性移植到.NET 中来，当语言的差别变得微乎其微的时候，原理的重要性不言而喻。\n重试策略核心理念 好了，截止到目前，我们分析了四种不同的重试策略，并且这四种重试策略是随着我们认知的加深而逐渐递进的。那么，通过这四种不同的重试策略，我们能否梳理出一个相对完整的应用程序重试策略呢？换言之，当为应用程序增加重试相关的功能时，我们都需要考虑哪些因素，因为使用这些框架会是非常简单的过程，而更重要的则是我们逐步演进的思考过程。当我们所依赖的是一个不稳定的场景，譬如远程调用、数据加载、数据上传等场景时，或者是在异常场景中需要重试以满足业务稳定性的要求等等，就可以考虑使用重试策略。这里简单地做一下梳理：\n重试逻辑与正常逻辑解耦，整个设计是非侵入式的。 支持自定义策略，譬如重试次数、重试间隔、重试源、重试超时时间等。 支持自定义断言，即可以使用 Predict或者类似表达式来定义返回值满足的条件。 支持多种异常，即可以针对特定的 Exception 或者自定义的 Exception 进行拦截。 断言实例和异常实例，作为正常逻辑和重试逻辑两者间的媒介进行交互。 通过命令模式，由 Retryer 对象完成对正常逻辑的调用，同时在内部封装重试逻辑。 一个简单的 Retry 实现 好了，熟悉我写作风格的朋友，一定知道我不喜欢空泛地讲一套理论，我更喜欢通过“造轮子”的这种方式，以加深对一个事物或者原理的认识。对于今天这篇文章，我的初衷是想告诉大家如何优雅地去实现 Retry，因为在现实中我们总会遇到各种各样的枷锁，这些枷锁约束着你写出糟糕的代码，我们比别人用心甚至更努力，反而常常被认为是有代码洁癖或者是炫技，可不管怎么样，人生是我们自己的，如果没有办法说服别人在项目中使用这些技术，那我们就在项目以外的地方去使用，或者是告诉别人我们有一种相对优雅的设计，如果这个设计恰好对别人有用，对我们来说就是一种莫大的幸福。参考 Polly 的 API 风格，这个 Retry 被我设计成了下面的样子：\ntry { var result = Retry.Default .Times(3) .Interval(2) .Catch\u0026lt;DivideByZeroException\u0026gt;() .Reject((count,ex)=\u0026gt; { var message = string.format(\u0026#34;第{0}次重试，异常来自:{1}\u0026#34;, count, ex.Message); Trace.WriteLine(message); }) .Execute\u0026lt;int\u0026gt;(()=\u0026gt; { var m = 0; return 3 / m; }); } catch(Exeption ex) { Trace.WriteLine(ex.Message); } 我承认它和 Polly 非常地像，不过我并没有去看 Polly 是如何实现的，目前它的实现完全来自这篇文章中我们提到的这些策略。我在为它增加了针对返回值的断言支持，通过 Return 方法来实现，而对异常的支持则是通过 Catch 方法来实现，除此以外，它支持异步方法调用，我们熟悉的 Task/async/await 这些 API 都可以使用。目前它还是一个玩具儿，因为我发现最难的部分，其实是断言或者说自定义表达式的设计，对于线程安全相关的问题，我会在慢慢地去完善它，如果你对它感兴趣的话，可以通过这里访问：RetryIt。好了，感谢大家关注我的博客，今天这篇先写到这里啦，欢迎大家在博客中留言！\n","date":"2018-03-31T19:20:54Z","image":"/posts/115524443/cover.png","permalink":"https://qinyuanpei.github.io/posts/115524443/","slug":"115524443","tags":["异常","重试","想法"],"title":"漫谈应用程序重试策略及其实现"},{"categories":["编程语言"],"content":"最近公司安排学习项目代码，前后花了一周左右的时间，基本熟悉了项目中的各个模块，感觉项目难度上整体偏中等。这是一个具备完整前端和后端流程的项目，在学习这个项目的过程中，我逐渐发现某些非常有趣的东西，比如在 Web API 的设计中采用严谨而完善的错误码、使用 OAuth 和 JWT 对 API 资源进行访问控制，在 JavaScript 中使用修饰器特性来实现日志记录等等，这些东西我会在后续的博客逐步去整理，今天想说的是如何通过 Unity 框架来简化应用程序异常处理和日志记录流程，而之所以关注这个问题，是因为我发现项目中接近滥用的异常处理，以及我不能忍受的大量重复代码。\n背景描述 由于业务场景上的需要，我们在产品中集成了大量第三方硬件厂商的 SDK，这些 SDK 主要都是由 C/C++编写的动态链接库，因此在使用这些 SDK 的过程中，通常频繁地使用返回值来判断一个方法是否成功被调用，虽然项目上制定了严格的错误码规范，可当我看到大量的 Log()方法和业务逻辑混合在一起时，我内心依然是表示拒绝的，甚至我看到在捕获异常以后记录日志然后继续 throw 异常，这都是些什么鬼操作啊，考虑到我的语言描述得可能不太准确，大家可以从下面两段代码来感受下整体画风：\npublic short LoginTerminal(string uid,string pwd) { try { Log.BeginLog() return SDK.Login(uid,pwd) } catch(Exception ex) { log.LogError(ErrorCode.E2301,ex) throw new TerminalException(ex.Message); } finally { Log.EndLog() } } 这是一段相对完整的业务逻辑代码，当然这里都是伪代码实现，这里我比较反感的两个地方是：第一，从头出现到尾的 BeginLog()/EndLog()这对方法；第二，在 Catch 块中记录完日志然后将异常再次抛出。经过我对项目的一番了解，BeginLog()/EndLog()这对方法会在日志中记录某个方法开始执行和结束执行的位置。在方法执行前后插入代码片段，这不就是面向切面编程(AOP)的思想吗？这里记录完日志然后再抛出异常的做法，我个人是不大认同的，因为我觉得拦截异常应该有一个统一的入口，因为异常会继续向上传递，既然如此，为什么我们不能统一地去处理异常和记录日志呢？难道就一定要让 Log 这个静态类无处不在吗？同样地，我们注意到项目还会有下面这样的代码：\npublic void ProcessTerminal(object sender,ProcessEventArgs args) { try { Log.BeginLog(); var terminal = (Termainal)sender; var result = terminal.Process(args); } finally { Log.EndLog(); } } 这种代码看起来不再关注异常，可和第一段一样，从头出现到尾的 BeginLog()/EndLog()简直不能忍，而且这里的 try\u0026hellip;finally 结构难免让人想起 using 的语法糖，那么这样是不是可以考虑让这个 Log 拥有类似的结构，换言之，我们总不能一直都在每一个方法里，重复写 BeginLog()/EndLog()这两个方法吧，既然 EndLog()方法总是在 finally 块里被执行，那为什么不考虑把它放到 Dispose()方法里(前提是有一个结构实现 IDispose 接口)。你问我是不是有代码洁癖啊？我真的没有，我就是懒，不喜欢重复做一件事情。所谓\u0026quot;管中窥豹，可见一斑\u0026quot;，大家可以想象整个项目会是什么样子。\n好了，为了避免让自己写这种糟糕的代码，我决心使用 Unity 框架来简化下这里的异常处理和日志记录流程，一个有追求的程序，如果可以交给自动化工具去做的事情，为什么要一次又一次地重复去写呢？我们可以吐槽一段代码写得有多糟糕，可我们所做的任何努力，都是为了让自己不变成这个样子。Unity 框架提供的 AOP，即面向切面编程，不就可以做这样的事情吗？所以，能动手的就直接动手，君子有所为有所不为，不要重复自己，\nUnity 框架与 AOP 好啦，交待完故事背景，今天的主角终于可以登场啦！经常关注我博客的朋友，一定知道我个人比较喜欢 IoC/AOP 这类所谓的\u0026quot;奇技淫巧\u0026quot;，就在今天我还在和一位同事在讨论 Ioc，这位同事认为 Ioc 增加了代码的复杂性，不认为 Ioc 会为项目带来明显的便利性。其实我相信大道至简，任何框架对我们而言都是高度抽象的，可正是因为有了这些抽象的层次，我们渐渐学会了关注核心的东西。这里提到了 Ioc，即控制反转，或者我们可以称之为依赖注入，那么 Unity 框架就是.NET 下众多依赖注入框架之一，这里称之为 Unity 框架，主要是避免和跨平台游戏引擎 Unity 产生混淆，以下全部称之为 Unity 框架。Unity 框架中提供了核心的依赖注入相关的接口，而微软的企业最佳实践库中为 Unity 扩展出了 AOP 相关的功能。除此以外，Spring.NET、Aspect.Core、AspectF 等都是.NET 下的 AOP 方案。那么在今天的故事中，我们遇到了的一个场景是在指定方法执行前、后插入代码片段，这是面向切面编程(AOP)的基本思想，为此，我们考虑使用 Unity 框架来简化应用程序中异常处理及日志记录流程。\nUnity 中的三种拦截器 Unity 中提供了三种典型的拦截器，为了选择一种合适的拦截器来实现我们的功能，我们首先来了解下这三种不同的拦截器各自的应用场景：\nTransparentProxyInterceptor：即透明代理拦截器，基于.NET Remoting 技术实现代理，它可以拦截对象的所有函数，缺点是被拦截对象必须继承自 MarshalByRefObject。 InterfaceInterceptor：顾名思义，即接口拦截器，仅拦截指定接口，显然只要目标类型实现了指定接口就可以拦截。C#不支持多继承，选择这种方式对代码的影响最小。 VirtualMethodInterceptor：顾名思义，即虚方法拦截器，仅拦截虚方法，这个对目标类型的要求就非常高啦，一般我们不会考虑这种方式。 对 Unity 框架而言，不管我们使用哪一种拦截器，我们都需要通过 UnityContainer 这个容器来为目标类型注入拦截器，这样 Unity 框架会帮助我们生成代理对象，我们只要在使用代理对象的时候，这些拦截器才会真正工作。博主曾经以为定义好下面这些 Handler 就可以了，简直是图样图森破。好了，一个基本的代码流程如下，请不要问我配置文件怎么配，我真的不喜欢配置文件，搞得跟某配置狂魔语言似的，反正这些配置文件这次记住了下次还是会忘的，可下面这几行代码是不会轻易忘记的啊：\nvar container = new UnityContainer().AddNewExtension\u0026lt;Interception\u0026gt;().RegisterType\u0026lt;IBussiness, Bussiness\u0026gt;(); container.Configure\u0026lt;Interception\u0026gt;().SetInterceptorFor\u0026lt;IBussiness\u0026gt;(new InterfaceInterceptor()); var bussiness = container.Resolve\u0026lt;IBussiness\u0026gt;(); 注意，这里不要直接从 Github 或者 Nuget 上下载 Unity 框架，因为最新版的 Unity 我实在是不会用啊！:joy: 我喜欢开箱即用的产品，我愿意钻研啊，可 DeadLine 永远会有终点！ 我们需要从微软企业最佳实践库中下载以下动态链接库：\nCommonServiceLocator.dll Microsoft.Practices.Unity.Configuration.dll Microsoft.Practices.Unity.dll Microsoft.Practices.Unity.Interception.Configuration.dll Microsoft.Practices.Unity.Interception.dll 考虑到我们这里需要实现两种功能，针对异常的异常处理流程，以及正常的日志记录流程，为此我们将实现 ExceptionHandler 和 LogHandler 两个组件。下面我们来一起了解这两个组件的实现过程，这里博主选择了最简单的 ICallHandler 接口，而非更一般的 IInterceptionBehavior 接口，主要希望让这个过程更简单些，同时实现在方法粒度上的可控，即我们可以选择性的去拦截某一个方法，而非全部的方法，因为在实际业务中并非所有的方法都需要拦截。 LogHandler 的实现 LogHandler 主要用于记录日志，所以我们需要记录方法的名字，方法的参数以及方法执行的结果，甚至是是否引发异常，这些功能在 AOP 中是相对基础的功能，Unity 框架为我们提供了这些基础设施，我们只要就可以获取到这些信息，然后将其记录到日志中即可。这里的代码如下：\npublic class LogHandler : ICallHandler { int ICallHandler.Order { get; set; } IMethodReturn ICallHandler.Invoke(IMethodInvocation input, GetNextHandlerDelegate getNext) { var methodInfo = input.MethodBase; var methodName = methodInfo.Name; Logger.Log(string.Format(\u0026#34;----------开始调用{0}----------\u0026#34;, methodName)); var parameters = methodInfo.GetParameters(); var arguments = input.Arguments; var logInfo = parameters.Select(e =\u0026gt; string.Format(\u0026#34;{0}:{1}\u0026#34;, e.Name, arguments[e.Position])); Logger.Log(\u0026#34;传入的参数为:\u0026#34; + string.Join(\u0026#34;,\u0026#34;, logInfo.ToArray())); var result = getNext()(input, getNext); if (result.Exception != null) Logger.Log(string.Format(\u0026#34;调用异常:{0}-{1}\u0026#34;, result.Exception.Message, result.Exception.StackTrace)); Logger.Log(string.Format(\u0026#34;调用{0}的结果为：{1}\u0026#34;, methodName, result.ReturnValue)); Logger.Log(string.Format(\u0026#34;----------结束调用{0}----------\u0026#34;, methodName)); return result; } } 为了让这个 Handler 更好用一些，我们希望它可以以 Attribute 的方式出现在方法上面，这样被标记过的方法就会被 Unity 框架拦截，所以我们需要一个继承自 Attribute 类的东西，知道我为什么不喜欢配置文件吗？因为我有 Attribute 啊！幸运的是 Unity 框架为我们提供了这样一个基类：HandlerAttribute，由此下面的代码可以这样写：\n[AttributeUsage(AttributeTargets.Method,AllowMultiple = true)] class LogHandlerAttribute : HandlerAttribute { public override ICallHandler CreateHandler(IUnityContainer container) { return new LogHandler(); } } ExceptionHandler 的实现 对于 ExceptionHandler 来说，它相比 LogHandler 增加的功能在于，它需要处理异常，按照目前项目的异常处理习惯，这种和硬件相关的方法都会被定义为一个 ErrorCode，为此我们的 ExceptionHandler 类中需要增加一个 ErrorCode 类型的成员，这是一个枚举类型。这里的代码实现如下：\npublic class ExceptionHandler : ICallHandler { int ICallHandler.Order { get; set; } public string ErrorCode { get; set; } IMethodReturn ICallHandler.Invoke(IMethodInvocation input, GetNextHandlerDelegate getNext) { var methodInfo = input.MethodBase; var methodName = methodInfo.Name; Logger.Log(string.Format(\u0026#34;--------------方法{0}执行开始--------------\u0026#34;, methodName)); var parameters = methodInfo.GetParameters(); var arguments = input.Arguments; var logInfo = parameters.Select(e =\u0026gt; string.Format(\u0026#34;{0}:{1}\u0026#34;, e.Name, arguments[e.Position])); Logger.Log(\u0026#34;传入的参数为:\u0026#34; + string.Join(\u0026#34;,\u0026#34;, logInfo.ToArray())); var result = getNext()(input, getNext); if (result.Exception != null) { Logger.Log(string.Format(\u0026#34;Error Code is {0}\u0026#34;, ErrorCode)); result.Exception = null; Logger.Log(string.Format(\u0026#34;--------------方法{0}执行结束--------------\u0026#34;, methodName)); throw new Exception(ErrorCode); } Logger.Log(string.Format(\u0026#34;--------------方法{0}执行结束--------------\u0026#34;, methodName)); return result; } } 可以注意到 ExceptionHandler 相比 LogHandler 的变化，主要发生在异常处理这部分，如你所愿，我在拦截到异常以后抛出了一个对应 ErrorCode 的异常，虽然我不赞同这种做法，但为了尊重现有项目的编程风格，我只能写有这样一行看起来非常拙劣的代码，我真的没有代码洁癖，我仅仅是觉得它还不够好，就像我觉得自己还不够好一样，同样，它需要定义一个对应的 Attribute 类，这样我们可以更加自由地使用这些特性：\n[AttributeUsage(AttributeTargets.Method,AllowMultiple = true)] class LogHandlerAttribute : HandlerAttribute { public override ICallHandler CreateHandler(IUnityContainer container) { return new LogHandler(); } } 本文小结 好了，现在我们可以来看，如何使用这篇文章中定义的两个组件：\nvar container = new UnityContainer().AddNewExtension\u0026lt;Interception\u0026gt;().RegisterType\u0026lt;IBussiness, Bussiness\u0026gt;(); container.Configure\u0026lt;Interception\u0026gt;().SetInterceptorFor\u0026lt;IBussiness\u0026gt;(new InterfaceInterceptor()); var bussiness = container.Resolve\u0026lt;IBussiness\u0026gt;(); var sum = bussiness.Add(12,23); Console.WriteLine(sum); var div = bussiness.Divide(1,0) Console.WriteLine(div) IBussiness 接口及其实现类 Bussiness 定义如下：\npublic interface IBussiness { int Add(int a, int b); int Divide(int a, int b); } public class Bussiness : MarshalByRefObject, IBussiness { [LogHandler] public int Add(int a, int b) { return a + b; } [ExceptionHandler(ErrorCode = \u0026#34;E2303\u0026#34;)] public int Divide(int a, int b) { return a / b; } } 好了，现在我们来看一下结果： 使用AOP简化后的异常处理和日志记录流程\r我们为此付出的代价是什么？第一，要有一个接口，写接口难道还有疑问吗？第二，要添加 Attribute 到指定方法上面，我保证这点时间足够你写好几遍重复代码了。第三，需要依赖注入机制，这个可能是到目前为止最大的影响，因为有了依赖注入以后，对象的实例化都交给了 Unity 框架，看起来我们好像被束缚了手脚，不能再任性地 new 一个对象实例出来，可这不正是依赖注入的精髓所在吗？我们就是需要 Unity 框架，来帮助我们管理这些模块间的依赖关系及其生命周期，如果你觉得这点代码不能接受，抱歉，任何依赖注入框架拯救不了你！\n今天这篇文章，我们从一个实际项目的背景出发，引出使用 Unity 框架来简化异常处理和日志记录流程这一想法，在正式实践这一想法前，我们首先了解了 Unity 框架中提供的三种拦截器及其各自优劣，在此基础上我们实现了 LogHandler 和 ExceptionHandler 两个组件，并展示了如何使用这两个组件，探讨使用整个 AOP 机制对现有项目的影响有多大，以及为什么我们需要 Unity 框架等问题，框架固然重要，了解为什么使用框架则更重要！好啦，这就是今天这篇文章的内容啦，再次谢谢大家关注我的博客，各位晚安！:smile:\n","date":"2018-03-21T19:35:40Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3291578070/","slug":"3291578070","tags":["AOP","异常","日志"],"title":"使用 Unity 框架简化应用程序异常处理及日志记录流程"},{"categories":["数据分析"],"content":"各位朋友，大家好，我是 Payne，欢迎大家关注我的博客。我的博客地址是：https://qinyuanpei.github.io。对于今天这篇文章的主题，相信经常关注我博客的朋友一定不会陌生。因为在 2017 年年底的时候，我曾以此为题写作了一篇文章：基于新浪微博的男女择偶观数据分析(上)。这篇文章记录了我当时脑海中闪烁着的细微想法，即当你发现一件事物背后是由哲学或者心理学这类玄奥的科学在驱动的时候，不妨考虑使用数学的思维来让一切因素数量化，我想这是最初数据分析让我感兴趣的一个原因。因为当时对文本的处理了解得非常粗浅，所以在第一次写作这篇文章的时候，实际的工作不过是在分词后绘制词云而已。等到我完成对微信好友信息的数据分析以后，我意识到微博这里其实可以继续发掘。关于微信好友信息的数据分析，可以参考这篇文章：基于 Python 实现的微信好友数据分析。在这样的想法促使下，便有了今天这篇文章，因为工作关系一直没有时间及时整理出来，希望这篇文章可以带给大家一点启示，尤其是在短文本分类方面，这样我就会非常开心啦！:slightly_smiling_face:\n故事背景 关于故事背景，我在 基于新浪微博的男女择偶观数据分析(上) 这篇文章中说得非常清楚啦。起因就是我想知道，男性和女性在选择伴侣的时候，到底更为关注哪些因素？在对微信好友信息进行数据分析的时候，我们可以非常直接地确定，譬如性别、签名、头像、位置这四个不同的维度，这是因为我们处理的是结构化的数据。什么是结构化的数据呢？一个非常直观的认识是，这些数据可以按照二维表的方式组织起来。可对于微博这样一个无结构的文本数据类型，我们除了对词频、词性等因素做常规统计分析以外，好像完全找不到一个合理有效的方案，因为我们很容易就明白一件事情，即：在短短的 140 个字符中，人类语言的多样性被放大到淋漓尽致 。为了将种种离散的信息收敛在一个统一的结构里，我们必须为这些文本构建一种模型，并努力使这种模型可以量化和计算。我们通过词云对微博进行可视化分析，更多是针对词频的一种分析方法，这种方法虽然可以帮助我们找出关键字，可是因为最初写作这篇文章时，对数据分析领域相关知识知之甚少，而且在分析的过程中没有考虑停用词，所以我认为在文本分类或者是主题提取层面上，我们都需要一种更好的方法。\n常见的技术方法 这篇文章涉及的领域称为文本分类或者主题提取，而针对微博、短信、评论等这类短文本的分类，则被称为短文本分类。为什么要进行文本分类呢？第一，提取出潜在主题以后可以帮助我们做进一步的分析。譬如博主这里想要从相亲类微博中分析男性和女性的择偶观，首先要解决的就是主题建模问题，因为在择偶过程中要考虑的因素会非常多，我们到底要选取哪些因素来分析呢？这些因素在特定领域中被称为特征，所以文本分类的过程伴随着特征提取。第二，**短文本数据通常只有一个主题，看起来这是在简化我们的分析过程，实则传统的基于文档的主题模型算法在这里难以适用。**因为这类主题模型算法都假定一篇文档中含有多个主题，而我们分析的是群体现象，这种个体上的差异必须设法将其统一于一体，比如美元和$属于同一个主题，我们需要一种策略来对其进行整合。\n传统主题提取模型通常由文本预处理、文本向量化、主题挖掘和主题表示等多个流程组成，每个流程都会有多种处理方法，不同的组合方法会产生不同的建模结果。目前，人们在传统主题提取模型的基础上，发展起了以CNN和RNN为代表的深度学习方法，在这里我们依然关注传统主题提取模型，因为这个领域对博主而言是个陌生的领域，这里我们更多的是关注传统主题提取模型。按照传统主题提取模型，文本分类问题被拆分为特征工程和分类器两个部分，其中，特征工程的作用是将文本转化为计算机可以理解的格式，并提供强特征表达能力，即特征信息可以用以分类，而分类器基本上是统计学相关的内容，其作用是根据特征对数据进行分类。下面来简单介绍下常见的技术方法。\n特征工程 特征工程覆盖了文本预处理、特征提取和文本表示三个流程。文本预处理通常指分词和去除停用词这两个过程，可以说分词是自然语言处理的基本前提。特征提取实际上囊括两个部分，即特征项的选择和特征项权重的计算。选择特征项的基本思路是：根据某个评价指标对原始数据进行排序，然后从中选择分数最高的评价指标，同时过滤掉其余的评价指标。通常可以选择的评价指标有文档频率、互信息、信息增益等，而特征权重的计算主要是经典的TF-IDF算法及其扩展算法。文本表示是指将文本预处理后转化为计算机可以理解的格式，是决定分类效果最重要的部分。传统做法是使用词袋模型(BOW)或者向量空间模型(VSM)，比如Word2Vec就是一个将词语转化为向量的相关项目。因为向量模型完全忽视文本的上下文，所以为了弥补这种技术上的不足，业界同时使用基于语义的文本表示方法，比如常见的LDA语义模型。\n分类器 分类器主要是统计学里的分类方法，基本上大部分的机器学习方法都在文本分类领域有所应用，比如最常见的朴素贝叶斯算法(Naive Bayes)、KNN、支持向量机(SVM)、最大熵(MaxEnt)、决策树和神经网络等等。简单来说，假设我们所有的数据样本可以划分为训练集和测试集。首先，分类器可以在训练集上执行分类算法以生成分类模型；其次，分类器可以通过分类模型对测试集进行预测以生成预测结果；最后，分类器可以计算出相关的评价指标以评估分类的效果。这里最常用的两个评价指标是准确率和召回率，前者关注的是数据的准确性，后者关注的是数据的全面性。\nTF-IDF 与朴素贝叶斯 TF-IDF(term frequency–inverse document frequency)是一种被用于信息检索与数据挖掘的统计学方法，常常被用来评估某个字词对于一个文件集或者是一个语料库中的一份文档的重要程度。在特征工程这里我们提到，特征工程中主要通过特征权重来对数据进行排序和分类，因此TF-IDF本质上是一种加权技术。TF-IDF的主要思想是：字词的重要性与它在文件中出现的次数成正比上升，与此同时与它在语料库中出现的频率成反比下降。这句话是什么意思呢？如果某个词或者短语在一篇文章中出现的频率(即TF)较高，并且在其它文章中出现的频率(即IDF)较低，那么就可以人为这个词或者短语可以作为一个特征，具备较好的类别区分能力，因此适合用来作为分类的标准。TF-IDF实际上是 TF * IDF，即 TF(term frequency，词频)与 IDF(inverse document frequency，逆文档频率)的乘积，具体我们通过下面的公式来理解： term frequency，词频\r显然，这里的 TF 表示某一词条在文档中出现的频率。再看 IDF: inverse document frequency，逆文档频率\r这里的 D 表示语料库中文档的数目，而分母表示的是含有指定词的文档的数目，这里两者求商后取对数即可得到 IDF。需要注意的是，当该词语不在语料库中时，理论上分母会变成 0，这将导致计算无法继续下去，因此为了修正这一错误，我们在分母上加 1，这样就可以得到 IDF 更为一般的计算公式。按照这样的思路，我们将两段文本分完词以后，分别计算每一个词的 tf-idf 并按照 tf-idf 对其进行排序，然后选取前 N 个元素作为其关键字，这样我们就获得了两个 N 维向量，按照向量理论的相关知识，两个向量间的夹角越小，其相关性越显著，这就是文本相似度判断的常规做法，在这个过程中，我们覆盖到了文本预处理、特征提取和文本表示三个过程，相信大家会对这个过程有更好的理解。\n好了，那么什么是特征呢？这里计算出来的 tf-idf 实际上就是一组特征，这个特征是上下文无关、完全基于频率分析的结果，现在这些结果都是计算机可以处理的数值类型，所以特征工程要做的事情，就是从这些数值中分析出某一种规律出来。譬如，我们通过分析大量的气象资料，认为明天有 80%的概率会下雨，那么此时下雨的概率 0.8 就可以作为一个特征值，在排除干扰因素的影响以后，我们可以做一个简单的分类，如果下雨的概率超过 0.8 即认为明天会下雨，反之则不会下雨。这是一个接近理想的二值化模型，在数学中我们有一种概率分布模型称为 0-1 分布，即一件事情只有两个可能，如果该事件会发生的概率为 p，则该事件不会发生的概率为 1-p。如果所有的问题都可以简化到这种程度，我相信我们会觉得这个世界枯燥无比，因为一切非黑即白、非此即彼，这会是我们所希望的世界的样子吗？ 为什么在这里我要提到概率呢？因为这和我们下面要提到的朴素贝叶斯有关。事实上，朴素贝叶斯的理论基础，正是我们所熟悉的条件概率。根据概率的相关知识，我们有以下公式，即全概率公式：P(A|B) = P(AB)/P(B)。我们对 A 和 B 进行交换，同理可得：P(B|A) = P(A/B)/P(A)。由此我们即得到了贝叶斯公式： 贝叶斯公式\r所以，朴素贝叶斯本质上是一种基于概率理论的分类算法。我们知道条件概率成立的前提是各个事件都是独立的，因此在朴素贝叶斯算法中假设所有特征间都是独立的，可当我们逐渐地了解这个世界，就会明白这个世界并不是非黑即白、非此即彼的，甚至一件事情会受到来自方方面面的因素影响，就像我们从前学习物理的时候喜欢用控制变量法一样，总有一天你会明白当时的想法太天真。朴素贝叶斯算法中的“朴素”，通常被翻译为 Naive，而这个词就是表示天真的意思，这正是朴素贝叶斯的名称由来，它简单粗暴地认为各个特征间是相互独立的，有人认为这种假设是相当不严谨的，所以相当排斥这种分类的理论，所幸朴素贝叶斯在实际应用中分类效果良好，尤其是在解决垃圾邮件过滤这类问题上，所以到今天为止，朴素贝叶斯依然是一个相当经典的分类算法，它是一个根据给定特性/属性，基于条件概率为样本赋予某个类别标签的模型。\n数据分析 好了，讲述这些理论知识实在是一件苦差事，因为让读者了解一套新的知识，远远比让自己了解一套新的知识容易，所以在描述这些理论的时候，我努力地避免给大家留下晦涩深奥地印象，可这样难免会让读者觉得我不太专业。可是，谁让我们生活在一个被无数前辈开垦过地世界里呢？作为一个资深的“调包侠”，这些理论我们能理解多少算多少，最终我们需要的只是一个库而已，所以在正式进入下面的内容时，我们首先来梳理侠整体数据分析的思路，这样我们就能对整个过程有一个相对感性的认识了。关于如何从新浪微博抓取数据，这个我们在上篇有详细的介绍，这里不再重复阐述，所有数据我们都存储在数据库里，下面的图示不再展示关于数据库的细节： 特征分析流程图\r简单来讲，这是一个有监督的、使用二元分类的特征提取过程。这里的语料库是由人工进行编制的文本资料，语料库的好坏将直接影响到分类的效果。比如说，我们希望提取的特征是陕西省的地理信息，那么我们就需要准备一个，由陕西省所辖的所有地级市组成的文本文件，这里为了方便后续处理，我们建议每行存放一个短文本信息。\n接下来，我们会从数据库中读取所有的数据，然后进行预处理操作，这里的预处理是指分词和去除停用词，停用词表是从网络上下载的，然后根据我们自己的需要再在基础上进行添加，我们会选取前 20 个词语作为关键词，这里使用了结巴分词的相关接口，其算法原理正是 tf-idf。我们会使用这 20 个关键词，和语料库中每一个主题下的内容进行比较，这里的相似度由 SnowNLP 提供支持，其计算结果是一个 20 维的向量，我们对向量进行归一化后，如果其向量中所有维度的值的最大值\u0026gt;=0.95，则认为该文本和这一主题相关，因此该主题的权重会增加 1，否则会继续计算下一个文本的相似度。\n我们汇总所有主题的权重，即可统计出各个主题出现的频率。比如我们这里关注 A、B、C 三个主题，而经过计算这三个主题各自出现的频率为 0.1、0.8 和 0.1，所以我们这里可以理解为：这里有 80%的把握认为文本和 B 主题有关，由此我们选取出了分类的特征，这里我们使用一个元组来表示特征，其表示为([0.1,0.8,0.1],\u0026ldquo;B\u0026rdquo;)。依次类推，我们就获得了全部的特征信息。接下来，我们使用 nltk 中提供的朴素贝叶斯分类器对内容进行分类，训练集和测试集合各占 50%，最终通过准确度来评估整个分类的效果。\n特征分析 特征分析的难点主要在特征的提取，在这里我们通过不同主题的频率来选取特征：\ndef buildFeatures(sentence,document): tokens = jieba.analyse.extract_tags(sentence) tokens = list(filter(lambda x:x.strip() not in stopwords, tokens)) features = {} for (subject,contents) in document.items(): for content in contents: if(similarText(tokens,content)): if(subject in features): features[subject]+=1 else: features[subject]=1 total = sum(features.values()) for subject in features.keys(): features[subject] = features[subject] / total # 特征归一化 for subject in subjects: if(subject not in features.keys()): features[subject] = 0 # 预测结果 max_value = max(features.values()) suggest_subject = \u0026#39; \u0026#39; for (key,value) in features.items(): if(value == max_value): suggest_subject = key return features, suggest_subject 其中，stopwords 我们从一个指定文件中读取：\nstopwords = open(\u0026#39;stopwords.txt\u0026#39;,\u0026#39;rt\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;).readlines() 这里有一个计算句子和主题相似度的方法 similarText()，其定义如下：\n# 文本相似度 def similarText(tokens,content): snow = SnowNLP(tokens) similar = snow.sim(content) norm = math.sqrt(sum(map(lambda x:x*x,similar))) if(norm == 0): return False similar = map(lambda x:x/norm,similar) return max(similar)\u0026gt;=0.95 我们通过下面的代码来构建特征，以及使用朴素贝叶斯分类器进行分类，核心代码如下：\ndef analyseFeatures(): rows = loadData() document = loadDocument(subjects) features = [buildFeatures(row[0],document) for row in rows] length = len(features) print(\u0026#39;数据集: \u0026#39; + str(length)) cut_length = int(length * 0.5) print(\u0026#39;训练集: \u0026#39; + str(cut_length)) train_set = features[0:cut_length] print(\u0026#39;测试集: \u0026#39; + str(length - cut_length)) test_set = features[cut_length:] classifier = nltk.NaiveBayesClassifier.train(train_set) train_accuracy = nltk.classify.accuracy(classifier,train_set) print(\u0026#39;准确度: \u0026#39; + str(train_accuracy)) counts = Counter(map(lambda x: x[1],test_set)) for key, count in counts.items(): freq = count/len(test_set) print(\u0026#34;主题\u0026lt;{0}\u0026gt;: {1}\u0026#34;.format(key,freq)) 下面是特征提取相关的结果，因为最近对语料库进行了调整，所以准确度只有 92%，用一位前辈的话说，数据分析就像炼丹，在结果没有出来以前，没有人知道答案会是什么。这里使用的是 nltk 内置的朴素贝叶斯分类器，而 nltk 是一个自然语言处理相关的库，感兴趣的朋友可以自行了解，这里推荐一本书：《NLTK 基础教程(用 NLTK 和 Python 库构建机器学习应用)》。下图中展示了各个主题在整个微博文本中所占的比重：\n特征提取及其分类效果\r年龄分布 对于男女性的年龄分布，我们通过正则来提取微博中年龄相关的数值，然后统计不同年龄出现的频数，并将其绘制为柱形统计图，相关代码实现如下：\ndef analyseAge(): ages = [] rows = loadData() pattern = re.compile(r\u0026#39;\\d{2}\\年|\\d{2}\\岁\u0026#39;) for row in rows: text = row[0].decode(\u0026#39;utf-8\u0026#39;) matches = pattern.findall(text) if(len(matches)\u0026gt;0): match = matches[0] if(u\u0026#39;年\u0026#39; in match): now = datetime.datetime.now().year birth = int(\u0026#39;\u0026#39;.join(re.findall(r\u0026#39;\\d\u0026#39;,match))) ages.append(now - 1900 - birth) else: ages.append(int(\u0026#39;\u0026#39;.join(re.findall(r\u0026#39;\\d\u0026#39;,match)))) ages = list(filter(lambda x: x\u0026gt;10 and x\u0026lt;40, ages)) freqs = Counter(ages).items() freqs = sorted(freqs,key=lambda x:x[0],reverse=False) freqs = dict(freqs) drawing.bar(\u0026#39;男女性择偶观数据分析:年龄分布\u0026#39;,freqs,\u0026#39;年龄\u0026#39;,\u0026#39;人数\u0026#39;,None) 通过图表，我们可以发现：择偶年龄重点集中在 24~28 岁之间，并且整个年龄区间符合正态分布。每年过年的时候，我们都会听到年轻人被催婚的声音，甚至作为一个单身的人，每一个节日都像是我们的忌日，因为无论在哪里，你都可以被秀恩爱或者被撒狗粮。“哪有人会喜欢孤独呢？不过是不喜欢失望”，当这句话出现在我的 Kindle 屏幕上，出现在村上春树的《挪威的森林》里，我突然有种扎心的感觉。有一天，当我不在视爱情为必需品时，我突然意识到生命里有太多比感情重要的事情。我不希望我们因为一句年龄到了就去结婚，如果人生的一切都有期限都要按部就班，那么为什么我们不能平静地面对衰老和死亡呢？人天生起点就是不一样的，所以你不必努力去迎合别人定制的标准，就像学生时代大家面对的是同一张考卷，有的人交卷交得早，有的人交卷交得晚，有的人考试成绩好，有的人考试成绩差，可这不过是一场考试而已，不是吗？如果我的时间不能浪费在我喜欢的人身上，我宁愿永远将时间浪费在自己的身上，除了生与死以外，结婚和繁衍并不是必答题，我可以不结婚啊，一如我可以交白卷啊！ 男女性择偶观数据分析:年龄分布\r性别组成 性别组成，我们主要从微博中的关键字入手，因为这些微博明确了择偶的是男嘉宾还是女嘉宾，我们通过这些特征就可以分析出男女性别比例。相关代码实现如下：\ndef analyseSex(): rows = loadData() sexs = {\u0026#39;male\u0026#39;:0, \u0026#34;female\u0026#34;:0} for row in rows: text = row[0].decode(\u0026#39;utf-8\u0026#39;) if u\u0026#39;男嘉宾[向右]\u0026#39; in text: sexs[\u0026#39;male\u0026#39;]+=1 elif u\u0026#39;女嘉宾[向右]\u0026#39; in text: sexs[\u0026#39;female\u0026#39;]+=1 drawing.pie(\u0026#39;男女性择偶观数据分析:男女性别比例\u0026#39;,sexs,None) 通过下面的图表，我们可以非常直观地看到，男性数量是超过女性数量的，两者比例接近 1.38:1。这和目前中国的实际基本相符，考虑到人们有更多的相亲渠道可以选择，我认为实际的比例应该会更大，媒体称适婚男性比女性多出 3000 万，性别比例的失衡难免会让男生找不到对象。可找不着对象有什么关系呢？人生短短一世，活着时候能见到最多不过四世同堂，血缘关系并不能让后辈替你完成未竟之事，当一个离开了这个世界，它与世界的关联就变得微乎其微，时间会让记忆逐渐模糊直至遗忘，你无法将这点微弱的安全感寄托在某一个人身上，人生而有涯，而知无涯，能在这个世界里流传下去的只有思想，我不想和任何人去攀比，因为生而为人，我很抱歉。 男女性择偶观数据分析:男女性别比例\r身高分布 身高分布，同样采用关键字匹配的方式实现，不同的是，择偶者通常会在微博中给出自己的身高以及对伴侣期望的身高，由此我们对微博中的身高进行了提取，分别获得了男性、女性身高分布及其身高差分布。这是我最开始研究这个问题的初衷，现在的结果印证了当时的想法，我内心其实是特别开心的，这正是为什么要花时间和精力写这篇文章的原因所在。这里，相关的代码实现如下：\n# 身高分布 def analyseHeight(): heights = [] rows = loadData() pattern = re.compile(r\u0026#39;1\\d{2}|\\d{1}\\.\\d{1,2}|\\d{1}\\米\\d{2}\u0026#39;) for row in rows: text = row[0].decode(\u0026#39;utf-8\u0026#39;) matches = pattern.findall(text) if(len(matches)\u0026gt;1): matches = map(lambda x:int(\u0026#39;\u0026#39;.join(re.findall(r\u0026#39;\\d\u0026#39;,x))),matches) matches = list(filter(lambda x: x\u0026lt;190 and x\u0026gt;150, matches)) if(len(matches)\u0026gt;1): height = {} height[\u0026#39;male\u0026#39;] = max(matches) height[\u0026#39;female\u0026#39;] = min(matches) heights.append(height) # 男性身高分布 male_heights = list(map(lambda x:x[\u0026#39;male\u0026#39;],heights)) male_heights = Counter(male_heights).items() male_heights = dict(sorted(male_heights,key=lambda x:x[0],reverse = False)) drawing.bar(\u0026#39;男女性择偶观数据分析:男性身高分布\u0026#39;,male_heights,\u0026#39;身高\u0026#39;,\u0026#39;人数\u0026#39;,None) # 女性身高分布 female_heights = list(map(lambda x:x[\u0026#39;female\u0026#39;],heights)) female_heights = Counter(female_heights).items() female_heights = dict(sorted(female_heights,key=lambda x:x[0],reverse = False)) drawing.bar(\u0026#39;男女性择偶观数据分析:女性身高分布\u0026#39;,female_heights,\u0026#39;身高\u0026#39;,\u0026#39;人数\u0026#39;,None) # 男女身高差分布 substract_heights = list(map(lambda x:x[\u0026#39;male\u0026#39;]-x[\u0026#39;female\u0026#39;],heights)) substract_heights = Counter(substract_heights).items() substract_heights = dict(sorted(substract_heights,key=lambda x:x[0],reverse = False)) drawing.bar(\u0026#39;男女性择偶观数据分析:男女身高差分布\u0026#39;,substract_heights,\u0026#39;身高差\u0026#39;,\u0026#39;人数\u0026#39;,None) 虽然女生都希望男生 180 以上，据说这样可以举高高、有安全感，可是作为一个成年人，我们必须勇敢地打破这种不切实际的幻想，因为身高和外貌都是父母给我们的，那些基因里决定的东西，往往是我们无法通过后天努力来弥补的。如果可以的话，我希望自己再长高 5 厘米，可如果我再无法长高，我希望你能接受现在的我，接受一个人身高上的缺陷，和接受一个人人性中的缺点，在我看来是一模一样的。可人类最大的问题， 就在于愿意相信自己眼睛看到的，耳朵听到的，并且这是两个人建立联系的前提，人家愿意了解你有趣的灵魂，前提是你有一副好看的皮囊，人类啊，说到底是一种比较高级的动物而已，就像动物用皮毛、肤色去吸引同类一样，如你所见，男生平均身高其实只有 175 而已！ 男女性择偶观数据分析:男性身高分布\r女性的身高通常不会被作为筛选条件，正如社会群体通常都是对男性提出各种要求一样，两个同等条件下的男、女性，人们理所当然地对男性提出了更高的要求，可其实大家都是母亲十月怀胎而来，同样地都在这个世界里生活了 20 多年。所以这个世界上有太多地问题，其实都是人们自己造成的。比如女性一定要找一个穿高跟鞋后还要比她高的男性，而男性一定要找一个身高上和他相差不大的女性，男性的身高不足 175，同女性的身高不足 165 一样，都是人们眼中比较尴尬的身高，可你看这图表中女性的平均身高是 160，那么，就让大家一起尴尬吧，不知道当年小平爷爷和拿破仑将军的夫人心里是怎么想的啦！ 男女性择偶观数据分析:女性身高分布\r最初我研究这个问题的时候，我发现微博上有好多身高不足 160 的女性，要求伴侣期望身高都是 175 以上，作为一个身高只有 170 的男生，我感到绝望和悲伤啊，后来和一位朋友聊天，他说他觉得我连 170 都没有，我想说人类为什么要这般奇怪，譬如体重一定要说得比实际轻、身高一定要说得比实际高、年龄一定要说得比实际小……难道这样不感觉累吗？那么到底有多少人希望两个人的身高差超过 20 厘米呢？网络上流传的所谓最萌身高差到底萌不萌呢？你看孟德尔通过豌豆杂交试验来研究遗传问题，两个身高差超过 20 厘米的人的后代，平均下来难道不是只有 170 吗？图表表明，男女性之间最佳的身高差是 15 厘米。 男女性择偶观数据分析:男女身高差分布\r地理分布 因为在这些微博中会出现相亲者的地理信息，所以我们整理了陕西省各县市的名称作为关键字，试图分析出这些相亲者的地理分布，这里我们简单绘制了一个柱形图，相关代码实现如下：\n# 地区分析 def anslyseLocation(): freqs = { } citys = [u\u0026#39;西安\u0026#39;,u\u0026#39;铜川\u0026#39;,u\u0026#39;宝鸡\u0026#39;,u\u0026#39;咸阳\u0026#39;,u\u0026#39;渭南\u0026#39;,u\u0026#39;延安\u0026#39;,u\u0026#39;汉中\u0026#39;,u\u0026#39;榆林\u0026#39;,u\u0026#39;安康\u0026#39;,u\u0026#39;商洛\u0026#39;] rows = loadData() for row in rows: text = row[0].decode(\u0026#39;utf-8\u0026#39;) for city in citys: if(city in text): if(city in freqs.keys()): freqs[city]+=1 else: freqs[city]=1 drawing.bar(\u0026#39;地区分布图\u0026#39;,freqs,\u0026#39;地区\u0026#39;,\u0026#39;人数\u0026#39;,None) 这里的结果令人出戏，因为西安作为陕西省的省会城市，在所有地区中一骑绝尘。考虑到在这些微博中\u0026quot;西安\u0026quot;存在干扰，所以这个结果并不是非常严谨，不能作为一个有效的分析指标，而且这里存在同义词，比如\u0026quot;本地\u0026quot;和\u0026quot;土著\u0026quot;其实都表示西安，而我们统计的时候并没有考虑这种情况，所以这里绘制的地区分布图表，大家看看就好啦！ 男女性择偶观数据分析:地区分布图\r星座分布 这里为什么要分析星座呢？理论上来讲，我是不大相信这些东西的，可当你经历的事情多了以后，你就会下意识地认为这些东西说得很对，我想古代的占卜算卦基本上是同样的东西，其实世间好多事情之间应该是没有直接的联系的，无非是在千百年的历史积淀中，逐渐地形成了一套建立在经验上的理论体系，这就像我们今天所追捧的机器学习，我们有千百年的历史长河去收集数据，每一个相信这些理论的人都是一个数据样本，这些理论体系通过不断地训练和模拟，逐渐可以正确地预测某些事情，让我们相信万事万物间存在某种联系。可即便如此，人类依旧免不了对各种事物存在偏见，比如星座中经常无辜躺枪的处女座、双子座和天蝎座，人类最擅长的认知方式，就是用一个群体现象来预测个人现象，可讽刺的是朴素贝叶斯就是这样的思想，所以这里我们简单地统计了下各种星座的频数分布：\n# 星座分析 def analyseStar(): stars = [\u0026#39;白羊\u0026#39;,\u0026#39;金牛\u0026#39;,\u0026#39;双子\u0026#39;,\u0026#39;巨蟹\u0026#39;,\u0026#39;狮子\u0026#39;,\u0026#39;处女\u0026#39;,\u0026#39;天秤\u0026#39;,\u0026#39;天蝎\u0026#39;,\u0026#39;射手\u0026#39;,\u0026#39;摩羯\u0026#39;,\u0026#39;水瓶\u0026#39;,\u0026#39;双鱼\u0026#39;] freqs = {} rows = loadData() for row in rows: text = row[0].decode(\u0026#39;utf-8\u0026#39;) for star in stars: if(star in text): if(star in freqs.keys()): freqs[star]+=1 else: freqs[star]=1 for star in stars: if(star not in freqs.keys()): freqs[star] = 0 freqs = Counter(freqs).items() freqs = dict(freqs) drawing.pie(\u0026#39;男女性择偶观数据分析:星座分布\u0026#39;,freqs,None) 这个结果相对客观些，因为 12 个星座基本上平分秋色啦，并不存在某种星座独领风骚的情况，简直是人与自然的大和谐了呢？ 男女性择偶观数据分析:星座分布\r本文小结 这篇文章写到这里，我其实已经非常疲惫啦，因为这篇文章的上篇与下篇中间相隔了差不多三个月，而且我写作上篇的时候，并没有打算写这一篇文章出来，再者两篇文章写作时的心境完全不同，所以现在写完这篇文章，终于有种如释重负的感觉，一来没有因拖延症而放弃这篇文章，二来为了了解相关的理论以及训练数据花费大量精力，我必须对自己的过去有一个总结，这是我今年年初给自己制定的目标，不管有没有喜欢我，我总要去做这些事情，不是因为我想要证明什么或者做给谁看，而是我认为这件事情比某些事情有趣而且重要。这篇文章首先承接上文，交待故事的背景，即为什么要做这样的数据分析；然后我们简单介绍了文本分类的常用的技术方法，主要以特征工程和分类器为主；接下来我们介绍了两个经典的理论：tf-idf 和朴素贝叶斯，这是本文文本分类的理论基础；在数据分析这部分，我们对特征、年龄、性别、身高、地区和星座等进行了分析，并借助 Python 中的图表模块完成了数据的可视化工作。好啦，以上就是这篇文章的全部内容啦，欢迎大家积极留言和评论，晚安！\n","date":"2018-03-17T15:28:40Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3083474169/","slug":"3083474169","tags":["朴素贝叶斯","文本分类","微博"],"title":"基于新浪微博的男女性择偶观数据分析(下)"},{"categories":["生活感悟"],"content":" 从昨天到今天，关于霍金逝世的消息，一直在朋友圈里刷屏。昨天同事告诉我这个消息的时候，我心底先是一片恍惚，而后习惯性地打开微信，发现朋友圈和公众号里都在讨论这件事情。而等到我吃饭的时候，居然听到临桌的一名男生，在向同伴讲述霍金辐射的理论，堪称我在这一天所见过的一股清流。不知道从什么时候开始，一个人物的突然离去，总是会让人们在短时间内亢奋起来，仿佛一场集体缅怀的狂欢。回顾最近这些年来已故的名人，例如杨绛先生、杨洁导演、词作家闫肃、作家黄易等等，每每提及不禁令人一阵唏嘘，正所谓“逝者已矣，生者如斯”。可我想说的是，不要总是等到离开的时候，才会想起一个人的存在。\n印象中第一次有这种感觉是在 2011 年，那时我刚刚考上大学，我只记得那时站在太阳底下的我，突然大声地向周围人宣布乔布斯逝世的消息。我清晰地记着父母惊愕的表情，因为在他们的人生字典里，全然不知道乔布斯是谁，可在 19 岁的我看来，那就像是某种重大的事情发生，至少从今天的角度来看，在我出生的 1992 年里，苏联正式解体，我们的生命总是不可避免地和某种历史进程关联起来。因为从中学时候就开始住校，印象中每次回家都既陌生而熟悉，偶尔会听到妈妈讲，家族里某一位长辈突然过世。这种事情听得多了，居然不会再觉得惊讶。可是想起这些人里，有人曾经出过数学题考问过我，有人你曾经帮过他们做过什么事情……刹那间觉察到时光的残忍——所谓的物是人非，大概就是你还在此处，而别人早已暗自走远。\n坦白地讲，我对霍金的认知永远都停留在《时间简史》这本书上，记得 16 年买了 Kindle 以后，的确买了这本书来读，大概读了十来页便读不下去。霍金和海伦·凯勒一样，是被我划定到身残志坚这类写作素材的范畴里。当时，语文老师让我们关注每年的感动中国人物评选，其初衷便是为了丰富我们的写作素材。回想那些年里，遭受无数次宫刑而忍辱负重的司马迁、实验了 3000 多种材料终于制造出灯泡的爱迪生、披发行吟泪洒汨罗而心系家国的屈原……这些在学生时代频频被消费的历史人物，在今天看来是否有些相似呢？据说知乎上一天内产生了 700 多个霍金相关的问题，一个曾经活在我们作文里的人物，在他离开这个世界以后，再次成为我们热议的话题，好像他从来没有在这个世界上存在过一样，这是一件可怕的事情。\n我认为尊重一名物理学家的基本要求是相信科学。伴随着霍金变成人们关注的热点，网络上开始流传伽利略、爱因斯坦和霍金三个人之间巧合的时间线问题，仿佛我们面对的不是一名为宇宙物理学做出巨大贡献的科学家，而是一个被神化的“活佛”转世。有人说，人类都是一群戏精。集体缅怀一个伟人吧，立马有人跳出来说，伟人的著作都没读过一本，蹭什么热度；大家都不关注这件事情吧，立马有人跳出说，“将军坟前无人问，戏子家事天下知”，根本没有人关心科技工作者。可你看关注的时候，大家都在关注什么，譬如定要给物理学领域内的专家学者们排出个优劣来，定要将一个人的私事深挖出来品判人品。杨绛先生说，“我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容。我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系。“，这是最浅显不过的道理，可惜想要做到实在太难。\n我一直相信“人生而孤独”，除了亲情血缘以外，人与人间的联系，有时脆弱得像一只挂在风筝上的线，随时都会有断开的危险。有些人不知不觉就渐渐走远了，我们一路踟蹰而雁行，在相遇中失散，在失散中相遇，可当两个人再次相遇时，已然不是当初的彼此。我是一个不太会维护亲密关系的人，不知道是我自己走得太快，还是别人走得太快，无数的人在我这里出现然后离开，仿佛是在追逐风中的花瓣，等到风停了的时候，花瓣已不见，花香已飘远。有时候会突然问自己，想把别人留在我的生命里，是不是一件自私的事情。游戏制作人陈星汉有一款游戏叫做《风之旅人》，在广阔无垠的沙漠场景中，最多只有两个玩家出现，出现的时间和地点随机，对方可能来自任何一个国家，你对他/她的的身份信息一无所知，两个人唯一的互动方式是“共鸣”。两个靠在一起的人，可以通过“共鸣”来为对方的围巾补充能量，最重要的一点是，一旦两个人走失，就永远不会再相遇，这是这款游戏超现实意义的一个体现。\n我不知道，两个人从无话不说到无话可说需要多久；我只知道，真正想要离开的人，从来都是不动声色的。自那以后，我不知道对方会在哪里，会变成什么样子，每天都会做哪些事情。我承认，别人的世界和你毫无关联，可你终究不愿意让自己成为孤岛，所以你会感到痛苦和挣扎，会想要找一个能永远陪伴你的人。可生老病死是人生里无可避免的结果，我们终其一生所寻找的灵魂伴侣，是否真的可以陪伴彼此到生命尽头。如果一切注定都要失去，我宁愿一直这样下去，我从来没有把生育看做是我生命的一种延续，因为每一个人的生命都注定独一无二，你不能想当然地认为，血缘关系会替你继承什么东西。从你死亡的那一刻起，一切都变成新的东西。\n人的一生会死亡三次，第一次是医生宣布你的死亡，这是肉体上的死亡；第二次是人们来参加你的葬礼，这是社会学意义上的死亡；第三次是这个世界没有人再记得你，这是哲学意义上的死亡。或许这个世界再无霍金，可他的思想和著作一直就在那里，时间会记录着人类的过去和未来，而他是搭乘时光列车满世界旅行的自由灵魂。一个人可以不结婚，可以不生孩子，因为这是你生而为人的选择，世俗的力量是如此的强大，以至于我们都以为，人生就是一个跳一跳游戏，每一个年龄就应该跳到相应的位置。我的人生目标里没有结婚生子，如果我注定留不下任何人，如果我注定永远要被这个世界所遗忘，我宁愿在我还活着的时候，努力去写字去发出声音，即使在这空荡荡的宇宙里听不到回声，可声音不是一直都在传播着吗？\n有时候，想想人生难免会觉得失望。我们明明知道世界是自己，和他人毫无关联，可我们还在努力地和这个世界发生着关联；我们明明不愿意让别人了解自己的生活，可我们对这个世界的表达欲从来没有衰减过。从镌刻在龟壳上甲骨文到以丝帛作为书写材料，再到造纸术的产生，再到今天的各种芯片，甚至内容的形式从文本演变为图片再演变为视频……可我们怎么就变成了一堆“亡灵”，从前 QQ 好友列表一片隐身，如今朋友圈剩下一条横线。如果一定要别人不再想起你，等你真正离开这个世界的时候，才会突然间被想起，我会很心疼一条鱼的记忆，因为一条鱼的记忆只有 8 秒。\n如果下一刻失去记忆的，是你和我这般普通人，我们没有机会像霍金一样，被大家集体缅怀，你希望被那一个人记着，记多长时间呢？就像我总和朋友们说，回家以后找时间相聚，可在家时会觉得家人最为重要，在某一瞬间发现自己并没有那么多时间；就像我总计划着找机会去看望语文老师，可和朋友约不到一起时便无从谈起。我突然间想到，高中的第一堂语文课上，老师安排大家写一篇作文，题目好像叫做《回首向来》亦或者是《行走在消逝中》，那时我的作文没有写完，反而被老师叫起来当众朗读，我说，“回首向来萧瑟处，也无风雨也无晴”，记忆明明是有的，可我突然叫不出来有些人的名字，甚至在某一个清晨惊醒，梦到过往的某一天考试迟到，或者是快要交卷发现作文没有写……我稍稍一定神，考试那好像是很多年前的事情了吧……\n我其实很想留下来陪你或者陪 Ta 呀，可时光列车从来不会留给我思考的时间，有时候我走得快，有时候你走得快，像无法逃离黑洞的光一样，拼命地往前走往前走。在广阔无垠的宇宙中，我们生活在彼此平行的世界里，有时看得见彼此，有时看不见彼此，靠着彼此间微弱的万有引力，不至于失散得太远，在成为一颗孤独的白矮星之前，请记住我。\n","date":"2018-03-15T21:29:47Z","image":"/posts/2809571715/cover.png","permalink":"https://qinyuanpei.github.io/posts/2809571715/","slug":"2809571715","tags":["霍金","时间","请记住我"],"title":"行走在消逝中"},{"categories":["读书笔记"],"content":"\r终于在除夕夜到来前，在 Kindle 上读完了 2017 年的最后一本书，来自夏目漱石先生的《我是猫》。起初买这本书的动机说起来非常滑稽，一来以为这会是一本诙谐幽默的书，二来对夏目这个名字莫名地充满好感。我读的是曹曼翻译的中文译本，读时觉得这位作者的文字清新素雅，即使全书行文节奏堪称缓慢到极点，想来应该是我们这个时代的人物。及至翻阅作者生平，始知这位被誉为“国民大作家”的日本作家，早在 100 年前就在日本文学史上享有盛名。这种感觉如何去形容呢？大概就是杨过从剑冢石刻的寥寥数语中，遥想独孤求败“生平求一敌手而不可得”的寂寥难堪。这位老先生的文字可以说非常”摩登“了，因为在 100 年后的今天再次读来，竟完全读不出违和感来，所谓”嬉笑怒骂皆成文章“，讽刺与幽默杂然相陈，这是我喜欢这本书的理由。\n对于《我是猫》这本书，按照作者的话说，它是一部没有什么情节的小说，因为它完全是以一只猫的视角来行文，这只生活在一个教师家庭里的猫，每天都会接触到形形色色的文人，譬如：不食人间烟火，空有一番理论而不去实践的独仙；整天磨玻璃球，做事一丝不苟甚至古板木呐的寒月；表面上每天都很乐观，实则唯恐天下不乱的米亭；做事三分钟热情，自命清高的苦沙弥……等等。在猫的眼睛这里，这些人整天聚在一起讨论没有意义的事情，对现实世界心怀不满，不思进取就会怨天尤人，甚至金田及其夫人的”拜金主义“，为金钱而陷害苦沙弥的邻居，唯利是图、虚伪圆滑的铃木，这些人在猫的眼睛里都是丑陋而黑暗的。这只猫平静地叙述着它的见闻，仿佛它早已经整个人类和社会看穿看透，或许带着些嘲讽，或许带着些同情。\n每年的 2 月 22 日是日本的猫节，这是我在读完这本书以后知道的。而猫在日本的文化形象中是非常神圣的，据说这是因为猫最早由遣唐使带来日本，首先作为宫廷宠物出现，直至江户时代进入”寻常百姓家“。除此之外，日本作为重度渔业国度，对稻米的珍惜使其在捕鼠护粮方面极为重视，猫作为老鼠的天敌自然而然地受到喜爱。相传招财猫起源于东京世田谷的豪德寺，因此猫在日本被人们当作神明供奉。再比如日本动漫中的机器猫、龙猫和 Hello Kitty 都是猫在日本文化中的经典形象，日本的文学作品比如《草枕子》、《源氏物语》等里面都有关于猫的故事。时至今日，依然有大量德川家族与猫的故事流传。因此，猫在日本人眼中有一种浓厚的贵族气息。陈凯歌导演的《妖猫传》，改编自日本作家梦枕貘的小说《沙门空海》，猫在其中的重要性不言自明。\n这是一本“猫眼看世界”的书，这是一个怎样的世界呢？1871 年，日本历史上最为大刀阔斧的一次改革——明治维新，开始在全国范围内推行。改革带来经济飞速发展的同时，带来了各种矛盾日益突出的社会问题。36 年的 1905 年，时年 38 岁的夏目漱石，以猫的视角，如初入人类社会一般，探讨当时知识分子的心理状态和对社会变迁的感慨，并因此一举成名，获得社会广泛关注，被认为是日本批判现实主义文学的丰碑。每一个时代都有它的无奈，或许我们今天难以想象老先生当时的心境，不过从这些猫的口吻里，从这些辛辣的讽刺和戏谑中，我们总能读出作者当时内心的苦闷。猫眼里那些荒诞不经的行为，恰恰就是你我每天的生活，我们总说人类和猫是好朋友，可那仅仅是我们以为的，在猫的眼睛里，我们就像一群神经病。\n猫是如何看待人类的呢？猫说：世间的奢侈往往是无能的表现。猫一年到头都穿着同一件衣服，而人类好像不把尽可能多的东西往身上照顾就难受，人类给羊添麻烦，受蚕照顾，承蒙棉花的恩泽，你看吧，我们的所作所为连只猫都看不下去。人类羡慕猫的悠闲，故而感慨道：什么时候能像猫一样轻松就好了。可明明是人类自己制造出一堆乱七八糟的事情给自己，到头来还抱怨真痛苦真痛苦，就像自己生起一堆火，到头来嚷着热死了热死了。这一切在猫看来都是庸庸碌碌的。猫甚至断言道：人类不可能永远繁荣昌盛下去。嗯，我愿静候属于猫族时代的到来。从前是“人类一思考，上帝就发笑”，而现在是“人类一思考，猫君就发笑”。猫觉得人类模仿它们的声音时是愚蠢的，尤其是在抚摸它们的时候，因为根本不存在撒娇声，只有被撒娇声，因为我们期待的是，猫向我们撒娇，可难道不是我们在向猫撒娇？\n个体的荒谬，在人类的个性面前根本不值一提，就如同人类的个性得到完全解放以后，永远像一锅众口难调的羹汤。小说中苦沙弥、迷亭、寒月、东风和独仙时常在一起聊天，话题涉及哲学、艺术，爱情、生活等多个方面，这只“毒舌”的猫，就在无意识地引导和放大这些观点，“我认为这个世界上，没有比爱和美更受人尊重的了”，所以这本书里的观点，其实并不是完全的消极的，就像这只猫平静地看着这个世界，它对人类有过嘲讽，有过同情，它甚至没有自己的名字，当它失足淹死在水缸里的时候，对这个世界更多的是种悲天悯人吧！我们这个世界上有五种毒药，佛家所谓的“贪嗔痴慢疑”，作者提到“可没有任何一个人，能够全然抛开自己去研究外界，如果人类能够把自己疏离出来，那么疏离的瞬间，也就没有了自己”，人类常常不愿放过自己，更不愿放过别人，因为所有无解的问题，都可以制造一个意义出来，而我们早已习惯这一切。\n曾经有朋友问我，为什么喜欢猫这种动物，我回答说，因为我就像一只猫，一只特立独行的猫，对所有人都很友善和蔼，却喜欢独来独往。因为维护这种若即若离的关系，对我来说比任何事情都要困难。人类以为猫都是傲娇的动物，其实这是人类的一厢情愿，因为从智力上来说，猫的智力是不及狗的。猫自然对人是有感情的，不过在人类驯养动物的历程中，狗更聪明、更懂得如何向人类索取，我们所认定的感情，在狗的世界里或许并不是。人类难以理解的事物，所谓阳春白雪，所谓曲高和寡，不自然地背负上高冷的名声，对一只猫而已，到底是我们不了解猫，还是不了解我们自己。人总在试图驯化猫这种动物，可猫无非是人类的一种折射而已，它就像那些独立潇洒的人一样，不藉由粘人和撒娇来获取安全感，在这个世上没有谁会离不开谁。你走，我不必送你；你来，不管多大风多大雨，我都去接你。这是我——一只猫的自白。\n有时候，难免会觉得人类自作聪明，喜欢给世间的事物贴上不同的标签，譬如二哈、喵星人、汪星人、猫主子……可你知道猫如何评价人类的吗？作者说，“这些人虽然看起来快活，但是如果叩问他们的心底，却可以听见悲凉的回响”。为什么会听见悲凉的回响呢？大概是人类丰富而有趣的个性，不断地尝试挑战世俗的眼光，结果被世俗打败而变得世俗，这听起来简直就像是，英雄杀死魔王又变成魔王的故事的翻版。“每个人地位都提高，等同于每个人的地位都下降。人类不再做让自己委屈的事情，正是个人力量变强的证明；几乎不再插手别人的事情，反而是群体力量变弱的证明”。一点亏都不愿意吃，一点小便宜就要占，无一不是为了证明个人意志的强化，可人与人间的空间越来越狭窄，日益窘迫，为了扩充自己膨胀到近乎爆炸。人与人之间那点空间，是一切痛苦的根源，你说这还不算作悲凉吗？没有谁可以完全了解一个人，不完全认知是人类关系的反应剂，痛苦、误会、偏见……等等纷至沓来，你以为你模仿猫叫，猫就真的听懂了吗？\n夏目先生在后记里写道，“世事变迁就像猫的眼珠一样变幻莫测，短短几个月世间，就可以去那极乐世界，或者可以把薪水花光光。年底过去了，正月过去了，花朵凋谢，新叶又生。以后世界将如何变化，我不了解，只不过水缸中猫的瞳孔，应该可以凝成永恒”。我想，世界会一如既往地这样无奈下去，时间会一如既往地这样消逝下去，而你和我会一如既往地平庸且烦恼下去。假如有这样一只猫，通过瞳孔记下了我的生平，不知道它会如何评价我呢？就像在某一个下雨天，突然想到某一个人，单单是因为怕这世界里，从此再没了对方的音讯。可单单是想到又有什么意义呢？《笑傲江湖》里令狐冲率领江湖群雄，前往少林寺解救被困的圣姑，这个将来会成为他妻子的人，而眼下更是生死未知、前途未明，可在一片寂静中听到雪花簌簌落下时，他想到的却是：小师妹不知这时候不知在干甚么。及至岳林珊为林平之所杀，临死托付令狐冲替她照顾小林子，内心却又不知做何感想了吧……\n","date":"2018-03-06T08:57:48Z","image":"/posts/352037321/anna-k-_4etX6hBpqk-unsplash.jpg","permalink":"https://qinyuanpei.github.io/posts/352037321/","slug":"352037321","tags":["夏目漱石","日本文学","我是猫"],"title":"我是猫，一只特立独行的猫"},{"categories":["独立博客"],"content":"各位朋友，大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是 https://qinyuanpei.github.io .在曾经的一篇博客：《持续集成在 Hexo 自动化部署上的实践》中，我为大家分享了在线持续集成服务 Travis CI 的相关内容，在这篇文章中我们通过 Travis CI 为 Hexo 提供了自动部署的支持。其原理是 Github 为 Travis CI 分配一个 token，当我们向 Github 推送新的代码以后，Travis 就会从代码仓库中拉取代码，并通过 npm 安装依赖生成静态页面，我们将这些静态页面推送到 master 分支，即可完成对 Hexo 的部署操作。这个流程从去年 10 月份建立以来，一直运行得非常稳定，对我个人而言，随着博客里得内容越来越多，在本地生成静态页面需要 20 多秒得时间，而有了持续集成服务以后，我可以用这个时间去做更多的事情，当持续集成流程发生异常的时候，微信上会收到 Travis 发送的邮件，整个过程简直令人心情愉悦。\n今天想继续写点这个话题相关的内容，即如何通过 Travis CI 实现 Hexo 在 Github 和 Coding 的同步部署。显然，部署 Hexo 到 Github Pages 我们已经实现，今天我们着重来说 Coding Pages。为什么我们需要 Coding Pages 呢？主要从两个方面考虑，首先，因为 Github Pages 屏蔽了百度的爬虫，所以我们托管在 Github 上的博客，无法被搜索引擎正常收录；其次，由于 Github Pages 的服务器在国外，所以在国内博客的速度会受到影响，而且**\u0026ldquo;防火墙\u0026rdquo;**的国情决定了 Github 是一个不稳定的网站。曾经经历过短时间内无法使用 Github 的情形，故而，为了保证博客可以更加稳定地运行，我们必须为博客提供一个备份镜像，这就是我们今天要提到的 Coding Pages 服务啦。在正式使用这个服务前，我们首先简单介绍下这个服务。\n我们知道 Github Pages 是 Github 提供的静态页面托管服务，其初衷是为个人项目或者组织项目创建演示或者文档站点，而 Coding Pages 则是国内的代码托管平台 Coding 提供的类似服务，国内类似的代码托管平台还有码云、Gitlab 等。Coding Pages 支持自定义域名、SSL 等基本特性，随着官方不断对这一服务进行升级，目前该服务除支持静态页面部署以外，同时支持 PHP 和 MySQL 这类动态页面部署的特性。对 Hexo 来说，静态页面部署的特性完全可以支撑我们这个想法。我的想法是以 Github 作为代码的主仓库，其上面的 blog 分支存放博客的源代码， master 分支存放博客的静态页面，在此基础上，我们同时推送静态页面到 Github 和 Coding 的代码仓库，这样就可以实现两个平台的同步部署，这里的部署自然是指由 Travis 完成的自动化部署。整体的流程设想如下图所示：\n博客同步部署流程图\r通过这个流程图，我们可以注意到，新增加的工作量，主要体现在 Travis 向 Coding 的代码仓库推送静态页面，因此我们首先要有一个 Coding 的代码仓库。关于如何注册 Coding 及在 Coding 上创建代码仓库，这里不再详细赘述啦，大家可以自行百度、Google 或者阅读官方文档。Travis CI 的行为主要由 .travis.yml 这个文件来决定，要推送静态页面到 Coding 的代码仓库，Travis CI 需要有代码仓库的读写权限。顺着这个思路，尝试让 Coding 授权 给 Travis CI，结果从文档中发现 Travis CI 并不支持 Coding，而 Coding 官方支持的持续集成 flow.ci 需要使用者从 Docker 创建镜像，所以看起来这条路无法走通。从搜索引擎中检索相关问题，从 Git 工作机制的角度入手，可以想到三种常见思路，即 SSH Key、Hexo 的 deploy 插件和 HTTPS 协议。\n第一种思路是考虑让 Travis CI 的远程服务器共享本机的 SSH Key，通过 ssh-copy-id 命令即可实现，可问题是 Travis CI 每次创建虚拟机环境是变化的，因此我们无法确定目标主机的 IP 或者计算机名称等信息，这种思路不适合 Travis CI。而 Travis CI 官方同样提供了命令行工具来完成这个工作，因为 Travis CI 是基于 Ruby 开发而来，所以需要 Ruby 的环境支持，作为一个为逃避 Jekyll 而选择 Hexo 的人，我是不会让自己再受到 Ruby 的摧残的，所以这种思路基本放弃。第二种思路是使用 Hexo 提供的 deploy 插件，例如 hexo-deploy-git 这个插件支持通过 git 部署，而 Coding 和 Github 都支持 Git 相关的协议，所以可以考虑使用这个插件来完成这个操作，目前网络上可以检索到的资料，都是使用这个插件来完成同步部署。可是经过我一位使用过这个插件的朋友确定，该插件需要再执行 git 命令行期间输入用户名和密码，Travis CI 是不会给你机会输入用户名和密码的，所以这种思路再次放弃。第三种 HTTPS 协议，这个想都不用想是需要输入密码的，所以果断直接放弃。\n正所谓\u0026quot;行至水穷处，坐看云起时\u0026quot;，山重水复之间，柳暗花明之际，我意外发现 Coding 提供了和 Github 类似的\u0026quot;访问令牌\u0026quot;，我们在使用 Travis CI 的时候，实际上做了两步授权操作，第一次是授权 Travis CI 读取我们在 Github 上的仓库列表，这是一个通过 OAuth 授权的过程；第二次授权是授权 Travis CI 向指定仓库推送或者拉取内容，这是一个通过 Token 授权的过程。我们会在 Travis CI 的后台设置中将 Token 作为全局变量导出，这样我们就可以在 .travis.yml 文件中引用这些全局变量。我意识到这是一个值得一试的想法，首先我们在 Coding 的**”个人设置\u0026quot;页面中找到访问令牌**，新建一个新的访问令牌，这里我们选第一个权限即可，因为我们只需要为 Travis 提供基本的读写权限，这样我们会生成一个 Token，这里注意保存 Token，因为它在这里只显示这一次，我们将 Token 填写到 Travis CI 的后台，取名为 CO_Token 即可，依次如下图所示：\n在Coding中新建访问令牌\r在Coding中保存访问令牌\r在Travis中新建全局变量\r好了，现在有了 Token，就意味着 Travis CI 有权限向 Coding 推送或者拉取内容了，那么怎么让它工作起来呢？我们记得 Travis CI 有一个叫做 .travis.yml 的配置文件对吧？这里我们需要简单修改下这个文件，让 Travis CI 在生成静态页面以后同时推送静态页面到 Coding。修改后的关键配置如下，我已经写好了详细注释，关于这个文件配置可以参考这里，这里不再详细说明：\nafter_script: - cd ./public - git init - git config user.name \u0026#34;qinyuanpei\u0026#34; - git config user.email \u0026#34;qinyuanpei@163.com\u0026#34; - git add . - git commit -m \u0026#34;Update Blog By TravisCI With Build $TRAVIS_BUILD_NUMBER\u0026#34; # Github Pages - git push --force --quiet \u0026#34;https://${CI_TOKEN}@${GH_REF}\u0026#34; master:master # Coding Pages - git push --force --quiet \u0026#34;https://qinyuanpei:${CO_TOKEN}@${CO_REF}\u0026#34; master:master - git tag v0.0.$TRAVIS_BUILD_NUMBER -a -m \u0026#34;Auto Taged By TravisCI With Build $TRAVIS_BUILD_NUMBER\u0026#34; # Github Pages - git push --quiet \u0026#34;https://${CI_TOKEN}@${GH_REF}\u0026#34; master:master --tags # Coding Pages - git push --quiet \u0026#34;https://qinyuanpei:${CO_TOKEN}@${CO_REF}\u0026#34; master:master --tags branches: only: - blog env: global: # Github Pages - GH_REF: github.com/qinyuanpei/qinyuanpei.github.io # Coding Pages - CO_REF: git.coding.net/qinyuanpei/qinyuanpei.coding.me.git 好了，现在我们就可以同时部署博客到 Github 和 Coding 了，现在大家可以使用下面两种方式来访问我的博客。需要说明的是，使用 Coding Pages 的特性需要开启仓库的 Pages服务，并且 Coding 支持免费托管私有项目，虽然目前仓库的容量存在限制，对我们部署 Hexo 来说完全足够啦，下图是 Coding 上展示的提交历史，排版效果棒棒哒，哈哈，好了，以上就是这篇文章的内容啦，希望大家喜欢哦！\nCoding上展示的提交历史\rGithub Pages 镜像 Coding Pages 镜像 本文使用的 .travis.yml 文件可以从这里 获取哦！\n","date":"2018-02-27T10:45:04Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1113828794/","slug":"1113828794","tags":["CI","Hexo","Travis"],"title":"基于 Travis CI 实现 Hexo 在 Github 和 Coding 的同步部署"},{"categories":["数据分析"],"content":"最近微信迎来了一次重要的更新，允许用户对\u0026quot;发现\u0026quot;页面进行定制。不知道从什么时候开始，微信朋友圈变得越来越复杂，当越来越多的人选择\u0026quot;仅展示最近三天的朋友圈\u0026quot;，大概连微信官方都是一脸的无可奈何。逐步泛化的好友关系，让微信从熟人社交逐渐过渡到陌生人社交，而朋友圈里亦真亦幻的状态更新，仿佛在努力证明每一个个体的\u0026quot;有趣\u0026quot;。有人选择在朋友圈里记录生活的点滴，有人选择在朋友圈里展示观点的异同，可归根到底，人们无时无刻不在窥探着别人的生活，唯独怕别人过多地了解自己的生活。人性中交织着的光明与黑暗，像一只浑身长满刺的刺猬，离得太远会感觉到寒冷，而靠得太近则害怕被刺扎到。朋友圈就像过年走亲戚，即便你心中有一万个不痛快，总是不愿意撕破脸，或屏蔽对方，或不给对方看，或仅展示最后三天，于是通讯录里的联系人越来越多，朋友圈越来越大，可再不会有能真正触动你内心的\u0026quot;小红点\u0026quot;出现，人类让一个产品变得越来越复杂，然后说它无法满足人类的需求，这大概是一开始就始料不及的吧！\n引言 有人说，人性远比计算机编程更复杂，因为即使是人类迄今为止最伟大的发明——计算机，在面对人类的自然语言时同样会张惶失措 。人类有多少语言存在着模棱两可的含义，我认为语言是人类最大的误解，人类时常喜欢揣测语言背后隐藏的含义，好像在沟通时表达清晰的含义会让人类没有面子，更不用说网络上流行的猜测女朋友真实意图的案例。金庸先生的武侠小说《射雕英雄传》里，在信息闭塞的南宋时期，江湖上裘千丈的一句鬼话，就搅得整个武林天翻地覆。其实，一两句话说清楚不好吗？黄药师、全真七子、江南六怪间的种种纠葛，哪一场不是误会？一众儿武功震古烁今的武林高手，怎么没有丝毫的去伪存真的能力，语言造成了多少误会。\n可即便人类的语言复杂得像一本无字天书，可人类还是从这些语言中寻觅到蛛丝马迹。古人有文王\u0026quot;拘而演周易\u0026quot;、东方朔测字卜卦，这种带有\u0026quot;迷信\u0026quot;色彩的原始崇拜，就如同今天人们迷信星座运势一般，都是人类在上千年的演变中不断对经验进行总结和训练的结果。如此说起来，我们的人工智能未尝不是一种更加科学化的\u0026quot;迷信\u0026quot;，因为数据和算法让我们在不断地相信，这一切都是真实地。生活在数字时代的我们，无疑是悲哀的，一面努力地在别人面前隐藏真实地自己，一面不无遗憾地感慨自己无处遁逃，每一根数字神经都紧紧地联系着你和我，你不能渴望任何一部数字设备具备真正的智能，可你生命里的每个瞬间，都在悄然间被数据地折射出来。\n今天这篇文章会基于 Python 对微信好友进行数据分析，这里选择的维度主要有：性别、头像、签名、位置，主要采用图表和词云两种形式来呈现结果，其中，对文本类信息会采用词频分析和情感分析两种方法。常言道：工欲善其事，必先利其器也。在正式开始这篇文章前，简单介绍下本文中使用到的第三方模块：\nitchat：微信网页版接口封装 Python 版本，在本文中用以获取微信好友信息。 jieba：结巴分词的 Python 版本，在本文中用以对文本信息进行分词处理。 matplotlib： Python 中图表绘制模块，在本文中用以绘制柱形图和饼图 snownlp：一个 Python 中的中文分词模块，在本文中用以对文本信息进行情感判断。 PIL： Python 中的图像处理模块，在本文中用以对图片进行处理。 numpy： Python 中 的数值计算模块，在本文中配合 wordcloud 模块使用。 wordcloud： Python 中的词云模块，在本文中用以绘制词云图片。 TencentYoutuyun：腾讯优图提供的 Python 版本 SDK ，在本文中用以识别人脸及提取图片标签信息。 以上模块均可通过 pip 安装，关于各个模块使用的详细说明，请自行查阅各自文档。 数据分析 分析微信好友数据的前提是获得好友信息，通过使用 itchat 这个模块，这一切会变得非常简单，我们通过下面两行代码就可以实现：\nitchat.auto_login(hotReload = True) friends = itchat.get_friends(update = True) 同平时登录网页版微信一样，我们使用手机扫描二维码就可以登录，这里返回的 friends 对象是一个集合，第一个元素是当前用户。所以，在下面的数据分析流程中，我们始终取 friends[1:]作为原始输入数据，集合中的每一个元素都是一个字典结构，以我本人为例，可以注意到这里有 Sex、City、Province、HeadImgUrl、Signature 这四个字段，我们下面的分析就从这四个字段入手：\n好友信息结构展示\r好友性别 分析好友性别，我们首先要获得所有好友的性别信息，这里我们将每一个好友信息的 Sex 字段提取出来，然后分别统计出 Male、Female 和 Unkonw 的数目，我们将这三个数值组装到一个列表中，即可使用 matplotlib 模块绘制出饼图来，其代码实现如下：\ndef analyseSex(firends): sexs = list(map(lambda x:x[\u0026#39;Sex\u0026#39;],friends[1:])) counts = list(map(lambda x:x[1],Counter(sexs).items())) labels = [\u0026#39;Unknow\u0026#39;,\u0026#39;Male\u0026#39;,\u0026#39;Female\u0026#39;] colors = [\u0026#39;red\u0026#39;,\u0026#39;yellowgreen\u0026#39;,\u0026#39;lightskyblue\u0026#39;] plt.figure(figsize=(8,5), dpi=80) plt.axes(aspect=1) plt.pie(counts, #性别统计结果 labels=labels, #性别展示标签 colors=colors, #饼图区域配色 labeldistance = 1.1, #标签距离圆点距离 autopct = \u0026#39;%3.1f%%\u0026#39;, #饼图区域文本格式 shadow = False, #饼图是否显示阴影 startangle = 90, #饼图起始角度 pctdistance = 0.6 #饼图区域文本距离圆点距离 ) plt.legend(loc=\u0026#39;upper right\u0026#39;,) plt.title(u\u0026#39;%s的微信好友性别组成\u0026#39; % friends[0][\u0026#39;NickName\u0026#39;]) plt.show() 这里简单解释下这段代码，微信中性别字段的取值有 Unkonw、Male 和 Female 三种，其对应的数值分别为 0、1、2。通过 Collection 模块中的 Counter()对这三种不同的取值进行统计，其 items()方法返回的是一个元组的集合，该元组的第一维元素表示键，即 0、1、2，该元组的第二维元素表示数目，且该元组的集合是排序过的，即其键按照 0、1、2 的顺序排列，所以通过 map()方法就可以得到这三种不同取值的数目，我们将其传递给 matplotlib 绘制即可，这三种不同取值各自所占的百分比由 matplotlib 计算得出。下图是 matplotlib 绘制的好友性别分布图： 微信好友性别分析\r看到这个结果，我一点都不觉得意外，男女比例严重失衡，这虽然可以解释我单身的原因，可我不觉得通过调整男女比例就能解决问题，好多人认为自己单身是因为社交圈子狭小，那么是不是扩展了社交圈子就能摆脱单身呢？我觉得或许这样会增加脱单的概率，可幸运之神应该不会眷顾我，因为我的好运气早在我 24 岁以前就消耗完啦。在知乎上有一个热门的话题：现在的男性是否普遍不再对女性展开追求了？，其实哪里会有人喜欢孤独呢？无非是怕一次又一次的失望罢了。有的人并不是我的花儿，我只是恰好途径了她的绽放。曾经有人说我是一个多情的人，可她永远不会知道，我做出的每一个决定都炽热而悲壮。所谓\u0026quot;慧极必伤，情深不寿；谦谦君子，温润如玉\u0026quot;，世人苦五毒者大抵如此。\n好友头像 分析好友头像，从两个方面来分析，第一，在这些好友头像中，使用人脸头像的好友比重有多大；第二，从这些好友头像中，可以提取出哪些有价值的关键字。这里需要根据 HeadImgUrl 字段下载头像到本地，然后通过腾讯优图提供的人脸识别相关的 API 接口，检测头像图片中是否存在人脸以及提取图片中的标签。其中，前者是分类汇总，我们使用饼图来呈现结果；后者是对文本进行分析，我们使用词云来呈现结果。关键代码如下 所示：\ndef analyseHeadImage(frineds): # Init Path basePath = os.path.abspath(\u0026#39;.\u0026#39;) baseFolder = basePath + \u0026#39;\\\\HeadImages\\\\\u0026#39; if(os.path.exists(baseFolder) == False): os.makedirs(baseFolder) # Analyse Images faceApi = FaceAPI() use_face = 0 not_use_face = 0 image_tags = \u0026#39;\u0026#39; for index in range(1,len(friends)): friend = friends[index] # Save HeadImages imgFile = baseFolder + \u0026#39;\\\\Image%s.jpg\u0026#39; % str(index) imgData = itchat.get_head_img(userName = friend[\u0026#39;UserName\u0026#39;]) if(os.path.exists(imgFile) == False): with open(imgFile,\u0026#39;wb\u0026#39;) as file: file.write(imgData) # Detect Faces time.sleep(1) result = faceApi.detectFace(imgFile) if result == True: use_face += 1 else: not_use_face += 1 # Extract Tags result = faceApi.extractTags(imgFile) image_tags += \u0026#39;,\u0026#39;.join(list(map(lambda x:x[\u0026#39;tag_name\u0026#39;],result))) labels = [u\u0026#39;使用人脸头像\u0026#39;,u\u0026#39;不使用人脸头像\u0026#39;] counts = [use_face,not_use_face] colors = [\u0026#39;red\u0026#39;,\u0026#39;yellowgreen\u0026#39;,\u0026#39;lightskyblue\u0026#39;] plt.figure(figsize=(8,5), dpi=80) plt.axes(aspect=1) plt.pie(counts, #性别统计结果 labels=labels, #性别展示标签 colors=colors, #饼图区域配色 labeldistance = 1.1, #标签距离圆点距离 autopct = \u0026#39;%3.1f%%\u0026#39;, #饼图区域文本格式 shadow = False, #饼图是否显示阴影 startangle = 90, #饼图起始角度 pctdistance = 0.6 #饼图区域文本距离圆点距离 ) plt.legend(loc=\u0026#39;upper right\u0026#39;,) plt.title(u\u0026#39;%s的微信好友使用人脸头像情况\u0026#39; % friends[0][\u0026#39;NickName\u0026#39;]) plt.show() image_tags = image_tags.encode(\u0026#39;iso8859-1\u0026#39;).decode(\u0026#39;utf-8\u0026#39;) back_coloring = np.array(Image.open(\u0026#39;face.jpg\u0026#39;)) wordcloud = WordCloud( font_path=\u0026#39;simfang.ttf\u0026#39;, background_color=\u0026#34;white\u0026#34;, max_words=1200, mask=back_coloring, max_font_size=75, random_state=45, width=800, height=480, margin=15 ) wordcloud.generate(image_tags) plt.imshow(wordcloud) plt.axis(\u0026#34;off\u0026#34;) plt.show() 这里我们会在当前目录新建一个 HeadImages 目录，用以存储所有好友的头像，然后我们这里会用到一个名为 FaceApi 类，这个类由腾讯优图的 SDK 封装而来，这里分别调用了人脸检测和图像标签识别两个 API 接口，前者会统计\u0026quot;使用人脸头像\u0026quot;和\u0026quot;不使用人脸头像\u0026quot;的好友各自的数目，后者会累加每个头像中提取出来的标签。其分析结果如下图所示： 可以注意到，在所有微信好友中，约有接近 1/4 的微信好友使用了人脸头像， 而有接近 3/4 的微信好友没有人脸头像，这说明在所有微信好友中对\u0026quot;颜值 \u0026ldquo;有自信的人，仅仅占到好友总数的 25%，或者说 75%的微信好友行事风格偏低调为主，不喜欢用人脸头像做微信头像。这是否说明\u0026quot;好看的皮囊\u0026quot;并非是千篇一律，长得好看的人实在是少数中的少数。所以，当女生的妆容越来越向着\u0026quot;韩式半永久粗平眉\u0026rdquo;、\u0026ldquo;瓜子脸\u0026quot;和\u0026quot;大红唇\u0026quot;靠拢的时候，当男生的服饰越来越向着\u0026quot;大背头\u0026rdquo;、\u0026ldquo;高领毛衣\u0026quot;和\u0026quot;长款大衣\u0026quot;靠拢的时候，我们能不能真正得个性一次。生命中有太多被世俗绑架着的事情，既要和别人不一样 ，同时还要和大多数人一样，这是人生在世的无可奈何。考虑到腾讯优图并不能真正得识别\u0026quot;人脸\u0026rdquo;，我们这里对好友头像中的标签再次进行提取，来帮助我们了解微信好友的头像中有哪些 关键词，其分析结果如图所示： 微信好友头像标签词云展示\r通过词云，我们可以发现：在微信好友中的签名词云中，出现频率相对较高的关键字有：女孩、树木、房屋、文本、截图、卡通、合影、天空、大海。这说明在我的微信好友中，好友选择的微信头像主要有日常、旅游、风景、截图四个来源，好友选择的微信头像中风格以卡通为主，好友选择的微信头像中常见的要素有天空、大海、房屋、树木。通过观察所有好友头像，我发现在我的微信好友中，使用个人照片作为微信头像的有 15 人，使用网络图片作为微信头像的有 53 人，使用动漫图片作为微信头像的有 25 人，使用合照图片作为微信头像的有 3 人，使用孩童照片作为微信头像的有 5 人，使用风景图片作为微信头像的有 13 人，使用女孩照片作为微信头像的有 18 人，基本符合图像标签提取的分析结果。\n好友签名 分析好友签名，签名是好友信息中最为丰富的文本信息，按照人类惯用的\u0026quot;贴标签\u0026quot;的方法论，签名可以分析出某一个人在某一段时间里状态，就像人开心了会笑、哀伤了会哭，哭和笑两种标签，分别表明了人开心和哀伤的状态。这里我们对签名做两种处理，第一种是使用用结巴分词进行分词后生成词云，目的是了解好友签名中的关键字有哪些，哪一个关键字出现的频率相对较高；第二种是使用 SnowNLP 分析好友签名中的感情倾向，即好友签名整体上是表现为正面的、负面的还是中立的，各自的比重是多少。这里提取 Signature 字段即可，其核心代码如下：\ndef analyseSignature(friends): signatures = \u0026#39;\u0026#39; emotions = [] pattern = re.compile(\u0026#34;1f\\d.+\u0026#34;) for friend in friends: signature = friend[\u0026#39;Signature\u0026#39;] if(signature != None): signature = signature.strip().replace(\u0026#39;span\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;class\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;emoji\u0026#39;, \u0026#39;\u0026#39;) signature = re.sub(r\u0026#39;1f(\\d.+)\u0026#39;,\u0026#39;\u0026#39;,signature) if(len(signature)\u0026gt;0): nlp = SnowNLP(signature) emotions.append(nlp.sentiments) signatures += \u0026#39; \u0026#39;.join(jieba.analyse.extract_tags(signature,5)) with open(\u0026#39;signatures.txt\u0026#39;,\u0026#39;wt\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) as file: file.write(signatures) # Sinature WordCloud back_coloring = np.array(Image.open(\u0026#39;flower.jpg\u0026#39;)) wordcloud = WordCloud( font_path=\u0026#39;simfang.ttf\u0026#39;, background_color=\u0026#34;white\u0026#34;, max_words=1200, mask=back_coloring, max_font_size=75, random_state=45, width=960, height=720, margin=15 ) wordcloud.generate(signatures) plt.imshow(wordcloud) plt.axis(\u0026#34;off\u0026#34;) plt.show() wordcloud.to_file(\u0026#39;signatures.jpg\u0026#39;) # Signature Emotional Judgment count_good = len(list(filter(lambda x:x\u0026gt;0.66,emotions))) count_normal = len(list(filter(lambda x:x\u0026gt;=0.33 and x\u0026lt;=0.66,emotions))) count_bad = len(list(filter(lambda x:x\u0026lt;0.33,emotions))) labels = [u\u0026#39;负面消极\u0026#39;,u\u0026#39;中性\u0026#39;,u\u0026#39;正面积极\u0026#39;] values = (count_bad,count_normal,count_good) plt.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;simHei\u0026#39;] plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;] = False plt.xlabel(u\u0026#39;情感判断\u0026#39;) plt.ylabel(u\u0026#39;频数\u0026#39;) plt.xticks(range(3),labels) plt.legend(loc=\u0026#39;upper right\u0026#39;,) plt.bar(range(3), values, color = \u0026#39;rgb\u0026#39;) plt.title(u\u0026#39;%s的微信好友签名信息情感分析\u0026#39; % friends[0][\u0026#39;NickName\u0026#39;]) plt.show() 通过词云，我们可以发现：在微信好友的签名信息中，出现频率相对较高的关键词有：努力、长大、美好、快乐、生活、幸福、人生、远方、时光、散步。果然我的微信好友都是温暖、正直的好青年啊！ :smile:其实，签名这个设定，从某种程度上是在反映人的一种心态，人在年轻时不免\u0026quot;为赋新词强说愁\u0026quot;，等到你真正到了这个精神境界，突然发现年轻时图样图森破，或许这就是我们不愿意让别人了解过去的原因，因为伴随着人的成长，某一种瞬间的状态简直不忍直视，QQ 空间陪伴了我们这代人的整个青春，令人印象深刻的\u0026quot;那年今日\u0026quot;功能，有时让我们感到回忆的温暖，有时让我们感到岁月的萧杀，\u0026ldquo;当时只道是寻常\u0026quot;的物是人非，\u0026ldquo;回首向来萧瑟处\u0026quot;的淡定从容，\u0026ldquo;今夕复何夕\u0026quot;的失落惆怅……都在这一行行签名里留下深深浅浅的印记。在知乎上有关于签名的话题讨论，对此感兴趣的朋友不妨找时间看看。:smile:\n微信好友签名信息词云展示\r通过柱状图，我们可以发现：在微信好友的签名信息中，正面积极的情感判断约占到 55.56%，中立的情感判断约占到 32.10%，负面消极的情感判断约占到 12.35%。这个结果和我们通过词云展示的结果基本吻合，这说明在微信好友的签名信息中，约有 87.66%的签名信息，传达出来都是一种积极向上的态度。朋友圈中基本上有两类用户，第一类用户使用朋友圈记录自己的生活，第二类用户使用朋友圈输出自己的观点。显然，对于第二类用户，它并不介意别人了解它的过去，它更在乎它从始至终输出的观点是否一致。所以，不管朋友圈里别人在或晒美食、或晒旅游、或秀恩爱、或晒宝宝、或煲鸡汤等等，在我看来这都是一种生活方式，精神层次和物质层次比你高的人群，觉得你朋友圈里的内容\u0026quot;无趣\u0026rdquo;，这是符合人类一贯的认知方式的，在大多数情况下，反而是那些和你层次差不多的人群，对不熟悉的人或者事物妄加判断，如果你不喜欢我朋友圈里的内容，请直接屏蔽我就好，因为这样我们还可以做朋友；如果你因为喜欢 A 而在我这里和我说 B 不好，这就真的是三观不合啦。我相信没有完全兴趣匹配的两个人，即使是男女朋友或者情侣之间，总之人与人相处嘛，真诚和互相尊重是基本要求。\n微信好友签名信息情感分析展示\r好友位置 分析好友位置，主要通过提取 Province 和 City 这两个字段。Python 中的地图可视化主要通过 Basemap 模块，这个模块需要从国外网站下载地图信息，使用起来非常的不便。百度的ECharts在前端使用的比较多，虽然社区里提供了pyecharts项目，可我注意到因为政策的改变，目前 Echarts 不再支持导出地图的功能，所以地图的定制方面目前依然是一个问题，主流的技术方案是配置全国各省市的 JSON 数据，这里博主使用的是BDP 个人版，这是一个零编程的方案，我们通过 Python 导出一个 CSV 文件，然后将其上传到 BDP 中，通过简单拖拽就可以制作可视化地图，简直不能再简单，这里我们仅仅展示生成 CSV 部分的代码：\ndef analyseLocation(friends): headers = [\u0026#39;NickName\u0026#39;,\u0026#39;Province\u0026#39;,\u0026#39;City\u0026#39;] with open(\u0026#39;location.csv\u0026#39;,\u0026#39;w\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;,newline=\u0026#39;\u0026#39;,) as csvFile: writer = csv.DictWriter(csvFile, headers) writer.writeheader() for friend in friends[1:]: row = {} row[\u0026#39;NickName\u0026#39;] = friend[\u0026#39;NickName\u0026#39;] row[\u0026#39;Province\u0026#39;] = friend[\u0026#39;Province\u0026#39;] row[\u0026#39;City\u0026#39;] = friend[\u0026#39;City\u0026#39;] writer.writerow(row) 下图是 BDP 中生成的微信好友地理分布图，可以发现：我的微信好友主要集中在宁夏和陕西两个省份。数字时代的神经牵动着每一个社交关系链的人，我们想要竭力去保护的那点隐私，在这些数据中一点点地折射出来。人类或许可以不断地伪装自己，可这些从数据背后抽离出来的规律和联系不会欺骗人类。数学曾经被人称为最没有用的学科，因为生活中并不需要神圣而纯粹的计算，在不同的学科知识里，经验公式永远比理论公式更为常用。可是此时此刻，你看，这世界就像一只滴滴答答转动着的时钟，每一分每一秒都是严丝合缝的。 微信好友地理分布图\r本文小结 写这篇文章的时候，我一直不知道该如何下笔，因为微信是一个神奇的存在，它是一个国民级别的全民 APP，所以，微信的产品设计一直都是一个有趣的现象，从最初底部 Tab 的数目、每个 Tab 的名称、\u0026ldquo;发现\u0026quot;页面的定制、小程序入口、朋友圈入口到朋友圈评论等等一系列的设计细节，都是值得我们透过人性和心理去研究的。即使是被人们封神的\u0026quot;张小龙\u0026rdquo;，在面对结构最为复杂的中国用户群体的时候，他的潇洒中依旧不免充满无奈，从对朋友圈的置之不理就可以看出，这是一个怎么做都不会让人满意的功能，任何一个生态在面对巨大的用户群体的时候，功能的增减就会变成一个难题，所谓\u0026quot;林子大了什么鸟都有\u0026rdquo;，知乎面对的是同样的问题，营销类公众号在不断消费社会话题的同时，引导着一批又一批粉丝的价值取向，人类总渴望着别人了解自己，可人类真的了解自己吗？这篇博客是我对数据分析的又一次尝试，主要从性别、头像、签名、位置四个维度，对微信好友进行了一次简单的数据分析，主要采用图表和词云两种形式来呈现结果。总而言之一句话，\u0026ldquo;数据可视化是手段而并非目的\u0026rdquo;，重要的不是我们在这里做了这些图出来，而是从这些图里反映出来的现象，我们能够得到什么本质上的启示，我一位朋友问我怎么什么都想抓取，为什么啊，因为我不懂人类啊！\n","date":"2018-02-24T12:50:52Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2805694118/","slug":"2805694118","tags":["Python","Wechat","matplotlib"],"title":"基于 Python 实现的微信好友数据分析"},{"categories":["独立博客"],"content":"各位朋友，大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是：https://qinyuanpei.github.io。首先在这里祝大家春节快乐，作为过完年以后的第一篇文章，博主想写点内容风格相对轻松的内容。自从博主的博客采用 TravisCI 提供的持续集成(CI)服务以以来，博客的更新部署变得越来越简单，所有的流程都被简化为 Git 工作流下的 提交(commit) 和 推送(push) 操作。考虑到博客是托管在 Github 上的，一直希望可以自动更新仓库主页的 README 文件，这样可以显示每次提交代码后的变更历史。基于这样一个构想，我想到了为博客生成目录并自动更新 README，其好处是可以为读者建立良好的文档导航，而且 Markdown 是一种简单友好的文档格式，Github 等代码托管平台天生就支持 Markdown 文档的渲染。关于博客采用 TravisCI 提供持续集成(CI)服务相关内容，可以参考 持续集成在 Hexo 自动化部署上的实践 这篇文章。\n好了，现在考虑如何为博客生成目录，我们这里需要三个要素，即标题、链接和时间。标题和时间可以直接从 _posts 目录下的 Markdown 文档中读取出来，链接从何而来呢？我最初想到的办法是读取每个 Markdown 文档的文件名，因为我的使用习惯是采用英文命名，这样当博客的永久链接(permalink)采用默认的 :year/:month/:day/:title/ 形式时，每个 Markdown 文档的文件名等价于文章链接。事实证明这是一个愚蠢的想法，因为当你改变永久链接(permalink)的形式时，这种明显投机的策略就会彻底的失败。相信你在浏览器种打开这篇文章时，已然注意到链接形式发生了变化，当然这是我们在稍后的文章中讨论的话题啦。至此，我们不得不寻找新的思路，那么这个问题该如何解决呢？\n我意识到我的博客配置了 hexo-generator-json-content 插件，这个插件最初的目的是为博客提供离线的搜索能力，该插件会在博客的根目录里生成一个 content.json 文件，而这个文件中含有我们想要的一切信息，因此我们的思路转变为解析这个文件，人生苦短啊，我果断选择了我最喜欢的 Python，这里我们会提取出所有的文章信息，按照日期由近到远排序后生成列表。Python 强大到让我觉得这篇文章无法下笔，所以这里直接给出代码啦：\n# -*- coding: utf-8 -*- import os import re import sys import json import datetime # 文档实体结构定义 class Post: def __init__(self,date,link,title): self.date = date self.link = link self.title = title def getTitle(self): return self.title def getLink(self): return \u0026#39;https://qinyuanpei.github.io/\u0026#39; + self.link def getDate(self): d = re.findall(r\u0026#39;\\d{4}-\\d{1,2}-\\d{1,2}\u0026#39;,self.date)[0] t = re.findall(r\u0026#39;\\d{2}:\\d{2}:\\d{2}\u0026#39;,self.date)[0] dt = \u0026#39;%s %s\u0026#39; % (d,t) return datetime.datetime.strptime(dt,\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) # 从JSON中加载文档数据 def loadData(): json_file = open(\u0026#39;./public/content.json\u0026#39;,\u0026#39;rb\u0026#39;) json_data = json.load(json_file) for item in json_data: yield Post(item[\u0026#39;date\u0026#39;],item[\u0026#39;path\u0026#39;],item[\u0026#39;title\u0026#39;]) # 从列表生成Markdown文件 def mkMarkdown(items): mdfile = open(\u0026#39;README.md\u0026#39;,mode=\u0026#39;wt\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) itemTpl = \u0026#39;* {0} - [{1}]({2})\\n\u0026#39; for item in items: mdfile.write(itemTpl.format( datetime.datetime.strftime(item.getDate(),\u0026#39;%Y-%m-%d\u0026#39;), item.getTitle(), item.getLink() )) if(__name__ == \u0026#34;__main__\u0026#34;): items = sorted(loadData(),key=lambda x:x.getDate(),reverse=True) mkMarkdown(items) 这里需要注意的有两个地方，第一，从 JSON 中解析出来的日期形式为：2018-02-23T01:32:45.000Z。对于这个形式的日期，博主先后尝试了内建的 time 模块和第三方的 datetime 模块，发现均无法直接转换为日期类型，所以首先采用正则匹配出日期和时间，然后再组合为标准的 %Y-%m-%d %H:%M:%S 的格式，这样就可以使用 datetime 模块进行处理啦，我还是想吐槽人类对各种各样 format 的执着，这些通配符在不同的语言中存在差别，就像 SQL 和正则引擎或多或少地存在兼容性问题一样。如果有朋友知道如何对这种日期形式进行转换，欢迎在博客中评论留言，再次谢谢大家。第二，使用内置函数 sorted()对数据进行排序，lambda 表达式使用起来非常棒，因为默认是升序排列地，而我们需要的是日期由近到远，所以这里选择了降序排列。\n现在我们更新博客时的流程将发生变化，首先通过 hexo generate 或 hexo g 命令生成博客，这样 Hexo 会为我们生成 content.json，然后我们执行这段 Python 脚本，就可以生成 REAMD.md 文件，这里我们将这个文件推送到 blog 分支。相对应地，我们修改 TravisCI 的脚本文件 .travis.yml 文件如下：\nscript: - hexo clean - hexo generate - cp README.md ./public/README.md 显然，这是告诉 TravisCI 在生成博客以后，将 README.md 文件复制到输出文件，这样当我们推送博客(指生成的静态页面)到 master 分支的时候，它会和 blog 分支同步共享同一份 README 。我想一定有朋友会问我，难道生成 README.md 文件的步骤不能交给 TravisCI 来处理？一定要在推送到 blog 分支以前手动地去执行脚本吗？我最初尝试过让 TravisCI 去执行这个 Python 脚本，可我发现一个残酷的事实时，我们这个虚拟机环境是 nodejs 的，这在我们定义 .travis.yml 文件时就指定了，因此这个环境中可能是没有 Python 支持的。起初我以为 Linux 系统自带 Python ， 因此尝试在 .travis.yml 文件中使用 pip 安装相关依赖，然后我发现持续集成服务华丽丽地挂了，因为 TravisCI 默认的 Python 版本是 Python2.7 , 除非我们指定的是一个 Python 的语言环境，所以这种想法不得不作罢，暂时就手动更新好啦。\n好了，这篇文章核心的内容就这么多，下面想说些关于 Hexo 的延伸话题。 Hexo 是一个基于 nodejs 的静态博客生成器，按理说使用 nodejs 去扩展功能是最佳的实践方式，所以即使 Python 再强大，我们在这里看到的依然存在着天然的割裂感， 我们能不能将执行 Python 脚本的这个过程合并到 hexo generate 或者 hexo g 这个步骤中去呢？ 通过官方文档中关于事件和生成器的描述，我们获得了两种新的思路，分别是在生成页面以后通过 child_process 模块调用 python 脚本、通过 Locals 变量获取全部文章信息后生成 Markdown。从方案是否优雅的角度上来讲，我个人更倾向于第二种方案。基本的代码如下：\n//方案一 hexo.on(\u0026#39;generateAfter\u0026#39;, function(post){ //TODO:通过content.json文件生成markdown文档 }); //方案二 hexo.extend.generator.register(\u0026#34;markdown\u0026#34;, function(locals){ var posts = locals.posts; //TODO:通过posts属性生成markdown文档 }); 显然，我是不会写 nodejs 的，如果有时间和精力的话，我可能会考虑采用第二种方案写一个插件，可是像我这么懒的一个人，还是不要提前立 flag 啦，毕竟人生苦短呐，我都选择使用 Python 这门语言来写啦，我干嘛非要再花时间去迎合它呢？好啦，这篇文章就是这样啦，本文中的脚本可以到 这里 来获取，本文生成的目录可以到 这里 来访问，再次谢谢大家！\n","date":"2018-02-23T09:32:45Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1329254441/","slug":"1329254441","tags":["Python","Github","Script"],"title":"使用 Python 生成博客目录并自动更新 README"},{"categories":["生活感悟"],"content":"\r周末花了一个晚上的时间看了部电影，由黄渤主演的《蛋炒饭》。有人说，这是一部刻意模仿《阿甘正传》的电影，充满胶片质感的纪录片风格，相似的镜头语言和表现手法，无一不在努力告诉你，这是一部本土化的《阿甘正传》。可是如同巧克力之于蛋炒饭，两种截然不同的食物会有不同的味道，电影所反映的实则是两种不同的内涵。如果说《阿甘正传》代表的是极具美国精神的励志故事，那么《蛋炒饭》代表的则是小人物为理想打拼的乌托邦。说《蛋炒饭》是本土化的《阿甘正传》其实不无道理，因为电影里充满了太多相似，这种在由时代感打磨出的细腻的情感，让我觉得这是一部值得一看的电影。\n电影《阿甘正传》经典开头\r电影开头苏茉莉坐在游乐园长凳上自下而上的长镜头，不禁让人联想到《阿甘正传》的开场，联想到那片空中摇曳着的羽毛；电影中反复出现的台词，来自大卫父亲的那句：做蛋炒饭要慢要慢，一如《阿甘正传》里的经典台词：人生就像一盒巧克力，你永远不知道下一块会是哪种？电影令我印象深刻的地方，在于它具厚重的时代感，中美建交、改革开放、金融危机、邓丽君、摇滚音乐、周杰伦、周笔畅……看起来王大卫好像和阿甘一样，参与并影响了众多历史事件。可我的理解是，阿甘最终成为了流浪街头的大富翁，而王大卫一直一无所有，因为他一辈子都在做一件事情，那就是做蛋炒饭。\n最美好的年纪\r整个电影采用的是倒叙的表现手法。主人公王大卫是一个智商不太高，而且有语言表达障碍的“傻子”，他第一次出场是替好兄弟打抱不平，结果他就像星爷《功夫》里的少年一样，以为自己的武功盖世无双，结果自然是被人打得头破血流；他书包里藏着的“复仇”的板砖，在外国友人访华的时候，一板砖破坏了中美友谊，结果自然是被校长通报批评；他家祖上是宫廷御厨，父亲即将退休食堂安排他去顶岗，一心想做厨师的他差点烧了厨房，结果自然是迫使父亲替他求情以保住工作；他从小喜欢的女孩苏茉莉要和好兄弟发生关系，她吻了他一下叫他在门口把风，结果自然是茉莉被渣男欺骗然后甩了他；他用母亲变卖古董的钱来了饭店，好兄弟嫉妒他做了老板，骗走他的产业后远走高飞，结果自然是他从老板变成杂工。\n有些爱就像一阵风\r看这个电影的时候，我一直觉得人生是绝望的，父母先后离开人世，哥哥在面前自杀而死，喜欢的女生抛弃自己，好兄弟发小欺骗自己……周围人对他全部是冷嘲热讽，每次发工资的时候，嘲弄他什么时候攒够钱赎回饭店；每次娱乐圈有绯闻传来的时候，说他“媳妇”苏茉莉又登上杂志封面；“好兄弟“因为金融危机背负债务，他就拎着辛苦攒下的一袋子零钱去帮他还债，结果半夜“好兄弟”开着车跑了……我不知道，我们该不该认同这种无条件“傻”的行为，电影想告诉我们的或许是“以德报怨，以德服人”这种儒家的传统思想，即在复杂的社会中，如何以一种淳朴单纯的心态，追寻本心，不畏曲折。所谓“众生虽苦，诸恶莫作”，这个社会复杂险恶是现实，可温润善良则是一种选择。我们抱怨时代给我们选择狭窄，因为我们放弃了那些艰难的选择，最终选择大多数人选择的那条路。\n李红兵：人都是会变的\r可我还是想说，无条件的善良是懦弱，我们选择善良，是因为我们不想伤害别人，可在今天这样一个时代，善良常常被当做是懦弱的表现，一个人努力让心变狠变硬，或许会达到通常意义上的成功，可成功的定义从来都是被人绑架着的，电影中“好兄弟”登上了“胡弄排行榜”，我不知道这是不是导演的一种讽刺，一个人靠着骗取好兄弟的产业而发家，即使收获了声名和利益，当他一个人在高速公路上疾驶，打算远走他乡的时候，内心是不是会有一点羞愧和遗憾呢？我们说，这是一部乌托邦式的电影，因为现实中像李红兵这样的人，绝对不会陡然间良心发现，把饭店归还王大卫，甚至王大卫拉开窗帘阳光照射进来，阳光下苏茉莉牵着女儿的手对他微笑，大卫的蛋炒饭得到溥仪认可，入选国际非遗名录……我宁愿相信，这是一种美好的理想。\n大卫做蛋炒饭\r大卫在电影中做过三次蛋炒饭，第一次是他顶替父亲的岗位到食堂工作，结果想做厨师的他差点烧了厨房，付出的代价是，因为御厨身份而自豪的父亲，向一辈子没低过头的食堂经理低头；第二次是他陪苏茉莉到医院流产，打完胎的苏茉莉说自己饿，他冒着被饭店人追打的危险，给苏茉莉做了一份蛋炒饭，付出的代价是，苏茉莉同他告别，独自到南方发展演艺事业；第三次是李红兵归还了饭店，成名后的王大卫，当着记者的面做了一次蛋炒饭，溥仪评价大卫的蛋炒饭，比他爷爷做得还要好，蛋炒饭入选国际非遗名录。大卫的一生都在做一件事情，那就是做蛋炒饭，这种专注是他和阿甘不一样的地方，在一个浮躁的时代，我们每天都在追逐都在忙碌，可我们追逐的是什么呢？或许是一盘蛋炒饭的安宁，从这个角度来说，蛋炒饭这种食物，真的是质朴而简单的存在啦。\n有些人像你生命里的天使\r如果你看过《阿甘正传》这部电影，会觉得苏茉莉和珍妮这两个角色是相似的。她们看起来都知道自己想要些什么，苏茉莉选择发展演艺事业进入娱乐圈，珍妮一直知道自己想要什么，她想做一名歌手，为了唱歌吃尽苦头，在酒吧、街头甚至任何可以唱歌的地方，只有有场合给她吉他，她就可以唱，甚至为了唱歌而穷困潦倒到卖身吸毒……她可以真挚地给成名的阿甘一个拥抱，而不愿依附他达到自己成名的目的，她深爱阿甘的同时，深知阿甘不会同自己结婚，因为童年的经历让她内心无比自卑，她始终打不开心里的结，她选择为阿甘生下孩子，这是一种自我成全。\n愿茉莉一直这样美好\r苏茉莉成名后取艺名苏菲，据说这个人物原型来自王菲，苏茉莉在成名过程中不断豪门势力，从最初回北京办演唱会拉赞助，到嫁入豪门以后受不了娱乐圈绯闻骚扰，进而宣布退出娱乐圈回到北京，她是一个不知道自己想要什么的人，直到故事最后她终于发现，原来那个深爱着自己的人，一直就在默默地等自己回来。所以对比两部电影中的女主，就可以发现，两部电影阐述的观点是截然相反的，即阿甘是迷茫的，而珍妮是坚定的；王大卫是坚定的，而苏茉莉是迷茫的。当你不知道想要什么的时候，不妨慢下来做份蛋炒饭，或许你就会找到答案。\n你饿不饿，我做蛋炒饭给你吃呀\r故事接近尾声的时候，王大卫再次登上三个人从小便常去的城楼，猛然间看到三个孩子正在那里玩耍，孩子们说这是他们的地盘，王大卫还是像从前一样，知趣地准备转身离开，阳光照耀城楼的刹那，我分明看见他脸上满意的笑容，那种历经沧桑后初心不改的从容。每个人，都既注定的命运，同时有偶然，两者都在同时发生，就像那片羽毛，努力飘啊飘啊，终于飘到阿甘脚下，然后又无可奈何的飘离阿甘。羽毛不是飞鸟，无法掌控飞行的方向，其实很想和你在一起，但偏偏风把距离吹得好远。如果可以的话，我想你和我一样，都喜欢蛋炒饭。你饿不饿，我做蛋炒饭给你吃呀？\n","date":"2018-02-10T16:12:25Z","image":"/posts/1933583281/de7ce9255d9d7bdcfc76dac13a881a46.jpg","permalink":"https://qinyuanpei.github.io/posts/1933583281/","slug":"1933583281","tags":["蛋炒饭","电影","影评"],"title":"愿你和我一样喜欢蛋炒饭"},{"categories":["编程语言"],"content":" 在过去一年多的时间里，我尝试改变博客的写作风格，努力让自己不再写教程类文章，即使在这个过程中，不断地面临着写作内容枯竭的痛苦。因为我渐渐地意识到，告诉别人如何去做一件事情，始终停留在\u0026quot;术\u0026quot;的层面，而比这个更为重要的是，告诉别人为什么要这样做，这样就可以过渡到\u0026quot;道\u0026quot;的层面。古人云：形而上者谓之道，形而下者谓之器。我们常常希望通过量变来产生质变，可是如果在这个过程中不能及时反思和总结，我们认为的努力或许仅仅是重复的劳作而已。如你所见，在这篇文章里，我们将通过 Python 和 Windows 注册表实现壁纸切换功能，主要涉及到的 Python 中的 requests、pyinstaller 这两个模块的使用，希望大家喜欢。\n故事缘由 人们常常相信事出有因，可这世界上有些事情，哪里会有什么原因啊，比如喜欢与不喜欢。做这样一个小功能的初衷，起源于我对桌面壁纸的挑剔。作为一个不完全的强迫症患者，我需要花费大量时间去挑选一张壁纸，丝毫不亚于在网上挑选一件喜欢的商品。我注意到知乎上有这样的话题：有哪些无版权图片网站值得推荐？，因此对于桌面壁纸的筛选，我渐渐地开始摆脱对搜索引擎的依赖，我个人比较喜欢Pexels和Unsplash这两个网站，所以我想到了从这两个网站抓取图片来设置 Windows 壁纸的方案。市面上类似的商业软件有百度壁纸、搜狗壁纸等，可这些软件都不纯粹，或多或少地掺杂了额外功能，个中缘由想来大家都是知道的。联想到微信最新版本的更新，\u0026ldquo;发现\u0026quot;页面支持所有项目的隐藏，甚至是盟友京东的电商入口和腾讯最赚钱的游戏入口，这让我开始正视腾讯这家公司，我收回曾经因为抄袭对腾讯产生的不满，腾讯是一家值得尊重的互联网公司。做一个纯粹的应用程序，这就是我的初心。\n设计实现 好了，现在我们考虑如何来实现这个功能，我们的思路是从Unsplash这个网站抓取图片，并将其存储在指定路径，然后通过 Windows API 完成壁纸的设置。Python 脚本会通过 pyinstaller 模块打包成可执行文件，我们通过修改注册表的方式，在右键菜单内加入切换壁纸的选项，这样我们可以直接通过右键菜单实现壁纸切换功能。在编写脚本的时候，起初想到的是抓包这样的常规思路，因为请求过程相对复杂而失败，后来意外地发现官方提供了 API 接口。事实上Pexels和Unsplash都提供了 API 接口，通过调用这些 API 接口，我们的探索进行得非常顺利，下面是具体脚本实现：\n# Query Images searchURL = \u0026#39;https://unsplash.com/napi/search?client_id=%s\u0026amp;query=%s\u0026amp;page=1\u0026#39; client_id = \u0026#39;fa60305aa82e74134cabc7093ef54c8e2c370c47e73152f72371c828daedfcd7\u0026#39; categories = [\u0026#39;nature\u0026#39;,\u0026#39;flowers\u0026#39;,\u0026#39;wallpaper\u0026#39;,\u0026#39;landscape\u0026#39;,\u0026#39;sky\u0026#39;] searchURL = searchURL % (client_id,random.choice(categories)) response = requests.get(searchURL) print(u\u0026#39;正在从Unsplash上搜索图片...\u0026#39;) # Parse Images data = json.loads(response.text) results = data[\u0026#39;photos\u0026#39;][\u0026#39;results\u0026#39;] print(u\u0026#39;已为您检索到图片共%s张\u0026#39; % str(len(results))) results = list(filter(lambda x:float(x[\u0026#39;width\u0026#39;])/x[\u0026#39;height\u0026#39;] \u0026gt;=1.33,results)) result = random.choice(results) resultId = str(result[\u0026#39;id\u0026#39;]) resultURL = result[\u0026#39;urls\u0026#39;][\u0026#39;regular\u0026#39;] # Download Images print(u\u0026#39;正在为您下载图片:%s...\u0026#39; % resultId) basePath = sys.path[0] if(os.path.isfile(basePath)): basePath = os.path.dirname(basePath) baseFolder = basePath + \u0026#39;\\\\Download\\\\\u0026#39; if(not path.exists(baseFolder)): os.makedirs(baseFolder) jpgFile = baseFolder + resultId + \u0026#39;.jpg\u0026#39; bmpFile = baseFolder + resultId + \u0026#39;.bmp\u0026#39; response = requests.get(resultURL) with open(jpgFile,\u0026#39;wb\u0026#39;) as file: file.write(response.content) img = Image.open(jpgFile) img.save(bmpFile,\u0026#39;BMP\u0026#39;) os.remove(jpgFile) 这部分代码非常简单，需要关注的地方有：第一，这个 API 对应的密钥是公共的，即所有人都可以使用，这里随机从指定的分类中去搜索图片。第二，这里使用 filter()函数过滤出宽高比超过 1.33 的图片，即分辨率为 1366 * 768 的图片。这里需要注意的是，在 Python3.X 下 filter 需要转化为 list，否则会引发一个异常。第三，下载的图片默认为 JPEG 格式，而 Windows 下设置壁纸使用的是位图格式，即 BMP 格式，所以在这里我们使用 PIL 模块来完成格式转换。这里需要注意的是，PIL 模块目前不支持 Python3.X 以后的版本，我们这里使用的是 Pillow 模块，该模块可以通过 pip 直接完成安装。\n现在，我们将壁纸下载到本地以后，就可以着手设置壁纸相关的工作。这些工作主要借助为 win32api 和 win32gui 这两个内置模块，我们一起来看具体代码：\nprint(u\u0026#39;正在设置图片:%s为桌面壁纸...\u0026#39; % resultId) key = win32api.RegOpenKeyEx(win32con.HKEY_CURRENT_USER, \u0026#34;Control Panel\\\\Desktop\u0026#34;,0,win32con.KEY_SET_VALUE) win32api.RegSetValueEx(key, \u0026#34;WallpaperStyle\u0026#34;, 0, win32con.REG_SZ, \u0026#34;2\u0026#34;) #2拉伸适应桌面,0桌面居中 win32api.RegSetValueEx(key, \u0026#34;TileWallpaper\u0026#34;, 0, win32con.REG_SZ, \u0026#34;0\u0026#34;) win32gui.SystemParametersInfo(win32con.SPI_SETDESKWALLPAPER, bmpFile, 1+2) print(u\u0026#39;成功应用图片:%s为桌面壁纸\u0026#39; % resultId) 这部分内容非常简单，基本没有复杂的东西在里面。接下来我们需要通过 pyinstaller 模块将脚本打包成可执行文件，实际上这个步骤完全可以省略，因为现在我们通过命令行就可以实现壁纸切换，为什么要做这样额外的工作呢？考虑到 Windows 下 GUI 更为便捷一点，所以我们打包成可执行文件，主要是为了给右键菜单添加功能，我们最终点击想要实现的功能是，点击右键菜单就可以完成壁纸的切换。首先通过 pip 安装 pyinstaller 模块，在终端下执行命令：\npython -m pip install pyinstaller 安装完成后按照官方文档即可在./dist/目录中找到生成的可执行文件，如果打包出错可以修改 Python 根目录下的./Scripts/pyinstaller-script.py 文件，修改第一行 Python.exe 的路径，删除两端的引号即可，如下图所示。关于 pyinstaller 模块打包时的详细参数设定，请自行查阅官方文档。\npyinstaller-script.py文件\r现在，在生成可执行文件以后，我们打开注册表，定位到以下节点： 计算机\\HKEY_CLASSES_ROOT\\Directory\\Background\\shell，然后创建一级子节点 WallPaper，其默认值填写\u0026quot;更换壁纸\u0026rdquo;，接下来创建二级子节点 command，注意这个名称不能修改，其默认值填写可执行文件路径，本例中为：E:\\Software\\WallPaper\\main.exe，如下图所示：\n为右键菜单增加更换壁纸选项\r好了，现在我们可以看看在右键菜单中增加\u0026quot;更换壁纸\u0026quot;选项以后的效果： 最终效果\r文本小结 本文使用 Python 实现了 Windows 下切换壁纸的功能，通过 requests 模块从网络上抓取图片，通过 PIL 模块实现 JPEG 格式到 BMP 格式的转换，通过 win32api 和 win32gui 模块实现壁纸设置，并通过修改注册表的方式，将这一功能整合到系统菜单中，可以非常便捷地更换桌面壁纸。作为一个设计上的扩展，我们需要考虑更多的问题，比如当网络断开的时候如何避免异常，如何接入更多的在线图库 API，如何支持可配置的图片分类信息以及如何将修改注册表的过程自动化等等，这些问题博主会利用空闲时间去解决，今天这篇文章就是这样啦，本文源代码可以通过这里获取，谢谢大家！\n","date":"2018-02-05T16:48:39Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2822230423/","slug":"2822230423","tags":["Python","脚本","Windows"],"title":"基于 Python 实现 Windows 下壁纸切换功能"},{"categories":["编程语言"],"content":"各位朋友，大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是https://qinyuanpei.github.io。今天我想和大家一起探讨的话题是 Python 中的装饰器。因为工作关系最近这段时间在频繁地使用 Python，而我渐渐意识到这是一个非常有趣的话题。无论是在 Python 标准库还是第三方库中，我们越来越频繁地看到装饰器的身影，从某种程度上而言，Python 中的装饰器是 Python 进阶者的一条必由之路，正确合理地使用装饰器可以让我们的开发如虎添翼。装饰器天然地和函数式编程、设计模式、AOP 等概念产生联系，这更加让我对 Python 中的这个特性产生兴趣。所以，在这篇文章中我将带领大家一起来剖析 Python 中的装饰器，希望对大家学习 Python 有所帮助。\n什么是装饰器 什么是装饰器？这是一个问题。在我的认知中，装饰器是一种语法糖，其本质就是函数。我们注意到 Python 具备函数式编程的特征，譬如 lambda 演算，map、filter 和 reduce 等高阶函数。在函数式编程中，函数是一等公民，即“一切皆函数”。Python 的函数式编程特性由早期版本通过渐进式开发而来，所以对“一切皆对象”的 Python 来说，函数像普通对象一样使用，这是自然而然的结果。为了验证这个想法，我们一起来看下面的示例。\n函数对象 def square(n): return n * n func = square print func #\u0026lt;function square at 0x01FF9FB0\u0026gt; print func(5) #25 可以注意到，我们将一个函数直接赋值给一个变量，此时该变量表示的是一个函数对象的实例，什么叫做函数对象呢？就是说你可以将这个对象像函数一样使用，所以当它带括号和参数时，表示立即调用一个函数；当它不带括号和参数时，表示一个函数。在 C#中我们有一个相近的概念被称为委托，而委托本质上是一个函数指针，即表示指向一个方法的引用，从这个角度来看，C#中的委托类似于这里的函数对象，因为 Python 是一个动态语言，所以我们可以直接将一个函数赋值给一个对象，而无需借助 Delegate 这样的特殊类型。\n使用函数作为参数 def sum_square(f,m,n): return f(m) + f(n) print sum_square(square,3,4) #25 使用函数作为返回值 def square_wrapper(): def square(n): return n * n return square wrapper = square_wrapper() print wrapper(5) #25 既然在 Python 中存在函数对象这样的类型，可以让我们像使用普通对象一样使用函数。那么，我们自然可以将函数推广到普通对象适用的所有场合，即考虑让函数作为参数和返回值，因为普通对象都都具备这样的能力。为什么要提到这两点呢？因为让函数作为参数和返回值，这不仅是函数式编程中高阶函数的基本概念，而且是闭包、匿名方法和 lambda 等特性的理论基础。从 ES6 中的箭头函数、Promise、React 等可以看出，函数式编程在前端开发中越来越流行，而这些概念在原理上是相通的，这或许为我们学习函数式编程提供了一种新的思路。在这个示例中，**sum_square()和square_wrapper()**两个函数，分别为我们展示了使用函数作为参数和返回值的可行性。\ndef outer(m): n = 10 def inner(): return m + n return outer func = outer(5) print func() #15 #内函数修改外函数局部变量 def outer(a): b = [10] def inner(): b[0] += 1 return a + b[0] return inner func = outer(5) print func() #16 print func() #17 对 Python 这门语言来说，这里的 outer()函数和 inner()函数分别被称为外函数和内函数，变量 n 的定义不在 inner()函数内部，因此变量 n 称为 inner()函数的环境变量。在 Python 中，一个函数及其环境变量就构成了闭包(Closure)。要理解闭包我认为我们可以把握这三点：第一，外函数返回了内函数的引用，即我们调用 outer()函数时返回的是 inner()函数的引用；第二，外函数将自己的局部变量绑定到内函数，其中变量 b 的目的是展示如何在内函数中修改环境变量；第三，调用内函数意味着发生出、入栈，不同的是每次调用都共享同一个闭包变量，请参考第二个示例。好了，现在讲完闭包以后，我们就可以开始说 Python 中的装饰器啦。\n装饰器 装饰器是一种高级 Python 语法，装饰器可以对一个函数、方法或者类进行加工。所以，装饰器就像女孩子的梳妆盒，经过一番打扮后，可以让女孩子更漂亮。装饰器使用起来是非常简单的，其难点主要在如何去写一个装饰器。带着这个问题，我们来一起看看 Python 中的装饰器是如何工作的，以及为什么我们说装饰器的本质就是函数。早期的 Python 中并没有装饰器这一语法，最早出在 Python 2.5 版本中且仅仅支持函数的装饰，在 Python 2.6 及以后版本中装饰器被进一步用于类。\ndef decorator_print(func): def wrapper(*arg): print arg return func(*arg) return wrapper @decorator_print def sum(array): return reduce(lambda x,y:x+y,array) data = [1,3,5,7,9] print sum(data) 我们注意到装饰器可以使用 def 来定义，装饰器接收一个函数对象作为参数，并返回一个新的函数对象。装饰器通过名称绑定，让同一个变量名指向一个新返回的函数对象，这样就达到修改函数对象的目的。在使用装饰器时，我们通常会在新函数内部调用旧函数，以保留旧函数的功能，这正是“装饰”一词的由来。在定义好装饰器以后，就可以使用@语法了，其实际意义时，将被修饰对象作为参数传递给装饰器函数，然后将装饰器函数返回的函数对象赋给原来的被修饰对象。装饰器可以实现代码的可复用性，即我们可以用同一个装饰器修饰多个函数，以便实现相同的附加功能。在这个示例中，我们定义了一个 decorator_print 的装饰器函数，它负责对一个函数 func 进行修饰，在调用函数 func 以前执行 print 语句，进而可以帮助我们调试函数中的参数，通过@语法可以让我们使用一个名称去绑定一个函数对象。在这里，它的调用过程可以被分解为：\nsum = decorator_print(sum) print sum() 接下来，我们再来写一个统计代码执行时长的装饰器 decorator_watcher:\ndef decorator_watcher(func): def wrapper(*arg): t1 = time.time() result = func(*arg) t2 = time.time() print(\u0026#39;time:\u0026#39;,t2-t1) return result return wrapper 此时我们可以使用该装饰器来统计 sum()函数执行时长：\n@decorator_watcher def sum(array): return reduce(lambda x,y:x+y,array) data = [1,3,5,7,9] print sum(data) 现在，这个装饰器打印出来的信息格式都是一样的，我们无法从终端中分辨它对应哪一个函数，因此考虑给它增加参数以提高辨识度：\ndef decorator_watcher(funcName=\u0026#39;\u0026#39;): def decorator(func): def wrapper(*arg): t1 = time.time() result = func(*arg) t2 = time.time() print(\u0026#39;%s time:\u0026#39; % funcName,t2-t1) return result return wrapper return decorator @decorator_watcher(\u0026#39;sum\u0026#39;) def sum(array): return reduce(lambda x,y:x+y,array) data = [1,3,5,7,9] print sum(data) 装饰器同样可以对类进行修饰，譬如我们希望某一个类支持单例模式，在 C#中我们定义泛型类 Singleton。下面演示如何通过装饰器来实现这一功能：\ninstances = {} def getInstance(aClass, *args): if aClass not in instances: instances[aClass] = aClass(*args) return instances[aClass] def singleton(aClass): def onCall(*args): return getInstance(aClass,*args) return onCall @singleton class Person: def __init__(self,name,hours,rate): self.name = name self.hours = hours self.rate = rate def pay(self): return self.hours * self.rate 除此以外，Python 标准库中提供了诸如 classmethod、staticmethod、property 等类装饰器，感兴趣的读者朋友可以自行前去研究，这里不再赘述。\n装饰器与设计模式 装饰器可以对函数、方法和类进行修改，同时保证原有功能不受影响。这自然而然地让我想到面向切面编程(AOP)，其核心思想是，以非侵入的方式，在方法执行前后插入代码片段，以此来增强原有代码的功能。面向切面编程(AOP)通常通过代理模式(静态/动态)来实现，而与此同时，在 Gof 提出的“设计模式”中有一种设计模式被称为装饰器模式，这两种模式的相似性，让我意识到这会是一个有趣的话题，所以在接下来的部分，我们将讨论这两种设计模式与装饰器的内在联系。\n代理模式 代理模式，属于 23 种设计模式中的结构型模式，其核心是为真实对象提供一种代理来控制对该对象的访问。在这里我们提到了真实对象，这就要首先引出代理模式中的三种角色，即抽象对象、代理对象和真实对象。其中：\n抽象对象：通过接口或抽象类声明真实角色实现的业务方法。 代理对象：实现抽象角色，是真实角色的代理，通过真实角色的业务逻辑方法来实现抽象方法。 真实对象：实现抽象角色，定义真实角色所要实现的业务逻辑，供代理角色调用。 下面是一个典型的代理模式 UML 图示： 代理模式\r通过 UML 图我们可以发现，代理模式通过代理对象隐藏了真实对象，实现了调用者对真实对象的访问控制，即调用者无法直接接触到真实对象。“代理”这个词汇是一个非常生活化的词汇，因为我们可以非常容易地联系到生活种的中介这种角色，譬如租赁房屋时会存在房屋中介这种角色，租客(调用者)通过中介(代理对象)来联系房东(真实对象)，这种模式有什么好处呢？中介(代理对象)的存在隔离了租客(调用者)与房东(真实对象)，有效地保护了房东(真实对象)的个人隐私，使其免除了频繁被租客(调用者)骚扰的困惑，所以代理模式的强调的是控制。\n按照代理机制上的不同来划分，代理模式可以分为静态代理和动态代理。前者是将抽象对象、代理对象和真实对象这三种角色在编译时就确定下来。对于 C#这样的静态强类型语言而言，这意味着我们需要手动定义出这些类型；而后者则是指在运行时期间动态地创建代理类，譬如 Unity、Ca\u0026rsquo;stle、Aspect Core 以及 ASP.NET 中都可以看到这种技术的身影，即所谓的“动态编织”技术，通过反射机制和修改 IL 代码来达到动态代理的目的。通常意义上的代理模式，都是指静态代理，下面我们一起来看代码示例：\npublic class RealSubject : ISubject { public void Request() { Console.WriteLine(\u0026#34;我是RealSubject\u0026#34;); } } public class ProxySubject : ISubject { private ISubject subject; public ProxySubject(ISubject subject) { this.subject = subject; } public void Request() { this.subject.Request(); } } 通过示例代码，我们可以注意到，在代理对象 ProxySubject 中持有对 ISubject 接口的引用，因此它可以代理任何实现了 ISubject 接口的类，即真实对象。在 Request()方法中我们调用了真实对象的 Request()方法，实际上我们可以在代理对象中增加更多的细节，譬如在 Request()方法执行前后插入指定的代码，这就是面向切面编程(AOP)的最基本的原理。在实际应用中，主要以动态代理最为常见，Java 中提供了 InvocationHandler 接口来实现这一接口，在.NET 中则有远程调用(Remoting Proxies)、ContextBoundObject和IL 织入等多种实现方式。从整体而言，生成代理类和子类化是常见的两种思路。相比静态代理，动态代理机制相对复杂，不适合在这里展开来说，感兴趣的朋友可以去做进一步的了解。\n装饰器模式 装饰器模式，同样是一种结构型模式，其核心是为了解决由继承引发的“类型爆炸”问题。我们知道，通过继承增加子类就可以扩展父类的功能，可随着业务复杂性的不断增加，子类变得越来越多，这就会引发“类型爆炸”问题。装饰器模式就是一种用以代替继承的技术，即无需通过继承增加子类就可以扩展父类的功能，同时不改变原有的结构。在《西游记》中孙悟空和二郎神斗法，孙悟空变成了一座庙宇，这是对原有功能的一种扩展，因为孙悟空的本质依然是只猴子，不同的是此刻具备了庙宇的功能。这就是装饰器模式。下面，我们一起来看一个生活中的例子。\n咖啡种类\r喜欢喝咖啡的朋友，看到这张图应该感到特别亲切，因为咖啡的种类的确是太多啦。在开始喝咖啡以前，我完全不知道咖啡会有这么多的种类，而且咖啡作为一种略显小资的饮品，其名称更是令人目不暇接，一如街头出现的各种女孩子喜欢的茶品饮料，有些当真是教人叫不出来名字。这是一个典型的“类型爆炸”问题，人们在吃喝上坚持不懈的追求，让咖啡的种类越来越多，这个时候继承反而变成了一种沉重的包袱，那么该如何解决这个问题呢？装饰器模式应运而生，首先来看装饰器模式的 UML 图示： 装饰器模式\r从这个图示中可以看出，装饰器和被装饰者都派生自同一个抽象类 Component，而不同的 Decorator 具备不同的功能，DecoratorA 可以为被装饰者扩展状态，DecoratorB 可以为被装饰者扩展行为，可无论如何，被装饰者的本质不会发生变化，它还是一个 Component。回到咖啡这个问题，我们发现这些咖啡都是由浓缩咖啡、水、牛奶、奶泡等组成，所以我们可以从一杯浓缩咖啡开始，对咖啡反复进行调配，直至搭配出我们喜欢的咖啡，这个过程就是反复使用装饰器进行装饰的过程，因此我们可以写出下面的代码：\n//饮料抽象类 abstract class Drink { public abstract Drink Mix(Drink drink); } //牛奶装饰器 class MilkDecorator : Drink { private Drink milk; MilkDecorator(Drink milk) { this.milk = milk; } public override Drink Mix(Drink coffee) { return coffee.Mix(this.milk); } } //热水装饰器 class WaterDecorator : Drink { private Drink water; WaterDecorator(Drink water) { this.water = water; } public override Drink Mix(Drink coffee) { return coffee.Mix(this.water); } } //一杯浓缩咖啡 var coffee = new Coffee() //咖啡里混入水 coffee = new WaterDecorator(new Water()).Mix(coffee) //咖啡里混入牛奶 coffee = new MilkDecorator(new Milk()).Mix(coffee) 在这里我们演示了如何通过装饰器模式来调配出一杯咖啡，这里我没有写出具体的 Coffee 类。在实际场景中，我们还会遇到在咖啡里加糖或者配料来收费的问题，此时装饰器模式就可以帮助我们解决问题，不同的装饰器会对咖啡的价格进行修改，因此在应用完所有装饰器以后，我们就可以计算出最终这杯咖啡的价格。由此我们可以看出，装饰器模式强调的是扩展。什么是扩展呢，就是在不影响原来功能的基础上增加新的功能。\n区别和联系 代理模式和装饰器模式都是结构型的设计模式，两者在实现上是非常相似的。不同的地方在于，代理模式下调用者无法直接接触到真实对象，因此代理模式强调的是控制，即向调用者隐藏真实对象的信息，控制真实对象可以访问的范围；装饰器模式下，扩展功能的职责由子类转向装饰器，且装饰器与被装饰者通常是**\u0026ldquo;同源\u0026rdquo;的，即派生自同一个父类或者是实现了同一个接口，装饰器关注的是增加被装饰者的功能，即扩展**。两者的联系在于都需要持有一个**\u0026ldquo;同源\u0026rdquo;**对象的引用，譬如代理对象与真实对象同源，装饰器与被装饰者同源。从调用的层面上来讲，调用者无法接触到真实对象，它调用的始终是代理对象，对真实对象的内部细节一无所知，这是代理模式；调用者可以接触到装饰器和被装饰者，并且知道装饰器会对被装饰者产生什么样的影响，通常是从一个默认的对象开始\u0026quot;加工\u0026quot;，这是装饰器模式。\n装饰器与面向切面 这篇文章写到现在，我发觉我挖了一个非常大的坑，因为这篇文章中涉及到的概念实在太多，务求每一个概念都能讲得清楚透彻，有时候就像莫名立起来的 flag，时间一长连我自己都觉得荒唐。有时候感觉内容越来越难写，道理越来越难同别人讲清楚。写作从一开始坚持到现在，就如同某些固执的喜欢一样，大概连我都不记得最初是为了什么吧。好了，现在来说说装饰器与面向切面。我接触 Python 装饰器的时候，自然而然想到的是.NET 中的 Attribute。我在越来越多的项目中使用 Attribute，譬如 ORM 中字段与实体的映射规则、数据模型(Data Model)中字段的校验规则、RESTful API/Web API 设计中的路由配置等，因为我非常不喜欢 Java 中近乎滥用的配置文件。\nC#中的 Attribute 实际上是一种依附在目标(AttributeTargets)上的特殊类型，它无法通过 new 关键字进行实例化，它的实例化必须依赖所依附的目标，通过分析 IL 代码我们可以知道，Attribute 并非是一种修饰符而是一种特殊的类，其方括号必须紧紧挨着所依赖的目标，构造函数以及对属性赋值均在圆括号内完成。相比较而言，Python 中的装饰器就显得更为顺理成章些，因为 Python 中的装饰器本质就是函数，装饰器等价于用装饰器函数去修饰一个函数。函数修饰函数，听起来感觉不可思议，可当你理解了函数和普通对象一样，就不会觉得这个想法不可思议。有时回想起人生会觉得充满玄学的意味，大概是因为我们还没有学会把自己看得普通。\n通过这篇文章的梳理，我们会发现一个奇妙的现象，Java 的 Spring 框架采用了动态代理了实现 AOP，而 Python 的装饰器简直就是天生的 AOP 利器，从原理上来讲，这两门语言会选择什么样的方案都说得通。Java 是典型的面向对象编程的语言，所以不存在任何游离于 Class 以外的函数，代理模式对类型的要求更为强烈些，因为我必须限制或者说要求 Proxy 实现里面的方法，而装饰器模式相对更为宽松些，遇到 Python 这样的动态类型语言，自然会显得事半功倍。这说明一个道理，通往山顶的道路会有无数条，从中找出最为优雅的一条，是数学家毕生的心愿。AOP 是一种思想，和语言无关，我常常听到 Java 的同学们宣称 AOP 和 IOC 在 Java 社区里如何流行，其实这些东西本来就是可以使用不同的方式去实现的，有些重要的东西，需要你剥离开偏见去认知。\n关于 C#中的 Attribute 和 AOP 如何去集成，在 Unity 和 Aspect Core 这两个框架中都有涉及，主流的 AOP 都在努力向这个方向去靠拢，Java 中的注解同样不会跳出这个设定，因为编程技术到了今天，语言间的差别微乎其微，我至今依然可以听到，换一种语言就能让问题得到解决的声音，我想说，软件工程是没有银弹的，人类社会的复杂性会永远持续地存在下去，你看微信这样一个社交软件，其对朋友圈权限的粒度之细足以令人叹服。有朋友尝试在 C#中借鉴 Python 的装饰器，并在一组文章中记录了其中的心得，这里分享给大家，希望对这个问题有兴趣的朋友，可以继续努力研究下去，AOP 采用哪种方式实现重要吗？有人用它做权限控制，有人用它做日志记录……允许差异的存在，或许才是我们真正需要从这个世界里汲取的力量。\n轻量级 AOP 框架-移植 python 的装饰器(Decorator)到 C#(思考篇)\n轻量级 AOP 框架-移植 python 的装饰器(Decorator)到 C#(编码篇)\n本文小结 本文是博主学习 Python 时临时起意的想法，因为曾经有在项目中使用过 AOP 的经验，所以在学习 Python 中的装饰器的时候，自然而然地对这个特性产生了兴趣。有人说，装饰器是 Python 进阶的重要知识点。在今天这篇文章中，我们首先从 Python 中的函数引出\u0026quot;函数对象\u0026quot;这一概念，在阐述这个概念的过程中，穿插了函数式编程、高阶函数、lambda 等等的概念，\u0026ldquo;函数是一等公民\u0026rdquo;，这句话在 Python 中出现时就是指装饰器，因为装饰器的本质就是函数。然后我们讨论了两种和装饰器有关的设计模式，即代理模式和装饰器模式，选择这两种模式来讨论，是因为我们在 Java/C#和 Python 中看到了两种截然不同的实现 AOP 的思路，这部分需要花功夫去精心雕琢。博主有时候觉得力不从心，所以写作中有不周到的地方希望大家谅解，同时积极欢迎大家留言，这篇文章就先写到这里吧，谢谢大家！\n","date":"2018-01-23T15:55:13Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2829333122/","slug":"2829333122","tags":["装饰器","Python","AOP"],"title":"深入浅出理解 Python 装饰器"},{"categories":["数据存储"],"content":"各位朋友，大家好，我是 Payne，欢迎大家关注我的博客。最近读一本并行计算相关的书籍，在这本书中作者提到了 MapReduce。相信熟悉大数据领域的朋友，一定都知道 MapReduce 是 Hadoop 框架中重要的组成部分。在这篇文章中，博主将以函数式编程作为切入点，来和大家聊一聊大数据中的 MapReduce。如今人工智能正成为行业内竞相追逐的热点，选择 MapReduce 这个主题，更多的是希望带领大家一窥人工智能的门庭，更多深入的话题需要大家来探索和挖掘。\nMapReduce 的前世今生 MapReduce 最早是由 Google 公司研究并提出的一种面向大规模数据处理的并行计算模型和方法。2003 年和 2004 年，Google 公司先后在国际会议上发表了关于 Google 分布式文件系统(GFS)和 MapReduce 的论文。这两篇论文公布了 Google 的 GFS 和 MapReduce 的基本原理和主要设计思想，我们通常所说的 Google 的三驾马车，实际上就是在说 GFS、BigTable 和 MapReduce。因此，这些论文的问世直接催生了 Hadoop 的诞生，可以说今天主流的大数据框架如 Hadoop、Spark 等，无一不是受到 Google 这些论文的影响，而这正是 MapReduce 由来，其得名则是因为函数式编程中的两个内置函数: map()和 reduce()。\n我们常常说，脱离了业务场景去讨论一项技术是无意义的，这个原则在 MapReduce 上同样适用。众所周知，Google 是一家搜索引擎公司，其设计 MapReduce 的初衷，主要是为了解决搜索引擎中大规模网页数据的并行化处理。所以，我们可以说，MapReduce 其实是起源自 Web 检索的。而我们知道，Web 检索可以分为两部分，即获取网页内容并建立索引、根据网页索引来处理查询关键字。我们可以认为互联网上的每个网页都是一个文档，而每个文档中都会有不同的关键字，Google 会针对每一个关键字建立映射关系，即哪些文档中含有当前关键字，这是建立索引的过程。在建立索引以后，查询就会变得简单，因为现在我们可以按图索骥。\n互联网诞生至今，网站及网页的数量越来越庞大，像 Google 这样的搜索引擎巨头是如何保证能够对 Web 上的内容进行检索的呢？答案是采用并行计算(Parallel)。硬件技术的不断革新，让计算机可以发挥多核的优势来处理数据，可当数据量庞大到单机无法处理的程度，就迫使我们不得不采用多台计算机进行并行计算。我们知道并行计算的思想是，将大数据分割成较小的数据块，交由不同的任务单元来处理，然后再将这些结果聚合起来。因此，可以将 MapReduce 理解为一种可以处理海量数据、运行在大规模集群上、具备高度容错能力、以并行处理方式执行的软件框架。MapReduce 是分治思想在大规模机器集群时代的集中体现(如图所示)，其中，Mapper 负责任务的划分，Reducer 负责结果的汇总。\nMapReduce原理图\rMapReduce 的推出给大数据并行处理带来了巨大的革命性影响，使其成为事实上的大数据处理的工业标准，是目前为止最为成功、最广为接受和最易于使用的大数据并行处理技术。广为人知的大数据框架 Hadoop，正是受到 MapReduce 的启发。自问世来，成为 Apache 基金会下最重要的项目，受到全球学术界和工业界的普遍关注，并得到推广和普及应用。MapReduce 的非凡意义在于，它提出了一个基于集群的高性能并行计算模型，允许使用市场上普通的商用服务器构成一个含有数十、数百甚至数千个节点的分布式并行计算集群，可以在集群节点上自动分配和执行任务以及收集计算结果，通过 Mapper 和 Reducer 提供了抽象的大数据处理并行编程接口，可以帮助开发人员更便捷地完成大规模数据处理的编程和计算工作。今天，Google 有超过 10000 个不同的项目已采用 MapReduce 来实现。\n函数式编程与 MapReduce 我们提到，MapReduce 之得名，其灵感来自函数式编程中的两个内置函数：map()和 reduce()。函数式编程中，有一个重要的概念叫做高阶函数，是指函数自身能够接受函数，并返回函数的一种函数。我们所熟悉的 C#和 Java 都是典型的面向对象编程(OOP)的语言，在这类编程语言中类(Class)是第一等公民，即不允许有独立于类的结构出现在代码中。虽然微软从未公开表示 C#支持函数式编程，可从 LINQ 中我们依然可以窥见高阶函数的身影，譬如我们熟悉的 Select()、Where()等扩展方法，就可以让我们按照函数式编程的风格去编写代码，这正是为什么 Java 8 开始支持 Stream API 的原因。最经典的函数式编程语言当属 Haskell 语言，我们今天见到的各种编程语言，在考虑引入函数式编程风格的时候，或多或少地都会受到这门语言影响。当你开始适应函数作为第一等公民、高阶函数、柯里化以及惰性求值以后，你或许就会感受到函数式编程特有的美感。\n这里我们选择 Python 来阐述函数式编程与 MapReduce 的关系。Python 可以让我们轻易地在多种不同的编程风格间切换，事实上现在的编程语言都有向着混合式编程风格发展的趋势。我们提到 MapReduce 来自两个内置函数：map()和 reduce()。其中，map()方法可以对指定集合中的元素按照指定函数进行映射，并将映射后的结果以集合形式返回。譬如我们有一个集合**[1,3,5,7,9]**，我们希望对集合中的每一个元素做平方运算。借助 Python 中的 map()方法和 lambda 表达式，这个问题可以通过 1 行代码得到解决。同理，如果我们希望对该集合内的元素做求和运算，我们可以借助于 Python 中的 reduce()方法，该方法位于 functools 模块中。在某些编程语言中该方法又被成为 fold()方法，实际上这两种叫法是等价的，我们关注函数式编程的本质即可。什么是本质呢？当然是函数啦。\nlist = map(lambda x: x * x, [1,3,5,7,9]) #[1,9,25,49,81]\rsum = functools.reduce(lambda x,y: x+y, [1,3,5,7,9]) #25 好了，现在我们来分析下这两个函数，这将帮助我们更好地理解 MapReduce。map()方法在这里被称为映射函数，它可以将一种类型映射为一种新的类型。举一个生活中的例子，譬如我们做菜的时候，必不可少的一个环节是将各种各样的食材切碎， 此时作用在这些食材的这个操作就是一个 Map 操作，你给 Map 一个洋葱就可以得到洋葱丝。同样地 ，你给 Map 一个番茄就可以得到番茄块。所以 Map 处理的原始数据，每条数据间没有相互联系，聪明的你告诉我洋葱和番茄有什么关系。相比 map()方法，reduce()方法复杂的地方在于，它要求 lambda 表达式中必须是两个参数。如果继续沿用做菜这个生活化的例子，reduce()方法是将 Map 操作中切好的食材混合在一起。假设我们要做一份辣椒酱，辣椒酱需要的材料有辣椒、姜和蒜，因此在第一步 Map 的时候，这些食材将具有相同的 Key。对同一类数据，我们就可以使用 reduce()进行左/右折叠操作，这相当于我们将同一道菜的食材一起放到锅里，所以 Reduce 阶段处理的数据是以 Key-Value 形式组织的，同一个 Key 下的 Value 具有相关性。这样，从理论上它就完全符合函数式编程里的 map()和 reduce()啦。\nC#并行编程里的 PLINQ 关于 MapReduce 中一个经典案例是，统计不同文章中出现的关键字的频率。对这样一个问题，我们基本上可以想到下面四种方法：\n写一个单线程程序，顺序地遍历所有文章，然后统计每个关键字出现的频率。 写一个多线程程序，并发地遍历所有文章，然后统计每个关键字出现的频率。 写一个单线程程序，部署到 N 台不同的计算机上，然后将文章分割成 N 份分别输入，再由人工汇总 N 份结果。 使用 MapReduce，程序部署、任务划分、结果汇总全部交给框架去完成，我们定义好任务即可。 通过对比，我们可以非常容易地分析出来，第一种方法最简单同时最耗时；第二种方法理论上比第一种高效，尤其是当计算机是多核或者多处理器的时候，缺点是要解决线程同步的问题；第三种方法初现集群的思想，可无法解决程序部署、任务划分和结果汇总等一系列问题；第四种方法本质上就是第三种方法， 可是 MapReduce 解决了第三种方法全部缺陷，所以它或许是目前最完美的方法。我们下面来考虑，如何模拟这个过程，因为博主不可能为了写一篇科普性质的文章，专门去准备一个 Hadoop 的开发环境啊，哈哈。 PLINQ，即 Parallel LINQ，是.NET 4.0 中增加的任务并行库(TPL)中的一部分。并行编程中有两个基本的概念，任务并行和数据并行。前者是指，将程序分割成一组任务并使用不同的线程来运行不同的任务，这种方式被称为任务并行；而后者是指，将数据分割成较小的数据块，对这些数据进行并行计算，然后聚合这些计算结果，这种编程模型称为数据并行。伴随着并行算法的出现，并行集合相继而来，显然 LINQ 的并行版本就是 PLINQ。这里我们来看一个使用 PLINQ 实现的词频统计代码，这将作为我们实现 MapReduce 的一个示例：\n//Origin Texts string strTarget = @\u0026#34;\u0026#34;; //Map string[] words = strTarget.Split(\u0026#39; \u0026#39;); ILookup\u0026lt;string, int\u0026gt; map = words.AsParallel().ToLookup(p =\u0026gt; p, k =\u0026gt; 1); //Reduce var reduce = from IGrouping\u0026lt;string, int\u0026gt; wordMap in map.AsParallel() where wordMap.Count() \u0026gt; 1 select new { Word = wordMap.Key, Count = wordMap.Count() }; //Show Results foreach (var word in reduce) Console.WriteLine(\u0026#34;Word: \u0026#39;{0}\u0026#39; : Count: {1}\u0026#34;, word.Word, word.Count); 本文小结 今天 Google 发布了全新的 AI 服务工具 Cloud AutoML，从这个产品的名字就可以看出，这是一个试图将人工智能大众化的产品。目前 AI 是行业中不容置疑热点，国外的科技巨头如 Google、微软，国内的科技巨头如腾讯、阿里和百度等，无一不在积极布局 AI 的上下游产业链。最近 CSDN 发布了人工智能方向的发展规划，整个产品线的基本都在做战略上调整，我们这些曾经的老用户被新的社区 运营搞得非常郁闷，因为所有的资源都在向着人工智能和区块链倾斜。上周在知乎上看到一篇讽刺国内区块链发展乱象的文章，大概就是说国人喜欢拿一个 Token 当做加密货币来买，实则连底层技术、分布账本、钱包等基础设施都没有。对于这一点我深有体会，任何新的商业模式在中国都火不过 1 年，譬如在 2017 年里被发扬广大的共享经济，有多少共享单车是靠技术和产品赢得市场的，我相信大部分都是沾了人口基数大的光。目前的人工智能，核心算法及技术都掌握在科技巨头手上。我们所追逐的人工智能，有多少是需要靠不断调整参数反复去训练来达到的呢？我觉得找到切实可靠的需求落脚点，比追逐一个又一个热点要更现实，我们大部分工程师都是在科学家工作的基础上做集成应用，所以拨开泡沫看清方向比盲目跟风更重要呀。\n这篇文章蹭了人工智能的热点，其实它对 MapReduce 并没有做多少深入的研究。我们从 Google 的业务场景着手分析，思考为什么 Google 需要 MapReduce，即提出 MapReduce 是为了解决一个什么样的问题？答案是为了适应 Google 在大规模 Web 检索业务方面的需要。通过梳理 Web 检索的一般流程，我们意识到，Web 检索可以分为两部分，即获取网页内容并建立索引、根据网页索引来处理查询关键字，从而引出了 Mapper 和 Reducer 两个基本的数据处理单元，MapReduce 是分治思想在大规模机器集群时代的集中体现，其中，Mapper 负责任务的划分，Reducer 负责结果的汇总。接下来，我们顺着函数式编程的思路，分析了函数式编程中的 map()和 reduce()，这两个核心的函数同 MapReduce 在思想上的一致性，这正是为了印证前文中 MapReduce 得名的由来。考虑到 C#中提供了 PLINQ，而在阅读《C#多线程编程》这本书时，同样提到了 MapReduce 这种并行计算模型，所以在这里将这两者进行结合，提供了一个通过并行计算统计单词频率的简单示例。以上就是这篇文章的所有内容了，如果大家对文章有什么意见或者建议，可以在文章评论区留言，这篇文章就是这样了，谢谢大家，晚安！\n","date":"2018-01-19T00:45:08Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2911923212/","slug":"2911923212","tags":["AI","MapReduce","FP","大数据"],"title":"AI 时代：聊聊大数据中的 MapReduce"},{"categories":["生活感悟"],"content":" 如果提前了解了你所要面对的人生，你是否还有勇气前来？\n这是电影《无问西东》里提出的一个问题。作为一部清华大学建校 100 周年的献礼片，在因为种种原因而被雪藏 6 年以后，终于以这样一种倔强而孤傲的姿态，进入到人们的视野。\n绘制彩红的张果果\r电影一开始，透过玻璃注视着四胞胎的张果果(张震饰)，第一次向我们发问：如果提前了解了你所要面对的人生，你是否还有勇气前来。而在电影接近尾声的时候，张果果再一次注视着四胞胎，在窗户上为它们勾勒彩虹，此时旁白再次想起，一问一答间，张果果心中已然找到答案。我想知道的是，屏幕前的你，是不是同样找到了答案？\n这是一部讲述传承的电影，它不同于目前电影市场上任何一种主流的类型，它从一开始就注定是一部不讨巧、不友善、不商业的电影。其故事跨度将近一个世纪，或许我们所看到的，不过是历史长河里的星星点点，可依然难掩背后熠熠生辉的文人风骨，这种由时代所赋予的文化气息，让它成为一部瑕不掩瑜的电影，所以即使这部电影在叙事、配乐和剪辑上存在缺陷，这依然是一部值得关注的电影。\n电影尝试用四个平行的板块，讲述四个不同历史阶段的故事，这些故事中的人物彼此关联，组成了一幅波澜壮阔的历史画面，而影片中想要去表达的精神内核，恰好是关联起这些人物的一种特质，我们称之为传承的东西。按照影片线性叙事的结构，我们可以梳理出这样四条线索。\n上进求学的吴岭澜\r1923 年，北平，清华学堂的高材生吴岭澜(陈楚生饰)，面对选择文理科时的迷茫与彷徨，校长梅贻绮告诉他什么是真实，适逢泰戈尔访华到清华园演讲，在听到“不要忘记你们的真心和真性”后，开始思考人生的意义。\n捐躯赴国的沈光耀\r1938 年，昆明，世家子弟沈光耀面对山河破碎的现实，在忠与孝的两难抉择中深感困惑，直到听到吴岭澜说“不要放弃对生活的思索，对自己的真实”，毅然选择投笔从戎，成为飞虎队队员，在国家危亡之际战死沙场。\n青春阳光的陈鹏\r1962 年，北京，清华毕业生陈鹏，在面对国家的大爱与恋人的小爱的选择时，饱含着对恋人王敏佳的爱远赴沙漠投身科研，他质问为支边而蒙蔽内心的李想什么是真实，他用爱托着恋人鼓励她努力活下去，为爱而奉献一生。\n寻找本心的张果果\r2010 年，北京，广告总监张果果，在明争暗斗的职场上遭人排挤，在面对职场中的名利诱惑时，在面对四胞胎永无止境的救助要求时，李想用生命拯救了张果果的父母，完成自我救赎，而张果果则在迷茫中找会初心，让爱传承下去。\n看这样一部电影的时候，像是在经历一件久远的事情。“这个时代缺少的不是完美的人，缺少的是从自己心底里给出的，真心、正义、无畏和同情”，这是电影中飞虎队教练在招收飞行员时所说的话。我想我们喜欢这部电影，恰恰是因为我们缺少这些东西，这些让我们感到温暖而纯粹的东西。借用吴岭澜在影片中的一句话，“此刻我终于不觉得这样的思考是羞耻的，甚而是你们人生必须的”，面对一个我们无力反驳的现实，或许思考有时候会显得更为苍白。彼时，清华大学校长梅贻绮对迷茫中的吴岭澜这样说道，“人把自己置身于忙碌之中，有一种麻木的踏实，但丧失了真实”，那么你知道什么是真实吗？\n循循善诱的清华校长梅贻琦\r聆听泰戈尔演讲的吴岭澜\r电影中第一个人物，是 1924 年就读于清华大学的吴岭澜，他见证过泰戈尔访问清华，最终从摇摆不定到听从内心声音，找寻人生的意义。吴岭澜当时是清华大学的一名学生，他的理科成绩属于不列，而当时最好的学生都会去读理科，所以他开始面临学业选择上的困惑。校长梅贻绮给他的答案是：“真实是你看到什么，听到什么，做什么和谁在一起，有一种从心灵深处满溢出来的不懊悔也不羞耻的平和与喜悦”。当时诗人泰戈尔访问清华，演讲的主题是\u0026quot;对自己的真诚\u0026quot;，这次演讲促使吴岭澜心态发生最终变化，转学文科，并最终成为西南联大的一名文科教授。电影对吴岭澜的描绘着重放在他和校长梅贻绮的谈话，以及他出现在泰戈尔演讲现场时那双坚定而执著的眼睛。影片中泰戈尔并没有给出完整的正脸，我们可以看到的只有他标志性的长头发和白头发，可见当时泰戈尔的演讲激起了这代青年人内心的热潮。\n历史上的西南联大\r西南联大师生合影\r历史上的西南联合大学，始于 1938 年 4 月。“七七事变”后，京津地区陷入日军炮火，彼时的清华、北大和南开三校一致决定南迁，遂组成西南联合大学。电影中投笔从戎的沈光耀(王力宏饰)就是这段历史时期的一个缩影，西南联大是中国高等教育史上最伟大的传奇，在短短 8 年间，汇聚并培养了一大批精英，譬如陈寅恪、钱穆、梁思成、朱自清、闻一多、冯友兰……可以说这是近代最为光华璀璨的时期，可谓群贤毕至、风华绝代。短短 8 年间，西南联大共毕业学生 3882 名，其中诺贝尔奖获得者 2 位、国家最高科学技术奖获得者 4 位、两弹一星功勋奖章获得者 8 位、两院院士 171 位及人文大师 100 多位。\n静坐停雨\r电影中这段我最喜欢的场景是“静坐听雨”。历史上西南联大的校舍，是由梁思成夫妇参与设计的。因为战争时期物资非常紧缺，校方无法提供充足的建筑材料，所以梁思成夫妇的设计方案一改再改，从楼房变成了平房，从砖墙变成了泥墙，最终 124 亩的校园里，只有图书馆和实验室以青瓦覆顶，教室用铁皮，宿舍则用茅草。铁皮屋顶最害怕下雨，而昆明偏偏是多雨气候，因此就有了雨声太大导致学生无法听清老师讲课，老师无奈地在黑板上写下“静坐听雨”四个字，这样苦涩中带着温情的画面。电影中沈光耀推开教室窗户，听到的是雨中体育系学生坚持出操的呐喊，看到的是雨中垂钓者的“一蓑烟雨任平生”的从容。我们今天的大学占地越来越大，楼越盖越高，可再找不回这种令人动容的场景。我本科有位老师喜欢讲《大学》，他说这大学不是“高楼大厦之谓”，而是“大师之谓”，这一刻我很想再听老师讲一次大学之道。\n汪曾琪有篇散文《跑警报》，讲述的正是当时的师生躲避日军飞机轰炸的故事，电影中沈光耀(王力宏饰)在锅炉房相遇煮冰糖银耳的桥段，正是取材于这篇散文。在影片中，吴岭澜(陈楚生饰)是一个对沈光耀产生深远影响的人，吴岭澜此时已成为西南联大文学系教授，他冒着被轰炸的危险把煮冰糖银耳的沈光耀从校舍拉了出来，一句“学生不走老师怎么能走”，足以让其形象瞬间高大起来。电影中吴岭澜躲避轰炸的时候带了只鸽子，而历史真实的故事则是金岳霖抱着一只鸡。当时流传着这样一个段子，“陈寅恪跑警报是为了保护国粹，刘文典跑警报是为了庄子，沈从文跑警报到底是为了什么“。在防空洞里吴岭澜讲述泰戈尔的“真实就是你所见所闻”，将真实传承到沈光耀心里，所以他会冒着受罚的风险向饥民空投物资，所以他会不顾家人反对毅然选择投笔从戎、加入飞虎队，直至弹尽粮绝的最后一刻，他毫不犹豫地选择同日军舰艇同归于尽，这是他们的青春故事。\n研制原子弹归来的陈鹏\r用深情撑住恋人的陈鹏\r陈鹏(黄晓明饰)在影片中是清华大学的一名学生，在毕业后参与原子弹的研制工作。他对恋人王敏佳矢志不渝的爱，成为了支撑她熬过人生低谷的强大动力。影片中的王敏佳一心想帮助曾经的老师摆脱“家庭暴力”，结果反倒被师母诬陷她勾引自己老师，而她的虚荣心迫使她被组织审查直至接受批斗。在批斗中王敏佳失去了美丽的面庞，她的好朋友李想为争取支援边区的名额，不敢承认自己的错误并坚决同她划清了界限。只有陈鹏自始至终守护在王敏佳身边。可陈鹏依然在国家和个人的抉择中，选择了远赴沙漠参与原子弹的研制工作，对他而言，照顾王敏佳和研制原子弹，就是他生命的全部。可当陈鹏回到曾经熟悉的地方时，却发现恋人因为“破四旧”运动而不见踪影。在爱人与国家面前，在爱情与理想面前，我们到底应该怎么去选择，我想大概就是电影中陈鹏告诉李想那句话，“死者已矣，生者如斯”，这是“晃晃”哥哥和神父教给他的大爱。\n眼神里都是戏的领导\r现代人的感情，就剩这么点了\r生活在现代都市的张果果(张震饰)，在尔虞我诈的人际关系中艰难前行，他恪守上下级的职场伦理却被有预谋地当作了替罪羊。面对女下属的质疑，不得不以高深莫测的“你猜”来掩饰内心的焦虑。对父母缺乏耐心以至于出去吃饭的时候直接将碗筷丢进垃圾框，他习惯性地防备着身边的一切，一如喜欢给自己覆盖一层冰冷盔甲的你我。他怕四胞胎家人纠缠而从医院匆匆离开，却又主动掏出名片在门口等人家追上来；目睹四胞胎家人住在没有暖气的出租屋里，冷峻着脸转身离开却又悄声让助理帮忙找个好点的两居室；他买来各种奶粉默默研究选出最满意的奶粉并长期供给，却又不愿意让四胞胎家人对他抱太高期望……起初会觉得这个角色和整个故事无关，可当张果果的父母交代出李想在边区为救自己而牺牲的故事，这一刻我们终于明白，这种传承从未改变，经历过迷茫和无助的张果果终于找回初心，自信地对这个世界说：“我和他们不一样”。\n我相信每个时代里的青年都曾这样迷茫过，就像我们今天所要面对的这种困局一样。媒体称我们这一代人是得过且过的佛系青年，我们为工作中无法得到别人认可而焦虑过，我们为空有适婚的年龄而无适婚的感情而烦闷过，我们为透支父母毕生积蓄来买房而羞愧过……可你看每一个在时代洪流中挣扎的人，其生命无一不被同时打上时代和民族的烙印。吴岭澜是因真心而重新选择自己的人生，沈光耀是因为正义而投笔从戎，陈鹏是因为无畏而托住恋人投身科研，张果果是因为同情而重拾本心迷途知返。列夫·托尔斯泰说过，幸福的人生都是相似的，不幸的人生各有各的不同。每个时代有每个时代的不幸，我们今天不单单要面对水涨船高的房价，更要面对逐渐枯竭甚至贫瘠的心灵。有人说，这部拍给清华学生看的电影，对改善我们的生活毫无意义，可这世上有多少东西，说出来以后能不变得庸俗呢？\n那时的爱情真美好\r这个校服好帅诶\r不管多远都要去寻找你\r最后的国学大师，王国维\r那时的青春真美好\r同样是在迷茫的 20 多岁，有人找到了真性在生命里惊鸿一瞥，有人在沉浮中随波逐流迷失本心，前者是沈光耀目睹战友牺牲与敌舰同归于尽，后者是李想为争取支边名额背弃朋友间的友谊。陈鹏的爱慷慨而深沉，王敏佳不幸而无能为力……每个时代都是写意的，有佚群绝类的意气，有献血淋漓的痛苦，今天的人们注定无法理解曾经艰苦的生活，而曾经的人们同样无法理解现代人所面对的压力。现实中的离婚和出轨，让美好的爱情变得遥不可及。我们今天越是期待什么，我们的心里就越是缺乏什么。白居易说《长恨歌》是虚构的，而感情是真实的，这或许完全是我们的内心在作怪。世道艰辛，梦想与现实，家国与大义，爱情与理想，名利与善良，仿佛冥冥之中被逼迫到进退两难境地的每一个人，众生皆苦，各自悲欢。张果果在酒吧里拎着半瓶红酒自嘲道，“现代人的情感，就这么多了”，当本心迷失，便是我们有火车，有高铁，有飞机，都不知道要去哪里了！\n路一直都在远方\r古人说“为天地立心，为生民立命，为往圣继绝学，为万世开太平”，这是传统知识分子对文化传承的一种使命感。在这部电影中，防空洞里师生背诵《楚辞》、讲述泰戈尔，这是我理解的文化上的传道；王国维在一片英语声中转身离去，那个萧疏没落的背景像是很久很久以前的事情；沈光耀的母亲不愿意他为功名荣辱，在不知人生为何物的时候匆匆离开；师母看到被人打得半死的王敏佳，内心残存的善念觉醒愧而坠井……这些游曳在光影里的残余画面，显得温暖而残酷，终于化作电影结束时一个又一个陌生而熟悉的名字。我们之所以怀念青春，是因为青春本该就是这个样子，真实勇敢，无惧无悔。世俗这样强大，强大到生不出改变它们的念头来。可是无论外界的社会如何跌宕起伏，都对自己真诚，坚守原则。内心没有了杂念和疑问，才能勇往直前。愿你在被打击时，记起你的珍贵，抵抗恶意；愿你在迷茫时，坚信你的珍贵，爱你所爱，行你所行，听从你心，无问西东。\n","date":"2018-01-19T00:42:06Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1983298072/","slug":"1983298072","tags":["电影","无问西东","影评"],"title":"无问东西：你曾是少年"},{"categories":["生活感悟"],"content":"也许是因为年纪大了的缘故，渐渐地如同上瘾一般喜欢上怀旧，喜欢怀旧到了什么程度呢？想着再次重温下小马哥的英雄本色，想着再次感受下星球大战里的刀光剑影……可是每次都因为不同的原因而落空，之所以没有去看《英雄本色》，是因为附近的电影院都没有排片，那么这一次呢，好像是因为一部和前任有关的电影《前任攻略 3：再见前任》。\n或许前任与现任永远都是一道无解的题目，就如同网上流传着的一个问题，“我和你妈同时掉水里你会先救哪一个”，我们发现对这个问题而言，即使代入现任和前任这样的设定，它看起来依然是说得通的。那么，在我们心里是如何对待现任和前任的呢？我想，在很多人的潜意识里，前任代表了那个你曾经认定要一直走下去的人。我们曾经用天真对一个人好，发自内心地去爱，最终却用尽全力去遗忘释怀。这部电影所讲述的恰好是这样一个写实的场景。最近看到一段这样的话，我们总是用前任的错误惩罚下一任，或者是用对前任的亏欠弥补下一任。这两种做法均无可厚非，甚至听起来第二种做法更好，可如果扪心自问，到底是爱前任多些，还是爱现任多些，我想这是一个问题。电影里的两对情侣，孟云和林佳，余飞和丁点，同时因为生活中的矛盾分手，好兄弟和好闺蜜再度走到一起，这个时候，我们觉得失恋应该是痛苦的一件事情，没有喝醉到不省人事，没有悲伤到歇斯底里，这都不算做失恋吧。\n可或许是我们想多了，有时候两个人分开以后，难过的只有自己，对对方而言或许是种解脱，于是我们在电影里看到了一幕幕单身男女嗨翻天的景象，我们常常听到这样一句话，两个人不合适就分手呗，讽刺的是，我们一边渴望着美好的爱情，一边对待感情弃若敝履，我们谈恋爱的时间越来越短，对彼此越来越没有耐心，有人说，这是因为我们长大了，知道 20 多岁的年龄，不能再像 10 多岁一样挥霍，像孟云和林佳这样的情侣，现实中举不胜举，为了彼此一点可怜的自尊，谁都不肯先放下身段同对方和解，等到矛盾积累得越来越多，任何一件事情都有可能成为，压死骆驼的最后一根稻草。这个时候，还要试图用扮演至尊宝说“我爱你”和疯狂吃芒果吃到过敏，这样幼稚可笑的事情来满足各自的虚荣心，好像我们每个人都实现了对彼此的承诺，真正爱你的人，怎么会舍得离开你呢，明明是自私得只懂得爱自己，非要坚持说是对方不够爱自己，真正爱你的人，爱一直都在心里。\n我不知道这样一部电影，是如何让大家产生共鸣，在观影的时候哭得稀里哗啦的，电影中两位男主在“隔壁老王”，讨论是生男孩好还是生女孩好，讨论和双胞胎姐妹约会的好处，和一众长着网红脸的 95 后在 Party 上狂嗨……各种段子让分手变成一场狂欢，而狂欢下的两种状态，有余飞和丁点这种“以分手的名义，撒着思念的娇”的，有孟云和林佳这种因为沟通不善而渐行渐远的。我们常常听到一个词叫做合适，其实以人类这种奇葩的性格，又怎么会找到完全合适的两个人呢？这一切都是需要去不断磨合的，我们看待感情的角度是功利的，如果两个人走到一起甚至结婚，我们觉得这就是合适的，那么像孟云和林佳这种相爱五年的情侣，我觉得我们很难用一句合适或者不合适来说说清楚，两个人在一起一定要多沟通，有问题不要想着第二天再解决。在感情中不要用“作”这种手段去测试对方，一个人喜欢你，愿意忍让你，并不代表你可以毫无底线。也许很多时候，我们都喜欢把话埋在心里，我们都觉得谁先主动谁就输啦，即使此刻赢得了面子，最终还是会输掉了里子。\n这部电影可谓是为“前任系列”画上了一个完美的结局，然而这样一部名不见经传的电影，可以在猫眼电影上获得 9.2 分的高分，一度超越近期口碑极佳的《芳华》，这种前半段走肾后半段走心的编排方式，成功地将观众的情绪带向高潮。有人说在这部电影里找到了自己的影子，可在我看来大家都在努力给自己加戏，两个人会分开一定是有某种原因的，我们总幻想着离开的时候，如何惊天地泣鬼神一般轰轰烈烈，可真正的离开从来都是悄无声息的。一场体面的分手，是两个彼此伤痕累累的的人，在面对这段感情时，即使知道其中存在的问题，但是发现一切再无法回到以前，因为在这个被称为成长的过程中，感情早已被当作燃料的消耗殆尽。所以，我们说：我依然爱你，但我不再喜欢你了，所以我们只能走到这里了。体面的分手，或许应该是这个样子的，是一切大彻大悟以后的慈悲，是用尽了力气后的平静，是虽然结局不如意，但我没有后悔遇见你，是接受成长带来的痛苦，是学会如何更好地爱自己、爱别人。\n有人说，前任就像黑夜中摇曳着的烛火，坍塌在无边的夜色中。或许会有无数根的蜡烛，能照亮你的黑暗，但再不会有这样一个人，能这样一往无前地在黑暗中，为你烫出一个洞，然后用温暖填满。成长是件非常痛苦的事情，你不一定会得到什么，但一定会失去什么，有些人注定是让你成长的，所以请好好再见，不负遇见。有些人会永远停留在你的心里，他们曾经出现，并成为你人生的一部分，我们会爱上下一个，再下一个。放过前任，放过自己，这或许是最好的结局。\n","date":"2018-01-12T00:39:39Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1358971951/","slug":"1358971951","tags":["电影","前任","公众号"],"title":"致前任：愿余生各自安好"},{"categories":["读书笔记"],"content":"本文是一篇读书笔记，由《C#多线程编程实战》一书中的内容整理而来，主要梳理了.NET 中多线程编程相关的知识脉络，从 Thread、ThreadPool、Task、async/await、并发集合、Parallel、PLINQ 到 Rx 及异步 I/O 等内容，均有所覆盖。为了帮助大家理解本文内容，首先给出博主在阅读该书过程中绘制的思维导图，大家可以根据个人需要针对性的查漏补缺。\n《多线程编程实战》思维导图\r线程基础 Tips1：暂停线程，即通过 Thread.Sleep()方法让线程等待一段时间而不用消耗操作系统资源。当线程处于休眠状态时，它会占用尽可能少的 CPU 时间。 Tips2：线程等待，即通过 Join()方法等待另一个线程结束，因为不知道执行所需要花费的时间，此时 Thread.Sleep()方法无效，并且第一个线程等待时是处于阻塞状态的。 Tips3：终止线程，调用 Abort()方法会给线程注入 ThreadAbortException 异常，该异常会导致程序崩溃，且该方法不一定总是能终止线程，目标线程可以通过处理该异常并调用 Thread.ResetAbort()方法来拒绝被终止，因此不推荐使用 Abort()方法来终止线程，理想的方式是通过 CancellationToken 来实现线程终止。 Tips4：线程优先级，线程优先级决定了该线程可占用多少 CPU 时间，通过设置 IsBackground 属性可以指定一个线程是否为后台线程，默认情况下，显式创建的线程都是前台线程。其主要区别是：进程会等待所有的前台线程完成后再结束工作，但是如果只剩下后台线程，则会直接结束工作。需要注意的是，如果程序定义了一个不会赞成的前台线程，主程序并不会正常结束。 Tips5：向线程传递参数，可以通过 ThreadStart 或者 lambda 表达式来向一个线程传递参数，需要注意的是，由 lambda 表达式带来的闭包问题 Tips6：竞争条件是多线程环境中非常常见的导致错误的原因，通过 lock 关键字锁定一个静态对象(static\u0026amp;readonly)时，需要访问该对象的所有其它线程都会处于阻塞状态，并等待直到该对象解除锁定，这可能会导致严重的性能问题， Tips7：发生死锁的原因是锁定的静态对象永远无法解除锁定，通常 Monitor 类用以解除死锁，而 lock 关键字用以创建死锁，Monitor 类的 TryEnter()方法可以用以检测静态对象是否可以解锁，lock 关键字本质上是 Monitor 类的语法糖。 bool acquiredLock = false; try { Monitor.Enter(lockObject, ref acquiredLock) } finally { if(acquiredLock) { Monitor.Exit(lockObject) } } Tips8：不要在线程中抛出异常，而是在线程代码中使用 try……catch 代码块。 线程同步 Tips9：无须共享对象，则无须进行线程同步，通过重新设计程序来移除共享状态，从而避免复杂的同步构造；使用原子操作，这意味着一个操作只占用一个量子的时间，一次就可以完成，并且只有当前操作完成后，其它线程方可执行其它操作，因此，无须实现其它线程等待当前操作完成，进而避免了使用锁，排除了死锁。 Tips10：为了实现线程同步，我们不得不使用不同的方式来协调线程，方式之一是将等待的线程设为阻塞，当线程处于阻塞状态时，会占用尽可能少的 CPU 时间，然而这意味着会引入至少一次的上下文切换。上下文切换，是指操作系统的线程调度器，该调度器会保存等待的线程状态，并切换到另一个线程，依次恢复等待的线程状态，而这需要消耗更多的资源。 Tips11：线程调度模式，当线程挂起很长时间时，需要操作系统内核来阻止线程使用 CPU 时间，这种模式被称为内核模式；当线程只需要等待一小段时间，而不需要将线程切换到阻塞状态，这种模式被称为用户模式；先尝试按照用户模式进行等待，如线程等待足够长时间，则切换到阻塞状态以节省 CPU 资源，这种模式被称为混合模式。 Tips12：Mutex 是一种原始的同步方法，其只对一个线程授予对共享资源的独占访问，Mutex 可以在不同的程序中同步线程。 Tips13：SemaphoreSlim 是 Semaphore 的轻量级版本，用以限制同时访问同一个资源的线程数量，超过该数量的线程需要等待，直到之前的线程中某一个完成工作，并调用 Release()方法发出信号，其使用了混合模式，而 Semaphore 则使用内核模式，可以在跨程序同步的场景下使用。 Tips14：AutoResetEvent 类用以从一个线程向另一个线程发送通知，该类可以通知等待的线程有某个事件发生，其实例在默认情况下初始状态为 unsignaled，调用 WaitOne()方法时将会被阻塞，直到我们调用了 Set 方法；相反地，如果初始状态为 signaled，调用 WaitOne()方法时将会被立即处理，需要我们再调用一次 Set 方法，以便向其它线程发出信号。 Tips15：ManualResetEventSlim 类是使用混合模式的线程信号量，相比使用内核模式的 AutoResetEvent 类更好(因为等待时间不能太长)，AutoResetEvent 像一个旋转门，一次仅允许一个人通过，而 ManualResetEventSlim 是 ManualResetEvent 的混合版本，一直保持大门开启直到手动屌用 Reset 方法。 Tips16：EventWaitHandle 类是 AutoResetEvent 和 ManualResetEvent 的基类，可以通过调用其 WaitOne()方法来阻塞线程，直到 Set()方法被调用，它有两种状态，即终止态和非终止态，这两种状态可以相互转换，调用 Set()方法可将其实例设为终止态，调用 Reset()方法可以将其实例设为非终止态。 Tips17：CountdownEvent 类可以用以等到直到一定数量的操作完成，需要注意的是，如果其实例方法 Signal()没有达到指定的次数，则其实例方法 Wait()将一直等待。所以，请确保使用 CountdownEvent 时，所有线程完成后都要调用 Signal()方法。 Tips18：ReaderWriterLockSlim 用以创建一个线程安全的机制，在多线程中对一个集合进行读写操作，ReaderWriterLockSlim 代表了一个管理资源访问的锁，允许多个线程同时读取，以及独占写。其中，读锁允许多线程读取数据，写锁在被释放前会阻塞其它线程的所有操作。 Tips19：SpinWait 类是一个混合同步构造，被设计为使用用户模式等待一段时间，然后切换到内核模式以节省 CPU 时间。 使用线程池 Tips20：volatile 关键字指出一个字段可能会被同时执行的多个线程修改，声明为 volatile 的字段不会被编译器和处理器优化为只能被单线程访问。 Tips21：创建线程是昂贵的操作，所以为每个短暂的异步操作创建线程会产生显著的开销。线程池的用途是执行运行时间短的操作，使用线程池可以减少并行度耗费及节省操作系统资源。在 ASP.NET 应用程序中使用线程池时要相当小心，ASP.NET 基础切实使用自己的线程池，如果在线程池中浪费所有的工作者线程，Web 服务器将不能够服务新的请求，在 ASP.NET 中只推荐使用 I/O 密集型的异步操作，因为其使用了一个不同的方式，叫做 I/O 线程。 Tips22：APM，即异步编程模型，是指使用 BeginXXX/EndXXX 和 IAsyncResult 对象等方式，其通过调用 BeginInvoke 方法返回 IAsyncResult 对象，然后通过调用 EndInvoke 方法返回结果，我们可通过轮询 IAsyncResult 对象的 IsCompleted 或者调用 IAsyncResult 对象的 AsyncWaitHandle 属性的 WaitOne()方法来等待直到操作完成。 Tips23：ThreadPool.RegisterWaitForSingleObject()方法允许我们将回调函数放入线程池中的队列中，当提供的等待事件处理器收到信号或发生超时时，该回调函数将被调用，这做鱼我们为线程池中的操作实现超时功能。具体思路是：ManualResetEvent + CancellationToken，当接收到 ManualResetEvent 对象的信号时处理超时，或者是使用 CancellationToken 来处理超时。 Tips24：CancellationToken 是.NET4.0 中被引入的实现异步操作的取消操作的事实标准，我们可以使用三种方式来实现取消过程，即轮询 IsCancellationRequested 属性、抛出 OperationCanceledException 异常、为 CancellationToken 注册一个回调函数。 Tips25：Timer 对象用以在线程池中创建周期性调用的异步操作。 Tips26：BackgroundWorker 组件，是典型的基于事件的异步模式，即 EAP，当通过 RunWorkerAsync 启动一个异步操作时，DoWork 事件所订阅的事件处理器，将会运行在线程池中，如果需要需要取消异步操作，则可以调用 CancelAsync()方法。 使用任务并行库 Tips27：TPL 即任务并行库，在.NET 4.0 中被引入，目的是解决 APM 和 EAP 中获取结果和传播异常的问题，TPL 在.NET4.5 中进行了调整，使其在使用上更简单，它可以理解为线程池之上的又一个抽象层，对程序员隐藏了与线程池交互的底层代码，并提供了更方便的细粒度的 API。TPL 的核心概念是任务，一个任务代表了一个异步操作，该操作可以通过多种方式运行，可以使用或者不使用独立线程运行。TPL 相比之前的模式，一个关键优势是其具有用于组合任务的便利的 API。 Tips28：Task.Run 是 Task.Factory.StartNew 的一个快捷方式，后者有附加的选项，在无特殊需求的情况下，可以直接使用 Task.Run，通过 TaskScheduler，我们可以控制任务的运行方式。 Tips29：使用 Task 实例的 Start 方法启动任务并等待结果，该任务会被放置在线程池中并且主线程会等待，直到任务返回前一直处于阻塞状态；使用 Task 实例的 RunSynchronously 方法启动任务，该任务是运行在主线程中，这是一个非常好的优化，可以避免使用线程池来执行非常短暂的操作；我们可以通过轮询 Task 实例的状态信息来判断一个任务是否执行结束。 Tips30：通过 Task 实例的 ContinueWith 方法可以为任务设置一个后续操作，通过 TaskContinuationOptions 选项来指定后续任务以什么样的方式执行。 Tips31：通过 Task 实例的 FromAsync 可以实现 APM 到 Task 的转换 Tips32：通过 TaskCompletionSource 可以实现 EAP 到 Task 的转换 Tips33：TaskScheduler 是一个非常重要的抽象，该组件实际上负责如何执行任务，默认的任务调度程序将任务放置在线程池的工作线程中。为了避免死锁，绝对不要通过任务调度程序在 UI 线程中使用同步操作，请使用 ContinueWith 或 async/await 方法。 使用 C# 6.0 Tips34：异步函数是 C# 5.0 引入的语言特性，它是基于 TPL 之上的更高级别抽象，真正简化了异步编程。要创建一个异步函数，首先需要使用 async 关键字标注一个方法，其次异步函数必须返回 Task 或 Task 类型，可以使用 async void 的方法，但是更推荐 async Task 的方法，使用 async void 的方法的唯一合理的地方就是在程序中使用顶层 UI 控制器事件处理器的时候，在使用 async 关键字标注的方法内部，可以使用 await 操作符，该操作符可与 TPL 任务一起工作，并获取该任务中异步操作的结果，在 async 方法外部不能使用 await 关键字，否则会有编译错误，异步函数代码中至少要拥有一个 await 关键字。 Tips35：在 Windows GUI 或 ASP.NET 等环境中不推荐使用 Task.Wait 和 Task.Result，因为非常有可能会造成死锁。 async 可以和 lambda 表达式联用，在表达式体中应该至少含有一个 await 关键字标示，因为 lambda 表达式的类型无法通过自身推断，所以必须显式地向 C#编译器指定类型。 Tips36：异步并不总是意味着并行执行 Tips37：单个异步操作可以使用 try……catch 来捕获异常，而对于一个以上的异步操作，使用 try…catch 仅仅可以从底层的 AggregateException 对象中获得第一个异常，为了获得所有的异常，可以使用 AggregateException 的 Flatten()方法将层级异常放入一个列表，并从中提取出所有的底层异常。 Tips38：通过 Task 实例的 ConfigureAwait()方法，可以设置使用 await 时同步上下文的行为，默认情况下，await 操作符会尝试捕捉同步上下文，并在其中执行代码，即调度器会向 UI 线程投入成千上百个后续操作任务，这会使用它的消息循环来异步地执行这些任务，当我们不需要在 UI 线程中运行这些代码时，向 ConfigureAwait 方法传入 false 将会是一个更高效的方案。 Tips39：async void 方法会导致异常处理方法，会放置到当前的同步上下文中，因此线程池中未被处理的异常会终结整个进程，使用 AppDomain.UnhandledException 事件可以拦截未处理的异常，但不能从拦截的地方恢复进程，async void 的 lambda 表达式，同 Action 类型是兼容的，强烈建议仅仅在 UI 事件处理器中使用 async void 方法，在其他情况下，请使用返回 Task 或者 Task 的方法。 使用并行集合 Tips40：ConcurrentQueue 使用了原子的比较和交换(CAS)，以及 SpinWait 来保证线程安全，它实现了一个先进先出(FIFO)的集合，这意味着元素出队列的顺序与加速队列的顺序是一致的，可以调用 Enqueue 方法向对接中加入元素，调用 TryDequeue 方法试图取出队列中第一个元素，调用 TryPeek 方法试图得到第一个元素但并不从队列中删除该元素。 Tips41：ConcurrentStack 的实现同样没有使用锁，仅采用了 CAS 操作，它是一个后进先出(LIFO)的集合，这意味着最后添加的元素会先返回，可以调用 Push 和 PushRange 方法添加元素，使用 TryPop 和 TryPopRange 方法获取元素，使用 TryPeek 方法检查元素。 Tips42：ConcurrentBag 是一个支持重复元素的无序集合，它针对以下情况进行了优化，即多个线程以这样的方式工作：每个线程产生和消费其自身的任务，极少发生线程间的交互(因为要交互就要使用锁)。可以调用 Add 方法添加元素，调用 TryPeek 方法检查元素，调用 TryTake 方法获取元素。 Tips43：ConcurrentDictionary 是一个线程安全的字典集合的实现，对于读操作无需使用锁，对于写操作则需要使用锁，该并发字典使用多个锁，在字典桶之上实现了一个细粒度的锁模型(使用锁的常规字典称为粗粒度锁)，参数 concurrentLevel 可以在构造函数中定义锁的数量。这意味着预估的线程数量将并发地更新该字典。由于并发字典使用锁，如无必要请避免使用以下操作：Count、IsEmpty、Keys、Values、CopyTo 及 ToArray，因为需要获取该字典中的所有锁。 Tips44：BlockingCollection 是一个针对 IProducerConsumerCollection 泛型接口实现的高级封装，它有很多先进的功能来实现管道场景，即当你有一些步骤需要使用之前步骤运行的结果时。BlockingCollection 类支持分块、调整内部集合容量、取消集合操作、从多个块集合中获取元素等。 Tips45：对 BlockingCollection 进行迭代时，需要注意的是，使用 GetConsumingEnumerable()进行迭代，因为虽然 BlockingCollection 实现了 IEnumerable 接口，但是它默认的行为是表示集合的“快照”，这不是我们期望的行为。 使用 PLINQ Tips46：将程序分割成一组任务并使用不同的线程来运行不同的任务，这种方式被称为任务并行 将数据分割成较小的数据块，对这些数据进行并行计算，然后聚合这些计算结果，这种编程模型称为数据并行 Tips47：结构并行确实更易维护，应该尽可能地使用，但它并不是万能的。通常有很多情况我们是不能简单地使用结构并行，那么以非结构化的方式使用 TPL 任务并行也是完全可以的。 Parallel 类中的 Invoke 方法是最简单的实现多任务并行的方法，Invoke 方法会阻塞其它线程直到所有线程都完成。 Tips48：Parallel 类中的 For 和 ForEach 方法可以定义并行循环，通过传入一个委托来定义每个循环项的行为，并得到一个结果来说明循环是否成功完成，ParallelOptions 类可以为并行循环定义最大并行数，使用 CollectionToken 取消任务，使用 TaskScheduler 类调度任务。 Tips49：ParallelLoopState 可以用于从循环中跳出或者检查循环状态，它有两种方式：Break 和 Stop，Stop 是指循环停止处理任何工作，而 Break 是指停止其之后的迭代，继续保持其之前的迭代工作。 Tips50：同 Task 类似，当使用 AsParallel()方法并行查询时，我们将得到 AggregateException，它将包含运行 PLINQ 期间发生的所有异常，我们可以使用 Flatten()方法和 Handle()方法来处理这些异常。 Tips51：ParallelEnumerable 类含有 PLINQ 的全部逻辑，并且作为 IEnumerable 集合功能的一组扩展方法，默认情况下结果会被合并单个线程中，我们可以通过 ForAll 方法来指定处理逻辑，此时它们使用的是同一个线程，将跳过合并结果的过程，除了 AsParallel()方法，我们同样可以使用 AsSequential()方法，来使得 PLINQ 查询以顺序方式执行(相对于并行) Tips52：PLINQ 中提供了丰富用以 PLINQ 查询的选项，例如 WithCancellation()方法用以取消查询，这将导致引发 OperationCanceledException 异常，并取消剩余的工作；例如 WithDegreeOfParallelism()方法用以指定执行查询时实际并行分割数，可以决定并行执行会占用多少资源及其性能如何；例如 WithExecutionMode()可以重载查询执行的模式，即我们可以决定选择以顺序执行还是并行执行的方式去执行查询；例如 WithMergeOptions()方法可以用以调整对查询结果的处理，默认 PLINQ 会将结果合并到单个线程中，因此在查询结果返回前，会缓存一定数量的结果，当发现查询花费大量时间时，更合理的方式是关闭结果缓存从而尽可能快地得到结果；例如 AsOrdered()方法，用以告诉 PLINQ 我们希望按照集合中的顺序进行处理(并行条件下，集合中的项有可能不是按顺序被处理的) 使用异步 I/O Tips53：异步 I/O，对服务器而言，可伸缩性是最高优先级，这意味着单个用户消耗的资源越少越好，如果为每个用户创建多个线程，则可伸缩性并不好，在 I/O 密集型的场景中需要使用异步，因为不需要 CPU 工作，其瓶颈在磁盘上，这种执行 I/O 任务的方式成为 I/O 线程。 在异步文件读写中，FileOptions.Asynchronous 是一个非常重要的选项，无论有无此参数都可以，以异步的方式使用该文件，区别是前者仅仅是在线程池中异步委托调用，而后者可以对 FileStream 垒使用异步 I/O。 Tips54：对 HttpListener 类，我们可以通过 GetContextasync()方法来异步地获取上下文。 Tips55：对数据库而言，我们可以通过 OpenAsync()、ExecuteNonQueryAsync()等方法异步地执行 SQL 语句。 好了，以上就是这篇读书笔记的主要内容啦，听说掌握了这 55 条 Tips 的人，都敢在简历上写”精通多线程编程“，哈哈，晚安啦，各位！\n","date":"2018-01-07T21:34:36Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/345410188/","slug":"345410188","tags":["读书","多线程","笔记","编程"],"title":"《C#多线程编程实战》读书笔记"},{"categories":["生活感悟"],"content":"当朋友圈开始集体缅怀 18 岁的时候，我们说，从此以后是 00 后的天下，因为所以的 90 后都成年了。或许我们都是喜欢怀旧的人，所以我们选择以这样一种方式，在狂欢中失去的同时，在失去中缅怀着，仿佛这种死磕到底的做法，会让这一切来得更晚一些。一群人抱着手机等待着新年的到来，和曾经无数个无眠的夜晚一样，我们并不知道明天会有什么，就像钟表的指针哒哒地作响，生命的齿轮永不停歇地转动。我们习惯性地期盼明天会更好，因为我们都知道“往者不可谏，来者犹可追”。那么，在新年钟声敲响以前，让记忆彻底来一场独自的狂欢吧！\n多情应笑我 我不知道该从哪里开始回顾我的 2017 年，如果一定要找一个起点开始说，我想应该是裴武刚离开西安。那时青龙寺樱花正在盛开，在他离开西安以前，我曾带他一起去那里游玩。他曾遗憾没有好好在西安游玩一番，在分别前我们一起去吃羊肉泡馍，真正体会到什么叫做相顾无言，直到十一回家去兰州转车，我们再次联系到彼此，所以当你想做一件事情的时候，最好就是趁着现在去做，一个过期的承诺是没有意义的，人生本就是大海里航行的一叶扁舟，你可以祈祷它不遭遇狂风暴雨，可事实上你并不能替任何人做这样一个决定，越想做好一件事情就越是要减少拖延。“人生不相见，动如参与商”，人生本不完美，缺憾能少一分便是一分。\n我记得我们吃完泡馍，两个人都抢着付钱，老板娘笑着说：“大家都是好朋友，谁付都是一样的，以后还有机会”。两个人一阵默然。从那时算起，到今天差不多 8 个月，我们再没有见过面，偶尔会在微信上聊聊天。人生的无奈就在于，你埋头忙碌的时候，对离别的处境毫不在意，等到你过头来再看时，不禁怅然若失，感情就这样一天天变淡，可是不是你努力融入对方的生活，就能够留住彼此在心中的位置呢？有人告诉我，她不属于你，她不适合你，可是不是换一个性格完全相反的人，就一定会是合适的人选呢？我们的大脑，同绝对理性的计算机最大的不同是，我们评价和认识一个人的标准，永远没有办法达到统一，就像你喜欢吃葱，而我喜欢吃香菜，你不能因此指责别人三观不正，这或许仅仅是你们的喜好不同而已，这个理性而感性的世界，有时真的让人抓狂。\n我一直没有告诉任何人，我和小古分开快两年的时间了。有人说，时间会慢慢磨去一切痕迹，而对于她，始终就像我心上的一颗痣，时间会把它变成一颗琥珀，时间越久越显得珍贵，我知道有人会嘲笑这固执的感情，就像我会在分开一年后，坐一个晚上的火车到洛阳，仅仅是为了见她一面而已，我永远记得她穿裙子的样子，我永远记得那条挂满灯笼的路，我永远都想再抱她一次……我听到莎莉花园的时候，会突然想起那个下午，当我们听到一家店里传来的音乐声，我们互相对彼此说我知道……我们在彼此不成熟的年龄深爱上彼此，等一天幡然醒悟的时候发现追悔莫及，离开她以后我发现我会单身很久，因为除了她我好像都不会同别人谈恋爱，这种迹象在我身上更加明显，因为无论你怎么尝试去改变，你永远无法逃开被拒绝的命运。我们喜欢喜欢一个人的时候，通常靠眼缘这种特别主观的东西，可太多时候我们无法完全了解一个人。\n从前我们喜欢一个人的时候，喜欢爱得轰轰烈烈，相信爱情可以战胜一切；等到长大以后，我们习惯爱得小心谨慎，试探和套路混杂不清真真假假。有人告诉你，要相信爱情是存在的，你只是还没有等到那个合适的人出现；有的人告诉你，不要在一棵树上吊死。成年人的感情和时间都非常有限，不要把精力浪费在一个不喜欢你的人身上。可兜兜转转就到了 25 岁，一个谈爱已晚、谈死尚早的尴尬年纪。有时候我会想，大学毕业了就可以结婚成家的人，或许是最幸福的人，因为你再找不到那样单纯的爱情。我见过情侣间吵架无数分分合合，我见过向现实妥协随便找个人结婚……我想找一个可以托付终生的人，因为父母总有一天会先我们而去，如果需要有人陪伴在我生命的一刻，我希望那个人会是你，可我决不会如此自私，因为我不想你在这个世界里孤单。我曾被人温柔地对待过，我想温柔地对待你，不是因为亏欠或者内疚而弥补什么，而是我知道“爱人者人恒爱之”。\n我曾经被一个女孩子拒绝过，我天真的以为，出场顺序会决定两个人的命运，直到她平静地说出，“就算我不和他在一起，我绝不会和你在一起“，原来一个在小古心里待她特别好的人，在别人眼中居然是如此的不堪，我记得我在她面前结结巴巴说英语的窘迫，我记得她告诉我她要订婚时我写满难过的脸……我好像被施诅咒一样，在和小古分开后，我喜欢过两个有男朋友的女生，有时候我都想告诉自己，我不再是那个十八九岁的少年，长大以后的爱情夹杂着现实，变得更加让人迷惑，可我再找不回那种“赌书泼茶”的感觉，或许求而不得是人生常态，王尔德说“人生有两种悲剧，一种是想得到的得不到，一种是想得到的得到了”，我对小古说，给我留一个梦好吗？她说好的呀。对这个世界而言，我们两个人的故事，或许一点都不美丽。可真正让我怀念的，恰好是这些微不足道的故事，有时候我会分不清，喜欢的到底是真实的她，还是想象中的她，或许我真正喜欢的，是曾经那个年轻而勇敢的少年，佛家有”贪嗔痴慢疑“，所谓五毒心者，都说人要学会放下执念，可如果从来没有得到过，又要放下什么呢？人因为“求而不得”而痛苦，从来没有拥有过的人，是无所谓拿起和放下的。\n做人没意思 大概从 8 月份开始学习做饭，而学习做饭的动机早就模糊不清，我只知道，从那一刻开始，我的生活开始慢慢发生变化，从一个不知道做饭需要买什么调味品的人，变成一个喜欢逛超市、喜欢看商品价格的人，变成一个喜欢周末去超市购物、喜欢钻研美食的人，变成一个懂得如何照顾自己、爱惜自己的人。西安的面食对我而言是粗犷的，如同这片关中大地上粗犷的民风，所以最初就买了本菜谱学习做饭，我学做饭的动机，其实是非常简单的，最低要求是以后讨不着老婆不会饿死自己，进阶要求是两口子过日子必须要有一个具备这项生活技能，现在我们看到有把吃作为爱好的吃货，可这样的人是不能称之为吃货的，因为吃货如果就是花花钱动动嘴这样简单，就对不起古往今来的吃货们，譬如苏东坡之于红烧肉，季鹰之于鲈鱼，袁枚之于《随园食单》，能做会吃这是真吃货。有人说，征服一个吃货的心，首先要征服她的胃。可也许这仅仅是一个理由，当你征服她的胃，你会发现还有更多的东西等你去征服。现在，先让我征服这本菜谱，我刚刚学会其中的 10 来道家常小炒。\n除了做饭这件事情以外，我一直在坚持的事情是读书，去年国庆的时候买了 Kindle，而年初则办理了省图的借书卡。一旦这两者结合起来以后，读书就变成了一件趣事。因为 Kindle 不擅长阅读技术类书籍，所以技术类书籍主要来自纸质书，而人文类书籍主要来自电子书。最早是 Wesley 推荐读陈忠实的《白鹿原》，这本书让我了解了许多陕西的风土人情。在同名电视剧中，张嘉怡饰演的白嘉轩，生平念念不忘的是妻子亲手做的油泼面。陕西的面食的确让人印象深刻，原本出生在北方的我，在这里仿佛第一次来到北方。此外，由电影《嫌疑人 X 的献身》追溯到原著，逐渐接触了《白夜行》和《解忧杂货店》这样典型的东野圭吾作品，程浩的《站在两个世界边缘》，毛姆的《月亮与六便士》，堀辰雄的《起风了》，《小王子》，沈复的《浮生六记》以及《追风筝的人》，而纸质书基本都是 Web 前后端(JavaScript/ASP.NET/SPA)相关的书籍，Python 相关的书籍和数学相关的书籍，这些书籍不能在这里展开讨论，感兴趣的朋友可以在博客里留言。\n我是一个喜欢看电影的人，就是单纯地喜欢听人讲故事，这和约会意义上的看电影不同，我就是单纯地去看个电影而已。不过作为一个日常单身狗，总是要学会面对这个世界里满满的恶意。2017 年看的第一部电影是韩寒的《乘风破浪》，这是一个笑点中夹杂着泪点的故事，剧情上借鉴了陈可辛导演的《难兄难弟》。接下来，2017 年看过的超级英雄电影有《神奇女侠》、《蜘蛛侠 : 归来》、《雷神 : 诸神的黄昏》、《猩球崛起》。2017 年看过的动画电影有《大护法》和《寻梦环游记》，其中前者是电影风格和政治隐寓吸引了我，而后者是讲述了一个亡灵节背景下的暖心故事，告诉我们如何去面对死亡、如何平衡家庭与梦想等。2017 年看过一部讲述野外探险的电影《七十七天》，由赵汉唐和江一燕主要，这个电影是部旅游风景片，江一燕的存在感略弱，男主最终在雷雨夜里丧生，告诫人们要懂得量力而行，我们觉得去趟西藏就能净化心灵，可比净化心灵更重要的是好好活着。2017 年最后一部电影是陈凯歌的《妖猫传》，改编自梦枕貘的《沙门空海》，以唐朝时期僧人空海和诗人白居易为主角，讲述唐玄宗和杨玉环的爱恨纠葛，这部电影视觉美是无与伦比的，虽然后期剧情呈现方式存在瑕疵，但可以让我们重新感受大唐盛景，以一种新的方式解读《长恨歌》，我觉得这样就很好啦！\nemsp; 2017 年写作方面相对去年明显退步，因为有两个月一直没有时间写东西，尤其是拖延症发作的时候，一篇博客大概两三个周甚至一个月方能写完，这大概是 2017 年最让人遗憾的事情。2017 年共撰写博客 16 篇，访客量增加约 30 万，受到 CSDN 运营梦姐的离开以及 CSDN 战略调整的影响，2017 年博客流量的主要来源是旧文章。2017 年技术博客的写作，基本延续 2016 年的策略，不再写面向新手的教程类内容，而是侧重对技术的整合和改进，尽可能地去写一种思路或者想法，与此同时，希望在技术博客以外扩展更多的，譬如写对生活的感悟、对电影的思考等等，计划中打算开通知乎专栏(已开通)、开通个人微信公众号(正在准备)。总而言之，希望拓宽博客的流量渠道，提升个人品牌的影响力，你可以不用长得特别帅，但要有一种不怒自威的气场，因为要想成为架构师，这是件任重而道远的事情！\nNew TodoList(\u0026ldquo;2018\u0026rdquo;) 写这样一个 TodoList，我总觉得像在立一个个 flag，因为 2017 年计划的 MongoDB 就没有达到目标，所以 2018 的 TodoList 我希望可以更接地气一点，具体来讲，我从技术、生活两个方面来制定目标：\n首先，希望 2018 年的我在技术方面：\n学会一个前端框架(Vue/Angular/React) 继续完善 HttpServer(支持 RESTful) Web 后端认证授权及中间件(OWIN/OAuth) 继续学习 Python 数据分析(Pandas) 写一个微信小程序(词典/小工具) 建立知乎专栏/个人微信公众号 其次，希望 2018 年的我在生活方面：\n认真谈恋爱，找个女朋友 提升整体审美及品味(穿衣/吃饭/健身) 去一趟莫高窟或者苏州园林(做想做的事) 争取涨薪，继续存钱 租一个可以更好做饭的房子 和任何人都聊得来(沟通技巧/情商) 2018，愿期望不负，愿你被温柔对待，岁月静好，现世安稳，心安处即吾家，晚安！\n","date":"2017-12-31T21:30:45Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2676125676/","slug":"2676125676","tags":["总结","回首","展望","随笔"],"title":"2017，在驻足间回首"},{"categories":["数据分析"],"content":"或许是因为我喜欢的姑娘从来都不喜欢我，而感情上的挫折一度让我陷入无尽的自卑。朋友在朋友圈里发布一条关于皮影戏的动态，我开玩笑说这个皮影戏结局应该是个悲剧，因为我注意到在剧中，无论一个人如何卖力地表演甚至双腿跪倒在地，有的人从故事开场到结束一直对此无动于衷。朋友回复我说，这不就是你现在的状态吗？我沉默半天终于熄灭手机屏幕。我听到过各种各样让我放弃她的话，即使这种念头在我脑海里萌生已久，是幻想让我硬生生地拖了这么久。当你努力想要融入对方的生活，而等待你的是一道冰冷的墙。这种感觉像什么呢？大概就是一个又一个“好友”安安静静地躺在联系人的置顶名单里，不敢发消息让对方知道，更不愿残忍地把对方删除。我安慰自己说，对我而言，我失去的是一个不喜欢你的人；而对对方而言，失去的是一个喜欢她的人。你当然可以说我没有那么喜欢她，如果一定要喜欢到卑微如尘土的地步，我宁愿一个人单身到天荒地老。\n当我意识到人与人间，即使亲近如父母尚且无法完全理解彼此的时候，我忽然发现一个有趣的现象，我们喜欢一个人的时候，首先注意到的会是外表，我们将其称之为眼缘，所以人与人间的感情纽带最初会是吸引，而后是了解彼此的优缺点，最终是相互理解和扶持。可我们知道，外表是可以伪装出来的，所以我们习惯通过外表和言语来评价一个人，这就像是数学归纳法，我们总认为推倒第一块多诺米骨牌，就意味着所有多诺米骨牌都会倒下。可现实世界矛盾的地方就在于，我们认为理所当然正确的事情，或许正是我们无法证明其正确性的，这在数学上称为哥德尔不完备定理。所以，一件残酷的事情是，当你无法吸引一个人的时候，通往内心世界的路就被堵死了。朋友圈里精彩纷呈的社交互动，并不代表有人愿意真正了解你的生活，何况是你吸引不到的人呢？我很想知道，我们在选择伴侣的时候到底看中什么，所以我一直在关注@西安月老牵线上发布的征婚交友类微博，本文的故事从这里正式展开。\n身高 175 的悲伤 或许你以为我会无聊到试图从微博上找到女朋友，可事实上作为一个程序员的我，即使整天投入精力在编程上，依然无法避免对象空引用的异常出现。如果说找到女朋友是个小概率事件，那么在我看来，找到一个真正懂我、喜欢我的女朋友，基本上是不可能事件。你不要觉得我对没有调整好心态、对生活过分悲观，如果你了解贝叶斯公式就会真正地理解我说的话。这个微博开始引起我的注意，是我发现身高在 155 到 165 左右的女生，对男生的要求基本上无疑例外地是 175+到 180+，我想知道到底有多少女生是有这样的想法，这是我想要抓取新浪微博的数据进行分析的初衷。更重要的是，身高不到 175 的我在面对这种要求的时候是悲伤的，因为我想起了《巴黎圣母院》中的卡西莫多，一个外表丑陋而拥有高尚人格的“丑八怪”。现代人整天都特别忙碌，以至于没有人会有耐心，园艺在忍受着你丑陋的外表的同时，同你讲一只小兔子亲了它喜欢的长颈鹿一下这种故事。\n我听到这样一句话，“好看的皮囊千篇一律，有趣的灵魂万里挑一”，可谁会觉得像卡西莫夫这样的人，会拥有或者配拥有高尚的人格呢？我们这副皮囊不管好看与否，它们都是父母给予我们的最好的礼物。难道一个所谓情商高的人，会在收到别人的时候因为礼物不好看而生气吗？ 我想起《画心》里懊悔受狐妖小唯皮相蛊惑而自毁双目的霍心，美丑都是父母赐予我们的，不该被我们拿来一番大肆炫耀，可我还是想知道，我们评价一个人的标准到底是什么？因为我渐渐明白，有些人不喜欢我们，并不是我们不好，而仅仅是某一点和对方不匹配。喜欢一个人的时候，像拔下身上的一根根刺，因为你越是得不到回应，就越像变成对方期待的样子，这个过程会让你觉得自己一无是处。直到今天看到一句话，一句足以热泪盈眶的话，如果不曾喜欢你，我本来非常可爱的。有时候，人做一件事情，或许就是在和自己过不去，比如说这件事情。\n花点时间爬爬微博 好了，现在我们来考虑从新浪微博上抓取@西安月老牵线上发布的微博，因为这是我们进行数据分析的前提。事实上，在写这篇文章前我曾花了大量时间来调试爬虫，然后用了一天的时间对数据进行清洗，最终利用晚上下班的时间生成词云。由此我们可以理出整体的思路：\n流程图\r通过流程图我们可以注意到，在这里我选择了 Python 来实现整个功能。转眼间我已经 25 岁了，这是种什么样的概念呢？两年前我 23 岁的时候，听别人讨论结婚这个问题，我觉得它离我还很遥远。如今看着周围人都结婚了，我竟有种“高处不胜寒”的感觉。所以呢，人生苦短，当你不能阻止时间一天天消逝的时候，你只能趁着现在去做你想做的事情，为了节省时间去做技术以外的尝试，我选择拥有全世界最丰富的库的 Python。\n这段时间学习数据分析，我渐渐意识到我们所熟悉的这个世界，如果以一种理性的角度，完全通过数据来解构的话，我们在这个数字时代里留下的每一条讯息，都冷冰冰地暴露着我们的喜怒哀乐，每一张照片里细微的表情变化，每一段文字里隐匿着的真实意图，都能被人脸识别和自然语言处理等等，这类人工智能为代表的技术所解读，我们努力想在朋友圈里隐藏些什么，当朋友圈的访问范围从半年逐渐缩小到三天，我们究竟能隐藏下什么呢？\n微博爬虫分析 首先，我们需要从微博上抓取数据下来，我没有去做抓包分析这样的重复性工作，因为我注意到这个问题，在网络上有很多朋友在讨论，我主要参考了以下内容：\nPython 爬虫如何机器登录新浪微博并抓取内容？ https://github.com/xchaoinfo/fuck-login 用 Python 写一个简单的微博爬虫 通过以上内容，我了解到在抓取新浪微博数据的问题上，我们基本会有以下思路：\n保存 cookie 信息，利用 requests 库发起请求并带上 cookie 利用 requests 库模拟登录新浪微博并在请求过程中保持 cookie 利用 selenium 库模拟登录新浪微博然后取得页面内容 利用 PhantomJS 库模拟登录新浪微博然后取得页面内容 可以看出差异主要集中在 cookie 的获取以及是否支持 headless 模式，并且我们得到一个共识，抓取新浪微博移动版要比PC 版要容易，因为移动版优先为小尺寸屏幕设备提供服务，因而页面结构相对整洁便于我们提取数据。起初博主认为第一种方式太简单粗暴，坚持要采用第二种方式去实现，最终证明还是太年轻了啊，新浪微博的登录给人的感觉就是蛋疼，这里就简单介绍下思路哈。\n首先我们会向服务器发出一次 GET 请求，它返回的结果是一段 JavaScript 代码，然后我们需要用正则匹配出其中的 JSON 字符，这样我们就获得了第二次请求需要用到的参数；接下来，第二次请求是一个 POST 请求，我们需要将用户名采用 Base64 加密，密码则采用 RSA 加密，需要用到第一次请求返回的参数。实际上，新浪微博官方给我们提供 API 获取微博数据，可这个 API 可以获取的微博数据非常有限，更让人难以接受的是新浪微博的应用授权方式，如果我们采用调用 API 的方式，在这里会有第三次 POST 请求，有朋友分析了完整的模拟登录过程，可我对此表示毫无兴趣啊。最早我采用了模拟这种方式，抓取第一页的时候还是登录的状态，可等到抓取第二页的时候变成了注销的状态，整个过程使用的是同一个 session 对象，所以我最后果断放弃了这种方式。\n好了，现在我们只需要在 Chrome 里 F12 找到 Network 选项卡，抓一次包取得 cookie，然后在请求的时候带上 cookie 即可。我们不用过分担心 cookie 过期的问题，在博主测试的时候，一个 cookie 可以持续工作 3 至 5 天，而且在后面我们会讲到，这个爬虫抓取的数据量其实并不大，在一两个小时内就可以完成抓取，没有必要将爬虫考虑得太严谨。在下图中我们标记出了博主计算机上存储的 cookie，我们通过 cookie 就可以免登录抓取信息啦。\n提取Cookie\r解决登录的问题以后，回到这个问题本身，我们需要抓取@西安月老牵线发布的所有微博，移动版对微博做了分页处理，所以我们只需要知道总共有多少页，然后循环去提取每一页里的信息即可，因为我们注意到每一页的地址都符合https://weibo.cn/u/3232168977?page=2这样的形式。首先页数，我们可以通过 name 为 mp 的隐藏控件来获得，其 value 属性表示总页数。其次，每条微博存放在 class 为 c，id 以 M_开头的 div 标签里，在这里我们只需要文本信息，顺藤摸瓜我们发现信息被存放在 class 为 ctt 的 span 标签里，这里博主遇到一个奇怪的问题，BeautifulSoup 默认的解析器 html.parser，不知道因为原因无法解析出标签，而 lxml 当时因为 pip 的问题无法安装，所以不能使用 XPath 来解析 DOM 解构，在这里我认为 XPath 更适合这个场景，如果有时间可以考虑对代码进行重构。\n在抓取微博的过程中，博主发现官方的反爬虫策略非常给力，连续工作超过 5 分钟 IP 就会被封锁，进而无法访问微博的服务器， 大概经过 20 至 30 分钟后会自动解封。或许主流的方案是花钱买动态代理，可我这个就是临时起意的一个想法，所以我采取了最简单粗暴的方法，让线程睡一会儿，在这样条件下，我花了大概 1 个半小时到 2 个小时左右的时间，从微博上抓取了 5600 条数据，并将其存储在了 SQLite 数据库中。什么？你问我为什么不考虑多线程，因为我这个人懒啊，这个问题最难的地方在数据分析，数据抓取方面我不太关注效率，因为我有足够的时间去等这些数据，所以关于性能方面的问题，有时间我们再做进一步讨论吧！\n数据处理过程 数据处理这块，我本来打算尝试下 MongoDB 这个数据库的，而实际上这是我今年计划要去学习的内容之一，后来因为种种原因一直搁置到现在，可当我注意到 Windows 下安装 MongoDB 的繁琐后，我果断放弃了这种念头回归简单的 SQLite，我基本上是交叉使用 Windows 和 Linux，而我知道 Linux 下安装 MongoDB 是非常简单的。我反复强调我喜欢小而美的东西，就是因为我想保留对方案的选择权。在这里我们的数据处理，主要是数据清洗和中文分词。首先，我们来一起看看数据库表的设计：\n数据表结构\r我这里一切从简，所以将这 5600 多条数据都存储在一张表里，表中有四个字段 ID、Post、Wish 和 Tags。显然，ID 是自增的主键，为每条微博提供一个唯一的标识；Post 存储我们从微博上抓取的原始信息，这里不含 HTML 标签，可是会含有微博表情字符啊摔；Wish 存储每条微博中对伴侣的要求具体有哪些，这里我们主要通过关键字来截取可谓简单粗暴，具体原因稍后会讲到:)；Tags 存储 Wish 字段经过分词以后的结果，这里我们使用结巴分词和 SnowNLP，该字段中存储的是序列化后的 JSON 字符串，下面我们具体来讲这些字段的处理。\n首先，分析@西安月老牵线发布的微博我们可以发现，所有征婚相关的微博都是以**#征婚交友#或者#月老爱牵线#这样的话题开始，并且每条微博都是先介绍个人情况，然后再描述对理想伴侣的期望，所以我们只需要找出每条微博里对理想伴侣的期望相关的描述，然后再根据这条微博是由男嘉宾还是女嘉宾**发布的，即可汇总出男、女性对各自伴侣的期望到底是什么，我们将这部分信息更新在 Wish 字段里，我们一起来看具体的代码：\n# Filter Data sql = \u0026#34;SELECT ID, Post FROM table_weibo WHERE POST LIKE \u0026#39;%%%%%s%%%%\u0026#39; OR POST LIKE \u0026#39;%%%%%s%%%%\u0026#39;\u0026#34; sql = sql % (u\u0026#39;#征婚交友#\u0026#39;,U\u0026#39;#月老爱牵线#\u0026#39;) self.cursor.execute(sql) rows = self.cursor.fetchall() # Adjust Data patterns = [\u0026#39;想找\u0026#39;,\u0026#39;希望找\u0026#39;,\u0026#39;要求\u0026#39;,\u0026#39;希望另一半\u0026#39;,\u0026#39;择偶标准\u0026#39;,\u0026#39;希望对方\u0026#39;, \u0026#39;希望\u0026#39;,\u0026#39;找一位\u0026#39;,\u0026#39;找一个\u0026#39;,\u0026#39;一半\u0026#39;,\u0026#39;找\u0026#39;,\u0026#39;想\u0026#39;,\u0026#39;喜欢\u0026#39;,\u0026#39;择偶条件\u0026#39;,\u0026#39;寻\u0026#39;,\u0026#39;期待\u0026#39;, \u0026#39;女孩\u0026#39;,\u0026#39;男孩\u0026#39;,\u0026#39;女生\u0026#39;,\u0026#39;男生\u0026#39;,\u0026#39;女士\u0026#39;,\u0026#39;男士\u0026#39;,\u0026#39;理想型\u0026#39;] sql = \u0026#34;UPDATE table_weibo SET Wish = ? WHERE ID = ?\u0026#34; for row in rows: id = row[0] post = row[1] match = -1 for pattern in patterns: if(pattern in post): match = post.find(pattern) + len(pattern) break if(match != -1): wish = str(post[match:]) wish = wish.replace(\u0026#39;#西安月老牵线#\u0026#39;,\u0026#39;\u0026#39;) wish = wish.replace(\u0026#39;[心]@月老蜀黍\u0026#39; ,\u0026#39;\u0026#39;) wish = wish.replace(\u0026#39;#月老爱牵线#\u0026#39; ,\u0026#39;\u0026#39;) self.cursor.execute(sql,(wish,id)) else: self.cursor.execute(sql,(\u0026#39;\u0026#39;,id)) self.connect.commit() 可以注意到，我们首先按照话题对微博进行了筛选，然后通过关键字列表 patterns 来截取我们所需要的 Wish 字段，实际上这里是需要反复去调整 patterns 的，直到所有满足我们期望的数据都被提取出来，所以这是一个渐进式的数据处理过程。或许我们能想到通过 NLP 相关的技术来分析这段文本，我尝试通过 SnowNLP 去分析这样一段长度为 100 到 500 的文本，因为 SnowNLP 具备分析一段话的摘要及关键字的能力。可我发现这样实践下来效果并不太好，这是因为 SnowNLP 本身是以电商网站的评论数据为基础的，所以遇到我们微博这样相对灵活的文本信息时，它提取出的关键字并不能完全地符合我们的期望。固然，我们可以通过训练 SnowNLP 来达到我们的目的，可训练需要准备大量的文本信息作为支撑。作为一个懒惰的人，我最终选择了通过关键字来提取关键信息，准确度基本可以保证 90%以上，因为暴力截取难免会拆分出不符合期望的信息。\n接下来，我们有了针对男、女择偶要求期望的 Wish 字段，可这些信息对我们而言，依然显得繁重而冗余，所以接下来我们考虑对 Wish 字段进行分词，最初的设想是通过词性和语法来分析，可当我分完词以后我就不得不佩服中文的博大精深，这里我选择了两个中文处理相关的库，即结巴分词和SnowNLP，它们都是开源项目并且有大量的文档作为参考，这里想说的是，SnowNLP 中支持中文文本的情感分析，这是我最初想要使用这个库的一个重要原因，因为我想从这些微博中找出评价一个人的形容词或者名词，而这些词的情感分析，可以作为我们是否将其作为一个评价指标的重要依据。\n可我们有句话叫做“认真你就输了”，尤其在女性的思维模式中，充满太多太多不能直接去理解的信息。这种我不用举例子啦，现在铺天盖地的直男癌/女权癌席卷而来，其实有太多问题无关对错，你输就输在没有照顾好对方的情绪，我们现在常常把情商挂在嘴上，可情商概念中的自我意识、控制情绪、自我激励、认知他人情绪和处理相互关系，有 60%说的是自我管理，而其余的 40%，恰恰就是我们日常理解中关于人际关系方面的，所以我们说人工智能不能完全代替人类，因为只有绝对理性的世界是恐怖的，可只有情绪化的感情而不讲道理的世界则是空虚的。我们可以去追逐人类内心中的灵性，即真、善、美，这是任何冰冷的计算机所不具备的东西。可我们能不能真诚一点呢，明明知道这一切都是套路可你还满心期待，我们并非不懂得什么是爱，爱不是我用一个又一个套路去套路你，而是我明知这是套路还愿意陪你表演下去，我没有在讽刺浙江卫视某节目，愿温柔的你被这个世界温柔地对待。\n关于结巴分词和 SnowNLP 地对比评测，大家可以参考：Python︱六款中文分词模块尝试，这里博主发现 SnowNLP 适合做大颗粒分词拆分，而结巴分词适合做小颗粒分词拆分。其实，从分词效果上来讲，结巴分词是要比 SnowNLP 效果更好一些的，可我这样说不是会显得情商比较高吗？这样你们会喜欢吗？最终我们采取的方案是两者混用，故而我们有了这样的代码：\ndef generateTags(self,text): snow = SnowNLP(text) sentences = snow.tags tags = [] for s in sentences: words = pseg.cut(s[0]) for w in words: tags.append({\u0026#39;word\u0026#39;:w.word,\u0026#39;flag\u0026#39;:w.flag}) return json.dumps(tags) 可以注意到，这里我们使用结巴分词获得了每个词的词性，不过到我写这篇文章的时候，对于词性的处理我依然没有什么好的想法，这里仅将其作为结果以 JSON 的形式存储到数据库中，现在我们基本上完成了所有数据处理的流程，在这个过程中会有些特殊的中文字符，我们采取暴力替换的方式进行去除即可，对此不在这里展开说啦。下图展示了数据库中部分数据：\n处理后的数据\r处理结果呈现 说起这篇文章，可以说这是我第一次接触数据分析，我们这个时代积累了大量的数据，所以我们有基于大数据的推荐和预测等等相关场景，知乎和微博的首页 Feed 流经过无数次算法调整，可人们依然在抱怨算法向人们推荐了无关的内容，这是否说明，我们所期待的智能，仅仅是让我们觉得智能而已，这一系列基于统计的数据分析理论，是否一定是符合某种冥冥之中的规则，我想起《模仿游戏》中和卷福扮演的图灵形成鲜明对比的，正是以休.压力山大为代表的统计方法派，电影中他们试图通过分析字母出现的频率来破解恩尼格玛。对于数据分析而言，如果说可视化是面向人类的分析手段，那么数据挖掘就是面向机器的分析手段。作为一个刚刚入门的萌新，我描述的是我对数据分析的一种感觉。回到本文主题，这里我选择以词云作为最终处理结果的呈现载体。\n词云，即 WordCloud，是一种展现关键字出现频率的表达方式，如果你对博客写作比较熟悉的话，就会知道诸如 WordPress、Ghost、Hexo 等都提供了标签云功能，我们每篇文章中都会给文章添加若干标签，而标签基本可以让读者了解这个博客都有哪些内容，在标签云中出现频率越高的标签其字体通常会越大，这样我们可以非常直观地了解到，每个因素在整体上占到的比重。本文之所以采用这种方案，正是希望通过词云来呈现男女在择偶观上更看重什么。生成词云的方式有很多，具体可以参考这篇文章：除了 Tagxedo 外，还有什么好的软件制作可以词云?，而博主最终选择了wordcloud，你可以看到 Python 基本上是万能的语言，有这么多优秀的第三方库可以用，我就问你怕不怕，关于这个库的用法，请参考：https://amueller.github.io/word_cloud/，如果通过 pip 无法直接安装该库，可以通过这里下载.whl 文件进行安装，注意升级 pip 到最新版本即可。\n参照官方的示例，我们从数据库中根据 Post 来过滤性别，根据 Tags 来获取关键字，然后将所有 Tags 串联成一个字符串，传递给 WordCloud 模块即可。下面给出代码片段：\n# Filter Data sql = \u0026#34;SELECT Post, Tags FROM table_weibo WHERE Tags \u0026lt;\u0026gt; \u0026#39;\u0026#39;\u0026#34; self.cursor.execute(sql) rows = self.cursor.fetchall() # Filter Tags male_tags = \u0026#39;\u0026#39; female_tags = \u0026#39;\u0026#39; for row in rows: post = row[0] tags = json.loads(row[1]) if u\u0026#39;男嘉宾[向右]\u0026#39; in post: female_tags += \u0026#39;,\u0026#39;.join(map(lambda x:x[\u0026#39;word\u0026#39;],tags)) elif u\u0026#39;女嘉宾[向右]\u0026#39; in row[0]: male_tags += \u0026#39;,\u0026#39;.join(map(lambda x:x[\u0026#39;word\u0026#39;],tags)) # WordCloud self.generateWordCloud(female_tags,\u0026#39;female.png\u0026#39;,\u0026#39;output_female.png\u0026#39;) self.generateWordCloud(male_tags,\u0026#39;male.png\u0026#39;,\u0026#39;output_male.png\u0026#39;) 在这里我们主要将男嘉宾/女嘉宾分别筛选出来，然后将分词结果用逗号串联起来，这样即可得到 male_tags 和 female_tags，我们会将其传递给 WordCloud 模块，可以注意到我们为男性/女性词云分别设置了不同的背景图片，最终会生成两张不同的图片，这里主要参考了Image-colored这个示例，代码片段展示如下：\ndef generateWordCloud(self,text,background,output): back_coloring = np.array(Image.open(background)) stopwords = set(STOPWORDS) stopwords.add(u\u0026#39;西安\u0026#39;) stopwords.add(u\u0026#39;生活\u0026#39;) wordcloud = WordCloud( font_path=\u0026#39;simfang.ttf\u0026#39;, # 设置字体 background_color=\u0026#34;white\u0026#34;, # 背景颜色 max_words=5000, # 词云显示的最大词数 mask=back_coloring, # 设置背景图片 stopwords=stopwords, #停用词设置 max_font_size=75, # 字体最大值 random_state=42, width=1000, height=860, margin=15,# 设置图片默认的大小,但是如果使用背景图片的话,那么保存的图片大小将会按照其大小保存,margin为词语边缘距离 ) wordcloud.generate(text) plt.imshow(wordcloud) plt.axis(\u0026#34;off\u0026#34;) plt.show() wordcloud.to_file(output) 这里我们注意到添加了两个停止词，这是因为我们发现，西安和生活这两个关键词，在整体中所占权重虽然较高，可是因为我们这里抓取的是西安本地的微博，所以这两个关键词对我们而言是没有意义的。再对这两个关键字进行剔除以后，我们最终生成的词云如图：\n男性心目中的伴侣\r女性心目中的伴侣\r这是个看脸的世界 对这样一个显然成立的结论，我是表示失望的，这种感觉像什么呢？就像你期待着对方说喜欢你，结果到最后她还是会说我们不合适。可花费如此大的篇幅来讲这样一个悲伤的故事，我们就象征性地分析下结论吧！首先，我们注意到男性心目中的伴侣，排名靠前关键字是性格、孝顺、善良、懂事、结婚、身高、眼缘，而女性心目中的伴侣，排名靠前的关键字是身高、稳重、责任心、上进心、工作、成熟。所以，我现在完全可以理解，为什么女生会对 180 的身高如此迷恋，因为她们想被男朋友举高高呀，我的一位朋友如是说。\n与此同时，我们发现很多指标譬如孝顺、善良、稳重、责任心、上进心等，其实都是需要两个人在相处久了以后慢慢去验证的，可这些最终会被眼缘和身高这种因素阻挡在外面。或许你会觉得这样浅显的道理，居然值得我花费时间和精力去思考。一个你吸引不到的人，终究是难以拉近两个人心间的距离的。喜欢是两个人的事情，我不是要卑微地乞求你来见我，而是你想要来见我我就主动迎上去。有天在 QQ 空间看到有人在看《怦然心动》，就忍不住下载下来一个人看。有时候我们喜欢的那个人，或许并没有那么好，你会渐渐发现，那种因为喜欢而附加在对方身上的光环，会随着时光而慢慢变淡。而有那么一瞬间，我只想比她变得更优秀，而不再幻想她会转身回来看我，或许这就是成长吧！\n本文小结 大概没有谁会像我这样，在写一篇技术文章的过程中，掺杂如此多的个人情感。可有时候，一个人做一件事情的动机，的确就是如此简单。我羡慕 175 以上的身高，可是不是我具备了这样的身高，你就会喜欢我呢？我想应该不会吧，因为你总能找到新的理由来拒绝我。所以，我写这篇文章，通过 Python 抓取新浪微博数据并对其进行分析，并不是想告诉你，你因为不具备哪些因素而不被人喜欢，而是想告诉你，我们每一个人都是这个世界上独一无二的存在，我们的优点同我们的缺点组合起来，这才是完整的我们。别人喜欢不喜欢我们到底有什么意义呢？就像我们喜欢许嵩、喜欢林依晨，难道就要让人家喜欢我们吗？我可以非常喜欢你，但我一定要骄傲地喜欢你，因为我骄傲时的样子最帅，谁让这是一个看脸的世界呢？\n","date":"2017-12-23T20:28:40Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1386017461/","slug":"1386017461","tags":["微博","词云","Python"],"title":"基于新浪微博的男女性择偶观数据分析(上)"},{"categories":["生活感悟"],"content":"接到妈妈打来的电话时，时间已然接近中午时分，从床上爬起来的刹那间，就听见妈妈熟悉的声音。妈妈问我年底公司有没有什么变动，顿时千万种思绪涌上心头，不知道该对电话彼端的妈妈说些什么。我突然想到二十四岁时的我，从第一家公司裸辞时的情景，可如今再度让父母为此焦虑，让身为人子的我感到惭愧不安。电话里妈妈让我照顾好自己，一个人在外不要太委屈自己。当一个人不被这个世界接纳的时候，就像是浑身长满刺的仙人掌。可在最亲近的家人眼里，我们永远是这个世界上独一无二的存在。我是一个不大主动同家里的人，在那一刻我忽然觉得，这个冬天没有那么冷了，即使我身处没有暖气的出租屋里，即使早晨带着体温的被子早已凉透，我想对自己说一句，冬天来了，春天还会远吗？\n发生在年底的 release 事件，就像这个冬天里的雾霾如约而至，即使早在去年就经历过这样的事情，可当它真实地发生在自己身上时，依然不免让人感到这个冬天的寒冷。纵观二十朝兴废更替，历史对我们而言常常是相似的。诸葛亮为“克复中原”六次北伐出师未捷而身先死的遗憾，唐玄宗宠溺杨玉环终致“安史之乱”而奔走蜀中避祸时的仓皇，这一切大概是我们如今坐西成高铁时无法想到的吧。有时候人的命运像极了历史的兴衰，记得三年前和同学第一次来西安，那个时候我们说，总有一天我们会再来这里的，那时的我或许完全不知道现在会面临这种处境，看着那些年龄比自己大依然碌碌无为的人，看着那些面临中年危机而不得不向生活妥协的人\u0026hellip;我告诉自己，我永远都害怕自己变成这样的人，所以让我内心无法平静下来的，永远是我近乎自责的自我反省式人格。\n在接受被 release 的事实以后，就开始频繁地去准备面试和跳槽。可年底时找工作注定是一个艰难的过程。其实回头想想今年面试的表现，前端、数据库等相关经验的匮乏，一度让我在面试中非常被动，而外冷内热的性格常常给人一种不自信的表现，特别是去葡萄城面试的这次体验，让我意识到在面试中我无法展示出和职位匹配的能力，我们说面试是双向选择的一个过程，这看起来有点像是找男女朋友一样，有时候我们太在乎对方而导致表现不佳。我花了时间去听知乎上有关面试技巧的 Live，甚至找朋友帮我分析如何给出面试官满意的答案，有时候别人会觉得我对严厉到苛刻的程度，是因为我对某些东西太在乎的缘故，可是不在乎这些问题就能解决了吗？矫枉过正至少意味着出发点是好的，总比发现问题后一直无所作为要好很多。\n我一直想找时间整理下这段时间面试遇到的题目，可令人窒息的拖延症让这种想法一直落空。有时候我怀疑，人和人的缘分都在第一眼就注定好了。或许你花时间和精力去追一个女生，但这种关系永远不会发生改变。人类总是肤浅地相信眼睛看到的，固执地认为自己的想法就是正确的，可人这种复杂的动物怎么会一眼就望穿呢，所以试图通过面试完全了解一个人，原本就是不切实际的想法。很多时候人与人接近并不是他们彼此熟悉，仅仅是因为大家的口味比较接近而已。无论多么熟悉的同事，在分开以后都会逐渐变得冷淡。每个人都像一只刺猬，离得太远会感觉到冷，而靠得太近会刺伤对方。大概是遇见小古花光所有运气，此后遇人不淑的厄运纷至沓来。人啊，简直是世界上最麻烦的一种动物。\n有时候我会埋头去做自己的事情，朝着自己内心的目标一点点靠近，即使曾经想要和她分享这点喜悦的人，早已消失在茫茫的人海里。我写爬虫、做数据分析、做聊天机器人，这其中有太多事情，是我对记忆的一种自我延伸，因为在那些曾经灰暗的日子里，陪伴我的人除了她，就是这些我比任何人都要在乎的技术。《嫌疑人 X 的献身》里，石神说道：“通往山顶的路或许会有很多条，而找出最优雅的那一条，是数学家永恒的追求”。也许他们说得都是对的，即使翻过了 2017 年，这个季节依然属于冬天，她只会比以前更让你觉得寒冷，可这一切的一切终究是会过去的，冬天来了，春天还会远吗？\n","date":"2017-11-19T10:16:17Z","image":"/posts/3111375079/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/3111375079/","slug":"3111375079","tags":["工作","生活","感悟","随笔"],"title":"冬天来了，春天还会远吗？"},{"categories":["独立博客"],"content":"如果说通过 TravisCI 实现博客的自动化部署，是持续集成这个概念在工作以外的一种延伸，那么今天这篇文章想要和大家分享的，则是我自身寻求技术转型和突破的一种挣扎。前段时间 Paul 同我聊到 Web 技术的发展趋势，Paul 认为 Web 应用会逐渐取代原生应用成为主流，我对此不置可否。真正让我陷入思考的是，在这个充满变化的时代，知识的更新速度远远超过你我的学习速度，我们应该如何去追随这个时代的步伐。如同那些淹没在时间河流里的技术名词，当青春不再的时候，我们喜欢把这个过程称之为成长，当发现距离第一次使用 FontPage 制作网站已过去十年，当发现曾经的网页三剑客在岁月蹉跎里频频改换姓名，当发现那些淹没在历史里的技术来不及学习就成为过往\u0026hellip;\u0026hellip;或许，这个世界真正迷人的地方，就在于它每天都在不断变化。\n新一代Web应用——PWA 接着 Paul 关于 Web 技术的这个话题，我认为 Web 技术在短期内会成为原生应用的一种补充。事实上，原生应用和 Web 应用哪一个会是未来，这个问题的争论由来已久，在业界我们可以看到 HTML5、PhoneGap、React/React Native、Electron/NW.js、小程序等方案百家争鸣，每一种方案都可以让我们去 Web 技术去打破平台间的差异。与此同时，我们注意到移动开发领域对原生技术的需求在缩减，虽然马克·扎克伯格曾表示，“选择 HTML5 是 Facebook 最大的错误“，可我们注意到，越来越多的 Web 技术被运用在原生应用中，Web 技术被认为是最佳的打造跨平台应用的技术，可以通过一套代码实现不同平台间体验的一致性。我们注意到知乎和天猫的客户端中都混合使用了一定的 Web 技术，因为纯粹使用原生技术去开发一个移动应用，其最大的弊端就在于我们要为 Android 和 iOS 维护两套不同的代码，从国内曾经疯狂火热的 iOS 培训就可以看出，单独使用原生技术去开发客户端，其成本实际上是一直居高不下的。\n虽然我们有 Xamarin 这样的跨平台技术，试图用一种编程语言和代码共享的方式，去开发两种不同平台的应用程序，可是我们注意到，平台间的差异和抗阻是天然存在的，就像SQL和面向对象这样我们再熟悉不过的例子。同样的，Facebook 的 React Native 项目，试图用Web技术去弱化平台间的差异，React Native 存在的主要问题是，它依然依赖原生组件暴露出来的组件和方法，所以像 DatePickerIOS、TabBarIOS 等控件是 iOS Only的，这意味着在开发过程中开发者还是要考虑平台间的差异性，其次 React 本身的JSX(对应HTML)、CSS Layout(对应CSS)本身是具有一定的学习曲线的，虽然底层因为没有使用WebView的原因提高了部分性能，然而整体上是牺牲了扩展性的。总而言之，这是一个介于 Web 技术和原生技术之间的中间技术，在我看来地位着实蛮尴尬的，因为无论在Web层还是Native层都选择了部分妥协，完美实现跨平台真心不容易啊。\n要掌握一门新技术，最好的方法就是去应用它。我的博客使用的是 Indigo主题，这是一个典型的 Material Design 风格的主题，所以我一直想尝试将其改造成原生应用，我曾经接触过移动端应用开发，如果通过 WebView 内嵌网页的方式来实现，我需要处理离线状态下页面的显示问题，以及所有混合应用开发都会遇到的一个问题，即原生应用层需要和Web应用层进行通信的问题。而如果采用 Hybrid App 的思路去开发一个混合应用，意味着我需要去学习 Cordova 这样的 Hybrid 开发框架，去了解 JavaScript 和 Native 交互的细节。那么有没有一种学习成本相对较低，同时可以提供原生应用体验的思路呢？答案是确定的，这就是我们下面要说的渐进式应用(PWA)。\n渐进式应用(Progressive Web Apps，PWA)是Google提出的新一代Web应用概念，其目的是提供可靠、快速、接近Native应用的服务方案。我们知道传统Web应用有两个关键问题无法解决，即需要从网络实时加载内容而带来的网络延迟和依赖浏览器入口而带来的用户体验，从某种意义上而言，渐进式应用的出现有望让这些问题得到解决，首先，渐进式应用可以显著加快应用加载速度，其提供的离线缓存机制可以让应用在离线环境下继续使用，关键技术为 Service Worker 和 Cache Storage；其次，渐进式应用可以被添加到主屏，有独立的图标、启动页、全屏支持，整体上更像 Native App，关键技术为 Web.App Manifest；最后，渐进式应用同操作系统集成能力得到提高，具备在不唤醒状态下推送消息的能力，关键技术为 Push API 和Notification API。\nPWA中关键技术解析 Google 对外提出 PWA 这个概念其实是在今天的二月份左右，所以现在我写这篇文章实际上是在赶一趟末班车。我最近比较喜欢的一个男演员张鲁一，在接受媒体采访时媒体称他是一个大器晚成的人，他的确让我找到了理想中成熟男人的一个标准，如果你要问我这个标准是什么，我推荐你去看他主演的电视剧《红色》。那么，好了，为了让大家了解渐进式Web应用(PWA)，相比其它跨平台方案有何优缺点，我们这里来简单讨论下PWA中的关键技术。\nServiceWorker 我们知道，传统的 Web 应用需要在网络环境下使用，当处在离线环境下时，因为 HTTP 请求无法被发送到服务器上，所以浏览器通常会显示一个空白页，并告知用户页面无法加载，因此会影响用户在离线环境下的使用体验，与此同时，因为 Web 页面在打开的过程中需要加载大量资源，因此在页面刚刚打开的一段时间内，用户看到的页面通常都是一个空白页面，考虑到缓存或者是预加载的 Web 应用，通常都会以预设资源作为占位符来填充页面，因此带来访问者的印象往往会更好。那么渐进式Web应用带给我们最大的惊喜，就是它可以在离线环境下使用，其核心技术就是 ServiceWorker，我们来一起看看如何使用 SeviceWorker：\nif (navigator.serviceWorker) { navigator.serviceWorker.register(\u0026#39;service-worker.js\u0026#39;) .then(function(registration) { console.log(\u0026#39;service worker 注册成功\u0026#39;); }).catch(function (err) { console.log(\u0026#39;servcie worker 注册失败\u0026#39;); }); } 我们这里看到一个基本的注册 ServiceWorker 的代码片段，并且它采用了业界流行的Promise的写法。那么首先第一个问题，ServiceWorker 到底是什么？ServiceWorker 本质上是一个 Web 应用程序和浏览器间的代理服务器，它可以在离线环境下拦截网络请求，并基于网络是否可用以及资源是否可用，来采取相对应的处理动作，所以 ServiceWorker 最基本用法是作为离线缓存来使用，而高阶用法则是消息推送和后台同步。通常来讲，ServiceWorker 会经历如下的生命周期：\nServiceWorker生命周期\r注：配图来自 http://web.jobbole.com/84792/\n按照官方文档中的定义，ServiceWorker 同 WebWorker 一样，是一段 JavaScript 脚本，作为一个后台独立线程运行，其运行环境与普通的 JavaScript 不同，因此不直接参与 Web 交互行为，从某种意义上来说，ServiceWorker 的出现，正是为了弥补 Web 应用天生所不具备的离线使用、消息推送、后台自动更新等特性，我们这里来看一个使用 ServiceWorker 缓存文件已达到离线使用的目的的例子：\nvar cacheStorageKey = \u0026#39;minimal-pwa-1\u0026#39; var cacheList = [ \u0026#39;/\u0026#39;, \u0026#34;index.html\u0026#34;, \u0026#34;main.css\u0026#34;, \u0026#34;e.png\u0026#34; ] self.addEventListener(\u0026#39;install\u0026#39;, e =\u0026gt; { e.waitUntil( caches.open(cacheStorageKey) .then(cache =\u0026gt; cache.addAll(cacheList)) .then(() =\u0026gt; self.skipWaiting()) ) }) 在这里例子中，我们在ServiceWorker的install事件中添加了待缓存文件列表，这将意味着这些静态资源，会在网页中的 ServiceWorker 被 install 的时候添加到缓存中，我们在某个合适的时机到来时就可以再次使用这些缓存资源。事实上考虑到安全性的问题，ServiceWorker 在设计时被约束为按照路径给予最高权限，即 ServiceWorker 在指定路径下是有效的。这里简单提下 ServiceWorker 的缓存策略，因为这个问题在我看来蛮复杂的，例如官方出品的 sw-tool 中定义的缓存策略就有如下五种：\n网络优先:：从网络获取, 失败或者超时再尝试从缓存读取 缓存优先:：从缓存获取, 缓存插叙不到再尝试从网络抓取 最快：同时查询缓存和网络, 返回最先拿到的 仅限网络：仅从网络获取 仅限缓存：仅从缓存获取 我们刚刚提到被缓存的静态资源会在合适的时机被再次使用，那么什么时候可以称之未合适的时机呢？在这个问题中，我们是指 fetch 事件，事实上通过拦截 fetch 事件，我们就可以拦截即将被发送到服务器端的 HTTP 请求，ServiceWorker 首先会检查缓存中是否存在待请求资源，如果存在，就直接使用这个资源并返回 HTTP 响应，否则就发起 HTTP 请求到服务器端，此时 ServiceWorker 担任的是一个代理服务器的角色。至此，我们就会明白，ServiceWorker 的作用其实就是在离线条件下利用缓存伪造 HTTP 响应返回，这样我们就达到了离线使用的目的，传统的 Web 应用在离线环境无法使用，根本原因是没有这样一个 Mock 的 Server 去伪造 HTTP 响应并返回，因为 HTTP 请求此时根本就无法发送到服务端。为了让ServiceWorker 全面接管 HTTP 请求以便利用请求，我们这里的实现方式如下：\nself.addEventListener(\u0026#39;fetch\u0026#39;, function(event) { event.respondWith( caches.match(event.request) .then(function(response) { // Cache hit - return response if (response) { return response; } return fetch(event.request); } ) ); }); 好了，以上就是 ServiceWorker 在离线缓存方面的基本用法，希望进行深入了解的朋友，可以参考文末链接做进一步研究。\nWeb App Manifest 接下来介绍 Web App Manifest，它其实是 Web 开发领域的一个\u0026quot;叛徒\u0026quot;，因为它所做的事情为大家所不齿，基本可以概括为，怎么样假装自己是一个 Native App，我们直接看它的定义：\n{ \u0026#34;name\u0026#34;: \u0026#34;Minimal app to try PWA\u0026#34;, \u0026#34;short_name\u0026#34;: \u0026#34;Minimal PWA\u0026#34;, \u0026#34;display\u0026#34;: \u0026#34;standalone\u0026#34;, \u0026#34;start_url\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;theme_color\u0026#34;: \u0026#34;#8888ff\u0026#34;, \u0026#34;background_color\u0026#34;: \u0026#34;#aaaaff\u0026#34;, \u0026#34;icons\u0026#34;: [ { \u0026#34;src\u0026#34;: \u0026#34;e.png\u0026#34;, \u0026#34;sizes\u0026#34;: \u0026#34;256x256\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;image/png\u0026#34; } ] } 这个我确认没有什么好说的，详细的参数可以参考这里，通常我们需要将以上文件命名为manifest.json，并通过以下方式引入到HTML结构中，通常是添加在标签下，我们所期望的图标、启动页、主题色等Native App的特性都是在这里定义的，这里想吐槽的是，随着越来越多的平台开始向标签中注入\u0026quot;新血液\u0026quot;，譬如标签和标签：现在HTML结构变得越来越复杂，更不要说主流的AngularJS和Vue这类MVVM框架，基本上都是通过扩展HTML属性来完成数据绑定的。对PWA应用来讲，我们只需要在标签下引入以下内容：\n\u0026lt;link rel=\u0026#34;manifest\u0026#34; href=\u0026#34;manifest.json\u0026#34; /\u0026gt; 这里简单介绍下 Web App Manifest 中常见的参数含义及其作用：\nname/short_name：表示应用被添加到屏幕上以后显示的名称，当屏幕空间不足以显示完整的 name 时，将显示 short_name。 start_url：表示用户从屏幕启动应用时所加载网页的URL，通常我们将其指向网站的首页。 theme_color：表示应用程序的主题颜色，PWA 事实上是建议使用 Material Design 设计风格的，因此该属性可以控制应用的主题颜色，并在页面加载完成前展示一个过渡动画。 scope：表示 PWA 应用的作用域，即哪些页面可以以 PWA 应用的形式呈现。 display：表示 PWA 应用呈现的方式，可以是 fullscreen、standalone、minimal-ui 和 browser 中的任意取值。 orientation：表示 PWA 应用的屏幕方向，如果你有移动开发的经验，对此应该不会感到陌生。 icons：表示 PWA 应用在屏幕上的图标，为了适配不同尺寸的屏幕，这里可以设置不同尺寸下的图标。同样地，如果你有移动开发的经验，对此应该不会感到陌生。 Push/Notification API 关于这两个东西，我们简单说一下啊，PWA 中的 Push 机制主要有 Notification 和 Push API 两部分组成，前者用于向用户展示通知，而后者用于订阅推送消息。网络上对这块介绍的并不多，关于推送这个问题，一直是国内 Android 用户和开发者的一块心病，因为 Google 的推送服务在国内水土不服，因此国内厂商或者是 SDK 提供商基本上都有自己的一套方案，这就导致在用户的设备上同时开启着若干个消息推送服务，用户手机里的电就是这样一点点被耗尽的，所以这个问题大家看看就好。在 PWA 中，我们可以通过 ServiceWorker 的后台计算能力结合 Push API 对推送事件进行响应，并通过 Notification API 实现通知的发出与处理：\n// sw.js self.addEventListener(\u0026#39;push\u0026#39;, event =\u0026gt; { event.waitUntil( // Process the event and display a notification. self.registration.showNotification(\u0026#34;Hey!\u0026#34;) ); }); self.addEventListener(\u0026#39;notificationclick\u0026#39;, event =\u0026gt; { // Do something with the event event.notification.close(); }); self.addEventListener(\u0026#39;notificationclose\u0026#39;, event =\u0026gt; { // Do something with the event }); 移植Hexo博客到PWA应用 现在，我们基本了解了PWA的概念以及实现PWA的关键技术，我们现在考虑将Hexo博客改造成一个PWA应用，我们这里不打算考虑消息推送的相关问题，所以对Hexo这样一个静态博客生成器而言，我们可以做的实际上只有两件事情，即通过Web App Manifest让它更像一个Native应用，通过ServiceWorker为它提供离线缓存的特性。我们从最简单的开始，我们需要在Hexo的根目录中增加一个manifest.json文件，该文件我们可以通过这个网站 manifoldjs.com 来生成。下面给出博主博客中使用的配置：\n{ \u0026#34;name\u0026#34;:\u0026#34;飞鸿踏雪的部落格\u0026#34;, \u0026#34;short_name\u0026#34;:\u0026#34;Payne\u0026#39;s Blog\u0026#34;, \u0026#34;description\u0026#34;:\u0026#34;人生到处知何似，应似飞鸿踏雪泥\u0026#34;, \u0026#34;icons\u0026#34;:[ { \u0026#34;src\u0026#34;:\u0026#34;assets/images/icons/bird36.png\u0026#34;, \u0026#34;sizes\u0026#34;:\u0026#34;36x36\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;assets/images/icons/bird48.png\u0026#34;, \u0026#34;sizes\u0026#34;:\u0026#34;48x48\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;assets/images/icons/bird72.png\u0026#34;, \u0026#34;sizes\u0026#34;:\u0026#34;72x72\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;assets/images/icons/bird96.png\u0026#34;, \u0026#34;sizes\u0026#34;:\u0026#34;96x96\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;assets/images/icons/bird144.png\u0026#34;, \u0026#34;sizes\u0026#34;:\u0026#34;144x144\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34; }, { \u0026#34;src\u0026#34;:\u0026#34;assets/images/icons/bird192.png\u0026#34;, \u0026#34;sizes\u0026#34;:\u0026#34;192x192\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;image/png\u0026#34; }], \u0026#34;background_color\u0026#34;:\u0026#34;#fff\u0026#34;, \u0026#34;theme_color\u0026#34;:\u0026#34;#000\u0026#34;, \u0026#34;start_url\u0026#34;:\u0026#34;/\u0026#34;, \u0026#34;display\u0026#34;:\u0026#34;standalone\u0026#34;, \u0026#34;orientation\u0026#34;:\u0026#34;portrait\u0026#34; } 好了，现在我们来考虑如何去实现一个ServiceWorker，Google官方提供了一个ServiceWorker的示例项目，以及网友提供的Minimal-PWA，这两个项目都可以帮助我们去了解，如何去实现一个ServiceWorker，甚至于我们有sw-toolbox和sw-precache这样的工具，配合gulp和webpack我们定制缓存策略并生成ServiceWorker。可是你要知道，懒惰对程序员而言是一种美德，在这里我选择了Hexo的插件hexo-offline，该插件可以帮助我们生成ServiceWoker，关于它的使用及配置，大家可以自行去了解，我重点想说说支持ServiceWorker以后，我的博客所呈现出来的变化以及PWA实际运行的效果。\nServiceWorker和Cache Storage\r通过这张图，我们可以清楚地看到，ServiceWorker确实在后台工作着，而Cache Storage确实对博客内的静态资源做了缓存处理。事实上对Hexo这样的静态博客而言，整个博客都是静态资源，所以在实际运行中它会对所有内容进行缓存，我们可以在终端中验证这个想法：\n在Hexo中监听到的缓存请求\r可我想说这一切并没有什么用，因为我并不能如愿地在离线状态下访问我的博客，甚至因为有了缓存机制，当我在撰写这篇博客时，虽然我改变了markdown文档的内容，但当我刷新博客的时候，因为缓存机制的存在，我不能像从前那样直接看到博客的变化，更重要的一点是，整个缓存大概有8M左右的体积，因此每次请求页面时，我能够明显地感觉到页面加载的延迟，看起来我们费了大量周折最终却一无所获，这听起来实在是讽刺不是吗？\n说完了ServiceWorker，我们再来说说Web App Manifest，我尝试从豌豆荚下载了移动版Chrome，可我自始至终无法将应用添加到主屏幕，貌似这需要Android系统底层的支持，我测试了两部手机，一部OPPO手机和一部小米手机，发现都没有明显的PWA支持，当我访问页面的时候，浏览器更加不会主动提示我\u0026quot;将应用添加到主屏\u0026quot;，像UC浏览器是将网站以应用的形式添加到浏览器首页，这的确没有什么值得令人惊喜的地方，因为在PC端的时候，我们就可以做到类似地实现，这篇文章耗费时间蛮长的啦，大概是因为我不知道，该如何描述这个失败的尝试。最近接触到一位前辈的项目，这是一个需要跨PC端和移动端的项目。目前面临的一个挑战就是，移动端有太多依赖原生接口的功能设计，所以一套代码在全平台适配，真的仅仅是一个美好的理想，离实现永远有一段不可逾越的距离。\n本文小结 本文主要以Google提出的渐进式Web应用(Progressive Web Apps)为主线，简单探讨了Google的渐进式Web应用及其关键技术。渐进式Web应用试图解决传统Web应用的两个关键问题，即需要从网络实时加载内容而带来的网络延迟和依赖浏览器入口而带来的用户体验。首先，渐进式应用可以显著加快应用加载速度，其提供的离线缓存机制可以让应用在离线环境下继续使用，关键技术为Service Worker和Cache Storage；其次，渐进式应用可以被添加到主屏，有独立的图标、启动页、全屏支持，整体上更像Native App，关键技术为Web.App Manifest；最后，渐进式应用同操作系统集成能力得到提高，具备在不唤醒状态下推送消息的能力，关键技术为Push API和Notification API。在此背景下，我们对静态博客Hexo进行了改造，尝试将其迁移到一个PWA应用上，虽然最终以失败告终，可是在整个过程中我们依然有所收获，我觉得一件事情能让我们有所思考或者有所感悟的话，这就已然是一种幸运、一种成功啦。\n其实Web应用与原生应用并非彼此水火不容，除了纯粹的Web技术和Native技术以外，在这两者之间我们看到的更多是混合技术的应用，所以我认为开发人员在未来一定要具备两种能力，即跨语言和跨平台开发的能力。比如小程序是在微信原生生态下建立的定制化Web应用，它有着类似HTML/CSS/JavaScript的技术方案，同时提供了统一的应用程序外观和使用体验；而跨平台游戏引擎cocos2d-x，通过JavaScript Bridge等类似技术，则可以实现将Web技术转化为Native技术\u0026hellip;..总而言之，在技术选型这个问题上，我们可以选择的方案越来越多，如何让想法可以伴随技术产生优秀的产品，这是我们在这个时代真正该去思考的问题。目前来讲，国内普遍重视iOS，可惜遗憾的是iOS不支持PWA；国内的Android系统经过阉割以后，国内用户无法使用Chrome，以及各个厂商定制的浏览器存在兼容性问题；国内因为政策及现实原因，第三方推送相对GCM推送要活跃很多，厂商并不会太关注对PWA应用推送的支持。虽然现实如此，可Web技术发展到今天为止，我们能做的就是希望它越来越好，在此引用黄玄的一句话：\n我们信仰 Web，不仅仅在于软件、软件平台与单纯的技术，还在于『任何人，在任何时间任何地点，都可以在万维网上发布任何信息，并被世界上的任何一个人所访问到。』而这才是 web 的最为革命之处，堪称我们人类，作为一个物种的一次进化。」\nPWA 初探：基本特性与标准现状 Service Worker API Using the Push API Service Worker初体验 PWA 入门: 理解和创建 Service Worker 脚本 PWA 入门: 写个非常简单的 PWA 页面 下一代 Web 应用模型 —— Progressive Web App ","date":"2017-10-24T23:13:41Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/450254281/","slug":"450254281","tags":["Hexo","PWA","Web"],"title":"迁移 Hexo 博客到 Google 渐进式 Web 应用(PWA)"},{"categories":["开发工具"],"content":"曾经听到过这样一句话，\u0026ldquo;不要用战术上的勤奋掩盖战略上的懒惰\u0026rdquo;，所以战术和战略更像是抽象类和具体类，而面向对象设计实际上是现实等级制度的一种映射。因此我们注意到，决策者通常关注的是战略层面的抽象概念，而执行者通常更关注战术层面的具体实现，正如在代码的架构设计中，处在顶层的代码以发送指令为主要使命，处在底层的代码以实现功能为主要使命。面对日新月异的互联网技术，当我们听到越来越多的新名词，譬如微服务、DevOps、单页面应用、前后端分离等等，这些概念曾让我们迷恋于追寻一个又一个风口，一如曾经的 O2O、VR、共享经济和人工智能，那么我们真的懂得如何让这些概念落地吗？在今天这篇文章中，我想和大家一起探讨持续集成相关的话题，并以 Hexo 结合 TravisCI 实现自动化部署为例，聊聊我心目中的 DevOps。\n从 DevOps 谈谈持续集成 不知从何时起，DevOps 开始成为大家竞相追捧的概念，同 ThoughtWorks 所倡导的微服务、敏捷开发一样，大家仿佛抓住了一根新的救命稻草一般，那么我们在说 DevOps 的时候，我们到底想要表达什么观点呢？想要搞清楚这个问题，我认为首先要明白，什么是 DevOps？从概念上讲，DevOps 是一个面向 IT 运维的工具流，以 IT 自动化以及持续集成(CI)、持续部署(CD)为基础，目的是优化开发、测试、运维等所有环节，所以 DevOps 本质上是一组部门间沟通协作的流程和方法，其目的是为了协调开发(DEV)、测试(QA)、运维(OPS)这三种角色，使开发运维一体化，通过高度自动化工具和流程，来确保软件构建、测试和发布更加快捷、频繁和稳定。\n所以，我们在说 DevOps 的时候，我们想表达的或许是流程和管理、运维和自动化、架构和服务、文化和组织等等的概念，那么在这些观点中，最重要的是什么呢？我认为是持续集成(CI)和持续部署(CD)，这是 DevOps 中从始至终贯穿的一条主线。通过 Git 这样的源代码控制工具，我们可以确保项目在一条主干上开发。而自动化测试/部署等周边工具，则为我们提供了实施持续集成/持续部署的必要条件。从公司角度出发，公司普遍更看重项目的交付能力，所以在传统持续集成/部署的基础上，我们时常会听到持续交付这样的声音，这时我们就会意识到，DevOps 实则是持续集成思想的一种延伸，它并不是一个新的概念，事实上我们这个行业，每年都喜欢这种“旧酒换新瓶”的做法，持续集成/部署/交付是 DevOps 的核心技术，如果没有自动化测试和自动化部署，DevOps 就是难以落地的空中楼阁。\n由此，我们就引出今天这篇文章的主题，即持续集成。我们提到，DevOps 是是一套面向 IT 的跨部门协作的工作流，它是持续集成思想的一种延伸，所以持续集成首先是一组工具链的集合。从某种意义上来讲，决策者喜欢 DevOps，并不是真正喜欢 DevOps，而是形式上的 DevOps 非常容易实现，因为有形的工具资源的整合是非常容易的，真正困难的是无形的流程资源的整合。你可以让两个陌生人在一起假装情侣，但你永远不可能真正拉近两个人心间的距离。通常而言，我们会用到下列工具：\n版本控制和协作开发：Github、GitLab、BitBucket、Coding 等。 自动化构建和测试：Apache Ant、Maven、Selenium、QUnit、NUnit、XUnit、MSBuild 等。 持续集成和交付：Jenkins、TravisCI、Flow.CI 等。 容器/服务部署：Docker、AWS、阿里云等。 从术和道的角度来看待持续集成，我们会发现在术的层面上，我们有非常多的选择空间，所以接下来我们主要从道的层面，来说说持续集成的核心思想。我们提到在实践 DevOps 的时候，需要有一条项目主干，那么持续集成的基本概念，就是指频繁地提交代码到主干分支，这样做的目的是，保证问题被及时发现以及避免分支大幅度偏离主干。\n在使用 Git 的场景下来看待持续集成，及时提交代码到主分支，可以避免因为分支改动过大而带来的冲突问题。按照敏捷开发的理论，每个 feature 通过迭代开发来集成到最终产品中，那么持续集成的目的，就是为了让产品可以在快速迭代的同时保证产品质量。在这里产品质量有两层含义，第一，本次 feature 提交通过测试；第二，本次 feature 提交无副作用。我们可以注意到，持续集成的第一个目的，即保证问题被及时发现，对应前者；持续集成的第二个目的，即避免分支大幅度偏离主干，对应后者。\n所谓持续集成，是指代码在集成到主干前，必须要通过自动化测试，只要有一个测试用例失败，就不能集成到主干，所以持续集成和自动化测试天生就是紧密联系在一起的。我们不能只看到持续集成/部署/交付，如果连流程上的自动化都无法实现，这些都是无从谈起的，从开发者的角度来看，理想的状态是编译即部署，我们提交的每一行代码，都是可以集成、交付和部署的代码，所以实际上是对开发者的代码质量提高了要求。所有我们觉得美好的事情，其实核心都在于人如何去运作，想到一位前辈说过的话，“软件开发没有银弹”，所有试图通过某种方法论解决软件工程复杂性的想法，都是天真而幼稚的。\nJenkins 持续集成落地实践 博主曾经在公司项目上实践过持续集成，深感持续集成想要真正在团队里落地，受到太多太多的因素制约。我们采取的方案是，使用 Git/Github 作为源代码版本控制工具，使用 Jenkins 作为持续集成工具，使用 MSBuild 作为项目构建工具，使用 MSTest/NUnit 作为单元测试框架，使用 Selenium/UI Automation 作为 UI 自动化测试框架，这些工具可以很好地同 Jenkins 整合起来。在持续集成工具的选择上，我们并没有太多的选择空间，因为公司需要同时支持 Java 和 JavaScript/Nodejs 项目的持续集成，在持续集成落地这件事情上，我们最终选择了妥协，我们不再追求自动化部署，而是选择通过这个过程来快速定位问题，具体原因我们下面来讲。\n首先，我们期望的是开发者在提交代码以后，可以触发编译、构建、测试和部署等一系列操作，我们会通过 Git 从远程仓库拉取最新代码，然后通过 MSBuild 来编译整个代码，由于 MSBuild 提供了定制化的脚本，可以对编译、测试和部署等环节进行精准控制，所以我们在 Jenkins 上触发的实际上是一系列动作，而这些都是可以在 Jenkins 上进行配置的，我们通常会将 Jenkins 上的日志以邮件形式发送给开发者，所以在很长一段时间里，每天到公司第一件事情，就是查看邮箱里的邮件，一旦发现有测试用例没有通过测试，我们就需要重复“修改代码“-\u0026gt;“提交代码“这个过程，直至所有用例都完全通过测试，理论上通过测试的代码就可以直接部署上线，因为 MSBuild 可以帮助我们生成最终文件，我们只需要将其打包然后上传到服务器即可，可是实际上这是我们假想的一种场景而已，因为现实场景中我们考虑得通常会更多。\n一个关键的问题是，我们没有可以量化的标准去评估，本次提交是否可以集成到主干。我知道你一定会说测试，事实是开发者不喜欢写测试，或者是写了不可测的测试，前一种观点认为写测试会占用开发时间，所以在开发时间相对紧张的时候，这就变成了我们不写测试的借口；后一种观点则是不会写可测试代码的表现，典型的表现是代码耦合度高、依赖大量无法 Mock 的对象实例、不会合理使用断言，所以在这种情况下，持续集成是没有意义的，我们不知道何时代码可以集成、交付和部署。我承认自动化测试无法全面替代人工测试，但当我们的关注点放在交付和部署上的时候，是否应该考虑先让持续集成落地，这实在是比 DevOps 更基础、更接地气，因为我相信持续集成是一种思想，它对开发团队中的每一个人都提出了更高的要求，持续集成是为了在保证产品质量的同时快速迭代，如果你心中没有产品质量的概念，DevOps 并不能帮你提高产品质量。\n第二个关键的问题是，开发和运维该如何去协作，DevOps 是为了促进部门间沟通协作而提出的一套工作流，自动化是这套机制能够良好运行下去的前提，可是在现实场景中一切并没有那么理想。以我们公司为例，开发组和运维组分属两个不同的部门，运维组在上线、部署等关键环节设置了严格的审批流程，即运维组牢牢地控制着线上生产环境，所以即使我们通过 MSBuild 在 Jenkins 上为程序打好了包，我们依然需要按照运维组的要求，提交上线请求、人工上传程序以及等待部门审批，通常我们上线只有等到每周五，而上线流程所需的东西，我们需要在一周前准备好，所以你可以注意到一个现象，虽然在流程上开发团队和运维团队是结合在一起的，但实际上两者的工作目标依然是分离的。那是不是将两个团队放在一起工作，就能解决这个问题呢？我想合作的前提是相互理解和信任，如果彼此都不愿意去了解对方的工作流程，DevOps 可能仅仅是我们用工具堆积出来的虚幻感。\n实现 Hexo 博客的自动化部署 好了，在公司使用 Jenkins 实践持续集成，在现实场景中总会受到各种各样的制约，这并不是因为持续集成这个想法不好，而是在现实面前我们都选择了妥协。有句话说，“如果没有见过光明，我本可忍受黑暗”，我们喜欢一个人或者是一样东西，都是因为我们觉得它是美好的，可以让我们觉得这个世界温暖，那么在公司以外的地方，我想更加自由地做些我喜欢的事情。在公司实践持续集成的时候，因为公司对权限的严格控制，我难以实现那种想象中的持续集成，即在成功地在提交代码以后直接触发编译和部署，我想在公司之外做成这件事情。\n为什么想到要给博客做持续集成呢？首先，持续集成和单元测试联系紧密，我自认为我的单元测试刚刚入门，为了写出更好的单元测试，我必须要这样做，来强迫自己努力去写好单元测试；其次，持续集成可以将开发和部署分离，所以我在任何一台计算机上撰写博客，都可以通过 TravisCI 实现编译和部署，对 Hexo 这种静态博客而言，部署其实就是推送页面到 Github 而已，整体难度并没有太高。最后，我平时更新博客都是手动推送页面，因为我不喜欢用 Hexo 提供的部署功能，现在我想让自己专注在内容写作上，而一切都可以在我的控制范围内。这正是我所想，如果能让一切更好一点，我都愿意去尝试和努力。\n关于 Hexo 这类静态博客生成器搭建博客的原理，我这里不想在赘述，因为我愿意相信，懂得搭建博客的人，一定是了解 Git、Github Pages 和 Markdown 等等的概念的，关于配置相关的细节大家可以参考官网。这里想着重介绍下 TravisCI，TravisCI 是一个在线的、分布式的持续集成服务，可以用来构建和测试托管在 Github 上的代码，并且其本身就是开源的。TravisCI 提供了主流编程语言如 C#、Java、JavaScript、Ruby、PHP、Node.js 等的支持，相比 Jenkins 而言，它是一个轻量级的持续集成平台，它会在每次提交代码后，根据配置文件来创建一个虚拟机，并执行用户定义的 Build 任务，这个虚拟机提供版本控制(Git)、项目构建(Node.js)等，在此前提下，我们下面着手 Hexo 的自动化部署。\n方案设计 Hexo 博客实际上可以分成两部分，即博客源代码和静态页面。其中博客源代码主要是指 Hexo 及其相关模块、博客内容(source)、博客主题(theme)，而静态页面由 Hexo 动态生成，通常放置在public目录中。对 Hexo 来讲，我们最终部署需要的是这些静态页面，所以我们设计得一个方案是，将静态页面存放在 master 分支，将博客源代码存放在 blog 分支。当用户提交代码到 blog 分支后，会触发 TravisCI 中定义的一系列操作，它会首先从 blog 分支拉取博客源代码，然后在 TravisCI 中完成静态页面的生成，最后将其提交到 master 分支以完成博客的更新，整个过程非常优雅，终于让我彻底摆脱了手动更新博客的过去，而更重要的是，从此写博客不再受地点的制约，因为写博客就是提交代码，生成静态页面以及部署到 Github Pages，现在全部交给了 TravisCI.\n配置 TravisCI TravisCI 是一个轻量级的持续集成方案，其轻量级主要体现在它的配置文件，即使用 TravisCI 并不需要我们安装任何软件，我们仅仅需要提供一个.travis.yml 文件即可，该文件通常被放置在项目根目录里。和 Jenkins 这样的持续集成工具不同，我们在这个文件中即可定制 Build 任务，下面给出一个基本的配置文件：\nlanguage: node_js node_js: stable # S: Build Lifecycle install: - npm install script: - hexo clean - hexo generate after_script: - cd ./public - git init - git config user.name \u0026#34;qinyuanpei\u0026#34; - git config user.email \u0026#34;qinyuanpei@163.com\u0026#34; - git add . - git commit -m \u0026#34;Update Blog\u0026#34; - git push --force --quiet \u0026#34;https://${CI_TOKEN}@${GH_REF}\u0026#34; master:master # E: Build LifeCycle branches: only: - blog env: global: - GH_REF: github.com/qinyuanpei/qinyuanpei.github.io 如果大家熟悉 Jenkins 的使用，就会发现这里定义的 Build 任务似曾相识。在这里我们首先指定了项目构建语言，即这是一个 node.js 的项目，然后我们会通过 npm 安装所有依赖，我们注意到在根目录里有一个 package.json 文件，该文件定义了整个项目依赖的项目。如果你使用过 Nuget，你会发现这一切都是如此的合理。那么当整个环境准备就绪以后，我们就可以着手博客的构建啦，和平时一样，我们会执行 hexo clean 和 hexo generate 命令，这样 Hexo 会帮助我们生成所有的静态页面，现在我们通过 Git 将其推送到 master 分支，通常基于 Github Pages 托管的页面都是存放在 gh-pages 分支的，可是对 Hexo 而言，我们放在 master 分支是没有问题的，这就是 TravisCI 构建整个博客的具体过程。\n关联 TravisCI 到目前为止，我们定义好了 TravisCI 将会在虚拟机中执行的 Build 任务。我们知道，这里 TravisCI 是需要访问我们托管在 Github 上的代码仓库的，所以我们必须将这个代码仓库和 Travis 关联起来，这样它就具备了从代码仓库拉取代码(Pull)和向代码仓库推送(Push)代码的能力。印象中公司是给每一个 Jenkins 服务器关联了一个 Github 账户，这样需要持续集成的项目只需要添加这个账号，并为其赋予基本的读写权限即可。在这里是类似的，我们有两种方案来关联 TravisCI，即为 TravisCI 虚拟机添加 SSH-Key 和使用 Github 提供的 Personal Access Token。\n前者和我们平时使用 Git 时配置 SSH-Key 是一样的，但考虑到公开密钥产生的安全性问题，TravisCI 建议我们使用官方的一个工具来对密钥进行加密，这是一个基于 Ruby 开发的命令行工具，加密后的内容可以在 TravisCI 中解密，这种方案需要安装 Ruby，博主选择放弃。如果你要问我为什么放弃 Ruby，大概是因为我忘不了曾经被 Jekyll 支配的恐惧感。而后者的原理是将 Github 生成的 Token 作为一个环境变量存储在 TravisCI 中，我们在定义 TravisCI 中的 Build 任务时可以引用这些环境变量，我们只需要在执行 Git 命令时带上这个 Token 就可以了。显然这种方式更合我的胃口，它的缺点是对此 Github 采用了粗放式的权限控制，即这个 Token 时可以访问所有代码仓库的，这一点大家自己可以根据自身情况来决定要使用哪一种方式。\n我们在 Github 中的 Setting-\u0026gt;Developer Settings 找到 Personal Access Token，然后选择所有 repo 相关的权限，生成这个 token 后将其复制下来备用，因为它只有在这个地方是可见的。接下来我们打开TravisCI，在使用 Github 登录后我们就可以在这里看到所有的项目，如图是我个人的 TravisCI 界面：\nTravisCI主界面\r大家可以注意到，这里我开启了 qinyuanpei.github.io 这个仓库的持续集成服务，如果大家没有在这里看到项目列表，可以点击\u0026quot;Sync account\u0026quot;按钮进行同步。好了，现在我们继续配置：\n配置TravisCI\r在这里我们配置了名为 CI_TOKEN 的环境变量，该值对应.travis.yml 文件中的${CI_TOKEN}。现在我们在本地提交代码到 blog 分支，就会触发 TravisCI 执行 Build 任务，在这里 Build 任务是从 blog 分支拉取博客内容及主题，通过 npm 安装依赖的 nodejs 模块，最终 Hexo 生成的静态页面会被推送到 master 分支，这样就完成了整个自动化构建的流程。下面是 TravisCI 执行 Build 过程中的日志界面：\nTravisCI日志\r从计划写这样一篇文章，到我一边写博客一篇将它发布在网络上，前后花了大概我 3 天左右的时间。这段时间发生了太多太多的事情，所以写东西受难免受到情绪影响，你现在看到这篇由 TravisCI 自动生成的博客，大概无法想象屏幕前的我有着怎样复杂的心绪，有时候我告诉自己要沉下心来学点什么，有时候我会觉得此时的我和过去没有什么区别。转眼间忙忙碌碌一年到头，可会想起来顿时觉得时间像虚度一般，有人说，当你对未来不再有什么期许的时候，就是你开始衰老的迹象，可我真的老了吗？我不是只有 25 岁吗？好啦，夜深人静，该去睡觉了，这篇文章就是这样子啦。\n","date":"2017-10-21T22:57:55Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3521618732/","slug":"3521618732","tags":["Hexo","Travis","CI"],"title":"持续集成在 Hexo 自动化部署上的实践"},{"categories":["生活感悟"],"content":"独自一个人在火车上望着窗外出神，而这种情景我再熟悉不过，或许风景会因为季节而不同，或许时间会因为年龄而不同，但对我而言，这个过程熟悉得就像一个我讲了无数遍的故事，从开篇布局到故事脉络都清楚到严丝合缝。印象中是从初中时候就开始寄宿生活，所以这种漂泊的感觉成为我生命里重要的烙印，而从那一刻起，我最期待的是寒暑假期，因为这象征着一段漂泊旅程的结束。大概是因为双子座属于风象星系，所以每次回家的旅程都伴随着忧愁风雨，我喜欢将这个过程理解为，生命不经意的装点。风有质而无形，或飞沙走石，或拈花弄叶；雨有质而无形，或坠散成珠，或凝聚成流。大概都是在世间漂泊的浪子，每一刻都摇曳不定。\n有人说，放假回家是学生时代做的事情，因为那时我们还没有脱离父母的庇护。可当我工作了以后，当我同他们的距离不再是学校和家的距离，当我同他们打电话不再是因为生活费告拮，我忽然发现我同他们交换了这场漂泊里的角色，从前是我期待着假期，因为我可以见到他们；现在则是他们期待着假期，因为他们可以见到我。我忽然发现我一年中我陪他们的日子屈指可数，从前向往远方觉得在那里能找到我的梦，现在越发地觉得他们一天天老去，想努力长大成熟，让他们能够放心，可更怕因为距离而疏远了他们，古人说『树欲静而风不止，子欲养而亲不待』，这个世界固然不再是古人所认识的世界，这种感情却超越了时间和空间轮回至今。\n国庆本来打算不回家的，因为即使无人可约无人可陪，我一个人同样可以给自己放假。我从来都是这样，如果有人愿意陪我做一件事情，我会很乐意接纳这番好意，并尝试用最好的状态去让彼此享受这个过程。如果没有人愿意陪伴我做一件事情，我不会勉强更不会因此而沮丧，因为当你习惯了一个人去面对所有事情，你会发现你从来不缺乏做一件事情的动力。可当家里人问我国庆要不要回家的时候，我突然心软得像融化了的雪，我意识到是我心底涌出的一股暖流，快速地打开微信买回家的火车票，结果发现 15 号的时候票就没有了，妈妈发微信给我说，『如果实在买不到火车票，就买机票回来吧，不用太心疼钱，人回来了就好』。\n我记得以前他们都不大会用这类 IM 产品的，但现在他们学会了怎么发消息发朋友圈。记得过年的时候，我教妈妈怎么样发红包，虽然只有三块钱这样来回发着玩，但她玩得比我都开心，或许父母就是这样，曾经青春期叛逆时觉得他们的世界和自己格格不入，等他们老去的时候依然在努力着融入我们的世界。小时候他们紧紧追逐着我们，是怕我们在成长路上摔倒；长大了他们依然紧紧追逐着我们，是怕我们的世界里没有了他们。可他们在一天天地老去，如果有一天他们追不动了，我们是否愿意停下来等等他们呢？窗外的若明若暗的，像极了我往常回家路上数过的每一盏灯，可是我最喜欢的那一盏，是来自那个叫做家的地方，它或许并不璀璨耀眼，但向来不吝于为你释放光和热，永远为你指示着家的方向。\n早上去北客站取票坐车，就接到朋友从中卫打来的电话，询问我国庆是不是要回家，这种感觉就像家里人一直记挂着你。我经常会想起高中时候，有时同朋友出去玩到很晚回家，常常会到朋友家里借宿，两个人寝则同床食则同桌，到今天我们依然是特别特别好的朋友。我幸运的是一直被朋友这样照顾着，即使我们两个在毕业后天各一方，我这个人不大懂得经营感情，被这样的朋友一直照顾着，我该是有多幸运啊。因为没有买到直达的车票，所以坐动车到兰州去转车，然后遭遇了人生中第一次火车晚点。火车晚点近三个小时，第一次迫切地感觉离家好远好远，每次回家都是披星戴月，或者在夜深人静时，或者在曙光初现时，让家人和朋友等我，我总是过意不去的。大概他们都太了解我，知道我会永远都是这副『长不大』的样子，可我总要长大成熟啊！\n我记得以前还在父母身边时，我脑子里想的是『父母在，不远游，游必有方』，我一位同学曾问我，何谓有方，当时的我真的是不知道该怎么样回答。后来，一个人去西安发展，终于明白，以前公司里一个女生所说的，在哪里都一样。想到父母孤零零地待在家里，想到每年屈指可数的回家次数，曾几何时，我们想去更广阔的世界寻找诗和远方，可父母逐渐蹒跚的步履注定无法，陪伴我们去那些遥远的地方。诗经里说『青青子衿，悠悠我心，纵我不往，子宁不来』，如果他们不能再紧紧追随我们的步伐，我们为什么不停下甚至转身回去去看望他们呢？以前和我徒弟聊天，她说男生都不大喜欢和家里人联络，我本来就是沉默寡言的性格，从初中时候就不主动和家里人联络，现在我想改变这种想法，因为我想有空就陪陪家里人。\n我喜欢在漫长的旅途中看书，这种习惯在我有了 Kindle 以后变得更为明显。早上看到这样一句话，『你懂得越多，就越觉得自己像这个世界里的孤儿』，人生而孤独，父母总有一天会离开我们，但他们教会我们如何去爱这个世界，我希望我可以用他们教会我的，在他们有生之年做些在他们眼里不再『孩子气』的事情，虽然在他们眼里我们永远都是孩子，我觉得真正的成熟并不是变得冷酷麻木，而是你知道这个世界有黑暗的一面，依然愿意相信那些温暖的事情。我到现在依然不喜欢听别人讲“道理”，因为我觉得人生最奇特的地方就在于，即使别人讲得这些“道理”都对，别人依然无法代替你去体验整个生活。每个人都是生活这片大海里的一朵浪花，为了不被岁月冲上荒凉寂寞的沙滩，我们唯有追逐巨浪努力生存。有那么一瞬间，我想和我喜欢的姑娘生活在一个城市里，等有空了就带上她回家陪陪父母。\n或许有些地方有比故乡更广阔的天空，或许有些地方有比故乡更湿润的土壤，或许有些地方有比故乡更精彩的旅程，我想说的是，不论你是在追求诗和远方，还是在忍受眼前的苟且，如果你觉得累了，请停下忙碌的脚步，找找回家的路，那里永远有人在等你！\n","date":"2017-10-21T22:31:48Z","image":"/posts/720539850/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/720539850/","slug":"720539850","tags":["回家","成长","亲情"],"title":"不如归去"},{"categories":["生活感悟"],"content":"\r连续数日的秋雨绵绵，依然固执地不肯转身离开，而之所以选择在国庆节前徘徊，或许是为了让离开家的人，多些同江湖风雨漂泊的味道。印象中这样的日子常常是相似的，譬如穿行在骤雨中被来往车辆溅得一身水，或者行走在上班的路上抬头看见第一场雪，或者是倚靠在公交车窗边上看风景转眼即逝，这些再熟悉不过的场景对我熟悉而又陌生，我惊异于记忆常常像盗梦空间般重叠，我感概于时间常常像钟表指针般流连。我不知道这个世界上是不是有平行世界，但我知道我再回不去曾经某一个时刻，我一直想写下这段时间的状态，可当我准备下笔时才发现，它需要我努力想好多事情，我依然还是曾经的我，风景依稀还是曾经的风景，到底是谁在一直变化呢？\n我不知道要从什么时候回忆这些事情，这种感觉就像是你期待了许久之后，在触碰到她的那一刻都不复存在了。我曾经答应过一个人要去看望她，如果你读过 《一个人的朝圣》 这本书，或许就会明白这是一种怎么样的执念，即使在明明知道一切再无法挽回的时候，这种执念还是让我想要达成这个愿望。我一直不知道两个人怎么就自然而然地在一起了，那种感觉如果一定要用语言来形容，我觉得是一种熟悉到灵魂里的默契。你想要牵着她的手的时候，她假装挣扎下后就一直让你牵着，那种娇羞中透着可爱的神情，在四目相对的时候眼睛里都是闪着光的。我忘不了在人潮中牵着她的手穿过整条街市，我忘不了抱着她的时候街市两边灯笼通红。有时候觉得人生充满了遗憾，好像错过她花光了我这辈子的好运气，从那以后我总是重复着昨天的故事。\n其实我自己都不清楚，我到底喜欢什么样的女孩子，甚至有时候我喜欢的是，我心中她最美好的样子。一个人少不经事的时候，大概会喜欢对女孩子说甜言蜜语，可当他经历过失去以后，他变得不再轻易许诺，这就好像我小时候是一个特别喜欢说话的人，在经历过因为紧张而变得口吃以后，我终于变成了今天这副沉默寡言的样子。有时候会陡然间觉得自己并没有怎么变，或许是因为她说过她喜欢我这个样子，所以我就固执地不肯改变，因为我怕有一天她回来的时候认不出我来，即使这是我脑补的一个剧情。曾经看过一个电影 《这个男人来自地球》，当我们熟知的宗教历史变成一个人的回忆，这种超越哲学意义上的时间我认为是荒凉的。曾经的小伙伴 Alex、Sandy、Kent、Andy、Kent、Kavin 和 Joe 都渐行渐远，到底是我停留在原地还是我超越了时间？\n不知道该怎么样描述这种感觉，或许我就是一个不擅长联络感情的人，生命中有太多太多东西，我眼睁睁看它离我远去而又无可奈何，想要安慰我的人总是劝我同昨天告别。但像我这样太看重感情的人，无论外表多么风平浪静，内心永远不肯残忍地删除回忆。所以，我记得 Jackson、Lynn、Candy 他们陪我度过的二十五岁生日，我记得 Candy 问我当时暗自许了什么样的愿望，坦白来讲，我没有想着脱单这样离我很遥远的愿望，我只想时间能够永远定格在那一刻，大家都可以开开心心地直到永远。你一定觉得我幼稚或者是不成熟啦，我问过人家要怎么样变得成熟，人家说你去找一个女朋友就好啦，然后就会在喜欢的人面前紧张甚至自卑，我曾一度很讨厌下雨天，因为我怕两个人遇到一起，我既没有伞亦没有外套。\n二十五岁的我，喜欢一个人还是和从前一样无所顾忌，我还是学不会那些复杂的套路，不喜欢单方面付出，不喜欢卑微地爱一个人，每一次都会因为喜欢某个女孩子而尝试改变，想和她站在一起的时候不会被她的光芒完全覆盖，想和她待在一块的时候不让她觉得我这个人枯燥，想和她抱在一起的时候给她讲我从书里看到的某个故事\u0026hellip;\u0026hellip;我一直在想，如果我们的感情不是以异地恋这种方式会不会有不一样的结局，我喜欢《星月神话》这首歌，是因为我们的确呼吸着同一片天空的气息而注定无法再相遇，就像两条相交的直线一样从陌生到熟悉再到陌生。我现在再看 《嫌疑人X的献身》 这部电影，我总在想，如果那天我们看的是这部电影会怎么样，此时的我比上大学时候胖了许多，大概一开始我在她心目中的样子，应该是张鲁一这样温润如玉的谦谦君子吧！生命就是这般离奇玄妙，你不能假设更无从假设应该发生什么，因为每一天都是无法重现的 Case，你觉得它相似，仅仅是因为相似而已。\n我喜欢穿裙子的女孩子，这一点完全是因为受到她的影响，虽然她一再告诉我，是我喜欢这样的女孩子，而她恰好喜欢穿裙子而已，可这些淹没在风声里的话语，谁会去盘问孰是孰非呢，如果她此刻愿意同我争论这个问题，我直接认输就好啦，我对输赢看得并不重要，这就像在工作中，没有人在意做的产品是不是好用，大家关注的是始终是它能节省多少个 FTE，所以为了达到这些光鲜亮丽的指标，没有人会在意工程师的代码被改成什么样子，我们所追求的东西是否显得舍本逐末，我们所在意的东西到底是否真正发自你我的本心。以前觉得两个人在一起简单，是因为我们没有想那么多；现在觉得两个人在一起困难，是因为我们习惯性想太多。你有没有在脑海中设想过，和一个人走完一生是种什么样的体验，我想说那是一个很美好的想象。\n","date":"2017-09-25T00:56:50Z","image":"/posts/2617501472/cover.png","permalink":"https://qinyuanpei.github.io/posts/2617501472/","slug":"2617501472","tags":["回忆","感悟","年华"],"title":"秋风劲似去年时"},{"categories":["生活感悟"],"content":"各位朋友，我是 Payne，大家好，欢迎大家关注我的博客，我的博客地址是https://qinyuanpei.github.io。最近前端技术圈因为 React 专利事件再次被大家关注，印象中 Angular 和 Vue 的纷争刚刚过去不久，果然前端技术圈对\u0026quot;造轮子\u0026quot;和\u0026quot;搞事情\u0026quot;有着近乎执著的追求。作为一个在知乎吃瓜的伪前端工程师，我对这凑热闹这种事情从来都是是颇为喜欢的。如果说 Angular 和 Vue 冲突主要来自大漠穷秋和尤小尤的个人战场，那么这次 React 专利事件则是商业公司之间对社区主导力量的一次争夺和抗衡。开源是一种近似乌托邦般的理想社会，它倡导的\u0026quot;人人为我，我为人人\u0026quot;这种近乎大同社会的观念，在面临商业化浪潮洗礼的时候难会和商业利益发生冲突，譬如 Google 因为使用 Java 而和甲骨文纠纷不断，最终不得不选择 Kotlin 作为 Android 开发的主力语言。所以这篇文章我想和大家通过 React 专利事件来聊聊开源软件许可，以及我们如何在商业化和开源社区间找到一个平衡点。\n事件始末 其实 React 专利事件由来已久，如果不是在知乎上看到\u0026ldquo;百度要求内部全面停止使用 React/React Native\u0026rdquo;的问题，我是完全没有意识到事态居然发展到如此严重的。每次前端技术圈\u0026quot;搞事情\u0026quot;的时候，基本上都会在我的知乎首页刷屏，可是对我这样的伪前端工程师而言，我仅仅是关注了\u0026quot;Web 开发\u0026quot;这个话题而已。忽略知乎首页推荐算法的缺陷，这的确动侧面说明了目前前端领域非常热门的事实，可它不能说明某些前端工程师的技术水平有多高，在引入前后端分离和前端构建工具以后，前端开发的基础设施渐渐地丰富起来了，可是前端开发目前经历着的一切，无一不在后端开发中涉及到，我没有想要成为全栈工程师的野心，在讨论这个事件以前我认为有必要了解下整个事件的始末：\n2016 年 7 月，Facebook 在 React.js 的开源许可协议中添加的附加专利条款首次在社区中引发广泛讨论。 2016 年 11 月，Facebook 发布官方问答，对附加专利条款进行了澄清，强化了其 BSD 许可证 + 专利许可证的概念。 2017 年 4 月，Apache Cassandra 项目正在考虑是哟过 Facebook 开源的数据库 RocksDB 作为存储引擎，可是考虑到专利授权的问题，Jeff Jirsa 向 Apache 法律社区寻求帮助。 2017 年 6 月，Apache 法律社区开始讨论 Facebook Patents License 协议专利授权的不对称问题，且该协议与 Apache Software License，即 Apache 2.0 等不兼容。 2017 年 7 月 15 日，Apache 软件基金会正式发表声明称：Facebook BSD + Patense License 正式被列入\u0026quot;Category X\u0026quot;列表，因此 Apache 项目中将不能含有或者依赖任何该协议的代码，而已发布的代码必须在 8 月 31 日前完成替换。 2007 年 8 月 19 日，Facebook 对 Facebook BSD + Patense License 有了新的解释，解释指出，专利许可证的存在是为了防御无量的专利诉讼，Facebook 增加专利许可证是为了保护核心技术。 2017 年 9 月 16 日，百度内部全面禁止使用 React/React Native 的消息在知乎上引发热烈讨论。 2017 年 9 月 17 日，Wordpress 官方称因为 React 专利问题而停止在博客程序 Wordpress 中使用 React 技术。 2017 年 9 月 23 日，Facebook 迫于社区压力对外宣称将在数周后将 React 授权许可修改为 MIT。 主流软件许可 其实作为一名软件工程师，这些和法律息息相关的内容，原本是不需要我们去关注的，因为即使公司在使用这些开源软件中发生法律纠纷，通常都会有法务人员协助公司去解决相关事宜，无论如何都轮不到我们这些人来关心的。不过这个事件的现实意义是，我们在做技术选型时，专利等可能引起法律纠纷的问题，一样是需要纳入考虑范围的。因为如果是个人性质或者纯玩性质的项目，我们的确无需在意太多。而如果你是商业性质项目、或者是公司自营项目，或者是服务于甲方，那么你必须考虑你使用开源软件的方式是否符合相关的软件许可。国内因为盗版软件盛行的原因，大家在心底里好像都不认同软件许可，但是像外企或者是对信息安全比较重视的企业，通常要么对许可证书比较看重，要么对开源软件不太感冒，所以像最近的 WePhone 创始人自杀这种事件，都在告诉我们一个道理，程序员不要整天都关注技术层面上的东西，虽然技术世界有很多纯粹而美好的事情，但当它和人类联系在一起、和政治联系在一起的时候，它就完全不在我们的控制之中了，所以我觉得我们有必要了解些法律相关的事情，那么从何处开始呢？我们不妨就来说说主流的开源软件许可吧！\n这个世界上的开源软件许可证书大约有上百种，我们不可能也没有必要了解所有的开源软件许可证书。对于主流的开源软件许可，我们有 GPL、BSD、MIT、MPL、Apache 和 LGPL，相信大家都没有兴趣去阅读这些晦涩深奥的 License，所以我们不打算在这里逐一介绍它们，事实上搞清楚它们在具体限制上的差异是件非常困难的事情。我们希望用最简洁的语言来描述这些开源软件许可：\nGPL： 即 GNU 通用公共授权(GNU General Public License)，其出发点是代码的开源/免费使用和引用/修改/衍生代码的开源/免费使用，但是不允许修改后和衍生的代码做为闭源的商业软件发布和销售。这就是为什么我们能用免费的各种包括商业公司在内的 Linux 版本，以及 Linux 上各种各样的由个人、组织和商业软件公司开发的免费软件。 BSD： 即Berkly Software Distribution， 基本上是一个给予使用者极大自由空间的一种开源协议，使用者可以自由地对代码进行使用、修改和二次发布，该协议鼓励代码共享，其出发点是尊重代码作者的著作权，要求保留原代码中的 BSD 协议，保留创作者署名权利，即不得以开源软件作者/机构的名义进行市场推广。 MIT： 即Massachusetts Institute of Technology，这是一个完全给予使用者自由空间的 简短而宽泛的授权协议，作者唯一的诉求是保留版权，使用者可以复制、修改、合并、发布、分、授权和销售软件副本，并根据程序的需要适度修改授权条款，唯一的要求是必须在发行版里附加原许可协议的声明，无论是以源代码还是二进制形式发布。 MPL：即The Mozilla Public License，该协议同 GPL 和 BSD 基本一致，差异主要体现在：源代码提供者不能提供已经受到专利保护的源代码、要求再发布者必须提供对代码程序修改的说明、允许通过 MPL 许可获得的源代码同其他类型源代码进行混合(第二条献给那些不好好在 Git 里写注释的同学)。 Apache：即著名的非盈利开源组织 Apache 采用的协议，该协议和 BSD 类似，同样鼓励代码共享和尊重原作者的著作权，同样允许代码修改，作为开源或商业软件再发布，主要关注点有：（1）需要给代码的用户一份 Apache License；（2）如果改动代码需要在被修改文件中做出说明；（3）衍生代码必须保留原有协议、商标、专利或者说明等；（4）不得对 Apache 协议进行修改。 LGPL：LGPL，即 GPL V2，是 GPL 的一个为主要为类库使用设计的开源协议，和 GPL 要求任何使用/修改/衍生之 GPL 类库的的软件必须采用 GPL 协议不同。LGPL 允许商业软件通过类库引用(link)方式使用 LGPL 类库而不需要开源商业软件的代码。这使得采用 LGPL 协议的开源代码，可以被商业软件作为类库引用并发布和销售的同时，保障作者的知识产权，避免有人利用源代码复制并开发类似产品。 好了，相信到这里大家就能够明白，为什么这次 React 专利事件能在社区里引起轰动。我认为主要的原因有两点：\n第一，React 在 BSD 协议许可的基础上增加的专利许可，对许可证书授权方和被授权方而言，存在待遇上的不对等性。实际上在 React 为前端带来虚拟 DOM、单向数据流和不可变对象等一系列函数式编程的概念的同时，Facebook 在开源社区中的话语权同样越来越大，Facebook 在开源协议中夹藏私货的确让人有种\u0026quot;挟天子以令诸侯\u0026quot;的感觉，曾几何时，社区指责微软没有开放全部的 OpenXML 标准，因为大家都觉得按照这个标准实现的 Office 文档和微软家的存在差异，可是面对这种和自家产品紧密联系的项目要开源，我觉得这不单是 Facebook 会有所提防，恐怕所有的商业公司都会有类似的想法吧，所以在这个事件中，隐含的一个点就是，一旦当使用 React 的公司和 Facebook 发生业务上的竞争，React 将成为 Facebook 获得诉讼胜利的一个重要筹码，因为根据 React 的专利协议，Facebook 有权在开展诉讼时从被授权方手中收回 React 的使用权，所以我们不难理解为什么百度和 Wordpress 都宣称要停止使用 React，除了不想受制于人以外，像百度这种未来可能会和 Facebook 在 AI 等领域发生竞争的公司，宁可自己造一套轮子而不愿让自家专利被对方使用的做法，我觉得是可以理解的。\n第二，React 在开源协议中附加专利许可的做法，从商业公司自我保护的角度来看，的确是无可厚非，不过这种做法未免会给开源社区带来不好的风气。我们都知道开源软件并不等同于免费软件，因为开源软件通过许可证书来保证开源软件代码是以一种合理的方式被使用。在很久很久以前，MySQL 是我非常喜欢的一个数据库，因为它可以让我摆脱 SQLServer 臃肿的体积。什么？你说.NET 技术体系中怎么会出现 MySQL？可这正是.NET 选择开源、选择了跨平台，我们才有机会在更广阔的世界里去做些有趣的事情不是吗，我们必须承认开源对这个世界的重要意义，当你发觉你身边的同事都在重复写些垃圾的代码时候，你或许就会意识到，其实在这个世界上有很多东西，我们是可以站在巨人的肩膀上看得更远的。当你因为目光短浅而小心谨慎地维护着那些破旧的代码的时候，我们除了一天天老去别无所获。自从 MySQL 被甲骨文收购以后，我觉得这个世界开始缺少些有趣的东西，甲骨文和 Google 关于 Java 的官司让 Google 最终选择了 Kotlin，所以你可以看到开源这件事情对这个世界是绝对有利的，很多人担心这些代码开源到互联网上对商业公司不利，其实我们都清楚，没有环境和生态的代码基本不会有人关心，我们是不是该重新审视下开源？\nOK，我知道现在大家都在思考一件事情，既然开源对这个世界的进步是有利的，那么是否开源就不应该成为我们思考的问题，我们真正应该考虑的问题是，如何选择一个合适的开源软件许可证书，在商业化和开源间找到一个平衡点。对于这个问题，我想大家一定会犯选择困难症，不过没有关系啦，我想此时下面这张图也许可以帮到大家：\n如何选择开源软件许可证书\r何去何从 或许在数日前，你还在为 React 专利事件而苦恼，或者考虑在 Preact 的基础上实现一个新的 React，或者考虑转向 Angular 和 Vue 这两个框架，此时此刻 Facebook 宣布将 React 的开源协议修改为 MIT，或许这算是开源社区的一次胜利，或许这算是整个专利事件的尘埃落定，或许有人继续担心 Facebook 搞其他事情，可是这个世界原本就是在每天都发生变化着的，对于未来我们常常是无从得知它的足迹会在哪里。人生本来就是一个人的逆旅，要想在这充满变化的世界里获得安全感，唯有努力让自己处于不败之地，技术何尝不是这样呢，想想这 20 年间我们经历了多少技术的变革，从来没有一门框架可以让我们一劳永逸，所以对于小公司而言，大可不必担心 Facebook 会因为专利问题和你产生法律上的纠纷，该用什么就用什么框架，因为没有绝对完美的框架，能结合业务场景选择合适的框架，为这个世界带来一点点微小的变化，这样子我们就足够开心啦！而对于 BAT 这样的互联网大厂，则应该考虑走自主研发的差异化路线，因为如果你不想受制于人，最好的方法就是让别人依赖你，而不是去努力依赖别人。作为一个伪前端工程师，我觉得不管什么时候，我们都要努力打好基础，而不是在一堆框架中疲于奔命，对热衷于搞事情和造轮子的前端技术圈来说，下一次的讨论热点会是什么，你我都未必能想到，这个时候还有什么比努力更重要的事情呢。好了，这篇文章就是这样了，希望大家能够喜欢，我们下一篇见。\n","date":"2017-09-20T23:06:45Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1166840790/","slug":"1166840790","tags":["前端","React","开源"],"title":"从 React 专利事件看开源软件许可"},{"categories":["数据存储"],"content":"各位朋友，大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是https://qinyuanpei.github.io。想起来大概有一个月没有更新博客啦。或许是因为这中间发生了太多的事情，想来人生原本就充满曲折和变数。在微信群里得知家中舅爷去世的消息，突然意识到时间早已摧毁你我的一切。那个曾经同你有千丝万缕联系的人，会在某一刻同你彻底失去联系。所以我更珍视彼此在一起的时光，因为在这个世界上每天都面临着改变。有时候工作上遇到不开心的时候，会想着一个人去一个陌生的地方，我们就在不断地相聚和离别中慢慢老去。这段时间一直在学习做饭，为此特意买了本菜谱，结果发现，最难的并不是如何去做好一道菜，而是你为了做好一道菜需要准备各种食材，就像人与人交流并没有什么困难，真正困难的地方，是你找不到一个可以一直陪你说话的人。熟悉的店面会被拆迁转让，熟悉的人事会被错过改变，上帝想把世界煮成一锅粥，可味道的调配却由我们来掌控。\n好了，所谓“如人饮水，冷暖自知”，人生奇就奇在你没有办法用三言两语去描述它。这段时间面试过两三家公司，整体上感觉自己的生活太安逸了些，虽然我现在依然住在租来的房子里，转眼间 2017 年接近尾声啦，可是回想起来今年年初制定的计划，在广泛阅读和提升技术上都是不及格的状态，印象中打算研究 Redis 和 MonogoDB 这两种数据库的(因为没有购买为知笔记会员导致部分笔记损坏或者丢失)，然而到现在为止我还有研究完 Redis。尤其当我面试的时候，我发现好多我写在简历上的内容，都会成为某种意义上的呈堂证供，这让我更加确信好多东西需要不断地去巩固，所以尝试在实际项目上使用 Moq、考虑怎么写出更好的测试方法以及时刻保持自我的不可替代性，这些都是我最近在考虑的事情，有时候发脾气是因为觉得自己在浪费生命，可越是被这种无力感笼罩的时候，就越是要对自己狠一点儿，所以在这篇博客中，让我们重新拾起对 Redis 的学习兴趣，今天我们来说说 Redis 中的 Lua 脚本。\n熟悉我博客的朋友一定都知道，我曾经开发过 Unity3D 相关的项目，而 Lua 脚本正是 Unity3D 中主流的热更新方案。关于 Lua 脚本相关的文章，大家可以通过下面的链接来了解，在这里我们不再讲述 Lua 的基础内容，本篇文章所讲述的是如何通过 Redis 内置的 Lua 解释器来执行脚本，我们为什么使用脚本语言进行开发呢，因为这样可以降低开发的难度啊。\n脚本语言编程：Lua 脚本编程入门 在 Windows 下使用 Visual Studio 编译 Lua5.3 Unity3D 游戏开发之 Lua 与游戏的不解之缘(上) Unity3D 游戏开发之 Lua 与游戏的不解之缘(中) Unity3D 游戏开发之 Lua 与游戏的不解之缘(下) Unity3D 游戏开发之 Lua 与游戏的不解之缘终结篇：UniLua 热更新完全解读 好了，既然我们已然了解到 Redis 是通过内置的 Lua 解释器来执行脚本，所以 Redis 中的 Lua 脚本其实可以理解为 Lua 语法 + Redis API。为了写作这篇文章，我不得不将我的操作系统切换到 Linux，因为这样我可以随时在写作过程中使用终端，我写作的一个重要特点，就是所有的内容都尽量保证有测试覆盖，我知道有许多人都不喜欢写测试，测试虽然不能保证你没有 BUG，可是有了 BUG 以后可以直接在测试中定位问题，这就是我们为什么要重视测试的原因所在。在 Redis 中我们有两类命令用以处理和脚本相关的事情：\nEval 系列 熟悉 JavsScript 的朋友应该会更熟悉这个方法，因为 Eval 在 JavaScript 是个神奇的存在，它可以执行任何合法的 JavaScript 代码，我和我的同事就曾经在一个项目中写过两层嵌套的 Eval 方法，显然这是为了实现某种奇怪的需求。那么在 Redis 中有 EVAL 和 EVALSHA 两个命令可以使用，这两个命令是从 Redis2.6.0 版本开始的，通过内置的 Lua 解释器来实现对脚本求值。EVAL 命令的基本格式如下：\nEVAL script numkeys key [key ...] arg [arg ...] 我们可以注意到在这里 EVAL 命令由三部分组成，即第一个部分，表示一段 Lua 脚本程序，并且这段脚本不需要更不应该定义函数；第二部分，表示参数列表，指在脚本中需要用到的键，因为 Redis 是一个键值数据库，这些键名可以通过全局变量 KEYS 来访问，默认索引将从 1 开始，事实上我们更推荐你使用这种方式来访问键名；第三部分，表示除建键名参数以外的附加参数，和第二部分类似，这里我们可以通过全局变量 ARGV 来访问，这里就不再赘述啦。我们一起来看下面的例子：\nEVAL \u0026#34;return {KEYS[1],KEYS[2]}\u0026#34; 2 ab cd 此时我们会返回一个由 KEYS[1]和 KEYS[2]组成的集合，集合中的两个元素分别是 ab、cd，注意到这里有一个参数 2,它表示我们这里将有两个参数，事实上 Redis 将从这个位置开始解析参数，所以我们必须告诉 Redis 参数解析到什么位置结束，因为主要参数(KEYS)和附加参数(ARGV)是从解析的角度上是无法区分的，所以我们期望的结果会是：\n1) \u0026#34;ab\u0026#34; 2) \u0026#34;cd\u0026#34; 现在我们来增加点难度，显然你明白我在说什么，请注意我要引入附加参数(ARGV)啦！\nEVAL \u0026#34;return {KEYS[1]..ARGV[2] ,KEYS[2]..ARGV[1] }\u0026#34; 2 ab cd ab cd 这里我们尝试对 KEYS 和 ARGV 进行拼接，需要说明的是 Lua 中连接字符串使用的是. .，所以这里将得到结果：\n1) \u0026#34;abcd\u0026#34; 2) \u0026#34;cdab\u0026#34; 好了，现在大家应该理解 EVAL 这个命令的使用方法啦，那么对 EVALSHA 命令来说，顾名思义，它就是使用了 SHA1 验证的 EVAL 方法，我们注意到现在脚本都是定义在 EVAL 命令的第一个参数上，假如我们需要复用一个脚本，而该脚本可以为我们提供 Sum 这样的功能，即它可以返回一组参数的和给我们，显然参数的个数是不同的，那么这个时候我们总不能每次都重复写这个脚本吧，所以 Redis 会为脚本创建一个指纹，我们使用 EVALSHA 命令来传入一个指纹，Redis 将从缓存的脚本中找到这个脚本，并结合我们的参数来调用它，这样我们就可以获得脚本执行以后的结果，关于指纹的这种说法，大家可以结合 Git 提交代码时的感受进行理解，除此以外，它和 EVAL 在使用方法上是完全一致的，所以不再举例子说明啦。\nScript 系列 好了，下面我们来介绍第二类和 Lua 脚本相关的 API，相比 Eval 给人云里雾里的感觉，Script 系列的命令处处洋溢着规范命名的美好气息，我们通过这些命令的名字基本上就可以知道它是做什么事情的，这告诉我们平时写代码的时候如何去写出优雅的代码。我们通过下面一组命令来了解 Script 系列命令的具体用法：\n/* 载入一个脚本到缓存中 */ SCRIPT LOAD \u0026#34;return \u0026#39;Hello Redis\u0026#39;\u0026#34; /* Redis返回该脚本的指纹信息 */ \u0026#34;e509eb0869056563287758d23146eb00e0518da5\u0026#34; /* 查询脚本是否存在于缓存中 */ SCRIPT EXISTS \u0026#34;e509eb0869056563287758d23146eb00e0518da5\u0026#34; /* Redis返回1表示脚本存在，反之不存在 */ 1) (integer) 1 /* 从缓存中清空所有脚本 */ SCRIPT FLUSH OK /* 此时脚本在缓存中是不存在的 */ SCRIPT EXISTS \u0026#34;e509eb0869056563287758d23146eb00e0518da5\u0026#34; 1) (integer) 0 至此，我们了解到了 Redis 中对 Lua 脚本支持的主要特性，坦白地讲，我认为 Lua 脚本在这里的应用极其薄弱，完全达不到我们印象中 Lua 脚本的强大，甚至我对 Redis 中的 KEYS 和 ARGV 依然有些模糊，大概越想搞明白的事情有时候就越搞不清楚。这里我没有提到的一个 SCRIPT 系列的命令是 SCRIPT KILL，这个命令的作用是杀死当前正在运行的脚本，并且当且仅当这个脚本没有执行过任何写操作时，这个命令才会生效，所以这个命令主要用于杀死长时间运行的脚本，执行完这个命令后，执行这个脚本的客户端将从阻塞的 EVAL 命令中退出，并收一个错误作为返回值，所以我们可以理解为这是一个强行终止脚本执行的方法，因为我这里这个脚本非常的简单，所以它执行起来非常快，而我没有这样一个足够长的脚本去验证这个命令，所以在上面的脚本示例中我没有去验证这个命令，对此感兴趣的朋友可以自行去研究啦。\nLua 脚本应用 通过本文前面两个部分，我们基本了解了 Redis 中 Lua 脚本是如何工作的，在演示示例脚本的时候，我是直接在终端下运行 redis-server 和 redis-cli 的，并且所有的命令都是在终端下手动键入的，难道在实际的使用中我们要这样子玩 Redis 吗？想起来都觉得好可怕是不是？所以我们下面来通过一个具体的案例，来演示 Redis 怎么去和一个 Lua 脚本脚本进行交：\n首先，我们来定义一个简单的 Lua 脚本文件script01.lua，该脚本将对集合中的元素进行求和：\nlocal sum = 0; local key = KEYS[1] local length = redis.call(\u0026#34;LLEN\u0026#34;,key) local index = 0 while (index \u0026lt; length) do sum = sum + redis.call(\u0026#34;LINDEX\u0026#34;,key,index) index = index + 1 end return sum 现在我们在终端中执行这个脚本，为了方便起见，我们这里将其放在 redis-3.2.8 目录下的 scripts 目录。我们首先在 Redis 中准备些数据来做好准备，在终端中执行命令：\nLPUSH data 2 4 6 8 10 (integer) 5 src/redis-cli --eval ~/文档/redis-3.2.8/scripts/script01.lua data (integer) 30 好了，我们下面来解释下这段脚本，我们向 Redis 中键名为data的集合中添加了 5 个元素，注意这句脚本是在执行src/redis-cli后执行的，这部分内容我们在前面讲解 Redis 中的数据结构的时候提到过，博主表示在写这篇文章的时候依然要去看文档，总之现在我们有一个集合，并且这个集合中有 5 个元素，与此同时呢，我们编写了一个 Lua 脚本文件script01.lua，这个脚本的作用是对集合中的元素进行求和。在这里我们注意到，我们可以通过 redis.call()这个方法来调用 redis 中的命令，具体到这里我们使用 LLEN 命令获取了集合的长度，使用 LINDEX 命令获取了集合中的元素。我们在前面提到两个全局变量 KEYS 和 ARGV，可以完全当作 Lua 脚本中的两个变量来处理，从编程角度来讲，我们可以将其直接在脚本中写死。可是考虑到 Redis 是一个键值数据库，所以我们很容易想到键名应该对外暴露出来，以满足复用 Lua 脚本的目的。这里我们直接用 redis-cli 来运行 EVAL 命令，所以我们注意到它的传参方式有点不一样，事实上 KEYS 和 ARGV 中间使用逗号隔开即可。\n所以我们可以想到一种 Lua 脚本自动管理的思路，即通过命令行读取指定目录下的 Lua 脚本文件，通过 SCRIPT LOAD 方法获得其在 Redis 中的 SHA1 指纹，然后我们将脚本名称或者 ID 和这个指纹关联起来并将其存储在 Redis 中，此时我们只需要传入脚本名称和参数即可返回脚本执行后的结果，这样是不是感觉非常优雅呢？虽然 Redis 是一个键值性数据库，它不具备传统关系型数据库的查询能力，但是现在我们有了 Lua 脚本以后一样可以通过脚本来定制出查询，而到此时此刻我或许才真正明白 Redis 中 Lua 脚本是一种怎样神奇的存在。我们心怀敬畏，同时对这个世界永远充满期待，因为我们从来不知道人类潜能开发的极限在哪里。我们创造了太多不可思议的事情，有时候甚至连我们自己都怀疑，为什么我们会走到今天这一步。在脚本语言里我最喜欢的编程语言是 Lua 和 Python，如果说我喜欢 Lua 源于我对游戏开发的兴趣，喜欢 Python 源于我对编写网页爬虫的兴趣，那么我很庆幸今天我又多了一个使用 Lua 的原因。世上美好的事情莫过于，你喜欢一样东西，恰好有人和你一样喜欢，可惜那是很久以前的事情啦。\n我们现在可以了解到，Redis 提供了一种机制可以让 Lua 脚本同 Redis 进行交互。可是事实上 Redis 和 Lua 在数据结构定义上存在一定差异。所以，下面我们来了解下这两种数据结构是如何进行转换的，了解完这些我认为这篇文章就可以结束啦，因为现在接近 1 点钟啦而明天还要上班。在 Lua 脚本中调用 call()或者 pcall()方法来执行 Redis 命令时，Redis 命令执行的结构会被转换为 Lua 中的数据结构。同理，当 Lua 脚本在终端中执行时，Lua 脚本的返回值会被转化为 Redis 的协议并经由 EVAL 返回给客户端。关于 call()和 pcall()这两个方法，一个显著的区别是前者在出错时返回的是错误信息，而后者返回的是经由 Lua table 包装后的结果。我们知道 table 在 Lua 语言中是一个非常强大的数据结构，显然后者对调用者更为友好些啦。通常在处理类型转换时我们有以下原则：\nLua table 结构中不能含有 nil，否则 Redis 将从第一个为 nil 的位置返回 Lua number 结构中不能区分浮点类型，默认会转换为整型并舍弃小数部分，如果需要保留小数部分请返回 string 类型 Lua boolean 结构在 Redis 中会被转换为 0 和 1 的取值 Redis 提供了 redis.error_reply()和 redis.error_status()两个辅助方法来完成 Lua-\u0026gt;Redis 的转换 好了，这篇博客就是这样子啦，关于为什么使用 Lua 脚本这个问题，我认为可以从减少网络开销、原子性和脚本复用三个角度来考虑，尤其是第二点，因为 Redis 执行脚本的时候是整体的、阻塞的执行，中间不会被插入新的命令，因此它完全可以不用担心出现竞态或者事务相关的问题，可是即使这样我们还是建议编写短小精悍的 Lua 脚本。以上就是这篇博客的全部内容啦，感谢大家关注，欢迎在博客留言及讨论相关技术问题，谢谢大家。\n参考文章 Xbynet - Redis 与 Lua 及 Redis-py 应用 Lua 一路向前走 - 【原】Redis 基本操作 小咚 - Redis Lua 总结 Redis 中文网 - Redis 脚本 ","date":"2017-09-17T10:49:07Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/4197961431/","slug":"4197961431","tags":["Redis","缓存","数据库","笔记"],"title":"Redis 缓存技术学习系列之 Lua 脚本"},{"categories":["编程语言"],"content":" 各位朋友，我是Payne，大家好，欢迎大家关注我的博客，我的博客地址是https://qinyuanpei.github.io。在这篇文章中，我想和大家探讨下数据校验的相关问题，为什么我会对这个问题感兴趣呢？这其实是来自最近工作中相关需求场景，而这篇文章其实是我在去年就准备要写的一篇文章，这篇文章一直存放在草稿箱里没有发布出来，所以结合这段时间项目上的思考，对当初的设计方案进行了改进，所有就有了大家现在看到的这篇文章，我始终认为思考是一个持久的过程，就像我们对这个世界的理解，是会随着阅历的变化而变化的。我们知道现实通常都会很残酷，不会给我们太充裕的时间去重构。可是思考会是人生永远的功课，当你忙碌到无暇顾影自怜的时候，不妨尝试慢下来抬头看看前方的路，或许原本就是我们选择了错误的方向呢，因为有时候作出一个正确的选择，实在是要比埋头苦干要重要得多啊。\n好啦，既然我们提到了思考，那么我们来一起看一个实际项目中的业务场景，在某自动化项目中，用户会将大量数据以某种方式组织起来，然后藉由自动化工具将这些数据批量上传到一个系统中，该系统实际上是一个由各种表单组成的Web页面，并且这些Web表单中的控件都有着严格的验证规则，当数据无法满足这些验证规则时将无法上传，因此为了提高自动化工具上传的成功率，我们必须保证用户组织的这些数据是合法的，假设我们的用户是一个仅仅会使用Office三件套的普通人，他们可以想到的最好的方式是将这些数据录入到Excel中，而Excel中的数据有效性验证依附在单元格上，一旦验证规则发生变化，我们就不得不去维护这个Excel文件，这绝对不是一个软件工程师该做的事情好吗？我们当然是需要在提交数据前做验证啦，然而我看到Excel中100多列的字段时，我瞬间就不淡定了，这么多的字段难道我们要逐个写if-else吗？不，作为一个提倡少写if-else的程序员，我怎么可能会去做这种无聊的事情呢？下面隆重推出本文的主角——Attribute。\n你的名字是？ 如你所见，本文的主角是Attribute，那么当它出现在你面前的时候，你是否会像《你的名字。》里的泷和三叶一样，互相问候对方一句：你的名字是？因为我们实在不知道应该叫它特性还是属性。可事实上这篇文章的标题暴露了这个问题的答案，这里我们应该叫它特性。好了，按照数学理论中的观点，任何问题都可以通过引入一个中间层来解决，现在我们有了一个新的问题，Attribute和Property到底有什么区别？虽然这两者都可以翻译为\u0026quot;属性\u0026quot;，可实际上它们表达的是两个不同层面上的概念，一般我们倾向于将Attribute理解为编程语言文法上的概念，而将Property理解为面向对象编程里的概念。\nAttribute/特性 我们将Attribute称为特性，那么我们在什么地方会用到特性呢？两个个非常典型的例子是超文本标记语言(HTML)和可扩展标记语言(XML)。首先这两种标记语言都是结构化、描述性的标记语言。结构化表现在节点间可通过父子或者兄弟的关系来表示结构，描述性表现在每个节点都可以附加不同的描述来丰富节点。例如下面的XML文件中，我们使用了描述性的特性来提高元素间的辨识度，即特性为元素定义了更多的额外信息，而这些额外信息并不作为元素数据结构的一部分：\n\u0026lt;bookstore\u0026gt;\r\u0026lt;book category=\u0026#34;COOKING\u0026#34;\u0026gt;\r\u0026lt;title lang=\u0026#34;en\u0026#34;\u0026gt;Everyday Italian\u0026lt;/title\u0026gt; \u0026lt;author\u0026gt;Giada De Laurentiis\u0026lt;/author\u0026gt; \u0026lt;year\u0026gt;2005\u0026lt;/year\u0026gt; \u0026lt;price\u0026gt;30.00\u0026lt;/price\u0026gt; \u0026lt;/book\u0026gt;\r\u0026lt;book category=\u0026#34;CHILDREN\u0026#34;\u0026gt;\r\u0026lt;title lang=\u0026#34;en\u0026#34;\u0026gt;Harry Potter\u0026lt;/title\u0026gt; \u0026lt;author\u0026gt;J K. Rowling\u0026lt;/author\u0026gt; \u0026lt;year\u0026gt;2005\u0026lt;/year\u0026gt; \u0026lt;price\u0026gt;29.99\u0026lt;/price\u0026gt; \u0026lt;/book\u0026gt;\r\u0026lt;/bookstore\u0026gt; 在这个例子中，bookstore节点由两个book节点组成，而每个book节点则由title、author、year和price四个节点组成，显然这些节点描述的是一种结构化的数据，而这些数据同时附加了相关描述性的信息，例如book节点有category信息，title节点有lang信息。在XML中最基本的一个内容单元我们称之为元素，即Element，而描述这些元素的最基本内容单元我们称之为特性。所以，这种在语言层面上进行描述而与实际抽象出的对象无关的概念就称为\u0026quot;特性”，人们认知和描述一个事物的方式会有所不同，所以在XML中会有这样一个历史遗留问题，我们应该使用Element还是Attribute，而产生这个问题的根源在于我们认识这个世界，是通过语言描述还是通过概念抽象。\n如果我们了解GUI相关技术的演进过程，就会发现历史总是如此的相似。为什么微软会在XML的基础上扩展出XAML这种专门为WPF而设计的界面设计语言呢？因为历史告诉我们GUI中的大量特性都应该使用声明式的、描述式的语法来实现，从苹果的Cocoa、微软的XAML、Qt的QML、Android的XML等无一不证明了这个观点，而采用过程式的MFC、WinForm、Swing等，我们常常需要为它们编写大量的交互性的逻辑代码，今天我们会发现前端领域的声明式编程、MVVM、组件化等技术点，其实都是这种思想的无限延伸，我们可以使用jQuery去直接操作DOM，但面向过程的命令式代码一定不如声明式容易理解。虽然在面向对象编程的世界里，我们最终还是需要将这些描述性的语法结构，转化为面向对象里的类和属性，可这已然是一种进步了不是吗？\nProperty/属性 我们认识这个世界的过程，恰恰折射出这两者截然不同的风格，从孩提时代理解的“天空是蓝色的”到学生时代认识到“大气是由氮气、氧气和稀有气体组成”，这种转变从本质上来看其实是因为我们认识世界的角度发生了变化。《西游降魔篇》里玄奘寻找五行山，第一次是风尘仆仆“看山是山”，第二次是由“镜花水月”启发“看山不是山”，第三次借“儿歌三百首”降伏孙悟空后“看山还是山”。面向对象编程(OOP)的一个重要思想是抽象，而抽象即是我们从描述性的语言中对事物属性进行构建的一个过程。例如现实生活中的汽车会有各种各样的数据信息：长度、宽度、高度、重量、速度等等，而与此同时汽车会有启动、刹车、减速、加速等等的行为，所以将事物的“数据”和“行为”提取出来进行抽象和模拟的过程，就是面向对象编程，我们在这个过程中可以注意到一点，所有的这一切都是针对对象而言的，所以Property是针对对象而言的。\n这里提到的一个重要概念是抽象，什么是抽象呢？我认为它恰好和具体相对的一个概念。所谓具体，即相由心生，你看到什么就是什么，与此同时通过一组描述性的语言将其描述出来，我以为这就是具体。例如\u0026quot;火辣辣的太阳挂在天上\u0026quot;，这是具体到太阳颜色和温度的一种描述；所谓抽象，即返璞归真，我们看到的并非世间阴晴圆缺的月亮，而是这浩瀚宇宙中国一颗遥远的行星，此时此刻我们将行星具备的特点概括出来，推而光之，我以为这就是抽象，所以对我们而言，属性是事物抽象后普遍具有的一种特征，它首先要达到一种抽象的层次，其次它要能表现出事物的特性，我更喜欢将Property称之为属性，它和我们在面向对象编程中的概念是完全统一的。\n方案设计及其实现 设计目标 免除配置开箱即用：无需任何配置文件，直接在实体上添加Attribute即可实现验证 非侵入式验证设计：验证与否对实体结构无任何副作用，可以随时添加验证或卸载验证 扩展灵活高度复用：可以自由派生自定义特性，通过泛型来支持不同实体类型的验证 设计思路 所有校验相关的Attribute都派生自ValidationAttribute这个父类，其核心方法是Validate()方法，该方法被声明为一个虚方法，因此所有的子类都必须对这个方法进行重写，它将返回一个叫做ValidationResult的结构，这是一个非常简单的数据结构，它仅仅包含Success和Message两个属性，前者表示当前校验是否成功，后者表示验证失败时的错误信息。显然，一个实体结构中将包含若干个不同的属性，所以在对一个实体结构进行验证的时候，会通过反射遍历每一个属性上的ValidationAttribute并调用其Validate()方法，所以最终返回给调用者的应该是由一组ValidationResult组成的集合，为此我们设计了ValidationResultCollection这个类，该类实现了ICollection接口，在此基础上我们增加了一个Success属性，当集合中所有ValidationResult的Success属性为true时，该属性为true反之为false。我们将数据校验的入口类EntityValidation设计成了一个静态类，它提供了一个泛型方法Validate()方法，所以对整体设计而言，它的灵活性和扩展性主要体现在：(1)通过派生自定义特性来增加验证规则；(2)通过泛型方法来支持不同类型的校验。下面给出UML类图供大家参考，最近刚刚开始学习UML，有不足之处请大家轻喷哈：\nUML类图\r技术要点 首先，在.NET中特性的基类是Attribute，Attribute从表现形式上来讲类似Java中的注解，可以像标签一样添加在类、属性、字段和方法上，并在运行时期间产生各种不同的效果。例如[Serializable]标签表示一个实体类可以序列化，[NonSerializable]标签则可以指定某些属性或者字段在序列化的时候被忽略。而从本质上来讲，Attribute是一个类，通常我们会将派生类以Attribute结尾，而在具体使用的时候可以省略Attribute，所以[Serializable]标签其实是对应.NET中定义的SerializableAttribute这个类。在我们定义Attribute的时候，一个需要考虑的问题是Attribute的作用范围，在.NET中定义了AttributeUsageAttribute这个类，它可以是Class、Property、Field、Method等，所以Attribute本质上是在运行时期间为元素提供附加信息的一种机制，即Attribute可以添加元数据。我们知道元数据是(MetaData)实际上是程序集(Assembly)中的一部分，显然这一切都是在编译时期间定义好的，所以Attribute的一个重要特征是在运行时期间只读(Readonly)。Attribute必须依附在指定目标上，当当前目标与AttributeUsage定义不符时，将无法通过编译。Attribute的实例化依赖于目标实例的实例化，无法直接通过new完成实例化。通常我们需要配合反射来使用Attribute，在运行时期间做些有意义的事情，例如ORM中实体字段与数据库字段的绑定、Unity中配合AOP使用的ExceptionHnadler等等，都是非常典型的Attribute的应用。\n了解了Attribute是什么东西，接下来我们要考虑的就是如何访问Attribute，在.NET中主要有两种方式来获取Attribute，即通过Attribute类提供的静态方法获取Attribute和通过Attribute依附的对象实例的元数据来获取Attribute。下面我们来看一段简单的代码实例：\npublic static T GetAttribute\u0026lt;T\u0026gt;(this PropertyInfo propertyInfo)\r{\rvar attrs = propertyInfo.GetCustomAttributes(typeof(T), false);\rif(attrs == null || attrs.Length\u0026lt;=0) return null;\rreturn atts[0] as T;\r} 这段代码展示了如何通过反射访问附加在属性上的Attribute，事实上除了PropertyInfo以外，它还可以从任何支持附加Attribute的元素，例如MethodInfo、FieldInfo、ConstructorInfo等。Attribute类提供了类似的静态方法，第一个参数可以是这些元素中的任何一个，第二个参数和第三个参数和这里的示例代码一致，分别是返回的Attribute的类型，以及是否要搜索父类的Attribute，它的返回值类型为Attribute[]。在这个方案中，我们通过下面的方式来对实体属性进行验证：\npublic static ValidationResultCollection Validate\u0026lt;T\u0026gt;(T entity)\r{\rvar type = entity.GetType();\rvar properties = type.GetProperties();\rvar results = new ValidationResultsCollection();\rforeach(var property in properties)\r{\rvar propertyValue = property.GetValue(entity,null);\rvar validationAttributes = property.GetCustomAttributes(typeof(ValudationAttribute),fasle);\rif(propertyValue == null \u0026amp;\u0026amp; (validationAttributes == null || valudationAttributs.Length \u0026lt;= 0)) continue\r//优先验证RequiredAttribute\rvar requiredAttributes = property.GetCustomAttributes(typeof(RequiredAttribute),false);\rif(requiredAttributes.Length \u0026gt; 0)\r{\rvar requiredResult = (requiredAttributes[0] as ValidationAttribute).Validate(propertyValue);\rresults.Add(requiredResult);\rif(propertyValue == null) continue;\r}\r//其次验证ValidationAttribute\rforeach(var validationAttribute in validationAttributes)\r{\rif(propertyValue != null \u0026amp;\u0026amp; !validationAttribute.GetType().Equals(typeof(RequiredAttribute)))\r{\rvar validationResult = (validateAttribute as ValidationAttribute).Validate(propertyValue);\rresults.Add(validationResult);\r}\r}\r}\rreturn results;\r} 在这里我们注意到在对ValidationAttribute进行处理的时候，优先验证了RequiredAttribute，因为如果它验证失败意味着下面的验证都不需要了，所以当一个Property上附加了RequiredAttribute并且它的值为null的时候，我们将不会进行下面的验证，这是在设计过程中发现ValidationAttribute的优先级不同而做出的一个简单地调整。关于ValidationAttribute，我们提到这是所有自定义特性的基类，实际在使用中我们会有各种各样的派生类，我们这里以RegexAttribute为例来看看它具体怎么实现：\npublic class RegexAttribute : ValidationAttribute\r{\rprivate string regexText;\rprivate string defaultMessage = \u0026#34;value is required to match a Regex rule {$regex};\rpublic RegexAttribute(string regexText,string message = null)\r{\rthis.regexText = regexText;\rthis.message = message == null ? defaultMessage : message;\r}\rpublic VelidationResult Validate(object value)\r{\rvar regex = new Regex(regexText);\rvar match = regex.match(value.ToString());\rvar success = match.Success;\rif(!success)\r{\rmessage = message.Replace(\u0026#34;{$regex}\u0026#34;,regexText);\rreturn new ValidationResult(){Success = success, Message = message};\r}\rreturn new ValidationResult(){Success = success};\r}\r} 好了，以上就是整个校验设计中关键的技术点啦，我认为整体上没有多少难点，因为这是我在项目上造的一个简单的轮子，相比ASP.NET MVC 中的校验要简单很多，相信大家可以根据这些内容轻松地实现一个自己的版本，虽然不主张\u0026quot;重复造轮子\u0026quot;，可博主在很多时候都是通过\u0026quot;造轮子\u0026quot;来学习的啊，哈哈。\n数据校验示例 下面我们来通过一个简单的示例来了解，如何在实际项目中使用这个验证方案：\npublic class Foo\r{\r[Required]\r[Regex(\u0026#34;(\\d+){3}-(\\d+){1}-(\\d+){6}\u0026#34;)]\rpublic string CardNumber {get; set;}\r[Required]\r[MaxLength(20,\u0026#34;AccountNumber is required within 20 characters\u0026#34;)]\rpublic string AccountNumber {get; set;}\r[Values(\u0026#34;FCY,DCP,ATM\u0026#34;)]\rpublic string TransactionType{get；set;}\r} 这里使用了三种验证规则，Required表示该字段不可以为空，Regex表示字段值要匹配指定的正则表达式，MaxLength表示字段长度不能超过指定长度，Values表示字段允许的取值范围，在实际使用中我们可以通过派生定义更多的验证规则，每一种验证规则都可以设置一个验证失败的信息，例如当AccountNumber的长度超过20时，将会返回指定的错误信息。我们可以通过下面的代码来验证Foo这个实体中的属性：\nvar foo = new Foo();\rfoo.CardNumber = \u0026#34;234-7-4567\u0026#34;;\rfoo.AccountNumber = \u0026#34;12345678900\u0026#34;;\rfoo.TransactionType = \u0026#34;DCP\u0026#34;\rvar results = EntityValidation.Validate\u0026lt;Foo\u0026gt;(foo);\rif(!result.Success) results.ToList().Foreach(r =\u0026gt; {\rConsole.WriteLine(r.Message);\r}); #本文小结 本文首先讲述了特性和属性两者在概念上的不同，即特性是编程语言文法上的概念，而属性是面向对象编程里的概念。接下来，我们针对.NET中的Attribute的表象和具象进行了讨论，Attribute从表象上看是和Java中的注解类似，可以像使用标签一样附加在类、方法、属性或者字段等元素上，而从具象上看Attribute提供了一种在运行时期间通过元数据访问附加信息的能力，Attribute是附加在类、方法、属性或者字段等元素上的一个类，需要继承自Attribute，它的实例化必须依赖这些附加对象的实例化，并且Attribute在运行时期间是Readonly的，Attribute通常需要配合反射来使用。在具备这些基础知识以后，我们开始和大家分享这个验证方案的设计思路及其技术要点，所谓抛砖引玉，本文的目的是想让大家借鉴这种思路，努力让业务代码更干净些，因为只有我们在乎这件事情，我们才会努力去将它做好。好了，今天这篇文章就是这样啦，谢谢大家关注！\n","date":"2017-08-21T14:25:41Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3873710624/","slug":"3873710624","tags":["C#","校验","特性"],"title":"基于特性(Attribute)的实体属性验证方案设计"},{"categories":["生活感悟"],"content":"\r猛然间驻足回首这些错落的旧时光，我渐渐意识到我已经有三个月没有写博客了。如果一定要我说出这是种什么样的感觉，大概就是你永远都不会知道永远到底有多远。或许你会喜欢上一个陌生的人，源自不经意间的惊鸿一瞥；或许你会开始厌倦一个熟悉的人，源自不经意间的怅然若失。时间如风起云涌，一边熟悉着一边陌生着，永远像极了一场你追我赶的拉力赛。从办公室里走出来被热风吹袭的一瞬间，我居然有种久违的暖人肺腑的感觉。每个人都像一粒炭火，都知道要通过抱团来取暖，可是有谁会愿意燃烧自己呢？所以孤独是人类如宿命一般的社会属性。我一位朋友曾向我讲述过，这种若即若离的感觉，而此时此刻，我想将这种感觉结合一部电影来说出来。\n这段时间好像看了挺多电影的，借我一位朋友的话说就是，“两只单身狗跑到电影院里去找刺激”。而对于《大护法》这部电影，我是选择了一个人去看的，因为我觉得这部电影的主题是“反乌托邦式”的，所以我宁愿自己独自去消化这些内容，而不是将消极悲观的情绪在观影时传播给别人。对这部电影我将其看作是一个成人童话，因为它的确不适合带小孩子去观看，而这部电影恰恰采用了PG-13的影片分级。我喜欢这个电影，某种意义是因为它在现有体制内，讲述一群被奴役的“花生人”，如何在外人的帮助下，从愚昧麻木转变为意识清醒，并最终产生自我意识推翻统治者暴政的故事，所以这其实是一个关于觉醒和反抗的故事。\n而这基本上是人类历史里永恒的话题啦，熟悉苹果公司历史的朋友一定知道，乔布斯当年曾经拍摄过一部名为《1984》的广告片，这部广告片取材自乔治.奥威尔的同名小说，该书中刻画了一个令人窒息的恐怖世界，在假想的 未来社会中，独裁者以追求权利为最终目标，自由被彻底剥夺，思想被严酷控制，人民被迫屈从于“老大哥”的统治。而将书中这个背景对应到苹果公司，我们就部难理解乔布斯是在用“老大哥”来影射当时的IBM公司，在这则广告片中“老大哥”被铁锤击碎后缓缓消失，此时旁白平静地念道：“1月24日，苹果电脑公司将推出麦金塔电脑，你将明白为什么1984不会变成《1984》”，这段传奇故事在《硅谷传奇》和《乔布斯》两部电影中均有反映，对此感兴趣的朋友可以自己去了解，显然乔布斯在当时试图向世界证明，苹果公司是唯一一家有希望打败IBM的公司。\n因此在很长一段时间里，我一直有想要通读《1984》的愿望，这种行为在某些人看来是矫情和装逼，可事实上连周星驰都表示没有读完《演员的自我修养》这本书，我就不明白这个想法为什么会遭人厌恶。这个世界上最令人厌恶的事情就是，我们所有人都生活在一个被道德和法律约束的世界，我们从出生就在适应接受某一种意识形态或者社会法则，可总有人试图告诉你生活是这样或者那样，并且出自尊重你必须接受和感谢这种建议，因为这些人最后会说我是为了你好。那么在导演不思凡的视角里，这种回归哲学意义上的最为追根溯源的问题，即我是谁，是如何通过电影表现出来的呢？欧阳吉安，即花生镇村名眼中的老神仙，他说鬼蘑菇是一种可怕的传染病，一旦花生人长了鬼蘑菇就必须被立即处死，花生人不能开口说话，即使他们都贴着假眼睛和假嘴巴，花生人不能拥有意识和思考，一旦说出事实就会被认为染了疯病必须被立即处死，所有的村民都循规蹈矩地听着老神仙的话，可事实上鬼蘑菇根本就不是传染病，它是花生人成熟的标志，老神仙这样一个统治者，从来不会将花生人视为人，它们活着的唯一意义就是等死后，由庖卯从脑袋里取出黑石头。古话说：流言止于智者，可在这个荒诞诡异的世界里，流言会因为恐惧而掩盖真相，村民始终生活在一种令人窒息的恐怖阴影里。\n故事开篇即点明主旨，即奕卫国大护法，即故事主角“红冬瓜”为寻找太子下落，而来到了充斥着腐烂气息的花生镇，虽然主角对这些像人而又不像人的“花生人”表示了反感，因为在大护法沿着山路来到小镇的时候，经过了一个类似拱门的建筑，可细思恐极的是这个拱顶堆满了花生人的头颅，而且头颅上的眼睛是真正的眼睛，而散落在地面上的贴纸其实是假的眼睛，联想整个故事情节，在花生镇敢于揭露真相、寻找真相的人都被杀鸡儆猴地处理掉了，这可以说是故事开篇埋下地一个伏笔了，可是为了寻找太子的下落，大护法不得不去向这些村民打探消息。可是大护法很快就发现，在花生镇这样一个奇怪的地方，随时随地都会有人杀戮村民和外来者，这些人被称为刑法者，负责帮助老神仙欧阳吉安杀死“该死”地村民，所以在故事一开始大护法在村子里就遭到了袭击，可是说起大护法来，这是一个战斗力爆表的反差萌系设定，而通过故事我们知道，这些刑法者由一个称为罗单的人管理，他不属于花生镇，和欧阳吉安这些人类不一样，他对彩这个神秘女子有种强烈的占有欲，他偷看她洗澡被发现便转身离开，可当他发现下属产生感情的时候，他毫不留情地杀死了他们，所以大概到现在为止，我们所认识的世界存在着严格的等级区分，整个故事从此定性，罗单压抑着自己的情欲，却不允许下属产生情欲，所以当他杀死欧阳吉安的时候，我们不会感到太意外，因为他心里隐藏了太多东西。\n太子是整个故事里，唯一一个清楚知道自己想要什么的人。他不喜欢朝廷里的纷争，便遁走江湖寄情山水，去寻找自己真正喜欢的事情。从来没有人将花生人当作人，他却视小姜为花生镇里最好的朋友。可我们都知道这样一句话：哪有什么岁月静好，不过是有人负重前行。在整个故事设定中，大护法的爷爷的爷爷起就一直是弈卫国的大护法，所以大护法的职责就是要保护太子，在故事安排上这部电影相对枯燥，因为后期基本上一直在找太子，所以太子有这样的机会，去选择做自己想做的事情，其实是因为有大护法在一直保护他，相反普通人可能不会有这样的机会，这一点我们稍后会提到。太子除了承担整部电影的笑点以外，我个人认为最出彩的地方是，他在被庖卯打得头破血流时，亲眼目睹了小姜的死，从那一刻开始，我相信他终于明白了身为帝王的那种担当，他不再是以前那个避世逃脱的太子，所以这种成长的感觉会非常好，他在看到大护法以后重复了两次“杀了他”，而在此之前他是坚决反对杀人的。\n小姜是唯一一个自我意识觉醒的花生人，他通过隐婆了解到自己是怎么来到这个世界上的，了解到毒蘑菇到底是怎么一回事情，了解到花生人来自蚁猴子却又以蚁猴子为食的真相，这里有一个有趣的设定，花生人是以蚁猴子作为食物的，这就好像喂猪的泔水里会有猪肉一样，想通了这一点，或许人吃猪肉和人吃人并没有本质的区别。小姜会说话这件事情，让欧阳吉安和疱卯都感到异样，前者是担心危及到自己的统治，后者是对自己的职业产生了怀疑。小姜最终还是死了，就像被庖卯杀死的那些花生人一样，不同的是那颗石头不再是黑石头，而是晶莹剔透的宝石。这让我想起蚌这种可以孕育出珍珠的海洋生物，普通的砂砾经过时间的磨洗可以变成美丽的珍珠。或许答案会是什么，更多的是因为你想要什么，内心贫瘠的土壤寸草不生，内心肥沃的土壤鲜花遍地。小姜内心善良所以懂得回报太子，但现在这个世界善良越来越被人忽视，小姜被老神仙视作圈养的猪啰，可太子会把他当作好朋友，所以说选择非常重要啦，遵从内心的选择更重要。\n庖卯这个角色其实挺悲哀的，他代表是那一类被理想绑架而失去自我的人。“庖”在古代就是指厨师这一类职业，我们熟悉的庖丁解牛这个词就是出自这里。可在影片中颇为讽刺的是，一个想成为厨师的人的最大理想，居然是想要一刀取人心脏，我们不能说这种想法不是一份理想，用大护法的话说就是“你的理想，杀气这么重，怕是实现得一天，会是你的终年”。我们注意到卯和丁一字之差，所谓“丁是丁，卯是卯”，当你被仇恨蒙蔽双眼的时候，看到的东西和实际相比大概会相差很多吧！庖卯在听见花生人说话以后就开始呕吐，这种感觉让他开始思考，自己每天屠杀的到底是些什么东西。试想我们每天吃的这些食物开口说话的话，我们同样会感到恐惧的吧，或许我们想和疱卯一样成为一名绝世大厨，但现实给我们的却是一份杀人的差事，我想坚持初心，知道自己从哪里来要到哪里去，就不会在路上迷失方向，他的死作为一种解脱，在这个故事里算是善终了吧。\n说了这么多，始终没有提到我们的大护法，个人感觉这个角色给我的印象确实不够深刻，它是一个古龙式的侠客形象，拥有和外表极不相称的武力值，这是一个用生命在战斗的人，从故事开头一直战斗到故事结尾，主角光环让它在断了数根肋骨后依然可以活到最后一刻，它是什么样子的呢？一个喜欢朗诵诗歌、自带莎士比亚腔调的文艺大护法，从台词的角度来讲，整部电影深刻中带着些许中二，但这部电影吸引的我，恰恰是这些硬伤很突出的地方，它整体的画风给人一种怪诞和虚无，可它的故事在国内审查体制下让人耳目一新，电影里说“假眼睛、假嘴巴都说贴着难受为什么还要贴？都摘掉它们会怎么笑话我？就因为怕被笑话，所以我们活成了笑话”，或许这是一部给成人看的童话，但透过这部电影多一点思考、多一点想象，大概是我们回报导演不思凡的最好方式之一，因为他想说的或许就是我们想说的。\n","date":"2017-07-30T20:38:22Z","image":"/posts/1684318907/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/1684318907/","slug":"1684318907","tags":["电影","大护法","童话","随笔"],"title":"《大护法》：花生镇里的成人童话"},{"categories":["编程语言"],"content":"正如你所看到的那样，今天我想和大家聊聊异常处理这个话题。对于异常处理这个话题，我相信大家都有各自的方法论。而我今天想和大家探讨的这种异常处理方案，我将其称之为基于过滤器的异常处理。我不知道这种定义是否准确，我们的项目上在要引入 AOP 的概念以后，我们对异常处理的关注点就从try-catch转向Interceptor。虽然首席架构极力推荐，使用 Unity 框架来拦截代码中的各种异常，可从我最初纠结于\u0026quot;return\u0026quot;和\u0026quot;throw\u0026quot;的取舍，到现在我可以灵活地使用和捕捉自定义异常，对我而言老老实实地实践异常处理的经典做法，比使用 AOP 这样一种高大上的概念要有意义地多，因为我相信在某些情况下，我们并不是真正地了解了异常处理。\n异常和错误 或许是因为人类对机器时代充满了近乎苛刻的憧憬，我们的计算机程序在开始设计的时候，就被告知不允许出现错误，甚至我们的教科书上会用一种充满传奇色彩的口吻，来讲述一个因为粗心的工程师计算错了小数点而导致航天飞行器机毁人亡的故事。可是人类常常会对自己选择宽容，而对他人则选择严格，这种观点在整个数字时代更为凸显，当我们无法容忍一个糟糕的应用程序的时候，无论曾经人们为此付出过多少努力，在这一瞬间他们的价值都将不复存在。我们的这种苛刻迫使我们不允许软件出现错误，我们尝试通过各种各样的测试来避免错误发生，可是事实上软件工程实践最终会演变为一个妥协的产物，这意味着我们任何的形式化方法最终都会失败，没有人可以保证一生都不会犯错，而软件工程师同样是人，为什么我们一定要求他们不可以犯错呢？\n我们不得不承认软件产品是一个持续演进的过程，如果抛开商业意义上的Deadline来说，实际上软件是永远没有写完的那一天的，这就是为什么工程师都有点理想主义的原因，不考虑外界环境因素的变化，而期待软件永远不会有新的问题产生，这实在是一种苛刻地要求。好了，我们在这里频繁地提到错误，那么在软件工程学意义上的异常和错误分别是指什么呢？具体来讲，异常是指我们可以明确预测到它会发生并且需要我们进一步处理的流程，而错误是指我们无法明确预测到它会发生并且它会程序流程中断而导致程序崩溃，所以我认为区分\u0026quot;异常\u0026quot;和\u0026quot;错误\u0026quot;最直观、最简单粗暴的方法就是，如果你捕捉到了一个异常并处理了这个异常，那么它就是异常。反之，如果任由异常导致程序 Crash，那么它就是错误。如果我们因为畏惧异常而给所有方法增加 try-catch，我不得不遗憾得告诉你，你还没有真正明白什么是异常。\n在早期的 Win32 API 中，微软大量使用了错误码来表示方法执行过程中发生的错误，这样就引出异常处理中的第一个问题，我们到底是应该是使用错误码还是异常来表示方法执行中发生的错误？事实上这两者在程序的表达能力上等价的，它们都可以向调用者传达\u0026quot;异常发生“这个事件，譬如我们在集合中查找一个元素，如果元素不存在则返回-1，这其实就是一个使用错误码来表示\u0026quot;错误“的经典案例，显然这种从 C/C++时代遗留下来的传统解释了 Win32 API 为什么会选择这样的设计方式，换言之，选择哪种方式，本质上是一种从 API 风格、代码风格和性能指标等方面综合考虑后的结果，错误码这种方式的缺陷主要在于，错误码不能明确地告诉调用者到底发生了什么错误，除非我们定义更多的错误代码，而且在没有引入可空类型以前，我们没有办法避免错误码污染返回值的值域，比如在这个例子，如果集合中恰好有一个元素-1，那么通过-1 这个返回值我们是没有办法判断出，这个-1 到底是不是因为方法内部发生了错误而返回-1.\n好了，现在我们来说说异常，异常在主流的编程语言里基本上是一个标配。异常可以保存从异常抛出点到异常捕获点间的相关信息，所以异常相比错误码可以持有更多的信息，或许你可以尝试去设计一种数据结构来让返回值更丰富:)。我们常常听到\u0026quot;使用异常会降低程序性能\u0026quot;这样的说法，可这部分性能上的差异仅仅是因为，我们需要在抛出异常的时候给调用者更多的信息，所以这是一个非常公平的事情。第二个问题，我们是不是在所有情况下都使用异常？使用异常的好处是它可以让我们以一种更安全的方式去处理异常，可一旦发生了异常程序的性能就会降低，所以我们可以看到.NET 中提供 TryParse 这样的方法，这其实是在告诉我们：如果预测到异常一定会发生，正确的策略不是去捕捉它而是去回避它。在《编写高质量的 C#代码》一书中曾建议：不要在 foreach 内部使用 try-catch，就是这个道理，即采用防御式编程的策略来回避异常，而不是总是抛出异常。\n那么，总结下行文至此的观点：异常是强类型的，类型安全的分支处理技术，而错误码是弱类型的，类型不安全的分支处理技术。元组等可以让函数返回多个返回值的技术，从理论层面上可以模拟异常，即将更多的细节信息返回给调用者，可是这种方式相比由运行时提供支持的异常机制，在性能指标和堆栈调用上都存在缺陷。异常在被运行时抛出来的时候，程序性能是下降的，这是因为调用者需要更多的细节信息，所以不建议在所有场合都抛出异常，建议使用防御式编程的策略去回避异常，直到确定程序没有办法处理下去的时候再抛出异常。理论上所有自定义的异常都应该去捕捉并处理，否则定义这些自定义异常是没有意义的。异常处理应该拥有统一的入口，在代码中到处 try-catch 和记日志是种非常丑陋的做法，理论上应该坚决摒弃。\nChecked Exception 最近垠神写了一篇新的文章《Kotlin 和 Checked Exception》，在这篇文章中垠神提到了 Checked Exception 这种针对异常处理的设计，而恰好我这篇文章写的同样是异常处理，并且我在下面提到的基于过滤器的异常处理方案，实际上就是为了解决这种 Checked Exception 的问题，虽然在.NET 中不存在 Checked Exception。\n要了解什么是 Checked Exception，要从 Java 中的异常机制说起。Java 中的异常类全部继承自 Throwable，它有两个直接子类 Error 和 Exception，通常情况下 Error 是指 Java 虚拟机中发生错误，所以 Error 不需要捕捉或者抛出，因为对此表示无能为力；而 Exception 则是指代码逻辑中发生错误，这类错误需要调用者去捕捉和处理。那么在这样的分类下，Java 中的异常可以分为 Checked Exception(受检查的异常)和 Unchecked Exception(未受检查的异常)，前者需要需要方法强制实现 throws 声明或者是使用 try-catch，如果不这样做编辑器就会直接报错，后者就相对宽容啦，没有这样霸道的条款，可是诡异的是 RuntimeException 是一个 UncheckedException，可它居然是继承自 Exception 而不是 Error，这实在令人费解，Java 的设计模式果然博大精深。\n那么对一个 Checked Exception，Java 的处理方式是十分地霸道的，我们一起来看下面这段代码：\nvoid Foo(string fileName) throws FileNotFoundException { if(...) throw new FileNotFoundException(); } 我们可以注意到 Java 强制让 Foo()方法实现了 throws 声明，原因是在该方法内部可能会引发 FileNotFoundException，如果我们不遵从这一\u0026quot;霸王条款\u0026quot;，那么我们的代码将无法通过编译，而在调用者层面上，Java 的霸道则体现在要求调用者使用 try-catch 结构处理这种异常，或者继续使用 throws 声明来使异常继续向上传递，我更喜欢将这种设计称之为一种理想状态下的异常处理机制，比如我们读写一个文件的时候，除了 FileNotFoundException 以外，可能还会遇到 FileLoadException、PathTooLongException、EndOfStreamException 等等的异常，如果这些异常在业务层面上是无差别的，那么我认为将异常细分到如此精细的程度是没有意义的，因为对用户而言这个时候它关心的是否成功读写了这个文件，具体的异常原因用户并不想真的知道，可是 Java 的 Checked Exception 在面对这种处境的时候，整体而言是显得力不从心的，因为我们不得不在方法从声明该方法会引出哪些异常，这对方法的编写者和方法的调用者来说都很痛苦。\n垠神这篇文章其实在说一个问题，Checked Exception 鼓励开发者主动告知调用者来捕获特定异常，这种思路完全是没有问题的，问题是调用者如何能够知道它需要捕获哪些异常，我们不可能每次都通过\u0026quot;转到定义”功能去看一个方法会引发哪些异常，垠神从 PL 的角度出发，想到了通过代码静态分析的方法来处理异常，垠神吐槽的其实是不分青红皂白滥用 try-catch 的做法，实际上 Java 标准库里对异常处理相当混乱，虽然官方鼓励使用 Checked Exception，但是标准库实现和工程实践上不乏将异常包装为 RuntimeException 来规避 Check 的做法，我认为 Checked Exception 在工程学意义上最大贡献是，在开发阶段该抛出什么异常就应该抛出异常，因为这样可以方便我们快读定位问题，而到了发布阶段则应该将这些异常都 catch 住即可，这样用户就不会看到这些奇葩的异常。换句话说，我们不必在程序中去处理所有的异常，而是将异常机制作为我们定位问题的工具，去捕获那些有可能出现的异常即可。\nC#中其实是由类似 Checked Exception 的概念存在的，不过所有的 Check 都不是强制去实现的，我们知道.NET 中一个方法会抛出哪些异常完全是由注释来说明的，XML 注释中的 exception 节点表示该方法会引发何种异常，我们一起来看下面的例子：\n/// \u0026lt;exception cref=\u0026#34;MasterFileFormatCorruptException\u0026#34;\u0026gt;\u0026lt;/exception\u0026gt; /// \u0026lt;exception cref=\u0026#34;MasterFileLockedOpenException\u0026#34;\u0026gt;\u0026lt;/exception\u0026gt; public static void ReadRecord(int flag) { if (flag == 1) throw new MasterFileFormatCorruptException(); else if (flag == 2) throw new MasterFileLockedOpenException(); // … } 可以注意到 C#采用的是一种相对温和的策略，即文档会明确告诉你，某个方法是否会引发异常以及引发哪些异常，但是是否要捕获这些异常则完全由调用者决定，我认为这是 C#之父 Hejlsberg 在权衡后在工程实践上选择的一种妥协，因为 Java 的 Checked Exception 理想主义色彩稍重，并不是在所有场景下我们都需要去处理所有的异常，所以 Checked Exception 带来的问题是，即使在只需要捕获基类异常的情况下，我们依然不得不去捕获各种子类异常，这难道不有点矫枉过正的感觉吗？事实上所有工程实践中，不分青红皂白直接捕获 Exception 父类的做法，就是因为调用者完全不想关注发生得异常细节，这是垠神在文章中吐槽的\u0026quot;糟糕的代码\u0026quot;，C#相对 Java 在异常处理上好的一点就是，优秀的工程师会自觉地处理异常，如果他们清楚地知道异常会发生就一定会去捕获异常。你不能强迫他们去做他们不喜欢做的事情。\n让异常处理更优雅 好了，现在我们来考虑这样一个问题，设计 Checked Exception 的初衷是为了让我们处理业务逻辑中不同的具体的异常，当这些异常在业务逻辑层面上无差别的时候，其实我们可以完全忽略这些异常的细节，因为不管是哪种具体的异常，在业务逻辑层面都被认为是任务执行失败，这种情况下我们直接捕获基类异常即可，例如在读写文件的例子中我们关注 IOException 即可。那么如果这些具体异常在业务逻辑层面上存在差异呢？这种情况下我们就应该向 Checked Exception 方向靠拢，下面我们来一起听一个实际的故事。\n我们的项目上需要从多个相互独立的系统中抓取数据并生成报表，因为这些系统在设计上都存在缺陷，所以在抓取数据的过程中非常容易出现错误，所以我们必须非常谨慎地处理这些异常，用户要求我们必须一种视觉友好的方式将报表输出出来，当异常发生时我们需要将抓取失败的数据高亮显示出来，并输出相关的错误信息来提醒用户来 Check 这些信息，而事实上每种异常发生的时候其处理逻辑是完全不同的。为此我们定义了将近 10 种的自定义异常，并为用户设计了完善的操作日志记录机制，这一切听起来非常不错，最终写出来的代码大概是下面这个样子：\ntry { Foo() } catch (ExceptionA ex) { } catch(ExceptionB ex) { } catch(ExceptionC ex) { } catch(ExceptionD ex) { } // ... 相信在 Java 的相关工程实践中，这种教科书般的代码基本上是异常处理的金科玉律啦，可是这种代码实现写起来会让人感觉头重脚轻，因为我们所有重要的逻辑都写在了 catch 块里，这让我非常不喜欢这种\u0026quot;臃肿”的代码，而且事实上在这个案例中 catch 块里的代码具备可重用的可能，所以我决定以一种更优雅的方式来重构这段代码。数学领域中有一个不变的真理，即任何问题都可以通过引入一个新的问题来得到解决，这个理论在编程中同样适用，因为在公司里受到同事 Wesley 的影响，我对 Ioc 和 AOP 都从思想上有了一定认识，公司在推行 Unity 的过程中我并没有看到多少实际的意义，所以我对\u0026quot;道\u0026quot;的重视远远超过对\u0026quot;术\u0026quot;的追求，因为我相信框架学习起来并不会花费多少实践，可怕的是你从来不试图去了解框架背后的秘密，所以我借由 AOP 中拦截器和 MVC 中过滤器的概念，想到了我接下来要说的这种异常处理的方案。\n其实我们在这里的核心目的是为了消除分支，所以采用多态是我们重构这部分代码的第一步。我们首先定义一个针对 Foo 方法的异常基类 ExceptionBase，然后让这里的 ExceptionA、ExceptionB、ExceptionC 等全部继承自 ExceptionBase，这样我们就可以将这些具体的异常子类向上转型为 ExceptionBase 统一进行处理。与此同时，我们注意到处理各种异常子类的逻辑各不相同，虽然我们可以直接将处理异常的逻辑写到异常中(通过一个虚方法来实现)，可这样会造成异常子类里的职责负荷，我更希望异常子类是一个朴素的贫血模型，所以我们这里引入过滤器的概念(不知道这样叫是否合适)，Filter 全部继承自 FilterBase，它有一个 Invoke 的方法，我们最终会在这里实现异常处理的方法，为此我们需要定义各种各样的 Filter，然后通过 Attribute 将每一种 Filter 和特定的 Exception 关联起来，比如我们希望在所有异常的地方打日志，那么我们只需要实现一个 LoggerFilter，然后给所有的 Exception 添加 Attribute，这可以显著改善我们的代码。\n现在，我们只需要给 ExceptionBase 类提供一个 GetFilter()方法，该方法的返回值类型为 FilterBase，我们将通过反射来创建一个 Filter 并将其返回，所以我们写出来的代码会是下面这个样子：\n[ExceptionFilter(FilterName = \u0026#34;FilterA\u0026#34;)] class ExceptionA : ExceptionBase { } [ExceptionFilter(FilterName = \u0026#34;FilterB\u0026#34;)] class ExceptionB : ExceptionBase { } [ExceptionFilter(FilterName = \u0026#34;FilterC\u0026#34;)] class ExceptionC : ExceptionBase { } 此时此刻，我们的关注点就从一堆 catch 块中转移到了不同的 filter 中，虽然我们编写代码的工作量没有减少，但这样的做法无疑增强了代码的可维护性，因为我们只需要到不同的 Filter 里去修改逻辑即可，我在书上看到这样一句话，无论什么时候合并代码总是比拆分代码要容易，当你不确定某个功能是否要放在特定模块中的时候，最好的解决方案就是将他们完全独立设计。降低代码的耦合度是一件我们常常挂在嘴边的事情，可是如何去真正地降低代码耦合度，这件事情需要我们一直思考下去。好了，现在我们完成了分支结构上的精简，而最终调用的代码会是：\ntry { Foo() } catch (ExceptionBase ex) { var filter = ex.GetFilter(); filter.Invoke(); } 现在的代码是不是比原来清新了好多呢？虽然这相比真正的 AOP 还是稍显稚嫩，可它的出现成功地让一段\u0026quot;丑陋\u0026quot;的代码变得优雅起来，我们不必再担心修改异常处理流程时会原来的代码产生副作用，如果需要增加新的处理逻辑我们继续派生异常类和过滤器即可，这就是代码设计上以不变应万变的道理，你要相信程序员都是非常懒惰的，如果有可以不用修改代码就适应变化的设计，他们是一定会喜欢的，因为这个世界对程序员并不友好，如果你不想你的代码被这个世界修改得面目全非，最好的选择就是不给它们这样的机会，他们说人长大一定会变成自己讨厌的样子，我想告诉这个世界永远不要忘记初心。\n好了，这篇文章唠叨到现在，写了大概 5000 多字，花了我整整两个下午的时间，我在向 Paul 和 John 询问这个问题的看法时，他们都告诉我这个问题没有好的解决方案或者是劝我不要做这样探索，可是事实上我只用了半天时间就完成了这个设计，在去年的时候我曾向房燕良前辈请教好异常处理的问题，当时我的关注点主要在使用错误码还是使用异常。在上一个项目中，我对于异常处理其实实践得并不好，因为我一直不知道哪里是捕获异常的入口，我个人并不认同直接捕获到异常直接 throw 这种做法，因为你在自定义异常的时候就应该想清楚，哪些异常是需要捕获并处理的，哪些异常时可以直接让它 Crash 的，如果每个人都仅仅是抛出异常而不去拦截异常，那么异常机制设计得再好又有什么用呢，关于 Java 的异常有一个梗，是说 Java 的异常给出了详细的堆栈信息，可就是不直接告诉你到底是哪里异常了，事实证明我设计的这个方案运行的很好，其实我很想吐槽操作日志真的有存在的必要吗？很多时候，我们要学会遵从自己内心的声音，所谓世间难道不就是我们吗？真正绑架我们的永远都只有别人，让我们时刻谨记：万物为虚，万事皆允。\n","date":"2017-05-20T20:10:28Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/570888918/","slug":"570888918","tags":["异常","设计","架构"],"title":"基于过滤器实现异常处理的探索"},{"categories":["编程语言"],"content":"各位朋友，大家好，欢迎大家关注我的博客，我是 Payne，我的博客地址是:http://qinyuanpei.com。今天博主想和大家探讨的是，.NET 中异步 Lambda 表达式的问题。为什么要讨论这个问题呢，这或许要从公司首席架构推广内部框架这件事情说起。我其实很久以前就有这种在团队内部做技术演进的想法，即通过公共类库、团队 Wiki 和技术交流等形式逐步地推进和完善团队整体架构的统一，因为一个团队在业务方向和技术选型上基本是一致的，因此团队内的技术演进对提高开发效率和交付质量意义重大，所以我能理解首席架构在内部推广公共类库这件事情，因为除了 KPI 这种功利性的目标以外，从长远来看这些东西对一个团队来说是积极而有利的，可是我们都知道工程师是这个世界上最傲慢的人，如果一个东西设计得不好，他们一定会尝试去改进甚至重新设计，所以架构并非是一种虚无缥缈的、凭空想象出来的东西，它的存在必须是为了解决某种问题。\n所以我始终认为，架构设计必须由一线开发人员来提炼和抽象，因为只有真正经历过\u0026quot;坑\u0026quot;的人，才会清楚地知道团队里最需要解决的问题是什么，一个良好的架构绝对不是由某些所谓\u0026quot;专家\u0026quot;闭门造车的结果，你只有真正了解了一个问题，懂得如何去定义一个问题，你才会知道目前这个团队中最迫切需要去解决的问题是什么，虽然说团队里技术层次存在差异，一个技术选型必然会和普通社会学问题一样存在众口难调的情形，可是一个东西设计得不好它就是不好，你不能强迫团队成员必须去使用它，因为这实在有悖于\u0026quot;自由\u0026quot;和\u0026quot;分享\u0026quot;的黑客文化。我相信软件开发没有银弹可言，这意味着它没有一种一劳永逸的解决方案，即使它的抽象层次再高、代码鲁棒性再好，所以团队内部技术演进应该采取\u0026quot;自下而上\u0026quot;的方式，对待工程师最好的方式就是给他们充分的自由，\u0026ldquo;自上而下\u0026quot;的行政命令不适合工程师文化，自计算机文明诞生以来，那种来自内心深处的\u0026quot;极客思维\u0026quot;决定了我们的基因，所以啊，\u0026ldquo;请原谅我一生不羁放纵爱自由\u0026rdquo;。\n好了，现在回到这个问题本身，问题产生的根源来自 ICommand 接口，而我们都知道该接口主要承担命令绑定作用。通过 ICommand 接口的定义我们可以知道，ICommand 接口的 Execute 方法是一个同步方法，因此常规的做法如 RelayCommand 或者 DelegateCommand，基本上都是传入一个 Action 来指向一个具体方法，最终 ICommand 接口中的 Execute 方法执行的实际上是这个具体方法。截止到目前为止，这个策略在主流的场景下都实施得非常好，可是我们在引入 Task、async/await 这些新的概念以后，我们突然发现 ICommand 接口存在一个亟待解决的问题，即它缺乏一个支持异步机制的 Execute 方法，显然这是一个历史遗留问题。 我开始关注这个问题是当我在同事 John 和 Charles 的项目中看到类似下面的代码，事实上他们都是非常优秀的高级工程师，在对这个问题理解和探讨的过程中，我要特别感谢他们愿意分享他们的想法。我们一起来看看下面的代码：\npublic RelayCommand RunCommand { get { return new RelayCommand(async ()=\u0026gt;{ /* await awaitable */ }); } } 请相信你的眼睛，因为你没有看错，让我倍感纠结的的正是这样一段简单的代码。这段代码让我迷惑的地方有两处，第一，RelayCommand 实现了 ICommand 接口，而 ICommand 接口的 Execute 方法是一个同步的方法，为什么我们可以在这个里传入一个异步方法，并通过 Action 这种委托类型来对其进行包装；第二，Action 是一个 void 类型，即无返回值的委托类型，我们这里显然使用 async 关键字修饰了一个无返回值的方法，因为我们在这个匿名方法内部使用了 await 语法。可是我们知道微软官方的建议是，使用 async 关键字来修饰一个返回值类型为 Task 或者 Task的方法。在我了解到 async 关键字还可以这样使用以后，对第二处疑惑我稍稍有些许释怀，因为事实上 Charles 就是正式通过这种思路来启发我，可我始终无法理解，为什么我们可以在一个同步的方法里执行一段异步代码，并试图去安慰自己说这段代码是异步的，在执行一个非常耗时的任务时界面不会阻塞。\n我们的项目需要在整个任务执行过程中输出操作日志，这意味着消息会实时地输出到界面上并且不会阻塞界面。我们在为此设计了一个基于观察者模式的消息队列，所有需要发送实时消息的模块被抽象为一个消息主题，而界面模块、日志模块等被抽象为消息观察者，所有订阅过的消息主题都会将消息推送到消息队列中，这一切目前在设计上是符合业务需求的。可是很快我们就会发现一个问题，使用 await 或者 Wait()方法时，消息并不是实时地发送到界面上去的，因为我们知道 await 或者 Wait()方法会一直等待一个异步任务执行完成，所以消息会在任务结束的一瞬间被全部发送到界面上，这显示是不符合我们的期望的，所以 Execute()方法里执行的必然是一个同步方法，它不会因为我们传入了一个异步方法而改变，况且同步和异步是相对而言的，如果我们将 await 语法修改为 Task.Run()，我们就会发现在异步任务执行完成前同步方法就开始执行了，而这正是我们想要的结果。\n在这里我更感兴趣的一个问题是，.NET 框架中的委托、匿名方法、Lambda 表达式和 Task 是不同时期.NET 的产物，那么我们在这里使用一个 async 关键字来修饰一个匿名方法，编译器在处理它的时候到底会怎么做呢？因为我们知道委托会被编译成一个包装类，那么现在在这篇文章中的提到的这个问题背景下，它会有什么不同呢？我们一起来看下面的代码：\nstatic void Main(string[] args) { Action action1 = async () =\u0026gt; await DoWorkAsync(); Action action2 = () =\u0026gt; DoWork(); } 我们注意到这里声明了两个 Action，即两个没有返回值的委托类型，它们的不同点在于前者使用了 async/await 这两个关键字，而后者则是一个普通的同步方法，那么这两者生成的 IL 代码是否有区别呢？我们可以通过 IL DASM 或者是 IL Spy 这两个工具来查看 IL 代码：\n查看IL代码\r我们可以注意到两点，第一，两个委托类型生成的中间代码完全一致，都是CachedAnonymousMethodDelegate，这在某种程度上说明不管 Action 里包装的是一个同步方法还是一个异步方法，最终生成的 IL 代码应该都是相同的。第二，同匿名方法和扩展方法一样，async/await 并未引入新的 IL 指令，async/await 内部应该是在维护一个状态机，这一点和 yield 关键字应该是相似的，并且对于异步的匿名方法(指 voild 类型)，通过 IL 代码可知它是由AsyncVoidMethodBuilder类来生成的，而对于异步的方法(指 Task 和 Task类型)，则是由AsyncTaskMethodBuilder类来生成，需要说明的是这两者在功能上相差无几，唯一的区别就在于异常处理。\n关于异步编程中异常的处理，老赵在其博客关于 C#中 async/await 中的异常处理（上）和 关于 C#中 async/await 中的异常处理（下）这两篇博客中做了非常详细的解释，建议大家有时间的话去阅读这两篇文章，我们在这里关注结论就好。 具体来讲，async Task 或者 async Task方法引发异常时，会捕获异常并将其放置在 Task 对象里，并且只有 Task 对象被 await 时会引发异常。特别地，在调用 Task.WhenAll()方法时，一个 Task 对象中可能会含有多个异常，此时 await 仅仅会重新抛出第一个异常，但是在 Task 上使用 Task.Wait 或 Task.Result 同步阻塞时，所有异常都会用 AggregateException 包装后引发。对于嵌套的 Task，即含有子任务的 Task，应该采用 AggregateException 来获取和处理所有的异常。Task/Task中未捕获的异常可以通过 TaskScheduler.UnobservedTaskException 来处理，这些异常不会继续向上抛导致程序异常退出。 async void 方法引发异常时，因为它没有 Task 对象来放置异常，因此它的异常 SynchronizationContext 上引发，而且因为AsyncVoidMethodBuilder内部并没有使用 TaskScheduler，因此对于 async void 方法来说，线程池中未捕获的异常将会一直向上抛并最终导致程序异常终止，虽然我们可以在 AppDomain.UnhandledException 这个事件中捕捉到这些\u0026quot;未处理的异常\u0026rdquo;，但这并不能阻止程序异常终止，通过我们可以通过注册这个事件来记录异常日志，以帮助我们快速定位问题。\n好了，现在我们回到这篇文章开始的问题，我们现在知道 async Task 和 async Task引发的异常，都不会是程序立即终止，除非我们显式地去 await 一个 Task 对象会引发异常，可是对 async void 来讲，一旦它引发异常，常规的 try-catch 时无法捕捉到异常的，这种\u0026quot;未处理的异常\u0026quot;会一直向上抛并最终导致程序异常终止。我为什么要说这个问题呢，因为我们在文章开始的时候写了一个异步的 lambda 表达式，最终它会被编译为 async void，我们现在应该会了解到，async void 非常容易引发未处理的异常并导致程序异常退出，所以这是微软官方最佳实践中不推荐使用 async void 的原因，因为使用 async void 就意味着我们要去捕获所有的异常。可是对标记为 async 的 lambda 表达式来讲，这个问题是非常隐蔽而且蛋疼的，或许不使用 async void 就是最为正确的选择了吧！\n最后，其实坦白讲，我自己是不清楚在这篇文章里我到底说什么的，因为这样一个在项目开发中遇到的问题，其实并不是一个特别重要的内容，因为它实在是太容易被我们给忽略啦。我最初关注这个问题完全是因为好奇，因为我从来没有见到过这种 lambda 表达式的写法，虽然纠结这样一个语法上的问题，和孔乙己讨论茴香豆的\u0026quot;茴\u0026quot;字由几种写法一样，都是一个相当迂腐不堪的表现，可我庆幸这份好奇让我了解到了更多的东西。其实总结下这篇文章中关注的点，主要有：\n由同步方法和异步方法包装的委托类型在 IL 层面上是无差别的，委托关注的是参数列表和返回类型，和是否有 async 关键字修饰没有关系。 匿名方法或者 lambda 最终依然会被编译为一个方法，在有 async 关键字修饰的情况下，建议使用 Func 而不是 Action，因为前者可以生成 async Task 或者 async Task，而后者仅仅可以生成 async void。 async Task/async Task和 async void 在异常处理机制上存在差异，前者未处理的异常不会继续向上抛导致程序异常退出，而后者未处理的异常会继续向上抛并导致程序异常退出，因此如果坚持要使用 async void，就一定处理各种异常。 参考文章： Microsoft - async/await - 异步编程中的最佳做法 TianFang - C# 5.0 async 函的提示和技巧\n","date":"2017-04-15T21:10:47Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/187480982/","slug":"187480982","tags":["Lambda","异步","编程"],"title":"异步 Lambda 表达式问题的探索"},{"categories":["数据存储"],"content":"各位朋友，大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。最近这段时间的天气可谓是变幻莫测，常常是周一到周五像夏天般热烈，而周六和周天像秋天般冷清。你不知道它到底会在何时下雨，即使你可以一直带着伞等雨落下来。但是对于没有伞的我来说，学会努力奔跑以至于不那么狼狈，或许是在这个世界上我唯一可以去做的事情。可是你知道一个人孤独的时候，即使是下雨这种再平常不过的事情，他都可以从雨声里听出孤独的感觉来，所以这个周末我决定继续研究 Redis 缓存技术，而今天我想和大家讨论的话题是 Redis 中的发布-订阅(Pub-Sub)，希望大家喜欢！\n从观察者模式说起 如果你熟悉常见的设计模式，就应该会知道在 24 种设计模式中，有一种称为观察者模式的设计模式，该模式又被称为发布-订阅模式。在正式讨论 Redis 中的发布-订阅特性前，我想先花点时间来为大家讲解下这种设计模式。观察者模式定义了一种一对多的依赖关系，让多个观察者同时监听同一个主题对象，当该主题对象在状态发生变化时，会通知所有观察者对象并使其自动更新自己。下面是该模式的 UML 类图：\n设计者模式的UML类图\r通常我们提到设计模式的时候，都认为实际模式是非常抽象而晦涩的概念，事实上设计模式是一种经过反复验证的编程经验。我们每天面对这个世界对其进行抽象并认识它，所以设计模式本质上是根植自生活的一种编程思想。以观察者模式为例，我们或许会在微信里订阅各种各样感兴趣的公众号，当这些公众号的内容发生更新时，就会主动向我们推送新的内容。在这里，我们订阅的公众号称为\u0026quot;主题\u0026quot;，而我们则称为\u0026quot;观察者\u0026quot;或者\u0026quot;订阅者\u0026quot;，而这正是观察者模式又被称为\u0026quot;发布-订阅模式\u0026quot;的原因所在，这种定义了一种一对多的依赖关系，让多个观察者同时监听同一个主题对象，当该主题对象在状态发生变化时，会通知所有观察者对象并使其自动更新自己的设计模式就被称为\u0026quot;观察者模式\u0026quot;。而通过这张图我们可以了解到，观察者模式试图解决的问题是，在不同的实例对象间相互协作的时候，如果在降低其各自耦合度的同时，维持这些示例对象间的一致性。在该模式中，主要存在四种角色，即：\n抽象主题(Subject)：抽象主题将所有观察者对象的引用保存到一个集合里，每个主题都可以有任何数量的观察者。抽象主题提供一个接口，可以增加(Attach 方法)和删除(Detach 方法)观察者对象。 具体主题(ConcreteSubject)：具体主题将在其内部定义相关状态，并将相关状态存入具体观察者对象。在具体主题内部状态发生变化时，通知所有注册过的观察者发出通知，即 UML 类图中定义的 Notify()方法。 抽象观察者(Observer)：抽象观察者将为所有具体的观察者定义一个接口，在获得主题更新通知时更新自己，即 UML 类图中定义的 Update()方法，执行该方法后观察者与主题的状态实现同步。 具体观察者(ConcreteObserver)：具体观察者将实现抽象观察者所定义的更新接口，来使得观察者自身的状态与主题状态协调，即具体观察者需要重写 Update()方法并维护其内部状态同主题保持一致。 至此我们就从思想上理解了观察者模式，观察者模式本质上是在维护一种一对多的依赖关系，因为观察者与主题都是依赖于抽象而非具体，两者分别属于两个不同层次上的抽象，因此观察者和主题两者间是解耦的。可是当你去实现一个具体的主题或者具体的观察者的时候，你会发现这两者间依然存在一定的依赖，因为观察者和主题在接口设计上需要协调，因为两者分别作为消息的\u0026quot;接收方\u0026quot;和\u0026quot;发送方\u0026quot;存在。观察者模式虽然在解耦上效果显著，可这并不代表它就是完美的。事实上，当观察者数目特别多的时候，为了通知所有的观察者将花费大量的时间；其次，当观察者间存在依赖关系时，观察者模式将导致这些观察者出现循环调用；再者，当主题通过异步的方式来通知观察者时，需要考虑通知本身是以自洽的方式进行的；最后，观察者模式可以确保观察者捕捉到主题的变化，可是观察者模式机制本身不具备知晓主题如何变化的能力。好了，下面我们来讲解如何实现一个基本的观察者模式。\n观察者模式的实现 现在，我们已然了解到在观察者模式中主要有四类角色，即抽象主题、抽象观察者、具体主题和具体观察者。因此，要实现观察者模式，实际上就是要实现这四种不同的角色。回到我们最初讨论过的场景，即微信用户订阅公众号，假设博主希望在博客更新的时候，以邮件或者公众号的形式来通知读者朋友博客更新的内容，这是一个典型的一对多的依赖关系维护的问题，显然此时观察者模式是一个最佳的设计思路。在这个设计中，邮件和公众号是两个具体的观察者，而博客是一个具体的主题。参照观察者模式的 UML 类图，我们应该首先提取出来两个抽象类，即 Subject 和 Observer。\n对 Subject 类而言，首先它需要提供一个订阅(Subscribe 的方法和取消订阅(Unsubscribe)方法，这和我们在日常生活中订阅报纸是完全一样的；其次，它需要有一个更新(Update)的方法，该方法负责向所有的订阅者广播消息。为什么叫做广播呢？因为所有的订阅者都会收到这条消息，这种订阅者被动接受主题推送消息的方式我们称为\u0026quot;推送模式\u0026quot;，即在 Update 的时候，主题会主动推送\u0026quot;参数\u0026quot;给订阅者；而订阅者主动拉取主题消息的方式我们称为\u0026quot;拉取模式\u0026quot;，即在 Update 的时候，主题并不主动推送\u0026quot;参数\u0026quot;给订阅者，而是由订阅者通过注入的主题来获取消息。这两种方式我们都可以称之为观察者模式，在这里我们选择\u0026quot;推送模式\u0026quot;，代码实现如下：\npublic abstract class Subject { private IList\u0026lt;Observer\u0026gt; observers = new List\u0026lt;Observer\u0026gt;(); public void Attach(Observer observer) { observers.Add(observer); } public void Deatch(Observer observer) { observers.Remove(observer); } public void Notify(string message) { observers.ToList().ForEach(o =\u0026gt; o.Update(message)); } } 对 Observer 而言，它在观察者模式中承担着消息接收者的角色，所以我们需要为其定义好接收消息的接口，需要注意的是该接口必须与具体主题保持一致，这便是我在文章中提到的，主题和观察者存在一定程度依赖的问题。考虑到不同的观察者所做的事情是完全不同的，例如邮件和公众号采取两种不同的方式来推送消息，因此 Update 方法应该被声明为虚方法，以为不同的观察者提供重写的扩展能力。它的代码实现如下：\npublic abstract class Observer { public virtual void Update(string message) { } } 对具体观察者而言，我们需要做的就是继承 Observer 类然后重写 Update 方法，在这里我们需要实现两个不同的类 EmailObserver 和 WechatObserver，它们分别来实现邮件和公众号接收到主题推送消息以后的逻辑，这里以 EmailObserver 为例，代码实现如下：\npublic class EmailObserver:Observer { public override void Update(string message) { Console.WriteLine(\u0026#34;邮箱接收到订阅消息:{0}\u0026#34;, message); } } 对具体主题而言，我们不再关心如何向所有的观察者发送消息，该功能在 Subject 父类中已然完成。我们可以为新的主题类添加更多的属性来描述其内部发生变化时的状态，例如文章数目、评论数目或者是内容更改等等。在这个例子中我们选择最简单的方式，即简单通知这两个观察者，因此我们直接继承 Subject 类即可。此时，完整的调用代码如下：\nBlogSubject blog = new BlogSubject(); blog.Attach(new EmailObserver()); blog.Attach(new WechatObserver()); blog.Notify(\u0026#34;Payne更新了Redis缓存技术学习系列文章\u0026#34;); 好了，现在通过下面的截图，我们就可以看到两个观察者 EmailObserver 和 WechatObserver，都接收到了来自主题 Blog 的消息推送。这就是观察者模式啦，看起来是不是非常简单。可是相信大家使用公众号以后就会发现一个问题，随着你订阅的内容越来越多，你的微信消息列表里出现的消息推送就越来越多，这个时候如果你不想接收消息推送该怎么办呢？答案好像只有一个，那就是取消订阅。这个场景可以看出\u0026quot;推送模式\u0026quot;让订阅者饱受消息骚扰，而为了解决这个问题，我们就有了\u0026quot;拉取模式\u0026quot;，此时主题仅仅是告诉观察者博客内容有更新，而更新的内容需要观察者自己去处理，这种模式大同小异，大家可以参照\u0026quot;推送模式\u0026quot;来自己实现。\n观察者模式基本实例\r这些就是观察者模式的核心内容啦，观察者模式的优点是它解除了主题和观察者间的耦合，并且使得这两者各自都依赖于抽象而非具体，观察者模式适用的场景是当一个对象的改变需要给变其它对象时，而且它不知道具体有多少个对象有待改变时。在 C#中我们可以通过委托、事件以及 Observable 接口这三种方式来更好、更快的实现观察者模式，自然这些都是后话啦，如果以后有机会我们可以继续进行探讨。\nHey Redis Pub-Sub 好了，了解完观察者模式即发布-订阅模式以后，我们现在就可以开始学习 Redis 中的发布-订阅模式啦。为什么我们要在开始学习 Redis 中的发布-订阅模式前，了解设计模式相关的概念呢？这是因为 Redis 中的发布-订阅模式和 Gof 设计模式一脉相承，譬如事件机制、消息机制等概念其实都是观察者模式的一种实际应用，一旦我们掌握了观察者模式的核心思想，即使这个世界充满了套路，可是这对你我而言又有什么不同呢？我们学习设计模式不是为了记住这些类图，而是能在最恰当的场景中合理使用这些模式来解决问题，这是我们学习的最终目的。\nRedis 中的发布-订阅模式是一种消息通信模式，即发布者发布消息，订阅者接收消息。在 Redis 中客户端可以订阅任意个频道，当该频道内接收到一个新消息时，所有订阅该频道的客户端都会收到这条新消息。我们可以这样理解这种消息通信模式，我们每个微信账号都是一个客户端，每个客户端都可以订阅任意个微信公众号，当微信的后台服务上接收到某个微信公众号的请求消息时，所有订阅了该微信公众号的客户端都会收到该推送。一个简单的图示如下：\nRedis中的消息模式\r我们可以注意到这和我们在文章中提到的\u0026quot;观察者模式\u0026quot;非常相似，在这个通信模式下，客户端作为消息的订阅者，即观察者。而频道作为消息的发布者，即主题。在 Redis 中频道是一个字符串类型的值，你可以将其理解为一个 Id。虽然我们在这篇文章中花费大量时间来讲观察者模式，事实上 Redis 中的发布-订阅是非常轻量并且强大的，下面是常见的命令：\nPSUBSCRIBE：该命令用于订阅一个或者多个符合模式匹配的频道\nPUBSUB：该命令用于返回由活跃频道组成的列表，即可以查询订阅与发布系统的状态\nPUBLISH：该命令用于发送消息到指定的频道\nPUNSUBSCRIBE：该命令用于退订所有符合模式匹配的频道\nSUBSCRIBE：该命令用于订阅一个或多个频道\nUNSUBSCRIBE：该命令用于退订一个或多个频道\n以上这些就是和发布-订阅相关的命令啦，从整体上而言它是相当简洁和紧凑的。在这篇文章中我们通篇都在说观察者模式，事实上 Redis 的发布-订阅从本质上来讲还是观察者模式，Redis 内部会维护一个频道的字典，首先它会从频道字典中查找所有的客户端，如果字典中不存在该频道，则将订阅该频道的客户端列表添加到字典中，否则它会返回字典中已经存在的客户端列表。在获取到所有客户端列表以后，Redis 将会遍历客户端列表中的客户端，然后给每个客户端发送消息，这部分代码的解读可以参考这篇文章：15 天玩转 redis —— 第九篇 发布/订阅模式。好了，这篇文章暂时就是这样子啦，为什么感觉最近学习 Redis 没有动力了呢？这篇文章没有实际的命令演示，这是因为我是在 Windows 系统下写完的这篇文章，深夜啦，睡吧！\n现在我们一起来看一个简单的示例，在这个示例中我们让两个客户端 A 和 B，订阅同一个频道 News，然后由客户端 C 来向这个频道 News 广播一条消息，理论上客户端 A 和客户端 B 都将会收到这条消息，需要注意此时服务端是开启的。首先，对于客户端 A 和客户端 B，我们在两个不同的终端窗口中打开 redis-cli，然后输入命令：\n\u0026gt; SUBSCRIBE News 在按下回车后，我们可以看到下面的信息：\n1) \u0026#34;subscribe\u0026#34; 2) \u0026#34;News\u0026#34; 3) (integer) 1 好了，现在我们在客户端 C 中来广播一条消息：\n\u0026gt; PUBLISH News \u0026#34;This is a message sent by 127.0.0.1:6379\u0026#34; 此时我们可以看到下图中所示的结果：\nRedis中发布订阅模式\r","date":"2017-04-15T21:03:57Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1444577573/","slug":"1444577573","tags":["Redis","缓存","设计模式","笔记"],"title":"Redis 缓存技术学习系列之发布订阅"},{"categories":["数据存储"],"content":" 在本系列的第一篇文章中，我们主要针对 Redis 中的“键”和“值”进行了学习。我们可以注意到，Redis 是一个 C/S 架构的数据库，在我们目前的认知中，它是通过终端中的一条条命令来存储和读取的，即它是一个非常典型的“请求-响应”模型。可是我们知道在实际的应用中，我们要面对的或许是更为复杂的业务逻辑，因为 Redis 中不存在传统关系型数据库中表的概念，因此在使用 Redis 的过程中，我们要面对两个实际的问题，即如何更好的维护数据库中的”键“、如何在高效执行命令的同时保证命令执行成功。对于前者，我认为这是一个设计上的问题，而对于后者，我认为这是一个技术上的问题。所以，这篇文章的核心内容就是找到这两个问题的答案。带着这样的问题出发，我们就可以正式进入这篇文章的主题：Redis 中的事务处理。\n从数据库事务说起 ​ 通常我们提及数据库都不可避免的要提到事务，那么什么是事务呢？事务是指作为单个逻辑工作单元执行的一系列操作。所以，首先事务是一系列操作，这一系列操作具有二态性，即完全地执行或者完全地不执行。因此事务处理可以确保除非事务单元内的所有操作的成功完成，否则不会永久更新面向数据的资源。我们这里举一个例子，数据库中除查询操作以外，插入(Insert)、删除(Delete)和更新(Update)这三种操作都会对数据造成影响，因为事务处理能够保证一系列操作可以完全地执行或者完全不执行，因此在一个事务被提交以后，该事务中的任何一条 SQL 语句在被执行的时候，都会生成一条撤销日志(Undo Log)，而撤销日志中记录的是和当前擦作完全相反的操作，比如删除的相反操作是插入，插入的相反操作是删除等。我们通常所说的事务回滚其实就是去执行这些插销日志里的相反操作，这同样告诉我们一个道理，只有事务中的一系列操作完全执行的情况下可以回滚，如果是在意外情况下导致事务中的一系列操作没有完全执行，这个时候我们是不能保证数据一定可以回滚的。\n​ 在数据库相关理论中，一个逻辑工作单元想要成为事务，就必须满足 ACID，即原子性、一致性、隔离性和持久性。(1)：原子性这个概念其实就是指，一个事务内的所有 SQL 操作都是一个整体，因此只有所有的 SQL 操作都完全执行成功，该事务方可以认为提交成功。如果在提交事务过程中某一条 SQL 语句执行失败，则整个事务必须回滚到事务提交前的状态。(2)：而一致性这个概念则是指，事务在完成的时候，必须要保证所有的数据都保持一致的状态，而落实到数据库的各个组成部分上，则要求开发人员能够保证数据、索引、约束、日志等在事务前后具备一致性。(3)：隔离性这个概念主要针对并发，其核心思想就是不同的并发事务对数据产生的修改必须是相互隔离的，假设有两个不同的事务 A 和 B 并发执行，那么对 A 来讲，它在执行前的状态只有两种，即 B 执行前和 B 执行后。同理，对 B 来讲同样是如此，这样的特性我们就称为隔离性。(4)：持久性相对简单，是指事务完成以后它对数据的影响是永久性的。\nRedis 中的事务处理 ​ 好了，截止到目前为止，我们对数据库中事务处理的相关理论有了一个基本的认识，或许这个世界上的数据库系统千差万别，但我相信在事务处理这个问题上它们最终会殊途同归，就像我们解决并发过程中的冲突问题，常规的做法依然是加锁一样，这是我之所以要花费精力去理解和解释这些理论知识的原因，技术可谓是日新月异，如果我们总是一味地为新技术而疲于奔命，那么或许我们会渐渐地失去对这个行业的热爱，我相信原理永远比框架更为重要，没有系统学习过计算机专业的课程，这件事情让我至今都颇为遗憾。Redis 中的事务是可以视为一个队列，即我们可以通过 MULTI 开始一个事务，这相当于我们声明了一个命令队列。接下来，我们向 Redis 中提交的每条命令，都会被排入这个命令队列。当我们输入 EXEC 命令时，将触发当前事务，这相当于我们从命令队列中取出命令并执行，所以 Redis 中一个事务从开始到执行会经历开始事务、命令入队和执行事务三个阶段。下面是一个在 Redis 中使用事务的简单示例：\n127.0.0.1:6379\u0026gt; MULTI OK 127.0.0.1:6379\u0026gt; SET Book_Name \u0026#34;GIt Pro\u0026#34; QUEUED 127.0.0.1:6379\u0026gt; SADD Program_Language \u0026#34;C++\u0026#34; \u0026#34;C#\u0026#34; \u0026#34;Jave\u0026#34; \u0026#34;Python\u0026#34; QUEUED 127.0.0.1:6379\u0026gt; GET Book_Name QUEUED 127.0.0.1:6379\u0026gt; EXEC 1) OK 2) (integer) 4 3) \u0026#34;GIt Pro\u0026#34; 我们可以注意到 Redis 中的事务和通常意义上的事务基本上是一致的，即\n事务是由一系列操作组成的单个逻辑工作执行单元。特别地，因为在 Redis 中命令是存储在一个队列中，所以，事务中的所有命令都会按顺序执行，并且在执行事务的过程中不会被客户端发送的其它命令中断。 事务是一个原子操作，事物中的命令只有两种执行结果，即全部执行或者全部不执行。如果客户端在使用 MULTI 命令开启事务后因为意外而没有执行 EXEC 命令，则事务中的所有命令都不会执行。同理，如果客户端在使用 MULTI 命令开启事务后执行 EXEC 命令，则事务中的所有命令都会执行。 Redis 中的事务可以使用 DISCARD 命令来清空一个命令队列，并放弃对事务的执行。如果命令在入队时发生错误，Redis 将在客户端调用 EXEC 命令时拒绝执行并取消事务，但是在 EXEC 命令执行后发生的错误，Redis 将选择自动忽略。 我们知道，常见的并发控制方案主要有悲观锁和乐观锁两种方案，这里首先来解释下这两种概念。所谓悲观锁，顾名思义是一种悲观的策略，悲观锁认为：在对任何记录做修改前都应该加锁，如果加锁失败则表明该机录正在被修改，此时应该抛出异常；如果加锁成功则修改记录并在事务完成后解锁；如果有其它人修改则应该等待当前修改解锁或者是抛出异常。而所谓乐观锁，顾名思义是一种乐观的策略，乐观锁认为：每次从记录中查找数据别人都不会修改，因此这个过程中不需要加锁，但是在更新记录的时候，会通过版本号来判断别人是否修改过当前记录。\n通常来讲，乐观锁适合在写冲突相对较少的场合下，悲观锁适合在写冲突相对较多的场合下。Redis 中提供了一种称为 check-and-set 的机制，该机制主要通过 WATCH 命令来实现，其原理正是基于乐观锁的策略，Redis 会在执行 EXEC 命令前检查被监视的键对应的值是否发生变化，如果该值发生变化表明有人修改过这个键中存储的值，此时 Redis 将会自动取消当前事务。我们来看这个简单的例子：\nWATCH Record_Count val = GET Record_Count val = val + 1 MULTI SET Record_Count $val EXEC 在这个例子中，我们尝试在事务中对 Record_Count 做一个自增操作，这段代码在非并发的情况下是没有问题的，可是在并发的情况下，如果在执行 EXEC 命令前有一个用户修改了 Record_Count 的值，那么我们此时的结果就会比期望的结果小 1，现在我们有了 WATCH，Redis 就会对 Record_Count 进行监听，当 Redis 监听到该值发生变化的时候，这个事务就会被自动取消，进而避免造成冲突。\n如何管理 Redis 的键 ​ 其实从切题的角度来讲，这篇博客基本上说清楚了事务处理问题，因此这篇博客虽然没有给大家带来多少惊喜，却依然可以非常恰到好处的结题，可是因为之前有朋友在博客中留言并问到 Redis 的键管理的问题，所以博主决定在这里简单的讨论下这个问题，鉴于博主和大家一样都是感刚接触 Redis，所以下面的观点仅仅是一家之言，如果有疑问可以在博客中留言，欢迎大家批评指正。我认为 Redis 中的键的管理，基本上有两种策略，即惰性删除和定期删除，而实际上这正是 Redis 默认的键删除策略：\nredis 使用惰性删除和定期删除两种策略来删除过期的键：惰性删除策略在碰到过期键时方进行删除操作，定期删除策略则每隔一段时间主动查找并删除过期键。\n所以，基于这两种键删除策略，我们可以想到的做法有：\n对于临时变量可以采用临时键来存储，在数据库全局设定一个过期时间，由 Redis 在键过期后自动删除。 对于持久化数据可以采用普通键来存储，通过服务器和客户端间定义协议来由客户端主动删除键。 对于不同模块中的键采取统一规范的命名规则来命名键，从而解决 Redis 中键管理混乱的问题。 设计合理的键回收机制，避免 Redis 使用超过 95%以上的内存，或者通过设置 Redis 中的最大内存容量及其内存策略来主动触发 Redis 对键的淘汰。具体可以参考：Sunnyxd - Redis 学习笔记-事务、键空间的维护与性能 好了，这篇文章就是这样了，希望大家喜欢，下篇见！\n","date":"2017-04-08T21:46:40Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/335366821/","slug":"335366821","tags":["Redis","缓存","数据库","笔记"],"title":"Redis 缓存技术学习系列之事务处理"},{"categories":["生活感悟"],"content":"春天，常常是万物复苏的日子，是以这段时间喜欢去各种地方赏花阅景。相比起三月中旬里裹挟着清冷的青龙寺，此刻到处人山人海的景象，仿佛洋溢着某种热闹的气息。从前读朱自清的《荷塘月色》，一直不明白“热闹是他们的，我什么都没有”这句话该做何解。当你面对梨花胜雪、桃花人面的景致的时候，心中却是如灰烬一般孤独的时候，大概终于明白，为何在熙熙攘攘的人群中会感到一丝清冷，因为唯有行走在人群里的时候，你会发现原来你一个人走了这么久。天地间万事万物更迭交替，本来是自然界中最普通的规则，可是如果每年的这个时候，你都是一个人去看这山山水水，相比时空上的孤寂感，人的孤寂感会更为强烈，“良辰美景奈何天，赏心乐事谁家院”，外面的世界再纷繁多变，对你而言不过是活着的时间。\n我常常像一个淡漠世情的路人，这个世界上发生什么都和我无关，路人纷纷成双入对，而我依然孑然一身，或许这就是我的生活。每天你会和不同的人相处，可每个人都在忙着自己的事情，我们和这个世界息息相关，与此同时，我们和这个世界毫无关联。我记得周五我帮 Kent 办理离职手续的时候，我一个人从七楼到十三楼再到十六楼，反复地奔波着。同事让他帮忙向公司归还一台笔记本，结果这台笔记本让整个过程都充满了落寞感。部门与部门间的相互推脱，同事与同事间的相互推诿，让我感到从未有过的窒息感：有一群人每天都坐在一起，每个人都看起来在努力工作，然而在你真正需要帮助的时候，你不能完全指望任何一个人。或许这和我外冷内热的性格有关，可是有那么一瞬间，我突然好想离开那个地方。\n组里的人都走得差不多啦，现下终于剩下我一个人。自此项目上全凭我一个人做主，可对我而言却没有多少欢喜。我还是喜欢和大家在一起，虽然我时常让他们生气，像个孩子一样，可你知道我从来都不是无理取闹的。『射雕英雄传』我看了不下十遍，世人都道东邪黄药师行事乖张、狂傲不羁，可他对妻子冯蘅的深情世间谁人能及，夜夜笙箫相伴墓前，打造花船赴死沧澜，及闻爱女葬身大海，悲痛之际以玉箫扣舷而歌，其深情亦如此。没有人可以一直像个孩子，可以永远都不长大。前段时间看韩寒的电影《乘风破浪》，电影的主题曲从发布就被人吐槽直男癌，其实那仅仅是丈夫在妻子面前“撒娇”的心态，有句话说『在你身边我是个孩子，可你需要了，我就是无坚不摧的勇士』，人人都在说情商如何如何重要，可在真性情面前它的确有点虚伪。\nKent 是坐周五晚上的火车离开，晚上两个人一块儿吃饭，聊到了家里的琐事，聊到了工作的想法，唯独没有提到离别，大概是我不愿意说起。结账的时候，两个人抢着付钱，老板娘笑着说两个人谁付都是一样的，反正以后还有的是机会。可是未来的事情有谁能够说得准呢？或许期望越高，失望就越大，就像我答应了她许多事情，即使我常常在脑海里想起，她现在都不会在乎了吧。可我总是很怀念那些日子，在这世上有那么一个人，绘画、舞蹈、诗文、书法无所不通，我如果能学会吹奏洞箫，为她跳舞时伴奏一曲可好？人生相识不易，或许不是我不愿意去认识别人，而是我知道知音难觅、知己难求，得不到的永远都是最好的。对 Kent 来说，他有根可寻，回去是最好的选择。而我则是无根枯蓬，风吹到哪里就在哪里。我知道我再难遇见那样的人，人生与我而言，离别总是常态，孤独是种绝症。\n或许 Kent 说得是对的，这个世界上并没有那么多，真正看重技术的公司，这个世界上普通如你我的人，无一不在做着普通的事情。寻找一家新的公司，对目前的我来说，是件极具挑战的事情。虽然邹老师最后帮我推荐了简历，但是对方并没有适合的项目，这种处境既危险而困窘，我不能再像以前一样，我必须让自己拥有自信的生活，一个人如果不能做到爱自己，又怎么能够做到爱别人呢？可她永远不会再出现了，看到路人成双入对的时候，或许爱对我而言是件太困难的事情，佛门有“贪嗔痴”三戒，求而不得是为执，一个人孤零零地在这个城市里，从明天开始，我会经历更多的一个人，一个人吃饭，一个人工作，一个人游曳……长夜听雨，自今日始，你知道一个人数梅花该是何等的寂寞，你就会知道一个人听雨滴下该是何等的孤独，当然这对我而言是没有区别的。\n有时候会在 B 站上看以前的影视剧，忽然发现原来一切都已然过去那么久。可当时的心性却不再回复，取而代之的是无限的回想，人生总是如此匆忙的别离，你每天都会认识不同的人，你每天都会错过不同的人，光影恍惚间，一切都仿佛是时间的灰烬爬上了镜子，这像什么呢？或许这就是世事无常，如白云苍狗；或许这就是人生不见，动如参商……时间啊，像极了爬满窗台的灰烬，我却还惦念着窗台外的爬山虎……\nPayne, 于 4 月 3 日夜\n","date":"2017-04-03T00:25:21Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1357715684/","slug":"1357715684","tags":["心情","孤独","别离"],"title":"时间如灰烬般遥远"},{"categories":["数据存储"],"content":" 作为一个反主流的开发者，在某种程度上，我对传统关系型数据库一直有点“讨厌”，因为关系型数据库实际上和面向对象思想是完全冲突的，前者建立在数学集合理论的基础上，而后者则是建立在软件工程基本原则的基础上。虽然传统的 ORM、序列化/反序列化在一定程度上解决了这种冲突，但是软件开发中关于使用原生 SQL 语句还是使用 ORM 框架的争论从来没有停止过。可是实际的业务背景中，是完全无法脱离数据库的，除非在某些特定的场合下，考虑到信息安全因素而禁止开发者使用数据库，在主流技术中数据库是一个非常重要的组成部分。为了弥补这个技术上的短板，从这篇文章开始，我将会学习一个经典的缓存技术：Redis。我们这里将 Redis 定性为一门缓存技术，这说明 Redis 和 MySQL 等主流的数据库存在本质上的区别，那么这些区别到底在哪里呢？或许在看完这个系列文章以后，你心中自然就会有了答案。\nRedis 是什么 Redis 是什么?这是本文第一个问题。Redis 是一个开源的使用 ANSI C 语言编写的、支持网络、 基于内存的、支持持久化的日志型、Key-Value 数据库。从如此丰富的修饰语中，我们基本可以抽离出这些信息：\nRedis 是一个 Key-Value 存储系统 Redis 的数据全部缓存在内存里 Redis 可以通过网络实现主从同步 Redis 支持丰富的数据类型可实现持久化 那么该如何给 Redis 一个准确的定义呢？或许这个定义可以帮助我们更好的理解 Redis，即 Redis 是一个高性能的 Key-Value 数据库。我们知道主流的数据存储方案，可以分为关系型数据库和非关系型数据库两大类。传统的 Oracle、MySQL 和 SQLServer 都是关系型数据库，关系型数据库将复杂的数据结构归结为二元关系，即二维表形式，而对数据的操作则建立在一个或多个关系表格中，并通过这些表格间的分类、合并、连接和选取等运算实现数据处理。如同天地万物，有阴影的地方就会有阳光。和关系型数据库相对应的，我们称之为非关系型数据库，这是一个泛指的概念。实际上非关系型数据库，根据设计原理的不同，具体可分为：键-值存储数据库、列存储数据库、文档数据库和图数据库四种。我们通常称非关系型数据库为 NoSQL，即\u0026quot;Not Only SQL\u0026quot;，从这个概念我们或许可以明白，SQL 和 NoSQL 并非是完全对立的两个世界，它们各自在其擅长的应用场景中发挥着重要的作用。\n所以我选择 Redis 这样一个非关系型数据库，从某种意义上来说，我是想说明一件事情，数据库技术并非绝对代表着关系型数据库和 SQL，实际上 SQL 这门语言存在一定缺陷，就像我们提及 Web 技术常常想到是如何去做一个网站(MVC)，可你同样会意识到 Web API 是更为重要的 Web 技术。这个世界并非是一成不变的，每一天都是新的挑战。\n开始使用 Redis 好了，在了解了 Redis 是一个什么东西以后，现在我们来正式开始使用 Redis。Redis 作为一个开源的键-值数据库，我们可以从它的官方网站或者是从Github来获取。这里推荐从官方网站下载相对稳定的版本，这里博主选择的是 3.2.8 版本，需要注意我们这里从官方网站下载的是源代码版本，所以首先第一件事情就是编译源代码。如果你非常擅长在 Window 下编译类似项目，可以尝试在 Windows 下进行编译。博主这里推荐大家使用 Linux 或者 MacOS 来编译，因为主流开源项目使用的 Makefile 都是 Unix 世界里的产物，所以使用 Linux 或者 MacOS 能够为我们节省大量的时间。博主这里使用的是 Elementary OS 这个 Linux 发行版(对应 Ubuntu14)，编译方法如下：\nRedis 的编译与安装 $ wget http://download.redis.io/releases/redis-3.2.8.tar.gz $ tar xzf redis-3.2.8.tar.gz $ cd redis-3.2.8 $ make 在这里，除了 make 的步骤严格依赖命令行以外，其余的步骤都可以手动完成，所以因为惧怕命令行而不愿意接触 Linux 的世界，事实证明，对一个真正的程序员来讲，命令行是一个唯一可以让人不被外界所干扰的高效地工具，Git 是这个世界上最好没有之一的版本控制工具，如果你喜欢 Git，那么你更应该尝试去喜欢 Linux。好了，在完成对 Redis 的编译后，我们就可以开始使用 Redis 了。Redis 是一个 C/S 架构的键-值数据库，这意味着我们需要 Redis 的服务端程序和客户端程序。在完成编译以后，我们将得到 redis-server 和 redis-cli 这两个内置服务端程序和客户端程序。实际使用中我们会接触到不同语言下的 redis 客户端，在这里我们直接使用 Redis 内置的客户端：\n//开启Redis服务 $ src/redis-server //开启Redis客户端 $ src/redis-cli 需要注意的是在这里服务端和客户端，是在两个不同的终端窗口中运行的，当我们看到下面的窗口时，即表明 Redis 服务开启就绪，此时我们就可以通过客户端来输入各种命令来完成数据的存取，默认情况下 Redis 每次会随机分配一个端口，这里 Redis 采用 6379 端口进行通信：\nRedis服务\rRedis 是一个采用键-值存储方案的数据库，因而传统关系型数据库里的 SQL 在这里将不再适用。你可以将 Redis 理解为一个字典，我们可以向这个字典中储存任何 Redis 支持的数据类型，并通过键名来获取字典中存储的对应数值。我们来看下面的例子，以下命令均在 redis-cli 中执行：\nSET foo bar OK GET foo \u0026#34;bar\u0026#34; Redis 中支持的数据类型 这个例子演示了如何在 Redis 中存储和读取一个简单的字符串类型的值，看起来这一切都非常简单啊，的确 Redis 就是这样一个简单而高效的键-值数据库。我们在前面提到 Redis 支持各种各样的数据类型，那么它到底支持哪些数据类型呢？具体来讲，Redis 支持 5 种基本的数据类型：\n字符串(Strings)：最基本的数据类型，使用 SET/GET 命令来存储和读取字符串类型的值。在 Redis 中最多可支持 512 兆字节的字符串长度，这意味着我们可以常见的数据类型序列化后再存储到 Redis 中。 散列/哈希(Hashes)：专门用来表示对象的数据类型。散列/哈希是键-值对的集合，可以维系字符串字段和字符串值间的映射关系，因此它主要用来表示对象。在 Redis 中可以使用 HMSET、HMGET、HGET、HGETALL 四种命令来存储和读取散列类型的值。 列表(Lists)：指按照插入顺序排序的字符串元素的集合，特别地，Redis 中的列表是采用链表实现的，因为对数据库系统而言，一个非常重要的特性是可以支持在含有大量元素的集合中快速添加元素。常见的应用于列表的命令主要有 LPUSH、RPUSH、LPOP、RPOP 和 LRANGE。 集合(Sets)：指不重复且无序的字符串元素的集合。针对列表，常见的命令主要有：SADD、SPOP、SCARD、SMEMBERS 和 SISMEMBER。例如 SADD 命令可以向集合中添加元素，SPOP 命令可以从集合中删除元素，SCARD 命令可以返回集合内元素个数，SMERMERS 命令可以枚举集合的所有元素，SISMEMBER 命令可以判断指定元素是否在指定集合内。 有序集合(Sorted Sets)：有序集合与集合相似，不同点在于集合中的每一个元素都会关联一个浮点型的数值，该数值称为 score，事实上 Redis 正是根据 score 来对集合内的元素进行排序的。集合内的元素是不允许重复的，但是 score 是可以允许重复的。常见的命令有：ZADD、ZCARD、ZCOUNT、ZREM、ZSCORE 等。 Redis 中和键有关的命令 我们知道 Redis 是一个键-值数据库，所以在了解了 Redis 中支持的数据类型，即“值”以后，现在让我们将关注点回归到“键”上面来，这是因为作为一个键-值数据库，键是我们从数据库中获取值的唯一方式，因此在这里说说 Redis 中那些和键有关的命令，这些命令基本都遵循下面的命名格式，常见的命令有：\nCOMMAND KEY_NAME DEL：该命令将在键名存在时从数据库中删除指定键，成功则返回 1，否则返回 0。 DUMP：该命令将序列化指定键，并返回被序列化的值。 EXISTS：该命令用以判断指定键是否存在。 EXPIRE：该命令用以给指定键设置过期时间。 KEYS：该命令用以返回所有满足匹配模式的键。 PERSIST：该命令用以移除指定键的过期时间。 RENAME：该命令用以重命名指定键。 ​ 好了，这就是这篇博客的内容了，自我感觉 Redis 中的内容相对分散，这种细小的知识点都隐藏在命令中，最初在介绍不同的数据类型的时候，在文章中均做了详细的介绍并辅以终端脚本，可是最后发现这样写下去还不如去看官方文档，像 Redis 这种即使学习了都不见得有机会使用的技术，当然我并不是说 Redis 不好啊，关系型数据库目前依然是主流的技术驱动力量，所以我觉得我们学习的时候最好是“观其大略”、“不求甚解”，首先注重整体知识体系上的理解，微枝末叶上的细节问题可以在使用的时候去查阅文档。在下面的文章中，我重点关注的内容是 Redis 的事务、脚本、发布/订阅及不同语言下 Redis 的使用，希望大家继续关注我的博客，本篇结束！\n","date":"2017-03-30T23:31:40Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3032366281/","slug":"3032366281","tags":["Redis","缓存","数据库","笔记"],"title":"Redis 缓存技术学习系列之邂逅 Redis"},{"categories":["编程语言"],"content":"各位朋友大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。本文是“使用 C#开发 HTTP 服务器”系列的第六篇文章，在这个系列文章中我们实现了一个基础的 Web 服务器，它支持从本地读取静态 HTML 页面，支持 GET 和 POST 两种请求方式。该项目托管在我的Github上，项目地址为https://github.com/qinyuanpei/HttpServer，感兴趣的朋友可以前往了解。其间有朋友为我提供了 HTTPS 的 PR，或许这偏离了这个系列开发 HTTP 服务器的初衷，可是我们应该认识到普及 HTTPS 是大势所趋。所以在今天这篇文章中，我将为大家带来 HTTPS 相关知识的普及，以及如何为我们的这个 Web 服务器增加 HTTPS 的支持。\n2017 年我们听到这样一个声音，苹果将强制实施 ATS，即 App Transport Security。首先我们要了解的是 ATS，它是苹果为了保证应用数据在网络中安全地传输而制定的一种规则，其核心是鼓励开发者使用安全的 HTTPS 协议和服务器进行通讯。在此之前考虑到大量的应用还在使用 HTTP 协议，所以苹果并未强制要求应用遵守这个规范，而此时苹果发出这样一种声音，我们终于意识到苹果这是在推广 HTTPS 啊！无独有偶，同样作为科技巨头之一的 Google，宣布在新发布的 Chrome 56 中会将仅支持 HTTP 协议的网页标记为“不安全”。HTTPS 到底是什么呢？为什么科技巨头纷纷开始对它青眼有加呢？这或许要从 HTTPS 协议说起。\nHTTPS，即 Hyper Text Transfer Protocol Over Secure Socket Layer 的简称，是指以安全为目标的 HTTP 协议。我们可以将其理解为在 HTTP 协议的基础上增加了安全机制，这里的安全机制是指 SSL,简单来讲 HTTPS 协议依然采用 HTTP 协议，不过它在 HTTP 和 TCP 间增加了加密/身份验证层，因此在保证数据传输安全的同时，为服务器提供了身份校验机制。任何采用 HTTPS 协议的网站，均可通过浏览器地址栏中的“锁”标志来查看网站的认证信息，或者是通过 CA 机构颁发的数字证书来查询。下图展示的是 HTTPS 协议中客户端和服务器端通信过程：\nHTTPS协议中客户端和服务器通信过程\r从图中我们可以看出，在 HTTPS 协议中客户端和服务器端分为六步：\n客户端请求服务器，发送握手消息给服务器。 服务器端返回客户端加密算法、数字证书和公钥。 客户端对返回的数字证书进行验证，如果验证通过则产生一个随机数，否则提示验证失败。 客户端使用公钥对产生的随机数进行加密，然后将其发送给服务器端。 服务器对该随机数进行解密，并以此作为密钥发送握手信息给客户端。 客户端收到消息后对消息进行解密，如果解密成功则表示握手结束。 这恰恰印证了我们最初的观点，即 HTTPS 协议依然采用 HTTP 协议(三次握手)进行通讯，不同的地方在于中间环节增加了加密处理，例如在客户端和服务器端相互验证的环节采用的是非对称加密，在客户端验证通过以后双方采用随机数作为密钥是对称加密，而三次握手以后验证消息是否被篡改则是采用 HASH 算法。所以我们应该可以注意到，HTTP 协议和 HTTPS 协议的一个显著的区别是，前者采用明文来传输消息，而后者采用密文来传输消息，因此 HTTPS 协议比 HTTP 协议在通讯上更为安全。而详细来说，两者的区别主要有：\nHTTPS 需要证书，而 HTTP 则不需要证书，证书由 CA 机构颁发。 HTTP 采用明文来传输消息，C/S 端无身份验证；HTTPS 采用密文来传输消息，C/S 端有身份验证。 HTTP 默认采用 80 端口进行通信，而 HTTPS 默认采用 443 端口进行通信。 好了，现在我们对 HTTPS 协议有了一个基本的认识：HTTPS 协议相比 HTTP 协议增加了身份验证和消息加密的机制，因此 HTTPS 协议能够保证通讯过程中的数据传输安全。在今天这样一个数字时代，当个人隐私安全彻底地暴露在浏览器、应用程序面前，能够提供更安全的互联网服务无疑会让人更有安全感，我想这是苹果和谷歌这样的科技巨头公司，之所以要去努力推广 HTTPS 协议的原因吧！因为客户端需要对服务器的证书进行验证，所以这意味着在客户端拥有访问所有受信证书的能力，例如我们在使用传统网银产品时都需要安装网银证书，这其实就是为了让客户端在向服务器端发起请求时方便对服务器进行验证，因此如果客户端请求的 URL 遭遇劫持，被重定向到某个不被信任的站点上，那么客户端发起的请求就会被拦截。同样的道理，服务器端会对客户端的请求进行验证，所以这里就不再详细展开去说啦。\n我们最初设计这个 HTTP 服务器的时候，没有考虑过要支持 HTTPS 协议。可是当我们了解了 HTTPS 协议后，我们发现，如果要让最初设计的 Web 服务器支持 HTTPS 协议，我们需要关注的是 Security，即身份验证和数据加密，我们知道这里的 Security 指的是 SSL，所以需要了解 SSL 相关的内容。其次，我们需要提供一个数字证书给服务器端，目的是在客户端发起请求的时候，将数字证书、加密算法和公钥返回，保证客户端可以完成证书校验。从这两点可以看出，我们首先需要从 CA 机构购买证书，这一点毋庸置疑。关于证书的购买及服务器的设置，我们通过搜索引擎可以找到相关参考。目前主流的服务器如 Apache、IIS、Tomcat 和 Ngnix 都可以非常方便地支持 HTTPS，这些问题更像是一种基础设施，所以我会在文章末尾列举出相关文章供大家查阅。\n这篇文章的核心是开发一个服务器，所以在保证这些基础设施完备的前提下，让我们将关注点落实到代码上面来。我们提到，HTTPS 除了证书以外关键点是 SSL，而在.NET 中提供 SSL 相关的 API，所以这里我们直接使用这些 API 就可以完成证书的创建、加载等工作。下面是相关的代码示例：\n// 使用OpenSSL.NET生成密钥 RSA rsa = new RSA(); BigNumber number = OpenSSL.Core.Random.Next(10, 10, 1); rsa.GenerateKeys(1024, number, null, null); CryptoKey key = new CryptoKey(rsa); //创建X509证书，Subject和Issuer相同 X509Certificate x509 = new X509Certificate(); x509.SerialNumber = (int)DateTime.Now.Ticks; x509.Subject = new X509Name(\u0026#34;CN=DOMAIN\u0026#34;); //DOMAIN为站点域名 x509.Issuer = new X509Name(\u0026#34;CN=DOMAIN\u0026#34;); x509.PublicKey = key; //指定公钥 x509.NotBefore = Convert.ToDateTime(\u0026#34;2011-1-1\u0026#34;); //起始时间 x509.NotAfter = Convert.ToDateTime(\u0026#34;2050-1-1\u0026#34;); //失效时间 x509.Version = 2; //使用私钥签名 x509.Sign(key, MessageDigest.MD5); //生成CRT证书 BIO x509bio = BIO.File(\u0026#34;CA.crt\u0026#34;, \u0026#34;w\u0026#34;); x509.Write(x509bio); //生成PFX证书 var certs = new OpenSSL.Core.Stack\u0026lt;X509Certificate\u0026gt;(); PKCS12 p12 = new PKCS12(\u0026#34;PASSWORD\u0026#34;, key, x509, certs); //PASSWORD为保护密钥 BIO p12Bio = BIO.File(\u0026#34;CA.pfx\u0026#34;, \u0026#34;w\u0026#34;); p12.Write(p12Bio); //加载证书 var certifiate = X509Certificate.CreateFromCertFile(\u0026#34;CA.crt\u0026#34;); 在我们获得证书以后，我们就可以通过 SSL 对 Socket 通信过程中传递的消息进行加密了，一个基本的示例代码如下：\nSslStream sslStream = new SslStream(clientStream); sslStream.AuthenticateAsServer(serverCertificate, false, SslProtocols.Tls, true); sslStream.ReadTimeout = 10000; sslStream.WriteTimeout = 10000; return sslStream; 个人感觉加密相关的问题深奥而晦涩，这篇文章中涉及到的相关概念和技术，都大大地超出了我目前的认知范围。不过既然这位朋友热心地提交了这个 PR，我就将这个过程视为向别人的一次学习吧！我会继续去完善这个项目：https://github.com/qinyuanpei/HttpServer。这篇博客终于算是写完了，周末开心！\n参考文章\nZery - HTTPS 原理解析 阮一峰 - SSL/TLS 协议运行机制的概述 维基百科 - 超文本传输安全协议 猫尾博客 - HTTPS 工作原理 MSDN - 如何在 IIS 中设置 HTTPS 服务 Dudu - 给 IIS 添加 CA 证书以支持 https 温柔易淡 - Apache 配置 HTTPS 功能 王浩宇 - 配置 Tomcat 使用 https 协议 ","date":"2017-03-05T14:01:39Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2734896333/","slug":"2734896333","tags":["HTTP","服务器","C#"],"title":"使用 C#开发 HTTP 服务器之支持 HTTPS"},{"categories":["生活感悟"],"content":"或许是今年的贺岁档电影全部遭遇“滑铁卢”的缘故，在这种情况下，电影《乘风破浪》或许会成为拯救整个贺一个岁档的奇迹。同往常一样，我依然选择一个人去看电影，而庆幸的是韩寒真的没有让我们失望。虽然前期在微博上经常看到韩寒在为这部电影做宣传，但我一直想知道它会一种什么样的方式来讲述这个故事，我隐隐约约觉得徐太浪(邓超饰)、徐正太(彭于晏饰)、小花(赵丽颖饰)三个人之间的关系非同寻常，我甚至臆想这是一部俗套的三角恋的故事。可结果却是完全出人意料的，我很喜欢这个故事。\n首先，电影一开头就项我们展示了一个非常常见的矛盾冲突，从小喜欢赛车的儿子徐太浪和坚决反对儿子以赛车作为职业的父亲徐正太。我们从小到大或许都会有这样的经历，我们曾经执著地喜欢过什么或是热爱过什么，可这种喜欢在父母眼中常常被视为叛逆。我大学四年里最反感的一件事情就是我的本科专业，我一直很喜欢计算机技术，确切地说是通过编程来理解这个世界，可是因为我高考成绩并不理想，当时在父亲的建议下选择了一个我不喜欢的专业，在这四年里我做过最多的事情，就是极力想要摆脱这个专业。我从那个时候开始将编程视为我主要的兴趣，我从大学时候就开始研究自己感兴趣的技术，开始在技术社区里撰写博客。\n这段经历让我至今都难以忘怀，这件事情教会我一个道理，当你认准去做一件事情的时候，其实没有任何一个人可以左右你的选择，2016 年上半年我经历了一段时间的失业。虽然周围人都在告诉我，考公务员或者选择我本专业或许会比我的选择更为光明，但那个时候我一直找不到喜欢的工作，我常常在心里问我自己：难道我注定不能去做我喜欢的事情吗？后来我独自一个人来到西安，我畏惧逃离北上广的那种无力感和孤独感，可其实一个人在西安同样是孤独的。父母自然是为了我们好，可这种两代人的矛盾不管什么时候都会持续下去，当徐太浪面对着镜头大声说出那些压抑在心底的话的时候，我们所看到的那个懦弱而苍老的父亲，或许在每个人身上都可以找到影子吧！\n或许是因为太想在父亲面前证明自己，所以当徐太浪取得赛车比赛冠军时，他迫不及待地让父亲坐在自己的副驾驶位上，一如电影开头那呼啸而过的一抹车影，当赛车穿过蜿蜒的山路、穿过废旧的城镇，在恍惚间我好像看到了，在《平凡之路》中一直蔓延着的无尽的长路，当镜头和画面频繁切换着，你以为是时间穿越了历史，其实时间就静止在那里，我们所看到的不过是那些转瞬即逝的风景，所听到的不过是那些呼啸而过的风声。我特别喜欢徐太浪的赛车与货车擦肩而过时，一切变得支离破碎的那组慢镜头，它就像一层层地褪去外表华丽的油漆，直接让斑驳的纹理一点点地裸露出来，当车窗玻璃破碎并跌落在空气里，当庆祝成功的香槟和奖杯混杂在空气里。\n或许此时此刻所有的一切都不再重要了吧，我们说洗去铅华返璞归真，大概只有在这种情况下，当我们的灵魂完全变得纯净的时候，或许穿越这种影视表现手法才会真正起作用，我们注意到徐太浪在医院病床上醒来的时候，眼角是带着泪水的，所以对这个电影里所叙述的故事，我宁愿将其当作是徐太浪的一个梦吧！他因为从小没有见过母亲，所以想要努力知道母亲长什么样子，在死亡边缘成为了他的一种执念。影片为了刻意制造笑点，让徐太浪身上多了许多“穿越”的特质，其实对观众而言这仅仅是一种代入的符号，换句话说，“穿越”以后的这个人是不是徐太浪本身已经不再重要了，重要的是我们通过这个人了解了整个故事。\n整个故事其实是一个解惑的过程，所有的线索都是为了让徐太浪明白，自己的父亲为什么会变成这个样子，而围绕这些线索，整理的冲突发生在以徐正太、六一、阿浪、小马为代表的“正太帮”和黄志强、罗力为代表的“黑社会”，双方的利益冲突交织在一间歌舞厅。我们知道徐正太的一个梦想是“歌舞厅里只唱歌，桑拿房里就洗澡”，而这是一种近乎乌托邦的存在，所以年轻的徐正太有理想和抱负，想成为杜月笙那样的老大，而这一切的初衷是为了让自己的妻子做歌舞厅的老板娘。而通过阿浪夜寻小花这个事情，我们同时知道，徐正太是一个可以因为朋友一句话就等对方一个晚上的人，这说明他是一个守信用的人，而导致徐正太坐牢的直接原因是他为朋友六一报仇，这是一个重义气的表现，那么他的局限性在哪里呢？他认为录像厅比电影院有前途，就大量购买录像带；他认为 BP 机代表了通信行业的未来，就大量囤积 BP 机。所以这个时候我们就会明白，他后来会对阿浪采取“棍棒教育”，其实是因为他的眼界就局限在这里，他在对待新事物上存在局限性，所以看不到世界的变化，他在天台上说的那句“世界是不会变的”就是最好的证明。六一代表了那种并不聪明但愿意为朋友两肋插刀的“傻气”，小马则代表了那种愿意去追逐梦想的“执着”，当 OICQ 这个名字从他口中说出来的时候，我就知道这个人的一生都将注定不平凡。黄志强代表的是商业利益既得者，而罗力则代表的是失去一切后迷途知返的回头浪子。\n如果说男人们的故事都是这般粗犷的话，那么这部电影里女性角色则显得细腻。首先从小花说起，小花和阿浪一样，都是对自己的父亲怀有敌意的，因为作为飞行员的父亲舍弃了和家人在一起的时间，投入全部身心到飞行事业上去，以至于在她婚礼的时候，父亲都没有来参加。影片中一个细节是，最后徐正太和阿浪杀死黄志强以后，怀有身孕的小花匆匆赶来，将满身血污的徐正太紧紧抱在怀里，小花说徐正太有时候像个孩子，其实男人都会有像孩子的一面，所以小花其实代表的是一种母性的关怀吧！值得一提的是电影里佳怡和松子这个女性角色，虽然六一喜欢佳怡，可依琳说她有喜欢的人的时候，他默默地选择放手，即使他心里依然有她，而最终六一躺在太平间里，此时陪伴在他身边的，不过是哭成泪人地佳怡和那条巡回犬，大概我们都习惯了忽视身边人的爱吧，从父母到伴侣，影片中正太和小花从四岁结识从始自终，这大概就足以令人唏嘘了吧。顶风作案的罗力，承诺要给松子一方天地，然而等他从狱中出来，见到的却是坐在黄志强车上神色复杂的那个人，你说这个世界是会变的，可它会变成什么样我们永远无从得知，小马希望的是那个拨号上网频频掉线的时代诞生一款 OICQ 的软件，徐正太希望的是像杜月笙那样收“物业费”……我们期望着什么，我们等待着什么，其实都不重要，世界一直都在变化，而我们如浮萍，或随风漂泊，或乘风破浪，一切都在我们的选择。\n电影结尾，阿浪和正太在同一个平行宇宙，一如影片开始，两个人坐在同一辆车上一样，阿浪问正太：“你的车技这么好是跟谁学的？”，正太回答：“天赋来的”。同样，正太问了阿浪相同的问题，阿浪说：“遗传来的”。其实到这里，我们看到的是父子间真正的握手言和，最终两个人互相理解了对方，电影开头两个人的矛盾冲突，以这样一种形式化解，是件让人欣慰的事情，如果那些被时代抛弃的 BP 机，或许你我都不再认同它们的价值，但那曾是父母眼中给我们的最好的爱，世界每天都在变化，而起风了，唯有努力生存。\n","date":"2017-02-04T22:31:33Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2314414875/","slug":"2314414875","tags":["韩寒","电影","青春"],"title":"愿浮萍乘风破浪"},{"categories":["读书笔记"],"content":"近年来函数式编程这种概念渐渐流行起来，尤其是在 React/Vuejs 这两个前端框架的推动下，函数式编程就像股新思潮一般瞬间席卷整个技术圈。虽然博主接触到的前端技术并不算深入，可这并不妨碍我们通过类似概念的延伸来理解这种概念。首先，函数式编程是一种编程范式，而我们所熟悉的常见编程范式则有命令式编程(Imperative Programmming)、函数式编程(Functional Programming)、逻辑式编程(Logic Programming)、**声明式编程(Declarative Programming)和响应式编程(Reactive Programming)**等。现代编程语言 在发展过程中实际上都在借鉴不同的编程范式，比如 Lisp 和 Haskell 是最经典的函数式编程语言，而 SmartTalk、C++和 Java 则是最经典的命令式编程语言。微软的 C#语言最早主要借鉴 Java 语言，在其引入 lambda 和 LINQ 特性以后，使得 C#开始具备实施函数式编程的基础，而最新的 Java8 同样开始强化 lambda 这一特性，为什么 lambda 会如此重要呢？这或许要从函数式编程的基本术语开始说起。\n什么是函数式编程 我们提到函数式编程是一种编程范式，它的基本思想是将计算机运算当作是数学中的函数，同时避免了状态和变量的概念。一个直观的理解是，在函数式编程中面向数据，函数是第一等公民，而我们传统的命令式编程中面向过程，类是第一等公民。为什么我们反复提到 lambda 呢？因为函数式编程中最重要的基础是 lambda 演算(Lambda Calculus)，并且 lambda 演算的函数可以接受函数作为参数和返回值，这听起来和数学有关，的确函数式编程是面向数学的抽象，任何计算机运算在这里都被抽象为表达式求值，简而言之，函数式程序即为一个表达式。值得一提的是，函数式编程是图灵完备的，这再次说明数学和计算机技术是紧密联系在一起的。虽然在博主心目中认为，图灵这位天纵英才的英国数学家，是真正的计算机鼻祖，但历史从来都喜欢开玩笑的，因为现代计算机是以冯.诺依曼体系为基础的，而这一体系天生就是面向过程即命令式的，在这套体系下计算机的运算实则是硬件的一种抽象，命令式程序实际上是一组指令集。因此，函数式程序目前依然需要编译为该体系下的计算机指令来执行，这听起来略显遗憾，可这对我们来说并不重要，下面让我们来一窥函数式编程的真容：\nsquares = map(lambda x: x * x, [0, 1, 2, 3, 4]) print squares 这是使用 Python 编写的函数式编程风格的代码，或许看到这样的代码，我们内心是完全崩溃的，可是它实现得其实是这样一个功能，即将集合{0, 1, 2, 3, 4}中的每个元素进行平方操作，然后返回一个新的集合。如果使用命令式编程，我们注定无法使用如此简单的代码实现这个功能。而这个功能在.NET 中其实是一个 Select 的功能：\nint[] array = new int[]{0, 1, 2, 3, 4}; int[] result = array.Select(m =\u0026gt; m * m).ToArray(); 这就是函数式编程的魅力，我们所做的事情都是由一个个函数来完成的，这个函数定义了输入和输出，而我们只需要将数据作为参数传递给函数，函数会返回我们期望的结果。好了，下面再看一个例子：\nsum = reduce(lambda a, x: a + x, [0, 1, 2, 3, 4]) print sum 即使我们从来没有了解过函数式编程，从命名我们依然可以看出这是一个对集合中的元素求和的功能实现，这就是规范命名的重要性。幸运的是.NET 中同样有类似的扩展方法，我喜欢 Linq，我喜欢 lambda：\nint[] array = new int[]{0, 1, 2, 3, 4}; int result = array.Sum(); 考虑到博主写不出更复杂的函数式编程的代码示例，这里不再列举更多的函数式编程风格的代码，可是我们从直观上来理解函数式编程，就会发现函数式编程同 lambda 密不可分，函数在这里扮演着重要的角色。好了，下面我们来了解下函数式编程中的常用术语。\n函数式编程的常用术语 函数式编程首先是一种编程范式，这意味着它和面向对象编程一样，都是一种编程的思想。而函数式编程最基本的两个特性就是不可变数据和表达式求值。基于两个基础特性，我们延伸出了各种函数式编程的相关概念，而这些概念就是函数式编程的常用术语。常用的函数式编程术语有高阶函数、柯里化/局部调用、惰性求值，递归等。在了解这些概念前，我们先来理解，什么是函数式编程的不可变性。不可变性，意味着在函数式编程中没有变量的概念，即操作不会改变原有的值而是修改新产生的值。举一个基本的例子，.NET 中 IEnumerable接口提供了大量的如 Select、Where 等扩展方法，而这些扩展方法同样会返回 IEnumerable类型，并且这些扩展方法不会改变原来的集合，所有的修改都是作用在一个新的集合上，这就是函数式编程的不可变性。实现不可变性的前提是纯函数，即函数不会产生副作用。一个更为生动的例子是，如果我们尝试对一个由匿名类型组成的集合进行修改，会被提示该匿名类型的属性为只读属性，这意味着数据是不可改变的，如果我们要坚持对数据进行“修改”，唯一的方法就是调用一个函数。\n高阶函数(Higer-Order-Function) 高阶函数是指函数自身能够接受函数，并返回函数的一种函数。这个概念听起来好像非常复杂的样子，其实在我们使用 Linq 的时候，我们就是在使用高阶函数啦。这里介绍三个非常有名的高阶函数，即 Map、Filter 和 Fold，这三个函数在 Linq 中分别对应于 Select、Where 和 Sum。我们可以通过下面的例子来理解：\nMap 函数需要一个元素集合和一个访问该元素集合中每一个元素的函数，该函数将生成一个新的元素集合，并返回这个新的元素集合。通过 C#中的迭代器可以惰性实现 Map 函数： IEnumerable\u0026lt;R\u0026gt; Map\u0026lt;T,R\u0026gt;(Func\u0026lt;T,R\u0026gt; func, IEnumerable\u0026lt;T\u0026gt; list) { foreach(T item in list) yield return func(item); } Filter 函数需要一个元素集合和一个筛选该元素结合的函数，该函数将从原始元素集合中筛选中符合条件的元素，然后组成一个新的元素集合，并返回这个新的元素集合。通过 C#中的 Predicate委托类型，我们可以写出下面的代码： IEnumerable\u0026lt;T\u0026gt; Filter\u0026lt;T\u0026gt;(Predicate\u0026lt;T\u0026gt; predicate, IEnumerable\u0026lt;T\u0026gt; list) { foreach(T item in list) { if(predicate(item)) yield return item; } } Fold 函数实际上代表了一系列函数，而最重要的两个例子是左折叠和右折叠，这里我们选择相对简单地左折叠来实现累加的功能，它需要一个元素集合，一个累加函数和一个初始值，我们一起来看下面的代码实现： R Fold\u0026lt;T,R\u0026gt;(Func\u0026lt;R,T,R\u0026gt; func, IEnumerable\u0026lt;T\u0026gt; list, R startValue = default(R)) { R result = startValue; foreach(T item in list) result = func(result, item); return result; } 相信现在大家应该理解什么是高阶函数了，这种听起来非常数学的名词，当我们尝试用代码来描述的时候会发现非常简单。相信大家都经历过学生时代，临近期末考试的时候死记硬背名词解释的情形，其实可以用简洁的东西描述清楚的概念，为什么需要用这种方式来理解呢？为什么我这里选择了 C#中的委托来编写这些示例代码呢？自然是同样的道理啦，因为我们都知道，在 C#中委托是一种类似函数指针的概念，因为当我们需要传入和返回一个函数的时候，选择委托这种特殊的类型可谓是恰如其分啦，这样并不会影响我们去理解高阶函数。\n柯里化(Curring)/局部套用 柯里化(Curring)得名于数学家 Haskell Curry，你的确没有看错，这位伟大的数学家不仅创造了 Haskell 这门函数式编程语言，而且提出了局部套用(Currin)这种概念。所谓局部套用，就是指不管函数中有多少个参数，都可以函数视为函数类的成员，而这些函数只有一个形参，局部套用和部分应用息息相关，尤其是部分应用是保证函数模块化的两个重要技术之一(部分应用和组合**(Composition)**是保证函数模块化的两个重要技术)。众所周知，在 C#中一个函数一旦完成定义，那么它的参数列表就是确定的，即相对静态。它不能像 Python 和 Lua 一样去动态改变参数列表，虽然我们可以通过缺省参数来减少参数的个数，可是在大多数情况下，我们都需要在调用函数前准备好所有参数，而局部套用所做的事情与这个理念截然相反，它的目标是用非完全的参数列表去调用函数。我们来一起看下面这个例子：\nFunc\u0026lt;int,int,int\u0026gt; add = (x,y) =\u0026gt; {return x + y;}; 这是一个由匿名方法定义的委托类型，显然我们需要在调用这个方法前准备好两个参数 x 和 y，这意味着 C#不允许我们在改变参数列表的情况下调用这个方法。而通过局部套用：\nFunc\u0026lt;int,int,int\u0026gt; curriedAdd =\u0026gt; (x) =\u0026gt; { return (y) =\u0026gt; { return x + y;}; }; 实际上在这里两个参数 x 和 y 的顺序对最终结果没有任何影响，我们这样写仅仅是为了符合人类正常的认知习惯，而此时我们注意到我们在调用 curriedAdd 时会发生质的的变化：\n//x和y同时被传入add add(x,y) //x和y可以不同时被传入curriedAdd curriedAdd(x)(y); 而如果我们将这里的函数用 Lambda 表达式来表示，则会发现：\nFunc\u0026lt;int,int,int\u0026gt; add = (x,y) =\u0026gt; return x + y; Func\u0026lt;int,Fucn\u0026lt;int,int\u0026gt;\u0026gt; curriedAdd = x = \u0026gt; y =\u0026gt; x + y; 至此，对一般的局部套用，存在：\nFunc\u0026lt;...\u0026gt; f = (part1, part2, part3, ...) =\u0026gt; ... 可转换为： Func\u0026lt;...\u0026gt; cf = part1 =\u0026gt; part2 =\u0026gt; part3 ... =\u0026gt; ... 则称后者为前者的局部套用形式。\n惰性求值 我们在前文中曾经提到过，在函数式编程中函数是第一等公民，而这里的函数更接近数学意义上的函数，即将函数视为一个可以对表达式求值的纯函数，所以我们这里自然而然地就提到了惰性求值。首先，博主这里想说说求值策略这个问题，求值策略通常有严格求值和非严格求值两种，而对 C#语言来讲，它在大多数情况下使用严格求值策略，即参数在传递给函数前求值。与之相对应的，我们将参数在传递给函数前不进行求值或者延迟求值的这种情况，称为非严格求值策略。一个经典的例子是 C#中的“短路”效应：\nbool isTrue = (10 \u0026lt; 5) \u0026amp;\u0026amp; (MyCheck()) 因为在这里表达式的第一部分返回值为 false，因此在实际调用中第二部分根本不会执行，因为无论第二部分返回 true 还是 false，实际上对整个表达式的结果都不会产生影响。这是一个非常经典的非严格求值的例子，同样的，布尔运算中的\u0026quot;||\u0026ldquo;运算符，同样存在这个问题。所以，至此我们可以领会到惰性求值的优点，即使程序的执行效率更好，尤其是在避免高昂运算代价的时候，我们要牢记：懒惰是程序员的一种美德，使用更简洁的代码来满足需求，是一名游戏程序员的永恒追求。我们可以联想那些在代码片段中优先 return 的场景，这大概勉强可以用这种理论来解释吧！例如我们强大的 Linq，原谅我如此执著于举 Linq 的例子，Linq 的一个特点是当数据需要被使用的时候开始计算，即数据是延迟加载的，而在此之前我们所有对数据的操作，从某种意义上来讲，更像是定义了一系列函数，这好像和数据库中的事务非常相近啦，其实这就是在告诉我们，懒惰是一种美德啊，哈哈！\n函数式编程的利弊探讨 好了，现在让我们从函数式编程的各种术语中解放出来，高屋建瓴般地从更高的层面上探讨下函数式编程的利弊。当你讨论一种东西的利弊时，一种习惯性的做法是找一种东西来和它作比较，如果 Windows 和 Linux、SQL 和 NoSQ、面向对象和函数式\u0026hellip;等等，我们常常关注一件事物的利弊，而非去寻找哪一个是最好。可惜自以为是的人类，常常以此来自我设限，划分各自的阵营，这当真是件无聊的事情，就像我一直不喜欢 SQL 和正则表达式，所以我就去了解数据库的设计、模式匹配相关内容，最终感觉颇有一番收获，我想这是我们真正的目的吧！好了，下面我们说说函数式编程有哪些优缺点？首先，函数式编程极大地改善了程序的模块化程度，高阶函数、递归和惰性求值让程序充分函数化，函数式让编程可以以一种声明式的风格来增强程序语义。当然，函数式编程的缺点是，我们这个现实世界本来就不是纯粹的，函数式编程强调的数据不可变性，意味着我们无法去模拟事物状态变化，因此我们不能为了追求无副作用、无锁而忽视现实，这个世界上总有些肮脏的问题，无法让我们用纯函数的思维去解决，这个时候我们不能说要让设计去适应这个世界，任何技术或者框架的诞生归根到底是为了解决问题，而函数式编程或者是面向对象编程，本质都是一种编程思想，我们最终是为了解决问题，就像这个世界有时候并不是面向对象的，我们用面向对象来描述这个世界，或许仅仅是我们自己的理解，这个世界到底是什么样子的，大概只有上帝会知道吧！\n本文小结 本文主要对函数式编程及其常见术语进行了简要讨论，主要根据《C#函数式程序设计》一书整理并辅以博主的理解而成。首先，函数式编程中强调无状态、不可变性，认为函数是一等公民，并且在函数式编程中每一个函数都是一个纯函数，它是数学概念咋计算机领域的一种延伸，和冯.诺依曼计算机体系不同，函数式编程的核心思想是以 lambda 演算为基础的表达式求值，并且函数式编程强调无副作用。本文对函数式编程中的常见术语如高阶函数、局部套用/柯里化、惰性求值等结合 C#语言进行了简单分析。或许对我们而言，函数式编程是一个新鲜事物，可正如我们第一次接触面向对象编程时一样，我们并不知道这样一种编程思想会持续到今天。我不认为函数式编程会彻底替代面向对象编程，就像 Web 开发无法彻底替换原生开发一样，函数式编程会作为面向对象的一种延伸和补充，所以本文对函数式编程的理解实际上是非常肤浅的，可这个世界本来就是在不断变化的，希望我们可以在恰当的场景下去权衡选择什么样的技术，对这个世界而言，我们永远都是探索者，或许永远都不存在完全能满足现实场景的编程范式吧！\n","date":"2017-02-02T19:21:12Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2171683728/","slug":"2171683728","tags":["函数式编程","读书","编程"],"title":"函数式编程常用术语"},{"categories":["编程语言"],"content":"最近 Visual Studio 推出 Mac 版本的消息迅速在技术圈里刷屏，当工程师们最喜欢的笔记本电脑 Mac，邂逅地球上最强大的集成开发环境 Visual Studio 的时候，会碰撞出怎样精彩的火花呢？在微软新任 CEO 纳德拉的“移动为先、云为先”战略下，微软的转变渐渐开始让人欣喜，从.NET Core、VSCode、TypeScript 再到近期的 Visual Studio For Mac，这一系列动作让我们感觉到，微软的技术栈越来越多地向着开源和跨平台两个方向努力。我们曾经固执地认为，微软的技术栈注定永远无法摆脱 Windows 的束缚，而事实上这个世界每天都在发生着变化。或许这次 Visual Studio 推出 Mac 版这件事情，本质上是微软收购的 Xamarin 公司旗下产品 Xamarin Studio 的一次改头换面。可是这件事情说明，微软正在努力让.NET 技术栈融入更多的应用场景。对我而言，我是没有钱去买一台 Mac 的，所以在这篇文章中，我们将在 Linux 下通过 Mono 和 VSCode 来打造一个轻量级的 IDE。而据说 Mono 会和 Xamarin 一样，将来会成为.NET 基金会的一部分。\n好了，我们首先在 Windows 世界里进行彩排，在开始下面的内容以前，请保证你的计算机上安装了 Mono 和 VSCode。假如你经常关注我的博客，你应该会知道 Mono 在这里的作用是什么？，简而言之，Mono 为我们提供了编译器环境和运行时环境，在这个基础上 VSCode 这个天生带着 Visual Studio 基因的编辑器，则可以为我们提供基础的代码调试功能，这是我们这篇文章写作的关键因素。如果你还对 Mono 一无所知，下面的两篇文章可以帮助你快速了解：\n使用 Mono 让.NET 程序跨平台运行 使用 Mono 打造轻量级的.NET 运行时 在我们了解了 Mono 以后，就可以考虑将 Mono 作为 VSCode 的运行时环境，这意味着我们可以在使用 VSCode 的同时直接编译代码。目前在 VSCode 中内建的运行时支持为 Node/Node2，所以如果我们希望在 VSCode 中调试更多的语言，我们就必须要为 VSCode 安装相应的插件。因为事实上在 VSCode 中编译代码我们可以直接通过 Task 来完成编译，但当我们希望在 VSCode 中对代码进行调试的时候，我们就必须借助插件来完成调试任务，这或许从侧面印证了 VSCode 的产品定位就是一个文本编辑器。\n而对于微软推出的这样一款产品，我们或许会疑惑，为什么这个编辑器提供的内建支持居然是 Node，而不是我们所熟悉的.NET 技术体系。这个原因非常容易理解，如果你听说过 Github 出品的编辑器 Atom，或者是使用过 Electron/Node-Webkit 相关技术，那么你一定会深刻地理解，VSCode 本质上和 Atom 一样，都是采用 Web 技术来构建跨平台应用，而 Node 天生就具备 Web 属性加成，所以我们就不难理解为什么 VSCode 内建的支持是 Node 而非.NET 技术体系。同样地，为了实现跨平台的目标，在对 C#语言的支持这个问题上，微软选择了 OminiSharp 这样一个跨平台的代码自动补全工具，而非我们在 Visual Studio 中所熟知的 Intellisense 技术。在.NETCore 推出以后.NET 跨平台不再是梦想，我们对技术的探索就不应该再局限在 Windows 平台上。\n博主关注 Mono 始于 Unity3D 引擎，因为 Mono 真正实现了.NET 技术的跨平台，而 Unity3D 引擎最为人所称道的当属其强悍的跨平台能力，在这一点上 Mono 功不可没。在此之前收费的 Xamarin 让人望而却步，所以 Mono 自然而然地就成为了我的选择。因为博主的计算机上安装了 Mono，所以在一开始使用 VSCode 的时候，就先入为主地认为在不安装插件的情况下，应该就可以直接在 VSCode 中编译和调试代码了。首先我们在 VSCode 中创建一个 C#代码文件，既然在程序世界里万事万物都从 Hello World 说起，那么我们这里依然遵循这个原则。在创建该代码文件以后，我们将其所在的目录在 VSCode 中打开，这是因为：\n在 VSCode 中仅支持以目录方式打开的文件的编译和调试\n所以这个时候我们在 VSCode 中的界面应该是如图所示：\n在VSCode中编写代码\r好了，下面我们直接按下 Ctrl+Shift+B 来编译代码，此时 VSCode 将提示我们“配置任务运行程序”，这里需要说明的是，在 VSCode 中你可以感受到微软对命令行和配置文件的偏执，这让适应了 Visual Studio 这样功能强大的我们相当不习惯，按照 VSCode 的提示或者是通过 Ctrl+Shift+P 打开命令面板，VSCode 将在当前工作目录下为我们创建.vscode 目录和 tasks.json 文件，在 VSCode 中任何和项目相关的配置信息都会存储在这里啦。此时我们配置 tasks.json:\n{ // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, // 该命令需要在系统变量内定义 \u0026#34;command\u0026#34;: \u0026#34;mcs\u0026#34;, // 或者使用完整的可执行路径 // \u0026#34;command: \u0026#34;C:\\Program Files\\Mono\\bin\\mcs.exe\u0026#34; \u0026#34;isShellCommand\u0026#34;: true, \u0026#34;args\u0026#34;: [\u0026#34;*.cs\u0026#34;], \u0026#34;showOutput\u0026#34;: \u0026#34;always\u0026#34; } 在这里需要说明的是一个tasks.json中可以通过 tasks 属性来配置多个任务运行程序，例如我们的项目中有 Python 和 C#两种代码需要编译，那么我们就可以配置两个 task，VSCode 将在运行程序的时候让用户由哪一个 task 来编译代码。如果你看过我在前面介绍过的两篇文章，就应该知道这里的 mcs.exe 其实是 Mono 提供的 C#编译器，它负责将我们的 C#代码编译为 IL 文件，然后 IL 文件再交由 CLR 来转换为本机代码。Mono 提供的 C#编译器可以将 C#代码编译为.exe 或者是.dll，可是在 VSCode 中好像默认都是编译为.exe，所以如果有知道如何在这里配置编译输出项的朋友，希望可以告诉我怎么去实现。\n现在，我们应该会得到一个 MainClass.exe 的文件，最初博主尝试直接去配置 launch.json，发现直接填写 type 为 mono 在 VSCode 中是无法识别的，最后决定去安装 mono-debug 的插件，安装插件在 VSCode 中是非常简单的，按下 Ctrl+Shift+X 打开插件界面，可以在这里查看最流行的插件列表、官方推荐的插件列表等等，我们直接搜索 mono-debug 然后安装插件即可。可是我不曾想到的是，我猜中故事的开头，却没有猜中故事的结尾，这个插件是不支持 Window 平台的，这个插件是不支持 Windows 平台的，这个插件是不支持 Windows 平台的。\n好吧，现在看起来 Linux 是我唯一可以去尝试的平台了，博主这里选择的是颜值最高的 Elementary OS，这是一个衍生自 Ubuntu 的 Linux 发行版。在 VSCode 正式版发布以后，在 Linux 下用 VSCode 来编程是我一直在尝试的事情，请不要说 Linux 系统使用起来会非常困难，博主在安装这些软件的过程中可以说是相当顺利。建议大家在 Linux 平台下安装 C#、Mono-Debug 和 Python 这 3 个插件，需要说明的是 C#和 Mono-Debug 在第一次使用的时候，需要在网络环境下下载相关依赖。下面是博主目前的插件安装情况：\nVSCode中插件安装界面\r我们现在按 F5 进行调试，和编译时一样，如果用户没有为当前项目配置“任务调试程序”，VSCode 会提示我们去创建一个配置文件 launch.json，我们这里选择 mono，该选项在安装 Mono-Debug 插件以前是没有的，该配置文件如下，我们注意到这里需要修改 program 属性为 MainClass.exe:\n{ \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;mono\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceRoot}/MainClass.exe\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceRoot}\u0026#34;, \u0026#34;preLaunchTask\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;runtimeExecutable\u0026#34;: null, \u0026#34;env\u0026#34;: {}, \u0026#34;externalConsole\u0026#34;: false }, { \u0026#34;name\u0026#34;: \u0026#34;Attach\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;mono\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 5085 } ] } 这里有一个小插曲，在博主运行这个简单的程序的时候，提示 Mono 的版本和 Mono-Debug 插件的版本要求不一致，因为 Mono-Debug 插件使用的是最新版本的 Mono。所以，果断卸载目前的 mono，然后安装最新的 mono，安装方法为：\nsudo apt-get install mono-complete 这样我们就可以看到眼前的成果啦，我们成功地在 VSCode 运行了一个 C#程序：\nVSCode 中调试代码\r虽然我很想在这篇博客中搞点干货出来，但是当我折腾数天以后，我大概就能够写出这样一篇相当零碎的文章，到目前为止我还是没有搞明白，为什么我在调试地过程中，VSCode 不会在我设置了断点地地方停下来，希望知道这个原因的朋友可以告诉我啊。这个过程最有意义的地方在于让我进一步熟悉了 Linux，在不一样的地方，会有不一样的风景，这个世界很大，不要给自己设限。后续我会去研究 VSCode 中的调试技巧以及.NETCore 相关内容，能看到 C#跨平台运行是件幸福的事情，而跨平台开发是我一直在探索的方向之一。夜晚已然来临了，而这篇文章就是这样了，谢谢大家的关注，晚安！\n","date":"2016-11-18T20:23:44Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3568552646/","slug":"3568552646","tags":["Mono","VSCode","跨平台"],"title":"基于 Mono 和 VSCode 打造轻量级跨平台 IDE"},{"categories":["编程语言"],"content":"其实我一直希望 Kindle 能够成为我知识管理的一部分，我们此刻所处的这个时代实则是一个信息爆炸的时代。我们每天都不得不去面对各种各样的信息，可这些信息中有多少是我们真正需要的呢？在一个信息碎片化的时代，有人说我们要懂得如何去利用碎片化的时间，有人说我们要懂得如何去高效查找需要的信息，微信和微博这类社交产品加速了信息的碎片化，或许当我们发现自己无法再集中精力去做一件事情的时候，我们就应该停下来反思如何去做好个人知识管理，我一直希望 Kindle 可以成为我知识管理的一部分，因为 Kindle 的阅读体验完全超越主流的电子设备，而且它可以让我们更加专注地去关注内容本身，Kindle 的同步机制为了提供了良好的知识管理契机，所以这篇文章我主要想分享我在以 Kindle 作为知识管理载体这件事情上的想法，希望对大家有所启发。\n记录，成为更好的自己 相信大家都有在阅读中收集和整理内容的习惯，尤其是当我们需要在写作过程中参考大量资料的时候。对博主而言，写博客其实是我学习和理解技术的一种方式，我始终相信：写作是一种自我鞭策式的学习方法。当我们通过描述来向别人传达一种概念时，如果连自己都没有想清楚其中的关节，那么我们必然无法想别人清晰地传达这种概念。或许我们会畏惧犯错，畏惧向别人传达出错误的概念，可是如果你永远不愿意迈出那一步，那么我们就永远无法知道自己的弱点在什么地方。所以，当我们尝试对自我进行知识管理的时候，我们就需要一种良好的方式来管理这些零散的知识，在这里 Markdown 和 Kindle 会成为我们的强力工具，来帮助我们收集和整理各种各样的信息，正如我在写这篇博客的时候，我需要 Markdown 文档撰写和 Kindle 电子书格式的相关内容，我可以快速地从我的为知笔记中找到参考内容。\n我最早一直使用网易的“有道云笔记”来做笔记，选择“有道云笔记”更多的原因是我使用着“网易云音乐”、“网易邮箱”和“网易云阅读”等众多的网易旗下产品。虽然国内不乏有 OneNote、为知笔记、有道云笔记、EverNote、马克飞象等众多笔记类产品，可是选择一款适合自己的笔记产品非常困难的。让我逐渐想要放弃有道云笔记的一个重要原因是，有道云笔记在推出“云协作”功能后，整体上显得非常臃肿，即使在后来推出了我喜欢的 Markdown 功能，我还是决定渐渐地从这个产品中过渡出来，或许是网易公司这样的大厂更注重产品线的全面化，“有道云笔记”在一段时间里产品定位一直相对尴尬，而且“有道云笔记”不支持导出笔记到同类产品这个举动，让我觉得这不符合网易相对国内厂商一贯良心的风格，所以我不得不去寻找一款它的替代产品。\n而最终我选择了为知笔记，它整体天蓝色的风格让我非常喜欢，虽然没有“有道云笔记”功能强大，而且内置付费模板，但对我来说，因为大部分记录都是在手机上，所以对我来讲，其实免费的功能基本足够我使用了，当然选择为知笔记的一个重要理由是它可以支持 Markdown，而且它的笔记存储格式为.zip 和.html，这意味着我们可以将笔记从云端同步到本地以后，我们可以将其导出到其它笔记产品中，这是我非常喜欢的特性。对为知笔记来说，它提供了类似 Kindle 通过邮箱传输电子书的功能，因此我们可以通过这个机制来将喜欢的资料推送到为知笔记中，目前主流笔记类产品在微信公众号中都提供了发送文章到笔记的功能，显然亚马逊官方公众号提供的 Send To Kindle 可以为我们提供类似的体验。即使微信公众号的出现让信息变成了新的信息孤岛，可我们通过这种方式来让我们感兴趣的内容被收集到笔记中，这是否在说明笔记类应用和 Kindle 阅读器冥冥之中就存在某种联系呢？\n使用 Markdown 来写作 可能你想象不到，你眼前看到的这篇博客，正是我通过 Markdown 这种“语言”来完成撰写的。而事实上，我使用 Markdown 超过三年了，对我而言 Markdown 是一种能让我专心写作的一种利器，不要告诉我 Word 是这个世界上标准的文档编写工具，如果我告诉你，使用 Markdown 你仅仅需要的是一个记事本，你是否还会对它的强大产生怀疑呢？事实上 Word 作为文档编制标准，对我们使用 Markdown 并没有太多影响，因为写作本来就应该是一件让自己开心的事情，如果我们不喜欢它，为什么不尝试更好的方式呢？一种让你可以关注核心内容的撰写，而非字体、段落或者是排版这种和样式息息相关的事情，我并不是说这些东西不重要，仅仅是因为从此时此刻开始，它开始变得不再重要起来。\n或许对普通人而言，Markdown 是一种陌生的语言，因为 Markdown 天生就有着极客的基因，我们最早接触 Markdown 或许是从 Github 上的某种一个开源项目开始，这种与生俱来就在全球最大的同性交友网站上活跃的语言，或许会被人们下意识地打上“程序员”的标签，这个世界从来不乏因为一知半解而肆意猜度的人，其实 Markdown 在写作领域是一种非常时尚的语言，我经常被别人问一个问题，Markdown 到底能为我们提升多少效率呢？这让我们从 Markdown 的语法说起。虽然我们称 Markdown 为一种语言，事实上 Markdown 是由 John Gruber 设计的一种标记语法，它的基本元素有：\n标题 Markdown 中定义标题采用#来完成，按照标题的级别，我们在需要提升为标题级别的内容前面，添加指定数目的#就可以了。例如我们可以定义下面的标题，以此类推总共有 6 级标题：\n# 一级标题 ## 二级标题 ### 三级标题 列表 熟悉 HTML 的朋友都知道，在 HTML 中存在有序列表和无序列表两种，而在 Markdown 语法中无序列表采用-或者*来完成，有序列表采用 1.、2.等来完成。我们来看一个简单的示例：\n//这是一个无序列表 * 无序列表元素1 * 无序列表元素2 * 无序列表元素3 //这是一个有序列表 1. 有序列表元素1 2. 有序列表元素2 3. 有序列表元素3 引用 我们在写作时经常需要引用相关观点来作为辅证，我们学习一种知识通常是一种由内而外的方式，我们从这个世界吸收知识和思想，通过内化来形成我们独立的世界观，世界原本就不是非此即彼的，就像自然界中的熵增定律一般，永远处在一种动态的平衡中。对 Markdown 来说，它采用\u0026lt;来完成引用的定义。例如，愿洞察之父指引我等：\n\u0026gt; 万物皆虚，万事皆允。当其它人盲目追寻真相的时候，记住万物皆虚；当其它人被道德和法律约束的时候，记住万事皆允。 图片与链接 互联网的重要精神是分享，我们说没有人是一座孤岛，对互联网来讲，没有链接就意味着没有一切。在 Markdown 语法中图片和链接的定义方式是非常接近的：\n[Google](http://www.google.com) ![这是一张图片](http://mouapp.com/Mou_128.png) 加粗与倾斜 我们通常会在文档中将重要信息加粗或者倾斜以表示其重要性，这种习惯在工作以后写邮件的时候得到了进一步的强化，虽然我们说 Markdown 更关注内容本身，即认为内容是写作的核心，可是这并不代表 Markdown 就此黯淡无光，相反地通过定义 CSS 样式，我们可以让 Markdown 文档更加美观和优雅。在 Markdwn 语法中，我们使用两个来表示加粗，一个表示倾斜。比如下面这个例子：\n**这是一个加粗的文本** *这是一个倾斜的文本** 代码高亮 没有什么比看到一段支持代码高亮的代码片段更让人开心的了，所以你至此就会明白，为什么 Markdown 会如此深受工程师们的喜爱，尤其当我们需要撰写一篇技术文章并且需要在文章中展示代码的时候，如你所见，这个博客中所有的代码片段都支持代码高亮，这样可以给阅读者更好的阅读的体验，我想吐槽的一件事情是公司内部的 Jira 居然不支持 Markdown 语法，虽然公司的第一架构经常会在这里 Share 技术文章，可是糟糕的代码片段完全让人没有继续看下去的冲动。在 Markdown 中定义一个代码块的语法使用三个`符号：\nusing System; class MainClass { public static void Main(string[] args) { Console.WriteLine(\u0026#34;Nothing is true, Everything is permitted.\u0026#34;); } } 表格 表格在 Markdown 中使用频率相对较低，因为如果没有编辑表格内容的需求，通常采用图片来展示表格内容会是一个更好的选择，而表格在 Markdown 中的表示同样是最复杂的，当然更复杂的是 Markdown 中的 LeTex 和 FlowChar，这些均属于 Markdown 的扩展语法，因为违背 Markdown 语法简洁的原则，所以我们在这里简单说下 Markdown 中的表格：\n| Column1| Column2| Column3| |:-------|:------:|-------:| | Left | Center | Right | | Left | Center | Right | | Left | Center | Right | 这里我们定义了一个四行三列的表格，我们这里使用:符号来表示表格中的对齐方式，显然这三列分别表示左对齐、居中对齐和右对齐，我们说表格这种元素复杂，主要是因为当表格中内容特别复杂的时候，这个表格定义就会降低可读性，而且它无法处理在单元格内换行的情况，所以它主要适用于表格内数据相对简单的情况下。\n好了，这些就是基本的 Markdown 语法所定义的元素啦，我觉得这是一种非常优雅的标记语言，如果你觉得纯文本的内容太单调，如果你觉得 Word 使用起来太复杂，那么 Markdown 就在这两者间找到一个平衡点，我没有劝大家放弃 Word 然后转而投身 Markdown 写作，可是作为一个经常码字的博客作者，我可以负责任的说，Markdown 是一种可以让你专注写作的工具，而且作为一名工程师，你会发现 StackOverflow、Segmentfault、Github 等知名技术社区，无一例外地都在支持 Markdown 语法，所以 Markdown 其实是开源社区里除了英语以外的第二大通用语言，所以如果你喜欢写作或者是喜欢开源，Markdown 都会是你不错的选择，而它的语法相信你此时已然学会了。\n从 Markdown 到 Kindle 有人说，Markdown 是一个人的狂欢，因为即使你再喜欢 Markdown 语法，如果你身边的同事都在使用 Word 等不同的工具，那么你该如何和他们写作呢？这听起来好像是一个严重的缺陷，所以 Markdown 注定是一种小众的写作语言。Markdown 可以转化为 HTML 或者 PDF，相比 HTMLPDF 相对会好些，因为 HTML 需要依赖 CSS 样式文件，没有样式文件的 HTML 是无论如何都不能彰显 Markdown 的优雅的。那么，难道我们就要坐以待毙接受这种妥协，Markdown 注定要被人们抛弃吗？不，让我们勇敢地说不，下面我们介绍常见的 Markdown 导出/转换工具：\n文档转换神器 Pandoc 如果说 Markdown 是这个世界上最为简洁、优雅的书写语言，那么 Pandoc 就是标记语言转换领域的瑞士军刀。我们知道 Markdown 是一种轻量级的标记语言，它可以允许写作者使用纯文本标记来来编写文档。通常我们会在 Markdown 编辑器中完成 Markdown 文档的编辑，然后将其发布到支持 Markdown 语法的站点上。以博主为例，博主会在 Sublime 中安装了 Markdown 插件，通常我会在 Sublime 中编写好文章，然后利用 Hexo 这个静态博客生成器生成静态 HTML 页面，利用 Github Pages 提供的静态网页托管服务，我将这些博客发布到了互联网上，大家此刻所看到的这篇文章就是通过 Markdown 语法撰写的。可是一旦 Markdown 语法出现在一个不被支持的场合，Markdown 的简洁、优雅都将大打折扣，这个时候就该我们的 Pandoc 登场啦！\nPandoc 是一个采用 Haskell 语法编写的命令行工具，或许你对 Haskell 这样的函数式编程语言闻名已久，但是我相信 Panndoc 对我们每一个人来说都是一个新颖的事物。Pandoc 采用 GNU GPL 授权协议进行发布，属于 Linux 世界的自由软件。庆幸的是，Pandoc 支持主流的 Windows、Mac 和 Linux 三大平台，这里我们以 Windows 平台为例介绍 Pandoc 这个工具。Pandoc 支持 HTML、.docx、Markdown、LaTex、.TXT、.epub 等常见格式的转换，以 Markdown 为例：\nMarkdown 转 HTML pandoc README.md -o README.html Markdown 转 Word pandoc README.md -o README.docx Markdown 转 Pdf(需要安装 LaTex) pandoc README.md --latex-engine=xelatex -o README.pdf 好了，现在通过 Pandoc 我们可以将 Markdown 转换为可读性更为良好的文档，而我们知道 Kindle 阅读器是可以支持.pdf 格式的文档的，虽然这种格式在 Kindle 并不能做到尽如人意，可是这对于我们而言，是将 Markdown 和 Kindle 紧密联系在一起的重要一步，这意味着我们只要 Markdown 文档转换为 Kindle 支持的文档格式，就可以实现 Markdown+Kindle 的个人知识管理方案。\n支持导出 HTML/PDF 的 Cmd Markdown 编辑器 下面推荐的是由作业部落出品的 Cmd Markdown 编辑器，这款自称为国内国内最强大的 Markdown 编辑器，实现了全平台覆盖，可以在线编辑同步预览，同时支持自动保存文档和云同步，支持一键发布文章到社区，是一个集文档编辑、预览、同步和发布等功能于一身的综合型 Markdown 编辑器，支持一键切换黑、白两种主题，支持直接导出 HTML/PDF，整体上是一个非常出色的 Markdown 编辑器。我不太喜欢这个编辑器的一个重要原因是，我喜欢在离线环境下写文章，然后将其发布在我的个人博客上面，而这款编辑器和社区耦合过紧，虽然提供了离线版本的 Markdown 编辑器，但是对我而言功能上略显臃肿，我在这里推荐这个 Markdown 编辑器的原因是，它提供了 HTML/PDF 的导出功能，当我们在线编辑 Markdown 文档时即可通过浏览器导出 HTML/PDF，如果你不需要经常导出 Markdown 为其它的文档格式，我会推荐你使用 Sublime、VSCode、马克飞象、为知笔记、小书匠等编辑器，虽然 Markdown 语法对使用者的要求并不高，但我相信有一个体验良好的 Markdown 编辑器，会让我们的写作过程更为开心，从而达到事半功倍的效果。我们这篇文章的主题是希望将 Markdown 和 Kindle 结合起来形成一套个人知识管理的方案，所以在这里的核心关注点就变成了如何快速、高效的导出 Markdown 为其它文档格式，而在这一点上，相信 Cmd Markdown 不会让你我感到失望。\n支持导出 PDF/Mobi/Epub 格式的 GitBook 好了，下面我们介绍一个使用 Github/Git 和 Markdown 来构建电子书的网站 GitBook，请不要误会，这个网站和全球最大的同性交友网站 Github 没有任何直接的关系，两者的关系可以理解为，GitBook 是一个基于 Github/Git 的静态页面生成器，其本身是一个由 NodeJS 编写而成的命令行工具，而通过这个工具和 Markdown 语法，我们就可以创建出足以媲美专业电子书籍的电子书，和 Hexo 类似，我们可以将这些生成的静态页面部署到 Github Pages 来供其它人浏览和阅读，除此以外 GitBook 本身就是一个相当优秀的内容发布平台，截止到今天 GitBook 上已经有 18036 本电子书。如果说在此以前，我们讨论的话题，即如何将 Markdown 转换为 Kindle 支持的电子书格式，是一个和 Kindle 电子书没有多少交集的话题，那么此时此刻我们已然和电子书密切地发生着关系，并且我们可以编写一本属于自己的电子书然后将其存放在 Kindle 上来阅读，这是一件非常酷的的事情，不是吗？如同王力宏在《开讲啦》节目中提到他幼时曾憧憬在广播里听到自己的歌一样，相信每一个喜欢写作的人，都渴望有朝一日看到自己的作品被发布出来，而 GitBook 就给了你这样一个梦想成真的机会。GitBook 提供了 epub、moni 和 pdf 等非常 Kindle 的文档格式导出功能，因此我们可以在编写完 Markdown 文档后直接导出为 Kindle 支持的文档格式，这是多么美好的一件事情呀！除此以外，我们还可以使用 Calibre 这个 Kindle 电子书管理软件来完成 Markdown 文件的支持，但这个世界上没有绝对完美的事情，经过博主尝试后发现，使用 Calibre 转换的电子书在 Kindle 上阅读体验并没有想象中出色，对此感兴趣的朋友可以自行尝试，这些都是后话啦！\n本文小结 个人知识管理其实是一个非常答案开放的问题，常言道“世事洞察皆学问，人情练达即文章”，当你渐渐地建立个人知识管理的意识，开始经常性地去梳理和完善自我的知识体系，这个时候到底选择什么样的方式来管理，其实是并不重要的，就像我们管理项目可以使用 Github，同样可以使用 Jira，这并不意味着这其中哪一种就是最好的，只有适合自己的才是最好的，就像我们使用 Jira 来管理项目并不意味着我们就在践行敏捷开发，我更愿意相信 Wesley 的观点，当你真正领会一件事情的思想和本质的时候，其实采用什么样的形式反而是次要的，我们从小到大常听到的一句话是，”如果连形式化的东西都不愿意去实践，又有什么资格来妄谈超脱形式化呢？”，我们暂且不管这句话对不对，我们必须认同的一点是，从 Git 到 Github 到 Markdown 到 GitBook 再到 Kindle，一个明显的趋势是，我们这个时代，信息会越来越趋于碎片化，而与此同时，开源让我们每一个人离信息越来越近，当每一个人都成为信息的提供者，我们这个时代是一个百家争鸣、集思广益的时代，所以不管你愿不愿意，不管你有没有意识到这一点，学会去高效地检索信息、学会高效地管理信息、学会高效地利用信息，这是这个时代对我们个人知识管理能力的一种要求。好了，这篇文章就是这样啦，再次谢谢大家的关注！\n","date":"2016-11-13T13:58:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1152813120/","slug":"1152813120","tags":["Kindle","Markdown","阅读"],"title":"在 Kindle 上阅读 Markdown 文档"},{"categories":["读书笔记"],"content":"\r最初开始读这本书的时候，并没有想到这本书会讲这样一个故事，甚至它不像一本畅销书一样让人充满期待，可是当你逐渐理清整个故事的来龙去脉以后，或许你会喜欢这个故事甚至被这个这个故事所震撼。我从未对宗教意义上的朝圣进行过深入了解，我所知道的朝圣，比如每年伊斯兰教历的第十二月，都会有数以百万计的伊斯兰教徒前往麦加参与朝觐仪式，而国内每年都会有从各地前往布达拉宫下的大昭寺朝佛的佛教信徒，而对藏传佛教信众来说“叩长头”是最为至诚的礼佛方式之一。所以朝圣是一项具有重大的道德或者灵性意义的旅程或者探寻，它关乎对信仰的思考同时注重身体力行，因为朝圣者始终相信前往一个重要的地方，能够从中获得灵性或者是得到治愈。\n本书的主人公哈罗德.弗莱，一个六十岁的老头儿，在酿酒厂工作了四十年后默默无闻地退休。在他平静如水的生活中，不曾经历过升迁的起起落落，他不曾得罪过别人到处树敌，更没有可以亲近到交心的朋友。他和心存隔阂的妻子住在乡下，生活平静却彼此感情疏离，日复一日，年复一年。直到有一天，一封来自二十年未见的老友奎妮地信，让哈罗德原本平静地生活开始发生变化。在信中奎妮告诉他自己患了癌症，哈罗德感到莫名的震惊和悲痛，此时他做了一个疯狂的决定：从英国最西南一路到最东北，徒步跨越整个英格兰去看望她。而让他产生这个想法的原因是他想当然地认为，只要自己去看奎妮她就能够活下来。这听起来确实是一个疯狂的想法，可是谁能够真的预料这个世界下一刻会发生什么呢？\n当哈罗德决定要开始做这件事情的时候，我们或许不会想到，这场以生命的名义发起的朝圣，其实是哈罗德内心深处的一种自我救赎。哈罗德是一个平凡如你我的普通人，甚至从他波澜不兴的人生轨迹中，我们完全找不出他的生命里有过哪些闪光点。当我们以今天这样一个世俗的眼光来审视哈罗德的时候，或许沉默寡言的哈罗德完全就是一个失败者，可当他徒步走完627英里的这段旅程以后，我们或许会明白，我们每一个人都是不完美的，而哈罗德所做的，无非是希望通过一个原始而质朴的方式，找回埋藏在内心多年的善良和温情，他曾经不是一个合格的丈夫、更不是一个合格的父亲，可是在经历过这段旅程以后，他终于找回了一个愈加真实的自己，这是一场关于生命地修行。\n有时候我们完全无法认清自我，因为随着年龄的增长，我们常常会因为这个冰冷的世界而变得麻木，我们开始学会沉默、学会妥协，那份与生俱来的骄傲终于被岁月磨去棱角。我们开始欺骗自己，认为这就是所谓的成熟。我特别喜欢这句话，“知世故而不世故，方为最善良的成熟”。当我们回首哈罗德的生平的时候，我们会意识到，这是一个一生都被挫败感塞满的男人，从小就害怕成为大家关注的焦点;一辈子都弯着腰生活，习惯像影子一样悄无声息，甚至于在他退休的时候，公司都没有为他举行过欢送仪式;他害怕被抛弃，结果因为儿子的死和妻子产生隔阂;他太害怕失去眼前，所以在儿子溺水时犹豫不决选择停下来解鞋带;他太害怕错过美好，所以在新婚之夜躲进厕所而不敢直视美丽的妻子.人们常常无法接纳不完美的自己，或许是因为我们心怀执念，或许是因为我们习惯自卑。可是不管怎么样，生活的意义就是去发现自我，更好地接纳这个世界。\n在这段旅行中，哈罗德遇到了加油站女孩、客店旅人、玛蒂娜等等不同的人，对哈罗德来说，这些人是他旅途中遇到的路人甲，可正是这些路人甲教会了哈罗德很多东西，让他学会了聆听别人的故事，学会了坦陈内心分享自己的故事。我们的人生是一个过程，它的起点和终点都是上天安排好的，我们要做的就是去让这个过程变得丰富起来。所以，试着去接受些你不了解的东西、去争取和相信自己可以改变某些事情。与其踟蹰不前犹豫不决，不如去接受这些来自未来的恐惧，当我们心中坚信自己一定可以做到的时候，其实我们离目标已然接近了一步。我们都还年轻，所以为什么不趁着现在，去做些真正疯狂的事情，如果当我们垂垂老矣的时候，发现生命是如此的苍白，大概我们会更加因为年轻时的碌碌无为而悔恨终生吧。人们都是憧憬着未来，期待着无限美好可能性。当我们明白爱是一种发自本性的情感，我们便真正具备了爱一个人的能力。\n我时常会因为控制不住情绪而伤害到别人，虽然我知道我的本意并不是为了去伤害别人，可是当对别人的伤害已然造成，它就会像钉满篱笆的钉子即使钉子被拔出来，可是永远无法再让篱笆变回它原来的样子。所以，在读这本书的过程中，我有时候会同情哈罗德的妻子莫琳，如果你面对的是一个终日木纳寡言的丈夫，甚至他从来都学不会浪漫或者是哄你开心，你会怎么想呢？最初莫琳对丈夫的困惑不解、担忧愤怒，随着故事的推进逐渐演变为探寻改变、尝试了解，甚至到故事的最后两个人终于重新走到一起，我最喜欢的情节是两个人尽释前嫌，手牵着手走向海边，一边走一边回忆第一次两个人在舞会上认识的情景，这个旅程是一个人的远行，可是它却是两个人的灵魂回归，当哈罗德逐渐接受自己懦弱的一面，开始学会承担责任；当莫琳开始反思过去的种种经历，回想起丈夫曾经温情的一面，我想说，世间所有的相遇都是久别重逢，有什么比相逢一笑泯恩仇更开心的事情呢？不愿意放下过去是执念，不愿意重新开始是执念，我们本来就应该温暖善良的样子，即使生活让我们暂时穷困潦倒、失意落寞，那又怎么样呢？\n一个让我更加感兴趣的地方是，当哈罗德因为独自远行这件事情而声名远播的时候，在那一瞬间形形色色的朝圣者都表示要加入这个旅程，可是很快我们就发现，这些朝圣者或多或少的都各怀心机，有的人是为了追求名利而加入队伍，有的人是为了出一本传奇书籍而加入队伍，有的人是为了挽救一段失败的婚姻而加入队伍，有的人是为了写一篇成为头条的新闻报道而加入队伍\u0026hellip;\u0026hellip;这就像我们这个世界，当所有人都开始尝试按照自己的理解来揣度哈罗德的用意的时候，朝圣这件事情本身地意义就会被无限的忽略，没有一个人尝试去理解哈罗德的本意，当所有人都对此趋之若鹜的时候，或许并不代表这件事情本身为人们所理解所推崇，人们喜欢的仅仅是这种被关注的感觉而已，所以请放下那颗浮躁的心，努力去聆听自己内心的声音，我们生来并不是因为我们需要这样一场添油加醋的作秀，就像我始终相信爱情是两个人彼此吸引自然而然地走到一起，我不擅长刻意的事情，或许是因为我天生就喜欢本色出演。\n哈罗德最初开始旅程的时候，基本上是一无所有的，他腿脚不便同时患有老年痴呆，正是这样的状态让妻子莫琳对他的所作所在困惑而愤怒，可是随着旅途的深入，那些在旅途中遇到的路人们，常常会馈赠给哈罗德如地图、指南针、药物等等这些东西，所以对哈罗德而言，这个过程是一个装备逐渐增加的过程，可是当他离目标越来越近的时候，他不得不将这些东西转赠给其他人或者是不慎在旅途中丢失，我印象比较深的一点是，他在旅途中收留了一条流浪狗，在相处的过程中逐渐培养出了感情，此时的哈罗德想起自己的儿子曾经想要养一条小狗，可是因为他的固执和懦弱，这个愿望最终成为永远的遗憾，正当他为此而费心伤神时，这条流浪狗忽然不知所踪，等哈罗德找到它时，发现它跟上一个年轻女孩儿上了一辆公交车，或许这告诉我们，我们的人生其实就是一个由简至繁再由繁至简的过程，我们曾经年少轻狂、我们曾经迷失方向，可当我们洗净铅华、丢掉一切附庸的时候，我们或许会不由得感谢，那些生命里教会我们某些东西的人。从始至终我们面对的这个世界的悲欢离合其实都是我们自己，可能有些人会在特定的时候出现然后陪伴我们一段时间，可我们最终会发现，有些路只能我们一个人来走。\n我们总要尝试去做一件想做的事情，我们总要学会去放下过去重新开始，这是一个人的涅磐，这是哈罗德的朝圣。“也许当你走出车门真真切切用双腿走路的时候，绵延不绝的土地并不是你能看到的唯一的事物”。我忘记这句话是故事中的哪个人物曾经说过的话啦，我对小说一直没有完全能股投入阅读的感觉，我想人的大脑里有太多的东西我们不明白，但是你想想，如果有信念，你就一定能把事情做成。你是否还记得最初开始这个旅程时曾说过的话：\n我现在马上出发。只要我一天还在走，她一天就要活着。请告诉她这次我不会让她失望。\nPayne 于 2016/11/12\n","date":"2016-11-05T21:44:52Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3657008967/","slug":"3657008967","tags":["朝圣","读书","生活"],"title":"生命的朝圣者"},{"categories":["编程语言"],"content":"最近在做的项目进入中期阶段，因为在基本框架结构确定以后，现阶段工作重心开始转变为具体业务逻辑的实现，在这个过程中我认为主要有两点，即保证逻辑代码的正确性和容错性、确定需求文档中隐性需求和逻辑缺陷。为什么我说的这两点都和用户需求这个层面息息相关呢？或许这和我这段时间的感受有些关系吧，我觉得当我们在面对用户提出的需求的时候，一个非常让我们不爽的一个地方是，我们总是需要花费大量的时间来和用户确定某些细节，而这些细节无论在 BRD 或者 PRD 中都无从体现。固然从用户层面上来讲，我们无法要求用户提供，详尽到每一个细节的需求文档。可我觉得这是一个修养的问题，我们习惯于宽以律己、严以待人，可是如果我们连自己都说服不了，我们该如何尝试去说服别人呢？我不认为我们就应该被用户限制自由，我们共同的目的都是想要好做一件事情，所以我们的关系应该是平等的伙伴的关系，这种上下级的、命令式的主仆关系让我感觉受到了侮辱。\n关于最近的碎碎念 其实对我而言，我更希望在工作中能找到一种释放天性的氛围，因为我觉得我们这个世界每天都有新的技术诞生。可是当我发现，我们的用户依然在使用着 20 多年前的技术的时候，我常常感觉到一种难以言表的紧迫感，或许对银行这类用户而言，它对安全和可靠的需要远远超过对新技术和新工具的需要，可是当我看到身边的同龄人甚至是人到中年的时候，我忽然间发现，原来这一切离我是如此的近，当你看到身边的同龄人对代码开始厌烦，继而将其当作糊口的工具的时候，我有时候就常常在想，我离这种状态会有多远，我讨厌自己不像期望中那样好，因为我曾错失过一个我爱的人，所以我有时候会像强迫症晚期一样，刻意地去追求完美。或许接受平庸会更像一个正常人，可我怕我再没有勇气去轻易喜欢一个人。我承认，我在这件事情上偏执是因为我在某种程度上自卑，可是如果我们能做得更好，为什么不去尝试做得更好呢？\n这段时间，我喜怒无常的性格，或许让我身边的同事受到了伤害，其实我从来都不是针对任何人，我只是对这种无法掌控的现实的一种愤怒，我们常常被用户要求，为他们开发某种自动化的工具，可是我们所有的工具流，都是建立在一套尚未健全的设计上的，甚至用户内部使用的相关系统存在各种各样的设计缺陷，而这些完全不适合做自动化的特性，常常面临被设计到需求文档中的尴尬。虽然工程师喜欢解决问题，可解决问题并不代表要以牺牲技术上的先进性为代价，就像今天我们同样可以使用汇编语言来开发应用程序，可是有谁会选择这样做呢？这是因为汇编作为工具本身就是一种相对低级的编程语言，所以在这种情况下，我不认为花费精力来为落后的工具填坑，是一种值得称赞的事情，我们早已告别了石器时代，可有人因为学会了钻木取火而沾沾自喜，这是一种悲哀。我们既然让计算机来替人们做事情，所以就应该明确告诉计算机到底想做什么。一切没有任何规则可言，同时妄图实现自动化的过程，都是在赤裸裸的耍流氓，而规则和约束常常让人性的缺点暴露无疑。\n所以，这种向现实妥协的做法，常常会让我们编写出肮脏的代码。我们总是想要编写出优雅、通用的代码，可因为工具流的落后、需求频繁变动、设计缺陷等等的原因，我们在面对这些东西的时候，常常感觉被人类的愚蠢的打败，人们说是人类发明了计算机，可是这是否就意味着我们一定会比计算机聪明，难道计算机无法通过深度学习超越人类吗？Google 的 AlphaGo 凭借当今火热的深度学习理论以 4:1 的战绩打败了韩国棋手李世乭，可是不愿意去学习新知识的人类居然可以自信到能够驾驭计算机，我说将来会有越来越多的工作被计算机代替，我的一位长辈不以为然的说，不管计算机如何智能它总需要人类来控制它吧，我真的很想问一句，如果计算机真的超越了人类它为什么还需要人类来管理，而人类依靠什么样的技术来管理这些计算机。我认为在这个世界上，总是存在某种永恒的规则，它可以超越生与死的界限，而这些规则永远不会被打破，人类就像一个任性的孩子一样，可真理不就是用来敬畏的吗？我们对这个世界了解的越多，发现自己越来越渺小，此时此刻，你是否还有信心说我们可以驾驭计算机？\n写这些碎碎念，其实是想反映我这段时间的心理状态，有人说，摆脱失恋最好的方法就是投入一段新的感情，可是其实你永远都清楚地知道，在你心里最看重什么，所以我对代码有一种特殊地感情，你可以清楚地从代码中读出一个人的所思所悟，因为那就是你独特个性的一种写照，所以每一次或许 Alex 让我改代码的时候，我都是在和我自己赌气吧，我不愿意让那些奇怪的逻辑破坏它的纯粹性，它必须是统一的、简洁的、纯粹的，它不能掺杂丝毫的丑陋的设计。而这种情况常常是因为用户在设计需求的时候忽略了某些细节，所以对我而言我生气、我愤怒，并非是我觉得这个需求无法实现，而是它在某种程度上是冗余的，即它可能破坏了一致性原则，灵活的人类是比呆板的计算机有趣，可和人相处得久了，你难免会觉得人显得不靠谱，这就是我厌恶的理由，在这个世界上所有一切计算机可以处理的问题，在某种程度上都可以转化为数学问题，一旦我们将设定突破这个规则，就会让代码因为妥协而变得丑陋不堪，我显然不允许这样的事情发生。\n花十分钟解锁新技能 好了，现在我们来回到这篇文章的主题，基于 C#中的 Trace 来实现一个简单地日志系统。我们的项目上存在大量和用户内部系统关联的特性，所以我们会在远程计算机上耗费大量的时间来测试代码，这个时候我们会遇到两个问题，第一，我们开发环境中的 Visual Studio 版本和生产环境中的 Visual Studio 版本不一致，所以如果直接远程调试，因为项目中使用的相关语法在低版本 Visual Studio 中不被支持，如果修改代码会非常痛苦，我们实在没有精力去兼容两个版本的开发环境。第二，项目中默认使用的日志系统 Log4Net，默认是在指定用户的我的文档目录中产生日志信息，而我们在远程调试时因为权限问题无法访问日志文件，所以虽然我们可以根据界面上反馈的信息，来粗略判断异常发生在什么时候，但这对我们追踪和定位问题来说是非常不利的。我们在研究了 Log4Net 的文档后，认为这个库的配置文件非常复杂，所以我们在想有没有一种更为简单地方案可以帮助我们解决这个问题。\n我们了解到.NET 中实际上提供了两个类 Trace 和 Debug 来满足类似的需求，而这两个类位于 System.Diagnostics 空间下，所以我们完全有理由相信基于这两个类，我们同样可以构建出一个相对简单的日志系统。首先我们通过 MSDN 了解到官方对它们各自用途的定义：\nTrace：提供了一组方法和属性，可以帮助您追踪您的代码执行，该类无法被继承。\nDebug: 提供了一组帮助调试代码的方法和属性，该类无法被继承。\n显然，我们通过这里给出的定义，可以非常容易的理解这两个类都可以用来追踪和调试代码，那么它们本质的区别在什么地方呢？如果我们的解决方案配置类型为 Release，则会忽略 Debug 类的输出。换句话说，当我们处在开发调试阶段时，使用 Debug 类能够帮助我们在控制台或者是文件以及任意自定义的输出位置输出相关的调试信息，而当产品上线发布以后这些调试信息则不会输出。而 Trace 无论是在 Debug 还是 Release 模式下都会输出相关的追踪信息。通常我们会在发布以后的产品中部署日志生成模块，这样可以方便开发者定位问题、维护产品，那么在这种情况下，我们采用 Trace 这种方式来追踪程序的执行情况是非常适合的，而这正是我想写这篇博客的一个原因。\n现在，在确定了使用 Trace 来开发一个简单的日志系统这样一个技术路线以后，现在我们来了解下 Trace 都提供了那些东西吧！对 Debug 和 Trace 这两个类来说，.NET 为它们提供了下面这些相同的方法：\nWriteLine: 该方法会在输出设备中写入一条调试信息，而通过实现不同的监听器(Listener)并对其中的方法进行重写(OverWrite)，就可以将调试信息以不同的形式输出。例如 Debug 类产生的调试信息默认输出在 Visual Studio 中的输出窗口，我们可以通过自定义监听器将调试信息输出到文件或者控制台中。同样地，对 Trace 类来说，它同样遵循这个原则，这体现出了一种宏观上的统一。所谓“和而不同”，我们可以尊重这个世界的规则、尊重宇宙苍生，可是我们每一个人都是一个完全独立的个体，人可以被打倒，但决不会被打败。 Trace.WriteLine(\u0026#34;This is a Debug message!\u0026#34;); Trace.WriteLine(\u0026#34;This is a Debug message!\u0026#34;,\u0026#34;Debug\u0026#34;); WriteLineIf: 该方法是 WriteLine 的增强版，仅当条件满足时会在输出设备中写入一条调试信息，同样，它支持通过实现不同的监听器(Listener)来完成重写，进而将调试信息以不同的形式输出，该方法在需要根据条件处理不同响应的场景下会非常有用。例如在项目中我们会通过一个窗口来输出程序执行过程中的细节信息，这些信息对我们开发人员来讲是非常重要的，因为我们可以通过这些信息来快速地定位问题。可是这些信息对用户而言是可以完全忽略的啊，难道肤浅的我们要在这里处理这两种情况吗？不，我们只需要定义一个全局开关，从此整个世界都变得安静了。 Trace.WriteLineIf(i\u0026gt;10,\u0026#34;This message will only output when i\u0026gt;10\u0026#34;); Indent/Unindent: Log4Net 中提供了对日志输出样式的支持，它被定义在一个 Xml 形式的配置文件当中，我们发现一件有趣的事情，复杂和简单是矛盾而统一的，就像我对编辑器这类工具，我会喜欢它提供的各种强大的扩展能力，而对集成开发环境这类工具，我会喜欢它提供的简单上手、零配置、开箱即用这种良好特性。当你发现你提供的功能越来越多的时候，就应该停下来思考这种做是否是正确的举动，一个东西的灵活性越强，它的复杂性就会越高，因为这意味着你需要去兼顾各种各样可能的组合。在这里 Indent 方法可以为输出提供缩进样式，相反 Unindent 方法可以为输出清除缩进样式。\nAssert: 断言不一定就出现在单元测试中，就像骑白马的不一定都是唐僧。严格的来讲，这里的断言相对单元测试中的断言会显得相对薄弱，因为它没有 Assert 这个类的功能丰富。在这里我想说的是，Assert 方法会在条件不满足时显示“断言失败”对话框，在对话框中会显示当前程序堆栈调用的详细情况，这是非常有意思的一个功能。有时候我们或许会因为业务而忽略技术，业务是现实规则的一种映射，所以我们可以理解业务本身地复杂性，可我们从古到今所认识的世界难道都是这样子的吗？或许由人类定义出来的这些规则本身就是错误的呢？\nTrace.Assert(i\u0026gt;10,\u0026#34;This message will only output when i\u0026lt;=10\u0026#34;); Flush: Flush 方法可以理解为一个通知监听器的方法，因为在调用 Flush 方法以后，每一个 Listener 对象将接收到它的所有输出，我们可以理解为，WriteLine 方法执行以后，无论 Trace 还是 Debug，其监听器都不会理解响应输出，只有当 Flush 方法被调用以后调试信息才会被响应和输出。 好了，再了解了这些以后，现在我们就可以开始设计一个日志系统了。按照国际惯例，我们当然是从设计接口开始，其实在做一项设计的时候，是不是要从接口开始，完全取决于你对接口持怎样的态度，人生或许有各种各样的套路，可是需不需要遵守这些套路完全是取决你的啊，编程同样是这个道理，我的习惯是在没有理解一个东西以前，永远不要尝试去使用它，可能你会说如果永远都不去尝试，那你就永远失去了了解它的机会，我想说的是，请不要滥用：\ninterface ILoger { void Warn(object msg); void Info(object msg); void Debug(object msg); void Error(object msg); } 可以注意到在这里，我定义了四种级别的 Log，这自然是模仿 Log4Net，更重要的是这些不同的级别，我并不清楚他们之间的区别。这听起来好像挺尴尬啊。定义好接口以后，我们就可以考虑具体的实现啦！日志系统对整个应用程序而言哼，是独立且贯穿整个软件开发的生命周期的，所以将其设计为单例模式会更加友好：\npublic class SimpleLoger : ILoger { /// \u0026lt;summary\u0026gt; /// Single Instance /// \u0026lt;/summary\u0026gt; private static SimpleLoger instance; public static SimpleLoger Instance { get { if (instance == null) instance = new SimpleLoger(); return instance; } } /// \u0026lt;summary\u0026gt; /// Constructor /// \u0026lt;/summary\u0026gt; private SimpleLoger() { Trace.Listeners.Clear(); Trace.Listeners.Add(new LogerTraceListener()); } public void Debug(object msg) { Trace.WriteLine(msg, \u0026#34;Debug\u0026#34;); } public void Warn(object msg) { Trace.WriteLine(msg, \u0026#34;Warn\u0026#34;); } public void Info(object msg) { Trace.WriteLine(msg, \u0026#34;Info\u0026#34;); } public void Error(object msg) { Trace.WriteLine(msg, \u0026#34;Error\u0026#34;); } } 现在我们来重点关注 SimpleLoger 的构造函数，显然在这里它应该是私有的，在这里我们首先从 Trace 类的 Listeners 中移除所有的监听器，这样做的目的是改变 Trace 类的输出行为，因为在前面介绍 Trace 的时候我们了解到，Trace 类和 Debug 类默认将调试信息输出在“输出”窗口中的，而我们现在希望将调试信息输出到日志文件中，所以我们需要改变 Trace 类的输出行为，改变的方式非常简单啦，移除默认的监听器，然后添加我们自己定义的监听器哇，对对对，就是这样简单粗暴。下面我们来看看如何定义这样一个监听器 LogerTraceLitener，它继承自 TraceListener 这个类，这意味着我们如果要实现一个自定义监听器，只需要继承 TraceListener 然后重写相关方法即可：\npublic class LogerTraceListener:TraceListener { /// \u0026lt;summary\u0026gt; /// FileName /// \u0026lt;/summary\u0026gt; private string m_fileName; /// \u0026lt;summary\u0026gt; /// Constructor /// \u0026lt;/summary\u0026gt; public LogerTraceListener() { string basePath = AppDomain.CurrentDomain.BaseDirectory + \u0026#34;\\\\Logs\\\\\u0026#34;; if(!Directory.Exists(basePath)) Directory.CreateDirectory(basePath); this.m_fileName = basePath + string.Format(\u0026#34;Log-{0}.txt\u0026#34;, DateTime.Now.ToString(\u0026#34;yyyyMMdd\u0026#34;)); } /// \u0026lt;summary\u0026gt; /// Write /// \u0026lt;/summary\u0026gt; public override void Write(string message) { message = Format(message, \u0026#34;\u0026#34;); File.AppendAllText(m_fileName,message); } /// \u0026lt;summary\u0026gt; /// Write /// \u0026lt;/summary\u0026gt; public override void Write(object obj) { string message = Format(obj, \u0026#34;\u0026#34;); File.AppendAllText(m_fileName, message); } /// \u0026lt;summary\u0026gt; /// WriteLine /// \u0026lt;/summary\u0026gt; public override void WriteLine(object obj) { string message = Format(obj, \u0026#34;\u0026#34;); File.AppendAllText(m_fileName, message); } /// \u0026lt;summary\u0026gt; /// WriteLine /// \u0026lt;/summary\u0026gt; public override void WriteLine(string message) { message = Format(message, \u0026#34;\u0026#34;); File.AppendAllText(m_fileName, message); } /// \u0026lt;summary\u0026gt; /// WriteLine /// \u0026lt;/summary\u0026gt; public override void WriteLine(object obj, string category) { string message = Format(obj, category); File.AppendAllText(m_fileName, message); } /// \u0026lt;summary\u0026gt; /// WriteLine /// \u0026lt;/summary\u0026gt; public override void WriteLine(string message, string category) { message = Format(message, category); File.AppendAllText(m_fileName, message); } /// \u0026lt;summary\u0026gt; /// Format /// \u0026lt;/summary\u0026gt; private string Format(object obj, string category) { StringBuilder builder = new StringBuilder(); builder.AppendFormat(\u0026#34;{0} \u0026#34;,DateTime.Now.ToString(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;)); if (!string.IsNullOrEmpty(category)) builder.AppendFormat(\u0026#34;[{0}] \u0026#34;, category); if (obj is Exception){ var ex = (Exception)obj; builder.Append(ex.Message + \u0026#34;\\r\\n\u0026#34;); builder.Append(ex.StackTrace + \u0026#34;\\r\\n\u0026#34;); } else{ builder.Append(obj.ToString() + \u0026#34;\\r\\n\u0026#34;); } return builder.ToString(); } } 在这里我重写了好多好多方法，可是实际上我在 SimpleLoger 中仅仅用到 WriteLine 这个方法，大家可以发挥自己的想象力，因为我始终相信编程是一件有趣的事情，我们有时候会感到沮丧，完全是因为这个糟糕的世界里充满了同样糟糕的事情。其实程序员是一个理性与感性并存的职业，如果是操作系统、编译原理和图形学可以并称为程序员的三大浪漫，那么 Big Clean Problem 将是我们最这个世界最好的敬畏，我们喜欢解决问题本质上是因为我们对这个世界充满好奇，可这并不意味着我们对问题来者不拒，这个世界产生的大部分问题都是因为人类的无知，可人类到此刻依然认为这一切非常合理。\n现在，让我们来检验我们的这个小玩意儿，我们将编写一个非常简单的单元测试案例，我们都知道当除数为 0 时在数学上是没有任何意义的，所以在计算机中当我们尝试除以 0 的时候会引发异常，由此我们会写出下面的代码：\n[TestMethod] public void Test() { try{ int i=0; Console.WriteLine(5/i); }catch (Exception e){ SimpleLoger.Instance.Debug(e); } } 理论上它会在程序根目录下生成一个 Logs 的文件夹，然后每天会生成一个以日期命名的文本文件。现在，它看起来工作得很好，我没有想要做出一个更好的日志系统的野心，我更喜欢去探索一种全新的可能性，我更在意在这个过程中我们收获了什么，人生本来就充满了各种各样无意义的事情，我们之所以热爱生命，是因为我们希望它变得有趣，这样就足够了，不是吗？\n效果演示\r","date":"2016-10-25T20:16:13Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1254783039/","slug":"1254783039","tags":["日志","Trace","调试"],"title":"基于 C# 中的 Trace 实现一个简单的日志系统"},{"categories":["读书笔记"],"content":"\r其实一直想读《黑客与画家》这本书，所以在我买了 Kindle 以后，这本书就成为我读完的第一本书。本书作者是美国互联网界举足轻重、有“创业教父”之称的哈佛大学计算机博士保罗·格雷厄姆 (Paul Graham )，而这本书是由他的思考整理而成的一本文集，虽然这本书的名字叫做《黑客与画家》，可实际上作者在这本书中观点，并非局限于黑客与画家本身，相反地它涉及编程、软件、创业、财富、设计、研究等等多个领域。我认为这本书带给我的，更多的是一种思想上的提升，当我们沉迷在代码中无法自拔的时候，我们其实应该意识到，这个世界原本是由理性和感性两种认识混合而成的。当科技与人文发生碰撞进而共鸣，这本书会告诉你这一切是如此的美妙。\n我们目前所处的这个时代，本质上是一个机器的时代，这是自工业革命以来，人类历史上又一个革命性的时代。越来越多迹象表明，未来的人类生活不仅是人与人的互动，而且更多的将是人与计算机的互动。所以想要把握这个时代，就必须理解计算机。而理解计算机的关键，则是要理解计算机背后的人。我们的时代是程序员主导的时代，而伟大的程序员就是黑客。\n普通人认为“黑客”就是入侵计算机的“计算机罪犯”，其实“黑客”的本意是指出于兴趣而解决某个难题，不管它有没有用的这类人(出自自由软件基金会创始人理查德·斯托尔曼)，所以黑客从一开始就是有精神追求的，它代表着求解问题过程中产生的精神愉悦或享受，黑客们只是比我们普通人更崇尚分享、开放和民主，他们对任何被禁止的东西都怀有特别强烈的好奇心，他们喜欢去思考那些似乎不应该被思考的问题，他们相信计算机会深刻地改变人们的生活，由此我们可以认识到“黑客伦理”的一个推论：黑客不服从管教，具有叛逆精神。黑客就像一群有知识的海盗。编程与绘画异曲同工，黑客是数字时代的艺术家，他们都是创作者，都试图创造出优秀的作品。\n书呆子之所以不受欢迎，是因为他们不如普通人聪明。可是作为普通人的我们没有意识到，这些书呆子和周围环境显得格格不入，或许从某种程度上说明他们领先了一步，或许书呆子已经在思考的东西，正是真实世界看重的东西，他们并非不想让自己受大家环境，他们只是没有时间来做这些普通人所看重的事情。虽然现实中的图灵并不像电影《模仿游戏》中塑造的那样怪癖、不合群，可是通过这部电影，我们依然能够感受到那种天才般的疯狂，图灵将全部精力都专注在制造“克里斯托弗”，因为不通世故而被同事嘲笑和指责，是琼教会他如何和大家相处。我们可以发现当图灵尝试改变以后，他可以像普通人一样收获友谊，所以所谓天才无非是，他们比常人更专注他们认为重要的事情。\n中国企业更加关注软件作为科学和工程的一部分，即我们都将编程作为一种技术，其实编程同样有它作为人文与艺术的部分，我们必须认识到编程是一种艺术创作。编程语言是用来帮助我们思考程序，而不是用来表达我们已经想好的程序。你只有以一种“设计”软件，而非“实现”软件的思路来应用程序，你才能在这个过程中发现编程的乐趣，这和我们的兴趣相辅相成，因为如果你不爱一件事，你不可能把它做得真正优秀，要是你很热爱编程，你就不可避免地会开发你自己的项目。\n坚持一丝不苟，就能取得优秀的成果，因为那些看不见的细节累加起来，就变得可见了。当一个黑客认为他是一个创作者的时候，他从事的就不再是机械性的工作，因为这就要求他具备灵感。如果编程时与绘画和写作同一类的工作，黑客是否有机会像伟大艺术家一样备受推崇、流芳百世呢？我认为这是有可能的，譬如 Linus 一生有 Linux 和 Git 两个作品就足以彪炳史册，而更重要的是他能让更多的人从中收益，这是黑客精神的价值所在。\n我们总是习惯于，给那些为我们所看不惯的人，贴上各种各样的标签。其实做自己喜欢的事情，是不需要伪装的，我只是我而已，不需要满足你们过高的期望。在我们这个世界里，程序员毫无例外地被人们误解，从阿里月饼事件我们就可以看出，普通人对技术的理解基本保持在白痴这样的水平上，可讽刺的是我们的命运被被掌握在这样的人手里，我们绞尽脑汁来复原产品经理们的脑洞，我们费尽心机来防止白痴用户们的错误……可我们依然被这场无情而可笑的舆论打败，我们对这个世界充满敬畏，因为它的复杂程度远远超过我们能够想象以致于抽象的范围。\n可我们依然在尝试让自己更努力一点，因为我们编写代码的同时，在不断地认识着这个世界，就像这个世界上本来没有算法、数据结构和设计模式，可当我们创造和了解这一切以后，我们对这个世界的看法或许会发生变化。我们不能接受的人类的愚蠢、狂妄和无知，因为当你越接近真理时，你就越会认识到自己的渺小。我们曾自以为是地相信，现代人比古代人更聪明、更高尚，可是当我们了解的历史越多，就会越明白事实并非如此，古人与我们是一样的人，他们不是更勇敢亦或着更野蛮，而是像我们一样的普通人，不管他们产生怎样的想法，都是正常人产生的想法。\n不管你是否愿意去相信，这个时代已经无可避免地，和计算机紧紧地联系在一起，计算机可以帮助人类完成大量工作。可你必须认识到，计算机存在的意义并非是让人类变得更懒惰。相反地，它希望人类向着更好的方向去发展、希望人类更有效率的工作和生活，因为计算机可以做的事情越多，人类面临的这种失业的紧迫感就应该越强烈。我们不应该想当然地认为，人类一定会凌驾于智能机器之上。人类在和大自然抗衡的过程中其无知与愚蠢昭然若揭，我们今天意识到地球环境面临威胁，这难道不是为儿时的任性付出的代价？\n图灵在这个世界上尚未出现计算机的时候，就能够想到有一天机器会比人更聪明，可我们普通人居然狂妄而无知地认为，所有的一切在工程师眼中都是非常简单的事情，工程师们懂得敬畏自然、敬畏真理，可普通人反而不愿意去理解这些原理和事实，更讽刺的事情是，一个软件工程的话语权是掌握在这样的人手中。我们从来不畏惧去挑战艰难的任务，仅仅是因为在这个世界上，有值得我们永远去敬畏和尊重的东西，那就是真理，那是像伽利略、布鲁诺、图灵等等对人类进步做出卓越贡献的人们，愿意用生命去捍卫的东西，现代计算机的理论基础是数学，所以你必须承认计算机是一门科学，其次它是一门艺术。\n我喜欢这种科学和艺术完美融合的感觉，就像现代科技发展到今天，我们面对地不再是冷冰冰的控制台终端，而是注重人性化和用户体验的图形化界面。苹果和微软都曾经窃取过施乐公司的图形化界面技术，这段故事在电影《硅谷传奇》中更是被演绎得淋漓尽致。可为什么乔布斯会一直评价微软的产品没有品味呢？或许这是因为乔布斯站在了一个科技与人文的十字路口。我并非果粉，甚至在某种意义上我一直是软粉，可这并不妨碍我对科技和美的一种热爱，技术本身并不能在商业化策略中起到决定性作用，可是一个优秀的产品一定是将技术和艺术完美地融合在一起，所谓“一张一弛，文武之道”，从这个意义上来说，黑客和画家志同道合，理性与感性相辅相成，即使我们不能这个世界改变什么呢？可那有什么关系呢？\n","date":"2016-10-09T18:38:03Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/4205536912/","slug":"4205536912","tags":["黑客","程序员","人文"],"title":"当黑客遇见画家"},{"categories":["读书笔记"],"content":"\r我不知道大家如何定义程序员这个工作，在我看来，在某种意义上，程序员和艺术家们具有相同之处，我们都是创作者，和诗人、画家、作家等等这些职业相近，我们都在试图创作出优秀的作品，我们借助编程语言来重构我们对这个世界的认识、借助抽象的概念来创造这个世界上不存在的东西，所以我们对自由和创造的渴望，来源自我们在这个世界上写下的第一行代码，或许这像是一个充满理想主义的臆想，可这并不重要，重要的是你如何看待这个世界、如何看待你自己，我更喜欢将程序员视为造梦者，就像每一个孩子在搭积木的时候，都有一个建筑师的梦一样，你可以选择让代码简洁、优雅，你同样选择让代码肮脏、丑陋，你相信什么，你执着什么，它就会是什么，所以为什么不给我们自己更多创造的机会。\n在这个世界上，伟大的艺术家都是孤独的。从梵高那忧郁而狂野的绘画作品，到永远如童话王国里天真孩子一般的顾城……或许这种与生俱来的孤独感会成为一种宿命般的诅咒吧！程序员同样是孤独的，虽然我们今天的现代文明，高度依赖着程序员们创作的各种程序代码，可程序员就像一个被隐藏在幕后的魔术师，永远被人们忽略甚至是遗忘，同样地，程序代码和程序员一样孤独，当一段代码经过链接和编译，然后转化为可执行程序以后，没有人会再记得它们的样子，这种感觉就像人们需要你，可是人们从来不想了解你，而这种孤独会从心灵上摧毁一个人。“古来圣贤皆寂寞，唯有饮者留其名”，这种感悟在司马迁的《报任安书》里表现得更加玲离尽致：盖文王拘而演周易；仲尼厄而作春秋；屈原放逐，乃赋离骚；左丘失明，厥有国语；孙子膑脚，兵法修列；不韦迁蜀，世传吕览；韩非囚秦，孤愤说难；诗三百篇，大抵圣贤发奋之所为作也。\n所以，我希望，程序员能够将每一行代码，都当做诗歌一样的艺术品，你必须在字里行间折射出明显的个人气质。在这个世界上，没有人会真正在乎这些代码是什么样子，因为人们需要的是一个可以对自己有用的东西，而对于它到底是什么永远没有人关心。我们整天面对着冷冰冰地液晶屏幕，所以我们理所当然地认为这些数字设备、应用程序都没有感情，它们就应该是被我们人类无情嘲弄的数字化工具。可讽刺的是，在一种称为手机的移动设备中，我们所有人的秘密都隐藏在这个我们不愿意去理解的东西里，这就相当于我们将我们的秘密，交给一个我们不愿意去深入了解的人一样，而仅仅是因为我们需要这样的一个人。这听起来像个笑话，所以，我们所有的努力，都是为了让这些代码，看起来不再那么孤独，你可以让它们成为你在这个世界上的烙印，你可以使用注释帮助人类理解代码、理解你自己，你可以创造出一种无限接近真相或者真理的信仰，你可以通过这一行行代码影响历史或者生活，我们不是仅仅为了活着。\n乔布斯曾经引用毕加索的名言：优秀的艺术家复制，伟大的艺术家偷窃，并以此作为理由对施乐公司进行了疯狂的“盗窃”，乔布斯身上最让我着迷的是那灵魂深处那种艺术家的气质，他始终相信任何产品都应该是人的一种自然延伸，而正是这种对质量、理想和心灵的专注，让苹果公司的产品在设计和品味上显得与众不同。在麦金塔电脑这个项目上，他对完美的极致追求，让他对艺术和设计的那种天性得到释放，可与此同时他被斯卡利排挤出苹果公司，或许在这个以成败论英雄的时代，乔布斯是一个失败者，可这个世界依然需要理想主义，多年后，当人们提到苹果电脑总会不由自主地想到 Mac 这个品牌，我想说这是理想主义的胜利。\n天才与普通人的区别在于，他们会做出在我们普通人眼中显得“疯狂”的事情，在电影《乔布斯》的结尾，乔布斯说：“只有疯狂到相信自己能改变世界的人，才能真正改变世界”，而这里对天才的定义是什么呢？他们特立独行、桀骜不驯、麻烦制造者，他们是格格不入的一群人，习惯用不同眼光看事情的人，不受规则约束的人，对既成事实不屑一顾的人，你可以引用他们，亦可反驳他们，或赞颂或诋毁他们，你唯一做不到的就是忽视他们，因为他们带来变革，他们推动人类向前，或许有些人将他们视作疯子，而我们将其视为天才。对大部分而言，我们都是普通人，可这并不代表我们就要停止努力，因为大部分的努力都尚未达到拼天赋的程度，何况我们可能连天赋都没有呢？\n这意味着我们需要去做些疯狂的事情，我们追求极致和完美，是因为我们想在这个世界上，留下属于我们自己的印记，没有人会明白，为什么乔布斯天生就有一种想要和 IBM 抗衡的想法，这种想法促成了让人们印象深刻的\u0026quot;1984\u0026quot;广告，其创意来自乔治·奥威尔的小说《一九八四》，并借助小说中的“大佬”映射当时的蓝色巨人 IBM，这个举动让苹果公司、让 Mac 成为人们心中理想主义的化身，这个举动相当地疯狂，不是吗？甚至在《硅谷传奇》这部电影中，这种疯狂被表现得更加淋漓尽致：我们正在创造一个新的信仰，你应该这样来认识，像艺术家或者诗人一样，我们正通过我们的所作所为，重写人类思想的历史。所以，当我们极力追求代码的优雅、性能的出众以及架构的良好，这一切都是因为我们想要做些，真正可以让我们在乎的事情。因为我们的生活常常被需求和问题左右，如果我们都不在乎我们所做的事情，我相信这个世界上不会有第二个人更加在乎。\n天才在某种意义上来讲是孤独的，因为天才的想法，注定会超越时间和空间。古往今来，无数仁人志士，为这个世界所付诸的努力，其实都是为了帮助我们更好的认识这个世界而已。遥想太古时期，在这个世界上尚流传着诸神创造世界的传说，地球上或许还没有人类出现，最早的古猿还没有学会直立行走，这段时间对我们人类来说非常漫长，可它对整个宇宙、整个地球来讲，或许是短暂到可以忽略的一段时间，甚至当我们将视野放大到整个宇宙的时候，我们可能会突然意识到，我们所处的世界，可能仅仅是宇宙中的时光，曾经驻足过的，一个小城镇，因此我们对这个世界既熟悉又陌生，所以我们有什么理由不去发现更多的事情呢？对这个世界永远保留好奇心，是天才的基本要求，我们必须认识到，相对整个宇宙，我们每个人都显得狭隘而无知，所以我们有什么理由不去探索这个陌生的世界呢？我们目前所处的这个时代，大量的开源项目和第三方 SDK，为我们提供了前人所不能想象的丰富的资源，无疑我们是幸福的，我们仅仅需要再努力一点。\n一位同事给我推荐了《模仿游戏》这个电影，在此之前我对图灵这个人物的了解，无外乎他对整个计算机行业做出的革命性的贡献，以及他像谜一样的人生经历，可是通过这部电影，我看到的是一种天才般的疯狂和执著，图灵在被通知去接受破解德国加密装置“恩尼格玛”的任务时，军方对其进行了一个简单的面试，在面试中我们发现图灵或许不懂得什么是幽默，这种被我们称为“书呆子”的性格，在实际生活中是不讨喜的，在《黑客与画家》中作者明确地指出，“书呆子”不受欢迎是因为他们比普通人聪明，可是图灵很快就能发现，团队中有哪些人是不合格的解密者，而在这个过程中他因为不太懂得如何和别人相处，而被团队里的成员指责和谩骂，可他从来没有放弃过制造他的机器，而最终的结果的确是依靠他的机器计算出来的，天才的孤独在于常常不被常人理解，可是当图灵被强迫关闭他的机器的时候，团队中的成员愿意站在这一边，这一幕是让我感受到温情存在的，而这一切来自一个走进他生命中的女孩子：琼。\n或许我们普通人穷尽一生都无法达到天才的万分之一，甚至有人想要告诫我，不要在一件事情上投入大量的热情，因为我们都期望获得成功，可现实难免会在这个时刻，无情地为我们浇下一盆凉水，这种感觉就像你对一个女孩子投入了大量感情，结果最后被她伤心到心灰意冷……可是人生其实本来是没有意义的，你从出生到走向死亡早已安排好，你需要做的就是“重写”这个过程，所以我们来到这个世界上，无非是想要认识些有趣的人、经历些有趣的事情，所以如果你对一件事情的成与败、一件东西得与失，都能做到坦然处之，或许你就不会再畏惧失败，当我们不再愿意为一件事情投入热情的时候，其实我们已经习惯了为自己的懒惰找到一个借口，你必须要相信你做的事情是最正确的事情，你必须要有一种敢于突破自我的激情，否则你注定只能做出平庸而普通的产品。如果我们不能比别人做得更好，那么就努力和别人做得不同;如果我们可以和别人做得不同，那么就努力比别人做得更好，即使普通如你我一般的人，我们依然可以选择像天才一样疯狂。\n在《模仿游戏》里，艾伦的“初恋”克里斯托弗告诉他，有时候正是人们以为的无用之人，成就了无人感想之事。同样地，在劝说琼参与到解密工作中时，他同样对琼说出了这句话，或许是因为琼因为迟到而无法参加测试时，她说的一句漫不经心的话“你凭什么认为我自己不可以解决呢”，让图灵想到了自己，所以他破例让迟到的琼参加测试，虽然图灵是一个同性恋，可他和琼两个人的感情依然让人动容，即使两个人都不是完美的彼此，即使他们按照各自不同的生活方式生活，可他们同样能按照各自不同的方式去爱对方，在这种情形下，世俗中理解的爱是否会显得平淡无奇？图灵的死亡，在我看来是一个国家甚至整个世界的巨大损失，从伽利略、布鲁诺再到图灵，在这个世界上人类自以为是地创造了各种“教条”，并以此来伤害这些本应该在历史中绽放异彩的天才们，图灵提出的“图灵测试”是人工智能领域的一个重要概念，而计算机领域最高的奖项以他的名字定义，甚至英国女王都曾经公开为其恢复名誉，所以同样都是活着，为什么我们不能给自己一种新的选择？\n或许你我的努力不会为人们所歌颂，或许你我的坚持不会为人们所理解，可我们来到这个世界，就是为了给这个世界留下我们的印记，不然我们来这里做什么呢？因为你要相信，你学过的每一样东西，你遭受过的每一次苦难，都会在你一生中的某个时刻派上用场。其实你我的生命都很短暂，我们没有多少时间值得去浪费。像诗人一样睿智，去了解我们的生活；像天才一样疯狂，去掌控我们的生活。\n","date":"2016-10-01T17:12:43Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3653716295/","slug":"3653716295","tags":["黑客","画家","诗人"],"title":"像诗人一样睿智，像天才一样疯狂"},{"categories":["读书笔记"],"content":"\r或许我不是一个懂得如何去爱人的人，我时常陷入一种自我否定的焦虑当中，当我发觉自己喜欢上一个人的时候，从某种意义上它会让我身上的缺点被无情地放大，我并不畏惧在喜欢的人面前暴露这些缺点，因为这就是真实的我，因此我从来不喜欢去塑造别人，让别人成为我心目中期待的样子，可是我会忍不住去塑造我自己，尤其是在和别人相处的过程中，发现我身上的缺点或者问题的时候，我习惯了对自我严格，虽然我知道这个过程注定痛苦，可是你能告诉我，爱到底是什么吗？如果爱不足以让我们改变，我们喜欢的究竟是一个怎样的自己、怎样的别人？\n弗洛姆这本《爱的艺术》是我自己为自己挑选的一本书，在我买了 Kindle 以后，我将我的时间安排在看书和学习上，因为我的确很喜欢读书，而我这种理性的性格有时候难免让人讨厌，所以读书特别是选择去读人文类书籍，从某种意义上来说是我在刻意地稀释这种理性思维造成的影响，《黑客与画家》里告诉我们一件事情，聪明人不被周围人喜欢是因为他们比周围人聪明，当谈恋爱越发地被人们改造成一种套路，我们对爱的定义或许会越来越模糊，可是爱作为一种大自然间普遍存在的情感，我坚信它是一种相当原始而简单的事情。\n弗洛姆认为，爱情是对人类生存问题的回答，人们在这个世界上逐渐意识到生不由己、意识到死的必然、意识到孤独和与世隔绝、意识到面对社会和自然的威力时的无能为力，所以，所有生活在不同文化和时代里的人，都面临着一个同一个问题，即：如何克服这种孤独感。古人说“古来圣贤皆寂寞，唯有饮者留其名”，其实何止是古来圣贤，我们生活在这个地球上的所有人，从出生到死亡都不可避免地被一种孤独感包围者，曾经和别人 讨论过这个问题，我对人情颇为淡漠，因为我觉得除了能够真正将彼此联系起来的两个人，在这个世界上你永远无法找到真正能让你灵魂皈依的地方，这种感觉并非是由血缘或者金钱这样的关系来维系，一个人的孤独与一群人的孤独，在整个宇宙间看起来，其实没有什么不同。\n爱的确是一门艺术，可对我们每个人而言，它像是某种缥缈甚至是难以揣测的情绪，你不能用一种非常理性的眼光来审视和定义它的存在，弗洛姆说：“不是拥有财物的人是富裕的，而是给予他人东西的人才是富裕者”，可现实是并非你不顾一切地对一个人好，就能赢得一份让你感动的爱情，所以在经历过挫折以后，我不再考虑一味地索取或者付出，我喜欢将这个过程叫做分享，人们在分享的过程中认识彼此、丰富彼此、提高彼此的生命感，这是我认为在爱情中我们需要去挖掘的一种潜质，如果一个人没有生命力，就不会有创造爱情的能力，所以当我们试图爱一个人之前，我们首先要学会爱我们自己，而弗洛伊德将这种人的自我欣赏叫做“自恋”，除了爱情自身积极性的因素以外，爱情具有所有爱的形式所共有的因素，如：关心、责任心、尊重和理解。\n爱情不是自私地占有对方或者是放弃除对方以外人的更为广泛意义上的博爱，“爱情是自由之子，永远不会是控制的产物”。或许我们穷极一生来认识自己、认识别人，可我们最终还是不认识自己、不认识别人，可我们无法阻止这种深入了解人的灵魂的秘密、了解人的核心，即“自我”的愿望将继续存在。毫无疑问，德尔斐的人箴言“认识你自己”表达了我们认识自己和他人的愿望。白昼和黑夜表面看起来是敌人，但它们却都是为了一个目标，因为相爱就是为了完成共同的事情。而从广义的爱的定义来看，中国古代先哲孟子的“老吾老以及人之老，幼吾幼以及人之幼”就很好的表达了这种观点，弗洛姆说：“一切爱的形式都以博爱为基础，我指的博爱就是对所有的人都有一种责任感，关心、尊重和了解他人，即愿意提高其他人的生活情趣”。以前以为爱是自私地占有一个人，是对除对方以外的人表现得漠不关心，可后来逐渐发现，我们对自己的爱远远超过别人，这或许不能叫做爱吧！\n我们来到这个世界上，更本质的意义在于我们希望认识些有趣的人，做些有趣的事情而已，这意味着我们常常试图在这个世界上留下自己的印记，我们习惯了在朋友圈里晒美食、晒旅游、晒自拍等等约定俗成的处事方法，亦如我们生来就渴望被人理解、被人喜欢一样，而这一切更本质的原因，我们现在称为“刷存在感”，恰恰迎合了这个观点，我们想要在这个世界上留下我们的印记，即使这些方式方法看起来并不是我们最初喜欢的样子，弗洛姆将在这个观点理解为“超越自己”的追求，这一追求属于人的最基本要求，即“人对自己的纯生物作用不满，他不能忍受自己仅仅是被扔进这一世界的小卒。他一定要感到自己是创造者，是能超越处于被创造者消极地位的生命。满足这一要求有许多可能性，最自然和最基本的途径就是母亲对自己创造物的关怀和爱”。所以，我们渴望被人喜欢、被人理解都是因为我们希望生来独特、生来不同，而这一切都源于父母对爱的一种创造力。\n我们常常表现出，一种试图要要证明比别人过得更好的心态，仿佛在朋友圈或者微博这种社交平台上，我们能找到更多的自豪感，可我同样知道，这个世界上最大的社交网站 Facebook，背后却是由一个有社交障碍的人创造出来的，我们都渴望让别人了解自己、认识自己，这不同于社交场合里那种客套的场面话，对此，弗洛姆认为，人与人之间可以通过讲述这种方式来打破人与生俱来的这种孤独感，“讲述自己的生活，叙述自己的希望和恐惧，谈出自己幼稚的或者不成熟的梦想，以及找到面对世界的共同利益——所有这一切都是克服人与人之间隔离的途径，甚至表露自己的愤怒和仇恨，毫无顾忌地交心也都被看作是亲密的表现”，我们对爱的终极理解其实应该是，我们通过爱一个人，进而爱全人类，爱一切生命，我们从自我的生命的本质出发去爱对方，并且去体验对方的本质，爱情是意志的行为，是人作的一项把全部生命交付对方的决定。\n圣经中“爱他人如同爱己”的说法，说明了对自己的完整性和独特性的尊重，爱自己，理解自己同尊重、爱和谅解别人是不可分割的，爱我同爱他人是紧密相连的。中世纪德意志神秘主义哲学家和神学家，爱克哈特有一句关于自爱的格言：“你若爱己，那就会爱所有的人如爱己。你若对一个人的爱少于爱己，如果你不是爱所有的人如同爱己，如果你不是在一个人身上爱所有的人——因为这个人就是上帝和人。一个既爱自己又爱他人如同爱己的人就是这样的人，一个值得这样评价的人”。我们来到世界上学会与人相处、学会如何去爱一个人，其实是在寻找一种探索生活所需要的信仰，因为生存或许会非常容易，可是学会生活或许会非常困难，我们努力提高生活质量，源于我们对自己和伴侣的一种爱，希望让彼此变得更好。\n或许对弗洛姆本人而言，这本《爱的艺术》更像是 他对自我的一种内省，因为他的爱情基本上在持续地遭遇着失败，他曾经和四个不同的女人结过婚，所以他在这本书里提出的大量观点都来源自他自己的感情经历，他早期研究过弗洛伊德的相关理论，而事实上，因为对爱的无能为力让他真正找到了爱的能力，我们在年轻的时候总会遇到一个非常喜欢，可我们却无法给她想要的生活的女孩子，或许我们都需要用一生去领悟爱的真正含义吧，就像弗洛姆这本书是建立在将理论和实践结合起来的基础上的，我不认为我此刻已经读懂了这本书，可是它对我的确非常重要，我总要学着去爱别人，让自己变得更好！\n","date":"2016-09-24T22:42:44Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2275646954/","slug":"2275646954","tags":["爱情","读书","Kindle"],"title":"你了解爱的艺术吗？"},{"categories":["编程语言"],"content":"最近需要给公司内部编写一个随机生成人员名单的小工具，在解决这个问题的过程中，我认识到这是一个概率相关的问题，即使在过去我曾经设计过类似转盘抽奖这样的应用程序，可我并不认为我真正搞清楚了这个问题，所以想在这篇文章中说说我对概率问题的相关思考。首先，我们来考虑这个问题的背景，我们需要定期在内部举行英语交流活动，可是大家的英语水差异悬殊，所以如果按照常规的思路来解决这个问题，即认为每个人被选中的概率是相等的话，实际上对英语不好的人是显得不公平的。其次，作为一个内部活动它需要的是营造一种氛围，让每个人参与到其中，所以它要求英语好的人有一个相对高的优先级，这样能够方便在活动开始前“破冰”，可是同时它需要让英语不好的人能够参与其中，所以这个问题该如何解决呢？这就是我们今天想要讨论的话题！\n今天吃什么 虽然我的确是一个对穿衣吃饭没有太多追求的人，可是作为这尘世间芸芸众生里最为普通的一个人，我们每天不可避免地会遇到这个问题。如同哲学家会从“认识你自己”这样一个基本命题发散出无数哲学问题一样，“今天吃什么”在某种意义上可能是比哲学问题还要重要的问题，尤其是对一个喜欢美食的吃货来讲，“今天吃什么”可能是整个世界里优先级最高的事情。\n好了，我们从这个问题出发想要说明什么呢？通常我们解决这个问题最简单粗暴的方式是，罗列出附近所有的餐馆然后从中随机选择一家，现在主流的地图类 APP 基本上都有这样的功能，在每个餐馆被选中的概率相同的情况下，这种方案是没有什么问题的。可是我们都知道，人类作为这个世界上最复杂的一种动物，怎么会甘心让这个问题如此的简单呢？因为我们在选择的时候存在一个优先级的问题，例如我今天想吃红烧肉而明天想吃水煮鱼，显然因为个体偏好的差异每个餐馆被选中的概率是不相同的，因此在这种情况下，经典的概率理论是无法满足我们的要求的，那么此时该如何解决这个问题呢？\n考虑到个人偏好对餐馆是否被选中的影响比较明显，因此实际上不同的餐馆拥有不同的优先级，这里我们假定有 A、B、C 三家餐馆，以你的个人喜好作为优先级评价标准，其优先级分别为 2、3、5，则根据概率知识可知，P(A)=0.2、P(B)=0.3、P(C)=0.5。我们这里采用一种累加的思路来处理，令 P(A)=P(A)=0.2、P(B)=P(A)+P(B)=0.5、P(C)=P(B)+P(C)=1，此时我们将 P(A)、P(B)、P(C)三个概率值标注在数轴上，根据几何概型的相关理论，我们很容易地可以知道，0~0.2 范围内可以表示餐馆 A 被选中的概率、0.2~0.5 可以表示餐馆 B 被选中的概率，0.5~1 可以表示餐馆 C 被选中的概率，此时我们可以借助各种编程语言提供的随机数功能来生成 0~1 间的随机数，然后根据随机数落在哪个区间内来处理结果。\n我认为这个方案是比较不错的一种思路，在数学里有一种思想称为归一化，我们这里是将概率值从离散状态变为连续状态，而计算机更擅长我们生成一定范围内的随机数，所以这个方案为我们解决这类问题找到了一个不错的契合点，联想到转盘游戏其实是将 0~1 范围内的数字转换为 0~360 度范围内的角度，这一切就显得更加有趣啦！\n概率与累积概率 在解决了“今天吃什么？”这样一个终极命题后，下面我们来从理论上对这个问题进行解释。第一个问题，什么是概率呢？根据百科全书中的定义，概率是概率论中的一个基本概念，它是度量随机事件发生的可能性的一个量，通常使用 0~1 间的实数来表示一个随机事件发生的可能性的大小，当其值越接近 0 时表示该随机事件越不可能发生，当其值月接近 1 时表示该随机事件越有可能发生。经典的古典概型理论指出，如果一个实验满足同时下列两个条件，则这样的实验就是古典实验：\n实验只且只有有限个基本结果 每个基本结果出现的可能性相同 此时，对古典实验中的事件 A，其概率定义为：P(A)=m/n，其中 m 为事件 A 包含的基本结果的数目，n 为该实验中所有可能出现的基本结果的数目，这种概率定义的方法称为概率的古典定义。人们在重复实验的基础上进一步提出，在一定条件下，重复做 n 次重复实验，虽然实验次数的增加，如果某个事件的频率逐渐稳定在某一个数值 p 附近，则认为数值 p 即为事件 A 在该条件下发生的概率。这是建立在统计基础上的概率定义。显然我们发现这里存在问题，即古典概型是建立在所有事件发生的可能性相同这样一个基本假设的基础上的，于此同时我们可以注意到，概率是客观的而频率则依赖经验。对于概率甚至数学，人们一度认为它们都是严格的科学，对这种观点，我想引用庞加莱的一段话：\n概率仅仅是我们无知程度的度量，据定义，我们不晓得其定律的现象，都是偶然现象。\n好了，下面我们来关注一个新的概念，即累积分布函数(CDF，Cumulative Distribution Function)，它能够完整描述一个实数随机变量 X 的概率分布，是概率密度函数(pdf，probability density function)的积分。其数学定义是 F(X)=P(X\u0026lt;=x)，表示随机变量小于或者等于某个数值的概率。为什么我们需要连续的随机变量呢？因为计算机产生的随机数通常都是指某个范围内的随机变量，而通常意义上的古典概型实际上是一种离散分布的数学模型，显然这两者间需要某种形式上的转换，所以我们需要累积分布函数，并且对连续函数而言，所有小于或者等于某个数值 x 的概率都可以认为，它等于数值 x 处的概率，因为我们能够保证累积分布函数严格递增，印象中诸如正态分布、均匀分布、泊松分布都是可以采用这种思路来处理的。好啦，更多理论层面的内容大家有兴趣的话可以自己去探索，这里我们想在这种理论的基础上设计一个基本的抽奖系统。\n一个抽奖系统的设计 首先，我们必须指出计算机产生的随机数都是伪随机数，因此我们无法编写出 100%随机的程序，而且事实上这种“随机”程度对我们来讲应该是完全足够了，所以在排除了这个因素的影响以后，我们基本上不能再为我们的应用程序寻找任何的借口，在这里我认为重要的一点是，我们能够有一个在理论和实践上都相对可行的方案，我们这里选择以一个简单的抽奖系统的设计为例来探讨这个问题。\n随机概率公平吗 抽奖系统最重要的是什么呢？是公平合理，那么怎样保证每次抽奖对所有用户来讲都是公平的呢？我认为首先要能够指定一个公平合理的规则，因为规则就如同人世间的法律正义、就如同璀璨夜空中的星辰宇宙一样，当它被确定下来以后就会一种永恒的真理。我们现在来考虑这样一个概率问题，假设我们有 A、B、C、D 四种不同的物品，它们各自被选中的概率分别为 10%、20%、30%和 40%，我们应该如何解决这个问题呢？通常可以想到的一种方案是，因为这四种物品被选中的概率之和为 100%，因此我们将 0~100 范围内的数划分为[1,11)、[11,31)、[31,61)、[61,101)，我们注意到这四个区间都是左闭右开的，因此每个区间的长度和概率完全对应，此时我们产生一个(0,101)间的随机数，然后根据随机数落在那个区间内来判断抽取物品的结果，这种方法称为随机概率，我们来看看它是如何实现的：\nstatic void Main(string[] args) { List\u0026lt;Prize\u0026gt; prizes = new List\u0026lt;Prize\u0026gt;() { new Prize(\u0026#34;奖品A\u0026#34;,0.1d,1,11), new Prize(\u0026#34;奖品B\u0026#34;,0.2d,11,31), new Prize(\u0026#34;奖品C\u0026#34;,0.3d,31,61), new Prize(\u0026#34;奖品D\u0026#34;,0.4d,61,101) }; var seed = Guid.NewGuid().GetHashCode(); Random random = new Random(seed); int rand = random.Next(1, 101); var prize = prizes.Where(p =\u0026gt; rand \u0026gt;= p.RangeStart \u0026amp;\u0026amp; rand \u0026lt; p.RangeEnd).FirstOrDefault(); Console.WriteLine(\u0026#34;随机选取的物品为:\u0026#34; + prize.ID); } 这段代码是非常简单的，可我想要问的一个问题是，我们这样的做法到底对不对呢。面对一个概率学的问题，如果要检验我们的算法是否正确，一个最简单的方式是判断其是否符合我们的预期，因为从概率的定义中我们已经可以了解到，概率是一种数学定义中的概念，我们可以通过大量重复试验的客观性来证明概率的确是存在的，并且我们可以合理地解释它为什么这样，可是这样到底对不对，我相信没有人能够给出确定的答案。现在我们来借助计算机通过大量的重复实验来证明我们的方法是否正确，根据频率和概率的关系，我们知道频率应该会在概率的某一个范围内上下波动，但是它整体上会越来越接近于概率。下面的表格给出了我在这个问题上的试验结果：\n10 次 1000 次 1000000 次 物品 A 0.2 0.109 0.10066 物品 B 0.4 0.199 0.19977 物品 C 0.3 0.311 0.300071 物品 D 0.1 0.381 0.399499 这里因为博主在尝试做 1 亿次重复试验时电脑运行时间非常漫长，就像在电影《模仿游戏》中艾伦.图灵制作的计算机器在破解德国加密设备“恩格尼玛”时，常常需要长达数月的运算周期一样，这种类比可能不是非常恰当，因为现代计算机的硬件水平是图灵所处的二战时期难以企及的，可是我们忽然发现一个非常沮丧的事实，我们在这里处理 1 亿次左右的循环，依然需要一段我们感觉上非常“漫长”的时间，而根据电影中的情节，图灵制作的计算机器需要完成 159 亿次的运算来尝试各种可能的组合，所以此时此刻我们是应该向这些推动人类进步的杰出人物诚恳地致敬，因为我们今天的一切都是来自这些人在当时看似疯狂的举动，或许人们曾将他们视为疯子而我更喜欢将其视为天才。\n结果的确如我们所预期的那样，这里每组试验都是 3 次平行试验后的平均值，可以看到随着重复次数的增加，试验结果更加趋向我们理论上设计的概率值，因此这种情况下我们认为这个设计是合理的，即这个算法是公平的。可是这个世界让人头疼的一点是，我们每个人都理所当然地认为，只要是别人能得到而我们自己得不到地，就一定是有内幕、是不公平地，可是这个世界本来就是不公平的啊，谁掌握更多的社会资源、谁掌握绝对的话语权，“正义”的天平就会向谁倾斜，我们能做的无非是让自己变得更好，努力避免陷入某种被动的局面。\n好了，言归正传，现在我们会发现一个问题，这种方案在奖品种类非常多的情况下，调整概率会是一件非常困难的事情，这就像工程师不喜欢产品经理和游戏策划，其真实原因并非是工程师无法实现特定需求，而是在整个建筑完成规划和设计以后，频繁的需求变更让一座伟大的建筑变成了临时的脚手架，你必须认识到这是工程师经过创作以后的某种产出，你可以不在乎这些无人问津的代码，可是我作为工程师我一定要比任何人都要在乎啊。\n可是一个新的问题是，你永远无法为用户提供一种通用的解决方案，一个简单的抽奖在引入各种“自定义”规则以后，就注定不会再成为一个简单的抽奖，因为揣测一个人会说什么，对我这样一个不喜欢说话的人来说简直是种灾难，同样地，用户让计算机来做什么就应该明确地告诉计算机，而不是让工程师用各种各样的 if-else 来揣测用户想要做什么，我们常常说“优雅接口、肮脏实现”，在某种意义上就是指这种东西在永远浪费工程师的时间，用户愚蠢、用户懒惰，可是他们居然可以凌驾于工程师之上，这是对这个世界最大的恶意。好了，我们现在来动手解决新的问题，当用户需要在抽奖的时候对各种条件进行筛选同时还要考虑优先级和公平性这样一个问题吧！\n让一切可复用 首先我们来考虑，如何设计一个可以复用的抽奖系统，在这个问题中我们关注两点，第一，这个抽奖系统可以支持不同类型的“奖品”；第二，这个抽奖系统可以支持不同类型的“抽取”方式。因为在这个问题中，按照某种优先级随机抽取人员或者物品其实应该是一类问题，而抽取我们都知道应该有可放回抽取和不可放回抽取两种，所以我们可以考虑通过泛型和接口来实现这样的需求。我们在这里定义一个 IRankable 接口，所有的“奖品”都要实现该接口，其定义如下：\ninterface IRankable { int GetRank(); } 我们可以发现该接口中只有一个 GetRank()方法，这是因为我们这里的概率算法的基础是权重，所以我们只要为不同类型的“奖品”建立其相应的权重模型，就可以实现对不同类型奖品的支持。现在我们需要编写一个随机生成“奖品”的随机生成器，我们应该可以想到通过接口来约束泛型的思路，所以下面我们来实现一个随机生成器 RandomGenerator。\n首先我们可以想到的一点是，因为这里的泛型类型 T 需要实现 IRankable 接口，因此我们可以通过 IRankable 接口中定义的 GetRank()方法来获取不同奖品的权重，在此基础上我们对奖品按照权重进行分组，则我们可以计算出每种权重在整个奖品权重中占到的百分比，我们以此作为每种权重奖品的概率，利用累积概率的思想可以非常容易地获得各种权重奖品对应的概率范围。其代码实现如下：\n/// \u0026lt;summary\u0026gt; /// 计算概率 /// \u0026lt;/summary\u0026gt; private void CalculateProbability(IEnumerable\u0026lt;T\u0026gt; source) { this.m_groups = source.GroupBy(e =\u0026gt; e.GetRank()); //计算总权重 var totalRank = 0; m_source.ToList().ForEach((item) =\u0026gt; { totalRank += item.GetRank(); }); //计算每个权重对应的概率 m_probs = new Dictionary\u0026lt;int, double\u0026gt;(); foreach (IGrouping\u0026lt;int, T\u0026gt; group in m_groups) { var p = (double)(group.Key * group.Count() / (double)totalRank); m_probs.Add(group.Key, p); } //计算每个权重对应的累积概率(递增） var totalProb = 0d; m_totalProbs = new Dictionary\u0026lt;int, double\u0026gt;(); foreach (KeyValuePair\u0026lt;int, double\u0026gt; kv in m_probs) { totalProb += kv.Value; m_totalProbs.Add(kv.Key, totalProb); } } 好了，现在我们就获得了不同权重物品所对应的累积概率，即其概率范围，因此我们可以利用随机生成[0,1)范围内的随机数，然后判断随机数所在哪个概率范围内，我们就可以知道要对哪个权重分组中的奖品进行抽取，而对每个权重分组来说，因为其权重都是一样的，所以这里抽取试验可以认为是符合随机概率的，我们只需要从该分组中随机选取一个奖品返回就可以啦。那么这里该如何查找概率范围内，我们这里选择经典的“二分查找”算法：\n/// \u0026lt;summary\u0026gt; /// 返回概率所在的区间索引 /// \u0026lt;/summary\u0026gt; private int GetProbablityRange(Dictionary\u0026lt;int, double\u0026gt; totalProbs, int begin, int end, double value) { if (begin \u0026gt;= end) return begin; int mid = (begin + end) / 2; if (totalProbs.ElementAt(mid).Value \u0026gt;= value) return GetProbablityRange(totalProbs, begin, mid, value); else return GetProbablityRange(totalProbs, mid + 1, end, value); } 那么好了，现在我们该怎么从这些奖品中随机抽取一个奖品呢，我们这里提供了随机生成 1 个奖品和随机生成指定数目个奖品的方法重载，我们以前者为例来看看它的实现过程:\n/// \u0026lt;summary\u0026gt; /// 随机抽取一个奖品 /// \u0026lt;/summary\u0026gt; public T Generate() { //初始化随机数 var seed = Guid.NewGuid().GetHashCode(); var random = new Random(seed); //生成0到1间的随机数 double rand = random.NextDouble(); T result = default(T); //计算随机数落在哪个区间内 int index = GetProbablityRange(m_totalProbs, 0, m_totalProbs.Count - 1, rand); switch (m_option) { case GenerateOption.CanReplace: result = GenerateCanReplace(index, random); break; case GenerateOption.NoReplace: result = GetnerateNoReplace(index, random); break; } return result; } 在这里我设计了两种不同的抽取方式，即可放回抽取和不可放回抽取，两者的区别在于前者奖品池中奖品的数目保持不变，而后者奖品池中奖品的数目会发生变化，而更本质的区别在于前者奖品概率保持不变，而后者概率会发生变化。后者在每次抽取完以后需要将抽中的奖品从奖品池中取出，重新计算概率后方能进行下一轮抽取，所以这里我们直接给出这两两种抽取方法的代码实现，这里需要考虑的一个问题是，在抽取指定数目个“奖品”的时候我们通常不希望出现重复的元素，前者需要我们判断已抽取的奖品列表中是否存在指定元素，而后者因为抽取的奖品会被取出，所以不需要考虑这种情况的处理。\n/// \u0026lt;summary\u0026gt; /// 可放回抽取/// \u0026lt;/summary\u0026gt; private T GenerateCanReplace(int index, Random random) { int rank = m_totalProbs.ElementAt(index).Key; var group = m_groups.Where(e =\u0026gt; e.Key == rank).FirstOrDefault(); if (group == null) group = m_groups.ElementAt(random.Next(0, m_groups.Count())); return group.ElementAt(random.Next(0, group.ToList().Count)); } /// \u0026lt;summary\u0026gt; /// 不可放回抽取 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; private T GetnerateNoReplace(int index, Random random) { int rank = m_totalProbs.ElementAt(index).Key; var group = m_groups.Where(e =\u0026gt; e.Key == rank).FirstOrDefault(); if (group == null) group = m_groups.ElementAt(random.Next(0, m_groups.Count())); T result = group.ElementAt(random.Next(0, group.ToList().Count)); //从集合中移除当前抽取的元素 var list = m_source.ToList(); list.Remove(result); //更新集合、重新计算概率 m_source = list; CalculateProbability(m_source); return result; } 在此基础上实现指定数目的奖品就会变得非常简单啦，因为我们只需要重复调用这个方法就可以啦，那么现在我们该如何使用这个随机生成器呢？一起来看一段示例代码：\n//模拟优先级和员工级别对随机数的影响 List\u0026lt;Contract\u0026gt; contracts = new List\u0026lt;Contract\u0026gt;(); contracts.Add(new Contract() { Name = \u0026#34;People0\u0026#34;, Level = \u0026#34;Senior\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People1\u0026#34;, Level = \u0026#34;Senior\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People2\u0026#34;, Level = \u0026#34;SSE\u0026#34;, Priority = 10 }); contracts.Add(new Contract() { Name = \u0026#34;People3\u0026#34;, Level = \u0026#34;SSE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People4\u0026#34;, Level = \u0026#34;SSE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People5\u0026#34;, Level = \u0026#34;SE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People6\u0026#34;, Level = \u0026#34;SE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People7\u0026#34;, Level = \u0026#34;SE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People8\u0026#34;, Level = \u0026#34;SE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People9\u0026#34;, Level = \u0026#34;SE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People10\u0026#34;, Level = \u0026#34;Senior\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People11\u0026#34;, Level = \u0026#34;Senior\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People12\u0026#34;, Level = \u0026#34;SSE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People13\u0026#34;, Level = \u0026#34;SSE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People14\u0026#34;, Level = \u0026#34;SSE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People15\u0026#34;, Level = \u0026#34;SE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People16\u0026#34;, Level = \u0026#34;SE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People17\u0026#34;, Level = \u0026#34;SE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People18\u0026#34;, Level = \u0026#34;SE\u0026#34;, Priority = 1 }); contracts.Add(new Contract() { Name = \u0026#34;People19\u0026#34;, Level = \u0026#34;SE\u0026#34;, Priority = 1 }); RandomGenerator\u0026lt;Contract\u0026gt; generator = new RandomGenerator\u0026lt;Contract\u0026gt;(contracts,GenerateOption.NoReplace); var list = generator.Generate(10); foreach (var contract in list) { Console.WriteLine(contract.Name);} } 在这里 Contract 实现了 IRankable 接口，每个 Contract 的权重由 Level 和 Priority 两个属性来计算，示例中我们一次从集合中随机抽取了 10 个 Contract，而我们的算法能够保证它们都是不重复的，这个程序可以满足各种各样的抽取规则，比如按照 Contract 的口语、职位等不同的维度进行概率模型的建立即可，只要它实现了 IRankable 接口就可以使用这篇文章中的方法来随机抽取，这其实是一个业余时间的小项目啦，可我还是想让自己认真地考虑下这个问题，所以我花时间写了这篇文章，我对它的期望并没有太高，我喜欢将这些想法写下来而已。\n小结 或许有些时候我们对一个事情的态度，对事情最终的走向起不了决定性的作用，尤其是在我们没有掌握话语权的时候。可我在考虑这个方案的时候，是明显地意识到程序永远无法满足人类的脑洞的需要，所以当我们在做一件事情的时候，就应该有意识地让自己想到它可能会有扩展性上的需求，我认为这是我们在做项目开发过程中需要去关注的一个点，如何让你的代码具备扩展性和可维护性，虽然有时候你想得太多会造成过度设计，可是如果你在项目前期做出了一个糟糕的规划，到了后期遭遇项目需求变更的时候就会非常痛苦，可不幸的是我在最近同时遭遇了这两种情况，或许这就是我想要强迫自己写完这篇文章的原因吧，这篇文章足足花了我两周的时间，我的拖延症啊什么时候能好啊！\n","date":"2016-09-24T20:06:45Z","image":"/posts/3247186509/pins-1358849_1920.jpg","permalink":"https://qinyuanpei.github.io/posts/3247186509/","slug":"3247186509","tags":["概率","数学","算法"],"title":"一个关于概率的问题的思考"},{"categories":["生活感悟"],"content":"昨天下午，当她从公司办理完离职手续的时候，她在内部聊天工具上告诉我：“师父，我在公司的离职手续都办理完了，我要走了”。在那一瞬间，我突然非常平静地走过去对她说：“那就走吧”。我不知道她是不是想让我在她离开之前做些什么，或许这完全就是我的一厢情愿，因为她在此之前就明确地告诉我，她不喜欢我，所以这注定是一个悲伤的故事。一个月前，当她的同学从公司离职的时候，她就告诉我或许某一天她就离开这里了，当时我说我会想她的，而当她真正要离开的时候，我甚至都想不明白自己为什么会如此平静。下班路上同事 Kent 若有所思的告诉我，在我这个年龄“一见钟情”这种满足初恋情结的机会会越来越少，与其在感情的创伤中持续失落不如努力去争取一段新的开始。我惊诧于他突然间发现我隐藏在心底的秘密，我更加纠结于我丧失了喜欢一个人的能力。\n我像是忽然间发现我在感情上的天赋基本为负值似的，从我开始喜欢她开始，在这个过程中我感觉自己非常自卑，甚至当我真正站在她面前的时候，我常常会变得非常笨拙以至于完全不知道要说什么，我和她说我在她面前的时候有点儿“怕”她，她回复给我一个笑脸说：你可是我师父啊。你知道吗？当一个男生接受女生“成为朋友”这种现实的时候，从某种意义上来讲，他是将对她的喜欢以一种新的方式重新输出，就像她不在公司里的时候，她愿意委托我帮她处理某些事情，这对我而言是非常开心的一个时刻，因为你永远都期待着，她在有事情的时候能够第一个想到你，即使她曾经拒绝过你两次，即使她明确告诉你她不喜欢你，我曾经告诉过自己，喜欢是占有而爱是克制，至少曾经有很多很多的瞬间，我们两个人因为坦露心声而变得非常开心，这是我最让我怀念的瞬间。\n我承认我喜欢她是明显的“套路”感的，而我恰恰不是一个演技派，所以当我发现我在她面前变得笨拙的时候，我意识到我的情绪随时都可能因为她的一举一动而受到影响。我曾经因为她和某个男生一起而生她的闷气，可讽刺的是她告诉我在她心目中我比这个男生更受欢迎。同样地，这是一个悲剧的开始：你人挺好的，可我觉得我们不适合，我们还是继续做朋友吧！男人在年轻的时候，最难以接受而不得不接受的一个现实是，你非常喜欢一个人，可你此时此刻完全没有能力给她想要的生活，就像古古说她可能再遇不到我这样对她好的人，可她始终觉得我们两个人是没有未来的，而这一次，我忽然发现这一切是如此的相似，虽然我清楚地知道她们是完全不同的两个人，当她们同时对我说出“你会找到真正适合你的那个人”这句话的时候，我已无心分辨这句话的真假，我只知道我会陷入无休止的纠结，就像一个死循环导致的精神分裂。\n我是一个双子座男生，虽然我不是特别相信星座这种东西，可是通过知乎上的相关问题，我意识到双重人格可能是每一个双子座的终身宿命。同事说我走路的时候身体前倾、双手呈合拢状态，我承认这是因为我受到阿什顿.库彻主演的乔布斯同名传记电影的影响，在某种程度上我是在刻意模仿他的动作，所以当时我戏诩地回答道：因为我是风一样的男子啊。可事实是，无论是她还是我，我们都像风一样彼此走进各自的生命历程，然后又像风一样遁迹于无形，可是这一切真实存在过不是吗？她说我无法控制情绪生闷气的时候显得挺逗比的，所以从那个时候开始，我决定学会管理情绪。可我时常对自己产生怀疑， 比如我怀疑我是不是一个不懂得聊天的人，比如我怀疑我是不是自从古古离开我以后就失去了爱人的能力，比如我怀疑是不是太专注于技术而忽略了更多技术以外的东西等等，事实上这些问题确实存在，当你喜欢上一个人以后，你会发现你更加你身上的缺点会在瞬间被方大，大概这是我在她面前自卑的原因吧！\n所以双子座内心深处像是上帝打开了一扇新窗户，它是天使与魔鬼的结合。而更多的时候，我都被内心深处的两个灵魂左右者，我时常温柔、时常暴戾，就像阴晴不定的天气一样，我虽然讨厌这种严重分裂的性格，因为它常常让我在小事情上举棋不定，而一个成熟的男人是要学会当机立断的，可我同样知道，这是真实的我，不管它是好是坏，当你学会接受这一切的时候，你的灵魂和你的身体是完全同步的，所以这是我为什么讨厌虚伪的原因，因为它像藏在套子里的一个人你永远都不知道它哪一句话是真的。当你发现一个双子座男生对你忽冷忽热的时候，如果你信任他就让他自生自灭，当他安静下来像个孩子的时候，你会发现他的快乐和忧愁就像风一样转瞬即逝，所以我说：“我是风一样的男子”。而当他对你失去关注和热情的时候，他会使用最直接的话语告诉你他的真实想法，而这一切在我这里从来发生过，我希望所有被我这种阴晴不定的性格不经意间伤害过的朋友都能够原谅我，我从来都不是一个坏人，只是我的心里一直有两个人在打架。\n当我此时此刻写这篇博客的时候，我常常问自己，我还喜欢她吗？我确认我依然喜欢她，尤其是当我知道，她年龄比我大 1 岁以及她拥有硕士学位，这样两个事实以后，我更多的纠结和痛苦是来自来自我的否定，我们常常说爱情一半靠缘分一半靠争取，可当你真正试图走进一个人的生活的时候，你是否认真地考虑过两个人的未来，因为我不想重蹈覆辙。喜欢一个人并没有错，关键是你如何让两个人的世界巧妙地融合为一个世界，我时常自卑因为我对未来还没有确定。我喜欢过的人永远都会在我心底占据一片地方，因为我有严重的依赖性和怀旧心理，我可以去经常去的地方吃同样的东西，我可以和经常遇见的人说同样的话，对我而言，忠诚度和稳定性比新鲜感更为重要。或许我在生活和情感上的的确确存在弱点，很多时候我主观意识薄弱，对待事物的态度随意而温和，这在女生心目中可能会变成没有主见吧，其实在这些小事情上我一直都不怎么上心的，或许我需要改变吧！\n昨天晚上，不知道为什么突然特别难过，或许是因为被同事说中心事，或许是因为想到我喜欢过的女孩而心存不甘，或许是因为憎恶现在这个状态的自己\u0026hellip;\u0026hellip;总而言之，有那么一瞬间我感到我的心像是被挖去了一块，原来喜欢一个人可以让我如此难过，想到种种过往我以为自己会泣不成声，结果我声音哽咽着眼睛却是干的，或许我们最喜欢使用的那个“破涕为笑”的表情最能形容我当时的心境，我想了很久，一个人跑出去散步，当风轻轻地划过我的脸庞的时候，当秋天的寒意让我猝不及防的时候，我忽然觉得，当这个世界越发地充满“恶意”的时候，你更需要一个坚强地理由去生活，她们都教会我很多，和这个相比，悲伤和痛苦都显得微不足道，我想努力学好英语、我想努力练习写字、我想努力培养兴趣、我想努力克服恐惧，我是一个骄傲的人，在技术上是在生活上更应该是，如果我不能像编程一样掌控世界，我愿意在这纷繁的世界里不忘初心，喜欢和爱这种美好的情感，是不该让悲伤来消化的奢侈品。对未来的期望是让我变得优秀，如果在感情问题上做不了演技派，那我只好选择本色出演，真诚比套路更重要。\n着窗外像风一样飞驰的风景，我的思绪越飘越远，我不知道我想要表达什么，在那一刻我突然很想给她写一首诗，当别的女孩儿嫉妒我对她如宠溺一般的耐心时，我常常告诉她们，我曾经有一个不笨的徒弟，即使各种羡慕嫉妒恨，可我想说，这是我的心自己选择的啊，我不过是遵从它的意志，让这一切变得温暖美好起来：\n你像一阵风/我伸开手 抓到的只有/花落后的芬芳 我离你/时而很近/时而很远 像天上的云/舒卷成蝶 我不想去采摘它啊 月亮静待着它怒放 我愿意守护它一刻 即使它一直当月亮是太阳\n2016 年 9 月 10 日 Payne\n","date":"2016-09-10T19:29:57Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/21112647/","slug":"21112647","tags":["感悟","成长","爱情"],"title":"一见钟情，无疾而终"},{"categories":["编程语言"],"content":"各位朋友大家好，我是 Payne，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。最近因为项目上的原因开始接触 WPF，或许这样一个在现在来讲显得过时的东西，我猜大家不会有兴趣去了解，可是你不会明白对某些保守的项目来讲，安全性比先进性更为重要，所以当你发现银行这类机构还在使用各种“复古”的软件系统的时候，你应该相信这类东西的确有它们存在的意义。与此同时，你会更加深刻地明白一个道理：技术是否先进性和其流行程度本身并无直接联系。由此我们可以推论出：一项不流行的技术不一定是因为它本身技术不先进，或许仅仅是因为它无法满足商业化的需求而已。我这里的确是在说 WPF,MVVM 思想最早由 WPF 提出，然而其发扬光大却是因为前端领域近年来比较热的 AngularJS 和 Vue.js，我们这里表达的一个观点是：很多你以为非常新潮的概念，或许仅仅是被人们重新赋予了新的名字，当你理清这一切的来龙去脉以后，你会发现这一切并没有什么不同。这符合我一贯的主张：去发现问题的实质、不要被框架束缚、通过共性来消除差异，所以在今天这篇文章里，我想说说 WPF 中 MVVM 模式下命令与委托的关系。\n什么是 MVVM 既然提及 MVVM，那么我们就无可避免的需要知道什么是 MVVM。我们在本文开篇已经提到，MVVM 这个概念最早由微软提出，具体来讲是由微软架构师 John Gossman 提出的。我个人更喜欢通过将 MVC、MVP 和 MVVM 这三者横向对比的方式来加强理解，因为这从某种意义上来讲，这是一个逐步改进和演化的过程。我们常常谈及软件的三层架构，我们常常对 MVC 耳濡目染以致将其神化，可事实上它们是某种在思想上无限接近的理念而已。\nMVC模式示意图\r首先，我们从最简单的 MVC 开始说起，作为最常用的软件架构之一，我们可以从上面的图示中看到，MVC 其实是非常简单的一个概念，它由模型(Model)、视图(View)和控制器(Controller)三部分组成，建立在一个单向流动的通信基础上，即 View 通知 Controller 响应用户请求，Controller 在接到 View 的通知后会更新 Model 内的数据，然后 Model 会将新的数据反馈给 View。我们发现这个设计可以使软件工程中的关注点分离，我们注意到通过 MVC 模式，我们实现了视图和模型的分离，通过控制器这个胶水层让两者间接联系起来，所以 MVC 的优点是让各个模块更好的协作。那么，它的缺点是什么呢？显然，视图和控制器是高度耦合的，因为控制器中无可避免地要访问视图内的元素，所以控制器注定无法在这尘世间独善其身。要知道最早的 MVC 架构是基于观察者模式实现的，即当 Model 发生变化时会同时通知 View 和 Controller，所以我们很快就可以认识到：我们从古至今的所有努力，都是为了让视图和模型彼此分离，我们在这条路上越走越远，幸运的是一直都不忘初心。\nMVP模式示意图\r接下来，我们为了彻底地让视图和模型分离，我们发明了新的软件架构：MVP。虽然从感性的认识上来讲，它是将 Controller 改名为 Presenter，然而从理性的认识上来讲，它在让视图和模型分离这件事情上做得更为决绝果断。通过图示我们可以发现，视图和模型不再发生直接联系，它们都通过 Presenter 相互联系，而且各个部分间的通信都变成了双向流动。我们可以很快意识到，现在全新的控制器即 Presenter 会变得越来越“重”，因为所有的逻辑都在这里，而视图会变得越来越“轻”，它不再需要主动去获取模型提供的数据，它将被动地接拥抱变化，因为现在在视图里基本上没有任何业务逻辑。现在我们可以预见，人类会在隔绝视图和模型这件事情上乘胜追击，人们会尝试让 Controller/Presenter/ViewModel 变得越来越臃肿，我想说的是，求它们在得知这一切真相时的心理阴影面积，我们试图让每一个模块各司其职、通力协作，结果脏活累活儿都交给了 Controller/Presenter/ViewModel，我想说这件事情做的真是漂亮。\nMVVM模式示意图\r历史总是如此的相似，人类在作死的道路上匍匐前进，继续发扬改名的优良传统，这一次是 Presenter 被改名为 ViewModel，在命名这件事情上，我认为程序员都是有某种强迫症因素在里面的，所以当你发现一个事物以一个新的名字出现在你的视野中的时候，通常它会有两种不同的结局，第一，陈酒换新瓶，我们贩卖的不是酒是情怀；第二，看今天的你我怎样重复昨天的故事，我这张旧船票还能否登上你的客船。幸运的是，MVVM 相对 MVP 的确发生了些许改变，一个重要的特性是双向绑定，View 的变化将自动反映在 ViewModel 中，而显然 ViewModel 是一个为 View 打造的 Model，它可以容纳更多的普通的 Model，因此从某种意义上来说，ViewModel 依然作为连接 View 和 Model 的桥梁而出现，它是对 View 的一种抽象，而抽象有两层含义，即数据(Property)和行为(Command)，一旦你明白了这一点，ViewModel 无非是一个特殊而普通的类而已，特殊是因为它需要实现 INotifyPropertyChanged 接口，普通是因为它继承了面向对象编程(OOP)的基本思想。\n更像 MVC 的 MVVM 到现在为止，我们基本上理解了 MVC、MVP 和 MVVM 这三者间的联系和区别，可是这样真的就是最好的结果吗？我们首先来思考一个问题，即什么样的代码应该写在控制器里。比如我们在对项目进行分层的时候，到底应该让控制器负责哪些任务？我们可以让 Controller 处理单独的路由，同样可以让 Controller 参与视图逻辑，甚至我们在编写 Model 的时候，我们可以有两种不同的选择，第一，编写一个简单的数据聚合实体，具体逻辑都交给控制器来处理，我们将这种方式称为贫血模型；第二，编写一个持有行为的数据聚合实体，控制器在业务逻辑中调用这些方法，我们将这种方式称为充血模型。所以，在这里我们纠结的地方，其实是选择让控制器更“重”还是让模型更“重”，我曾经接触过 1 年左右的 Android 开发，我认为 Android 工程是一个相对符合 MVC 架构的设计，可是我们难免会发现，作为控制器的 Activity 中的代码非常臃肿，因为我们在这里需要和视图、模型关联起来，所以综合现有的这些软件架构思想，我们发现模型和视图相对来讲都是可以复用的，可是作为连接这两者的 Controller/Presenter/ViewModel 是非常臃肿而且难以复用的，所以我怀疑我们是否是在真正的使用 MVVM。\n我不知道 MVVM 架构正确的使用方法是什么样的，因为这是我第一次接触到这样一个新的概念，就如同很多年前，我在学校图书馆里看到的一本讲 Web 开发的书中描写的那样：当我们不了解 MVC 的时候，我们理所当然地认为通过文件夹将项目划分为 Model、View、Controller，这样好像就是 MVC 啦。可是事实真的是这样吗？以我目前公司项目的情况而已，我认为它更像是使用了双向绑定的 MVC，因为你经常可以在 ViewModel 中看到，某个属性的 Get 访问器中各种被 if-else 折磨的“脏”代码，而在 ViewModel 中我基本上看不到 Model 的身影，并且因为使用了 Binding 的概念严重弱化了 ViewModel 作为类的基本属性，因此它没有构造函数、没有初始化，我们可以在 Get 访问器中看到各种硬编码，因为视图上的需求经常变动，所以当整个项目结束的时候，我本人是非常不愿意去看 ViewModel 这部分的代码的，因为项目上要求避免写 Code-Behind 代码，所以大量的事件被 Command 和 UIEventToCommand 代替，这样让 ViewModel 变得更“重”了。原本我们希望的是让这三者各司其职，结果现在脏活累活儿全部变成了 ViewModel 一个人的。虽然双向绑定可以避免去写大量赋值语句，可是我知道 ViewModel 内心深处会表示：宝宝心里苦。\n如果说 WPF 对技术圈最大的贡献，我认为这个贡献不在双向绑定，而是它真正意义上实现了设计和编程分离，我们必须承认设计和编程都是一项创造性活动，前者趋向感性，而后者趋向理想，在没有实现这两者分离的时候，程序员需要花费大量时间去还原设计师的设计，可是对程序员来讲，一段程序有没有界面设计在某些场合下是完全不重要的，在没有界面设计的情况下，我们可以通过单元测试来测试代码的可靠程度，相反地在有了界面设计以后我们反而不容易做到这一点，所以你问我 WPF 对技术圈最大的贡献是什么，我会回答它解放了程序员，可以让理性思维去做理性思维更适合的事情。我不太喜欢声明式编程，这里是指 WPF 中 XAML 这种继承自 XML 的标记语言，因为 Visual Studio 对 XAML 没有提供调试的支持，所以当你发现视图显示出现问题的时候，你很难分清楚是前台视图绑定出现错误还是后台 ViewModel 出现错误，只要你输入符合 XML 规范的内容程序都会编译通过而非引发异常，因为它是用反射所以性能问题广为人所诟病，其次 ViewModel 中通知前台属性发生变化时需要使用 OnPropertyChanged，该方法需要传入一个字符串类型的值，通常是指属性的名称，可是如果你定义了一个字符串类型的属性，当你在这里传入这个属性的时候，因为它是字符串类型所以不会引发编译错误，可是我觉得这个东西还是比较坑。\n委托与命令 好了，现在我想说说 WPF 中的命令和委托，事实上在我计划写这篇文章前，我对这里无比好奇，可当我发现这东西的实质以后，我忽然觉得花费如此大的篇幅来讲解这样一个概念，这是不是会显得特别无聊。我们的项目上使用的是一个叫做 MVVM light 的框架，当然我们没有使用它的全部功能，公司的前辈们非常猥琐地从这个开源项目中挑了些源代码出来，这里我不想提及关于这个框架本身地相关细节，因为我认为理解问题的实质比学会一个框架更加重要。首先，WPF 为每一个控件都提供了一个 Command 的依赖属性，因为任何实现了 ICommand 接口的类都可以通过绑定的方式和前台关联起来，我们这里对比下命令和路由事件的区别可以发现，路由事件必须写在 Code-Behind 代码中，而命令可以写在 ViewModel 里，所以直观上来讲命令更加自由灵活。下面我们以一个简单的例子来剖析这两者间的关系。\n我们知道使用 Command 需要实现 ICommand 接口，所以实现起来是相对容易的，我们这里继续沿用 MVVM light 中的 RelayCommand 这个名字：\npublic class RelayCommand : ICommand { private readonly Action\u0026lt;object\u0026gt; m_execute; private readonly Predicate\u0026lt;object\u0026gt; m_canExecute; public RelayCommand(Action\u0026lt;object\u0026gt; execute) { this.m_execute = execute; } public RelayCommand(Action\u0026lt;object\u0026gt; execute, Predicate\u0026lt;object\u0026gt; canExecute) { this.m_execute = execute; this.m_canExecute = canExecute; } public bool CanExecute(object parameter) { if (m_canExecute == null) return true; return m_canExecute(parameter); } public event EventHandler CanExecuteChanged { add { CommandManager.RequerySuggested += value; } remove { CommandManager.RequerySuggested -= value; } } public void Execute(object parameter) { this.m_execute(parameter); } } 我们可以看到这里有两个重要的方法，Execute 和 CanExecute，前者是一个 void 类型的方法，后者是一个 bool 类型的方法。当我们需要判断控件是否应该执行某一个过程的时候，CanExecute 这个方法就可以帮助我们完成判断，而 Execute 方法显然是执行某一个过程的方法，可以注意到通过委托我们让调用者更加自由和灵活地传入一个方法，这是我喜欢这种设计的一个地方，因为我的一位同事就对普通的路由事件表示无法理解。\n这里需要说明的是 CanExecuteChanged 这个事件，这个和 INotifyPropertyChanged 接口中的 PropertyChanged 成员类似，是在当 CanExecute 发生变化的时候通知视图的，我对这里的理解是 CanExecute 本身就具备对某一个过程是否应该被执行的支持，可是遗憾的是在，在我参与的项目中，人们更喜欢声明大量的布尔类型变量来处理这里的相关逻辑，因此无论是对 Property 还是 Command 而言，在 ViewModel 里都是看起来非常丑陋的代码实现。\n好了，现在对我们而言，这是一个非常愉快的旅程，因为在完成对 RelayCommand 的定义以后，我们绑定命令和定义命令的过程是非常简单的。除此以外，WPF 提供了一个 RoutedCommand 类，该类实现了 ICommand 接口，我怀疑 MVVM light 中的 EventToCommand 正是通过这种思路实现了路由事件到命令的转换，因为只有 RoutedCommand 具备访问 UI 事件的能力，这里我们仅仅提出问题，进一步的思考和验证我们可以留到以后去做。下面我们来看看如何声明和绑定命令：\npublic RelayCommand ClickCommand { get { return new RelayCommand((arg)=\u0026gt; { MessageBox.Show(\u0026#34;Click\u0026#34;); }); } } 显然这个 ClickCommand 将作为一个属性出现在 ViewModel 中，我选择了一个我最喜欢用的方法，或许这样看起来非常低端。可是在调试界面的过程中，它要比断点调试更为直接和直观。当我们的 ViewModel 中出现这样的只读属性的时候，直接在 Get 访问器中定义它的返回值似乎是最直接有效的方案，可问题是 Get 访问器应该是非常“轻”的，因为大量业务逻辑的渗透，现在连这里都不能保留其纯粹性了吗？这让我表示非常郁闷啊。\n\u0026lt;Window x:Class=\u0026#34;WPFLearning.Window1\u0026#34; xmlns=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml/presentation\u0026#34; xmlns:x=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml\u0026#34; Title=\u0026#34;Window\u0026#34; Height=\u0026#34;300\u0026#34; Width=\u0026#34;300\u0026#34;\u0026gt; \u0026lt;Grid\u0026gt; \u0026lt;Button Content=\u0026#34;Button\u0026#34; HorizontalAlignment=\u0026#34;Center\u0026#34; VerticalAlignment=\u0026#34;Center\u0026#34; Command=\u0026#34;{Binding ClickCommand }\u0026#34;/\u0026gt; \u0026lt;/Grid\u0026gt; \u0026lt;/Window\u0026gt; 现在你可以发现，委托和命令结合得非常好，当你发现这一切如此美妙的时候，回归本质或许是我们最喜欢的事情，就像纯粹的你我一样，在这个世界上，我们彼此装点着各自生命里美好的风景，执著而勇敢、温暖而明媚，那些周而复始的日子里，总能听到梦想开花的声音。\n小结 在这篇文章里我们讨论了 MVC、MVP、MVVM 各自架构变迁的前因后果，由此我们知道了软件设计中，一个典型的设计目标是让视图和模型分离，可我们同样发现，带着这个目标去设计软件的时候，我们基本鲜有更换视图的时候，虽然从理论上来讲，所有的业务逻辑都是在 ViewModel 中，视图和模型应该是可以进行更换的，可是你告诉我，有谁会为同一个软件制作不同的界面呢？难道我们还能期望通过一个静态工厂，来为不同的平台返回不同的视图，然后理论上只要适配正确的控制器就可以实现软件对不同平台的“自适应”，可是软件开发领域发展至今，最有可能提供完整跨平台方案的 Web 技术目前都无法满足这个需求，所以我们是否应该去怀疑这个设计的正确性呢？同样的，以 Java 的 SSH 三大框架为代表的“配置文件”流派，认为应该将数据库的相关信息写在配置文件里，这样可以满足我们随时切换到不同数据库产品上的需要，可是你告诉我，这样的应用场景多吗？所以，技术本身的设计并没有问题，我们需要思考的是，是否应该被框架和架构束缚，说到底我们是为了设计出更棒的软件产品，以此为目标，其实框架和架构更应该衍生为一种哲学意义上的思想，我们想让每一行代码都充满智慧的光芒，它骄傲却不孤独，因为总有人理解它、懂它。\n","date":"2016-07-21T14:27:07Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/569337285/","slug":"569337285","tags":["MVVM","委托","命令"],"title":"浅析 WPF 中 MVVM 模式下命令与委托的关系"},{"categories":["Unity3D"],"content":"各位朋友大家好，欢迎大家关注我的博客，我是 Payne，我的博客地址是http://qinyuanpei.com。最近一位朋友问我，如何在 Unity 引擎中实现类似《英雄联盟》中选择皮肤时的 3D 滚动视图效果，虽然我非常不喜欢这个游戏，可是大学四年在宿舍里被周围同学们耳濡目染，对这个游戏中常见英雄的口头禅还是颇为熟悉的，曾经在周围同学的“硝烟”和“噪杂”中熬夜编程，此时此刻想起来大概是最能让我怀念和骄傲的记忆了。剑圣说“你的剑就是我的剑”，伊泽瑞尔说“是时候表演真正的技术了”，杰斯说“为了更美好的明天而战”……或许曾经的某一瞬间，我们曾经有过类似的让你我疯狂着迷的人生信条，可是不管怎样，我希望我们可以将这些永远地铭刻在心里，如同心中栽种下一棵红莲，在黑夜中静静地等待开放，这样当此去经年亦或时过境迁的时候，我们不会说是时光抹去了你我年轻的棱角，因为我相信真正的棱角会因为磨砺而变得更加明亮，绝对不会因为此刻的苟且就变的麻木甚至迷茫。好了，喝完我这碗心灵鸡汤，下面我们来一起学习如何在 Unity3D 中使用 uGUI 实现 3D 滚动视图效果。\n需求分析 首先，我们先来对这个需求进行分析，从这篇文章的题目我们获得的一个关键信息是，希望通过某种方式实现 3D 滚动特效。因此我们首先要解决的一个问题是，我们应该采用 2D 方式来实现还是采用 3D 方式来实现这种界面效果。我们假定这里希望实现的效果如下图所示，我们可以注意到从这张图片的设计初衷来看，它更像是一种介绍产品特性的文案设计，我们这里仅仅是想通过这张图告诉大家，我们需要实现一个什么样的效果。软件开发过程中最大的成本在我看来主要来自沟通。因为事实上对普通用户而言技术并不重要，重要的是能否实现用户想要的功能，可是大部分情形是用户并不知道自己想要什么，除非你将实际的产品放到用户眼前甚至手中。好了，在对需求有了一个基本的印象以后，我们来思考如何实现这个需求。\n具体来讲，我们有两种思路：\n其一是采用真实的 3D 来制作，即我们通过一个圆柱体或者是多棱柱将图片\u0026quot;粘贴\u0026quot;在不同的面上，通过对圆柱体或者多棱柱进行旋转，然后以真实的 3D 的形式来呈现给用户。 其二是采用伪 3D 来制作，即我们通过在 2D 平面内对图片的层次进行合理化调整实现伪 3D 效果，配合插值、缩放等技巧来实现 2D 平面上的旋转，然后给用户一种视觉上的 3D 效果。 需求设定\r核心原理 在这里我们选择采用伪 3D 来制作，为什么选择这种方案呢？因为它简单啊，哈哈。好了，我们现在将实际的需求进行抽象，我们会发现什么呢？我们注意到这本质上是一个曲线问题，我们可以将每个图片的中心用平滑的曲线连接起来，然后我们就得到了一条抛物线或者是圆锥曲线或者是贝塞尔曲线，在这里我们将其理解为什么样的曲线并不重要，因为这最终影响到的是曲线的平滑度问题，即细节上的调整。沿着这个思路，我们就意识到，这是一个根据曲线平均分布坐标点的过程，假设我们这里 5 张图片，并且曲线在中间位置可以找到一条垂直的对称轴，那么我们只需要将这 5 个点在水平方向上平均分布即可，事实上根据人类视觉的特点，这个距离应该是越来越小的，就像我们看到的一排并列的树木，越远的地方它们的间距会越来越小，而事实上它们的间距是一样的，根据这个特性我们可以表现出这种视觉上的纵深的感觉，在实际项目中它取决于美术设定和策划设定，我们这里就从最简单的情况开始分析。\n好了，在解决了精灵放置的这个问题以后，我们接下来要解决的是什么呢？答案是精灵的层级，因为层级能够帮助我们营造一种视觉上的层次感和立体感，比如在跑酷游戏中我们常常使用视差滚动这种技术来表现 3D 效果，以及传统的斜 45 度瓦片地图来实现 2.5D 效果都是使用 2D 来模拟 3D 效果的经典案例。所以在这里除了确定每个精灵的放置位置以外，我们还有一个问题，如何对这些精灵进行排序，所幸的是在 uGUI 中我们可以通过 SetSiblingIndex 方法来设置一个精灵的深度，当每次通过按钮切换精灵的时候，我们都需要对所有精灵重新计算坐标和深度，而为了更好的视觉表现力，我们可以在切换的时候做一个简单的位移动画，至此我们就可以开始动手实现功能啦。\n具体实现 首先我们来搭建一个基本的场景，我们这里将一切浮华褪尽，我们可以看到在场景中有两个按钮，它们可以让我们当前选中的卡片，而界面底部的标签会显示我们当前选择的角色名称。虽然在这里采用触屏滑动的效果更好，可我们这里主要的目的是为了说明如何实现我们的思路，当引入这部分功能的设计以后，会增加大家在整体理解上的难度，所以我们这里以快速实现功能为主。注意到场景中的卡片此时都是相当“任性”地放置在界面上，这是因为我们稍后会采用算法计算每个卡片的实际位置，所以在这里完全可以忽略其“美观性”。\n场景展示\r这里，我们设定场景的大小为 800x460，那么在这种情况下，我们可以按照下面图中所示的曲线轨迹来构造一条曲线，考虑到椭圆方程比贝塞尔曲线更加简单易用，所以我们这里选择椭圆方程来作为场景中这些卡片排列的曲线方程。\n曲线方程\r此时以屏幕中心为原点构建平面直角坐标系，则这个椭圆是一个以长轴 2A=400、短轴 2B=640、中心在(0,320)上的椭圆。根据这个原理，我们可以将其代码实现分为三个步骤来实现。首先，我们将场景中的所有卡片存储在 GameObject 数组中，这里我们这里规定卡片的数目必须为奇数，然后我们从左到右依次计算每个卡片的位置和深度，这样就可以让卡片按照我们期望的方式进行排列啦。下面一起来看代码如何实现：\n//初始化精灵数组 int childCount = transform.childCount; //计算两侧精灵数目 halfSize = (childCount-1) / 2; //初始化精灵 sprites = new GameObject[childCount]; for(int i = 0; i \u0026lt; childCount; i++) { sprites[i] = transform.GetChild(i).gameObject; SetPosition(i); SetDeepin(i); } 这里 sprites 显然是一个 GameObject[],因为卡片的数目为奇数个，所以 halfSize 是指中间位置卡片的索引，这里需要两个辅助方法，SetPosition 和 SetDeepin，从名字我们就知道这两个方法分别是设置卡片位置和设置卡片深度。当我们提到代码注释的时候，好多人以代码自注释为理解逃避注释，孰不知这建立在命名规范的基础上，如果你连这点基本的要求都做不到，我建议你还是多写点注释、少写点代码。好了，这两个方法的实现细节如下：\n/// \u0026lt;summary\u0026gt; /// 设置精灵位置 /// \u0026lt;/summary\u0026gt; private void SetPosition(int index) { //计算第index个精灵的角度 float angle = 0.0f; if (index \u0026lt; halfSize) { angle = startAngle - (halfSize - index) * DeltaAngle; } else if (index \u0026gt; halfSize) { angle = startAngle + (index - halfSize) * DeltaAngle; } else { angle = startAngle; } //计算第index个精灵的坐标 float x = A* Mathf.Cos((angle/180) * Mathf.PI) + Center.x; float y = B* Mathf.Sin((angle/180) * Mathf.PI) + Center.y; Vector3 v3 = Camera.main.WorldToScreenPoint(new Vector3(x,y,0)); v3 = Camera.main.ScreenToWorldPoint(v3); Vector2 v2 = new Vector2(v3.x,v3.y); sprites[index].GetComponent\u0026lt;RectTransform\u0026gt;().anchoredPosition = v2; } 可以注意到，在这里我们根据精灵索引 index 和两侧精灵数目 halfSize 的关系，按照 DeltaAngle 这个增量来计算每个精灵实际的角度，在此基础上结合椭圆的参数方程，我们可以非常容易地计算出每个精灵实际的位置，这样就可以保证精灵中心都在椭圆曲线上。好了，接下来我们会遇到一个新的问题，这些精灵的层级应该是从中间位置向两边依次递减的，所以为了解决这个问题，我们还需要对每个精灵的层级进行计算，这部分代码的实现细节如下：\n/// \u0026lt;summary\u0026gt; /// 设置精灵深度 /// \u0026lt;/summary\u0026gt; private void SetDeepin(int index) { //计算精灵深度 int deepin = 0; if(index \u0026lt; halfSize) { deepin = index; } else if(index \u0026gt; halfSize) { deepin = sprites.Length - (1 + index); } else{ deepin = halfSize; } sprites[index].GetComponent\u0026lt;RectTransform\u0026gt;().SetSiblingIndex(deepin); } 事实上，我在这里并不清楚 SetSiblingIndex 这个方法的真正作用:)，可是它的确能够实现我们想要的功能。有时候在维护一个古老的项目的时候，可能你会在代码中看到各种有趣的注释，而这些注释中有相当一些都充满了一种“形而上学”的味道在里面，我们不知道这个世界为什么会是这样，可是看起来它们都运行地非常良好。或许这就是这个世界的奇妙之处，无论我们是否想要尝试打破这些规则，这个世界上总是有些我们难以理解的东西存在，可是存在即合理，不是吗？理性思维的缺陷在于想要为一切问题找到一个答案，所以这次苏格拉没有底，我们就感性一次又何妨呢，这个问题就让它没有答案吧！\n现在，显然我们需要解决一个新的问题，就像上帝在我们关上一扇门的同时，会为我们开启一扇窗口。理论上任何问题都可以通过引入一个中间层来解决，而引入中间层的同时毫无疑问地引入了一个新的问题。在这里我们已经完成了让所有精灵按照椭圆曲线进行排布以及精灵的层级关系这两个问题，可是我们这是一个静态的过程啊，我们需要的是让它能够滚动起来，所以怎么解决这个问题呢？我们可以注意到的一点是，精灵的这种“滚动”效果，实际上是将数组中的第一个元素 sprites[0]或者最后一个元素 sprites[sprites.Length-1]，依次和数组中的第 i 个元素进行交换。比如精灵整体向右侧“滚动”，我们只需要从第一个元素开始依次和最后一个元素进行交换就可以啦，所以这里的实现实际上是：\n/// \u0026lt;summary\u0026gt; /// 向后翻页 /// \u0026lt;/summary\u0026gt; public void OnNext() { int length = sprites.Length; for (int i = 0; i \u0026lt; length; i++) { GameObject temp = sprites[i]; sprites[i] = sprites[length - 1]; sprites[length-1] = temp; } for (int i = 0; i \u0026lt; length; i++) { SetPosition(i); SetDeepin(i); } } 我们在对数组内的元素重新组织后，需要重新计算每个精灵的位置和深度。我这里在思考的一个问题是：精灵的位置和深度实际上是确定的，所以我们可以考虑将它们存储起来“复用”，这样可以减少每次的重复计算。其实，代码的优化和重构是一个需要时间来酝酿的过程，没有人能够在写代码的时候，就可以意识到代码中的瑕疵，而这种发现问题的眼光通常需要长时间的培养，这是我们之所以提倡不要过早优化的原因，除非你能够快速地找到代码中的优化点。好了，现在采用类似的思路，我们可以实现向前翻页的逻辑啦，这里的代码非常简单不再赘述。\n好了，现在我们可以看看到目前为止我们实现了一个怎样的功能吧！\n效果展示\r其实这篇文章我还想继续再往下写的，可是因为我比较懒一直拖着不写，以及接下来相当多的内容都是和界面相关的东西，所以我决定这篇文章就暂时写到这里，目前这个方案可以实现一个简单的“3D”滚动的效果，按照这个思路，接下来我们要做的事情是让滚动更加平滑以及支持鼠标或者触屏操作，毕竟这个需求的出发点是来自一个游戏，所以我们可以考虑在“滚动”的时候增加插值特性，与此同时，为了让它更加具有“3D”的感觉，可以在设置精灵层级的时候为不同的精灵设置不同的缩放比例，这样会更加符合美术中的透视关系，效果应该会更好吧！我认识的一位朋友使用 uGUI 中原生控件 ScrollRect 实现了类似的功能，感觉她还是非常厉害的啊，果然我不再从事 Unity 开发以后，我在这块的技术完全跟不上整个技术圈的节奏啊。\n小结 本文介绍了一种基于曲线方程来构建伪 3D 效果的思路，主要借助椭圆的参数方程来计算精灵位置，使其实现按照椭圆曲线进行排布的效果，在此基础上配合层级调整、插值、缩放等技巧，在一定程度上可以实现 2D 平面内的伪 3D 旋转效果。因为博主身患拖延症晚期，所以这篇文章在拖延了很久以后，终于成功的成为了一个没有填完的坑，不过我相信掌握原理比获取代码更为重要，所以这篇文章更多的是希望能给大家提供相关思路，博主在这篇文章中没有实现的功能，各位读者有兴趣的话可以考虑自行实现，写完这篇文章表示心好累，好了，就这样吧,各位晚安！\n","date":"2016-07-10T14:29:33Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1150143610/","slug":"1150143610","tags":["游戏开发","uGUI","Unity3D"],"title":"在 Unity3D 中使用 uGUI 实现 3D 旋转特效"},{"categories":["Unity3D"],"content":"各位朋友大家好，欢迎关注我的博客，我的博客地址是：https://blog.yuanpei.me。最近因为受到工作上业务因素影响，所以博主在 Unity 引擎上的研究有所停滞。虽然目前的工作内容和 Unity3D 没有直接的关联，可是我觉得工程师应该有这样一种情怀，即工作和兴趣是完全不同的两个概念。编程对我而言，首先是一种兴趣，其次是一份工作。所以我宁愿在每天下班以后继续研究自己感兴趣的东西，而非为了取悦这个世界、为了加班而加班。最近广电总局让整个游戏行业都坐立不安了，因为其新发布的一系列规定，让中国的独立游戏开发者怨声载道。可是我们更应该看到积极的一面是，无数的小游戏公司会在最近数月内大量消失，或许对中国野蛮生长的游戏行业这是一次“形式”上的整顿，可对我们开发者来说，在这个过程中努力提升自我、巩固基础永远比追求时髦、流行的技术或者框架有意义的多，因为热闹的从来都是昙花一现般的璀璨，而永恒的永远都是历久弥新的真理。好了，闲言少叙，今天我们的话题是在 uGUI 中使用不规则精灵制作按钮。\n从用户体验说起 我们都知道在现代应用程序设计中，用户体验(UX)和用户界面(UI)是两个非常重要的内容。为什么用户体验(UX)和用户界面(UI)会显得如此重要呢？这是因为从普通用户的角度来讲，用户界面(UI)是其接触到一个产品时最先看到的最直观的东西，而在这个过程中产生的直观感受就是用户体验(UX)，所以说到底这是一个产品给用户的“第一印象”。\nUX和UI\r最近百度 UE 总监刘超在 IXDC 峰会上的演讲引起了大家的关注，抛开百度在人才选拔机制中存在的问题以及刘超本人在设计领域是否具备专业能力这两个问题，这件事情真正让大家吐槽的是什么呢？答案是用户体验。虽然 IXDC 并非国际级别的大型会议，但是我相信大家组织这样的活动，其本意是为了探讨交互、设计领域内的新方法和新思维，因为随着互联网行业的发展，交互和设计这个领域越来越被人们所关注，所以在这样一个场合下，当与会嘉宾都在试图向人们输出干货的时候，刘超以一个非常糟糕的“用户体验”来给大家讲什么是用户体验，这件事情起源自刘超的一个个人行为，结果牵一发而动全身，最终升级为百度继“魏则西事件”以后的又一次公关危机。\n什么叫设计\r我到底想说什么呢？我说的本质上就是用户体验的问题，在这个事件中，刘超穿着上的不得体(短裤搭配拖鞋?)、PPT 制作的粗制滥造(校招时所用修改)、演讲过程的敷衍糊弄(说相声、猜谜语)等因素，让刘超在与会者心目中的地位瞬间滑落到冰点，进而引发人们对百度在交互设计领域内的能力的怀疑，联想到百度最近这些年内出现的问题，这件事情难免会被人作为指责百度这家企业价值观问题，我想这是这个事情为什么会让大家如此关注的一个原因吧。\nWTF!\r那么，我们说这些到底和今天的主题有什么关系呢？我想说这当然有关系啊，因为我们提出的这个问题就是一个用户体验的问题。我们知道游戏行业对美术资源高度依赖，不管是 2D 游戏还是 3D 游戏，一个项目组中前期主要的工作量其实都在美术这边，虽然不同的游戏引擎、GUI 框架都为我们提供了标准的控件样式，然而在这样一个注重多样性的时代，默认样式、系统字体都会让人觉得这个产品缺乏新意，因此这种要求体现在游戏项目中就变成了，我们使用大量的图片资源来解决界面和字体的问题。\n例如，我们通常使用 BMFont 来制作位图字体，这是为了同时满足字体的多样性和资源的容量这两个要求。再比如我们在使用 cocos2d-x 和 Unity3D 引擎开发游戏的时候，我们将大量的时间花费在了 UI 的制作上，这一切的一切从本质上来讲都是为了提升产品的童虎体验。这样我们就会遇到一个问题，UI 中的按钮默认情况下都是规则的矩形，而实际上美术提供的素材常常是不规则的，因此如果继续使用以矩形为标准的这套机制，在实际使用中可能出现“用户点击在不该响应的区域结果程序响应了用户操作”这样的问题，为了解决这个问题，提升这一点点细微的用户体验，我们需要花费时间和精力来了解下面这些内容。\n两种不同的方案 目前，关于这个问题如何，解决通过搜索引擎我们能找到两种不同的方案：\n多边形碰撞器: 该方法是指给精灵(Sprite)添加一个多边形碰撞器(Rolygon Collider)组件，利用该组件来标记精灵的边界，这样通过比较鼠标位置和边界可以判断点击是否发生在精灵内部。这种方法的详细说明可以参考宣雨松的这篇文章：UGUI 研究院之不规则按钮的响应区域（十四） 精灵像素检测: 该方法是指通过读取精灵(Sprite)在某一点的像素值(RGBA)，如果该点的像素值中的 Alpha\u0026lt;0.5 则表示该点处是透明的，即用户点击的位置在精灵边界以外，否则用户点击的位置在精灵边界内部。这种方法的详细说明可以参考这里 多边形碰撞器 多边形碰撞器这种方案从本质上来讲，其核心思路是验证某一点是否在任意多边形内部，因为在这里 RolygonCollider2D 组件的作用体现在：第一，它可以在编辑器下进行可视化编辑对用户友好；第二，它可以在帮助我们标记精灵边界的同时保留顶点信息。所以在这里 RolygonCollider2D 组件相当于为我们提供任意多边形的顶点信息，而接下来我们要做是将鼠标位置转化为屏幕坐标，这样我们就获得了某一点的坐标。整体思路看起来是没有问题的，但我个人以及网友AwayMe都认为宣雨松这个算法存在问题，具体的理由如下：\n1、uGUI 中的元素采用的是以屏幕中心为原点(0,0)的平面直角坐标系，而普通屏幕坐标采用的是以左下角为原点(0,0)的平面直角坐标系，所以多边形顶点数组和鼠标位置不在一个坐标系内，使用 AABBB 这样的碰撞检测算法存在问题。\n2、RolygonCollider2D 中的 points 属性即多边形顶点数组存储的是相对于 UI 元素的相对坐标，在进行计算的时候应该统一转化为绝对坐标，这个过程在宣雨松的代码中有所涉及，但我认为对 UI 元素来讲，应该使用 transform.GetComponent().position 而非 transform.position，因为 transform.position 最初是给 3D 物体使用的，而实际上这里是存在误差的。\n3、我怀疑宣雨松提供的这个 ContainsPoint 方法的正确性，因为按照我的理解修改这个方法以后，发现界面响应的情况和实际情况是有所出入的，如下图所示，在整个区域内该方法都返回 false。为了排除因为我的方法而对结果产生的影响，我使用宣雨松的代码进行了测试，结论是这个方法不管进行坐标系的转换与否，它在整个区域内的返回值都是 false，因此我认为这个方法是错误的，虽然从理解算法的角度来看，它应该是根据线性差值来判断点在多边形中每条边的哪一侧的。\n响应区域说明\r在评论中网友AwayMe指出可以使用多边形碰撞器的 OverlapPoint 方法来判断一个点是否在多边形内部，可是经过我测试，这种方式和宣雨松提供的方法有着类似地问题，无论是否对坐标系进行转换，这个方法都返回 false，响应区域与上图完全一致。\n所以不管网络上有没有高质量的内容，一个核心的问题是你能否从中找到答案。如果你可以直接找到解决方案这可能是最好的结局；如果找不到直接的解决方案，却能够有所启发并独立解决问题，这是我们希望看到的结果。可是有时候人们并不这样想啊，人们想得到的是可以运行的代码而非解决问题的思路，因为可能人们并不想解决这个问题。\n好了，经过知乎上相关答案我找到了这篇文章，文章中提到了判断一个点是否在任意多边形内部的两种方法，分别为 Corssing Number 和 Winding Number。这两种方法在理论层面的相关细节请大家自行阅读这篇文章，我们这里选择的是前者，其基本思想是计算从该点引出的射线与多边形边界橡胶的次数，当其为奇数时表示该点在多边形内部，当其为偶数时表示在多边形外部。这里有一个有意思的事情是宣雨松选择的方法应该是著名的Ray-Crossing算法，可是为什么在这里会出现这样的问题呢？\n孰是孰非，一切都交给实践来证明吧！下面是我根据文章中提供的算法改写的一段 C#代码：\nbool ContainsPoint2(Vector2[] polyPoints,Vector2 p) { // 统计射线和多边形交叉次数 int cn = 0; // 遍历多边形顶点数组中的每条边 for(int i=0; i \u0026lt; polyPoints.Length-1; i++) { // 正常情况下这一步骤可以忽略这里是为了统一坐标系 polyPoints [i].x += transform.GetComponent\u0026lt;RectTransform\u0026gt; ().position.x; polyPoints [i].y += transform.GetComponent\u0026lt;RectTransform\u0026gt; ().position.y; // 从当前位置发射向上向下两条射线 if(((polyPoints [i].y \u0026lt;= p.y) \u0026amp;\u0026amp; (polyPoints [i + 1].y \u0026gt; p.y)) || ((polyPoints [i].y \u0026gt; p.y) \u0026amp;\u0026amp; (polyPoints [i + 1].y \u0026lt;= p.y))) { //compute the actual edge-ray intersect x-coordinate float vt = (float)(p.y - polyPoints [i].y) / (polyPoints [i + 1].y - polyPoints [i].y); //p.x \u0026lt; intersect if(p.x \u0026lt; polyPoints [i].x + vt * (polyPoints [i + 1].x - polyPoints [i].x)) ++cn; } } // 实际测试发现cn为0的情况即为宣雨松算法中存在的问题 // 所以在这里进行屏蔽直接返回false这样就可以让透明区域不再响应 if (cn == 0) return false; // 返回true表示在多边形外部否则表示在多边形内部 return cn % 2 == 0; } 这段代码说实话我理解的不是很透彻，而且令人费解的是实际结论和算法结论完全相反，因为按照我现在这样的设计，当 cn 为偶数时返回为 true，此时应该表示该点再多边形外部啊，可是事实上我测试这段代码的时候，它居然是可以正常工作的，即当该方法返回 true 的时候我的点击确实是在多边形内部，所以这是一段可以正常工作同时让我感到费解的代码，而且当我屏蔽了 cn 为 0 的这种情况以后，现在它已经可以完美的工作了\n正五边形精灵\r同样的，我们这里使用一张正五边形的精灵图片，然后编写下面的代码：\n/* * 基于多边形碰撞器实现的不规则按钮 * 作者：PayneQin * 日期：2016年7月9日 */ using UnityEngine; using System.Collections; using UnityEngine.UI; using UnityEngine.EventSystems; public class UnregularButtonWithCollider : MonoBehaviour,IPointerClickHandler { /// \u0026lt;summary\u0026gt; /// 多边形碰撞器 /// \u0026lt;/summary\u0026gt; PolygonCollider2D polygonCollider; void Start() { // 获取多边形碰撞器 polygonCollider = transform.GetComponent\u0026lt;PolygonCollider2D\u0026gt;(); } public void OnPointerClick(PointerEventData eventData) { // 对2D屏幕坐标系进行转换 Vector2 local; local.x = eventData.position.x - (float)Screen.width / 2.0f; local.y = eventData.position.y - (float)Screen.height / 2.0f; if(ContainsPoint(polygonCollider.points,local)) { Debug.Log (\u0026#34;这是一个正五边形!\u0026#34;); } } /// \u0026lt;summary\u0026gt; /// 判断指定点是否在给定的任意多边形内 /// \u0026lt;/summary\u0026gt; bool ContainsPoint(Vector2[] polyPoints,Vector2 p) { // 统计射线和多边形交叉次数 int cn = 0; // 遍历多边形顶点数组中的每条边 for(int i=0; i\u0026lt;polyPoints.Length-1; i++) { //正常情况下这一步骤可以忽略这里是为了统一坐标系 polyPoints [i].x += transform.GetComponent\u0026lt;RectTransform\u0026gt; ().position.x; polyPoints [i].y += transform.GetComponent\u0026lt;RectTransform\u0026gt; ().position.y; //从当前位置发射向上向下两条射线 if(((polyPoints [i].y \u0026lt;= p.y) \u0026amp;\u0026amp; (polyPoints [i + 1].y \u0026gt; p.y)) || ((polyPoints [i].y \u0026gt; p.y) \u0026amp;\u0026amp; (polyPoints [i + 1].y \u0026lt;= p.y))) { //compute the actual edge-ray intersect x-coordinate float vt = (float)(p.y - polyPoints [i].y) / (polyPoints [i + 1].y - polyPoints [i].y); //p.x \u0026lt; intersect if(p.x \u0026lt; polyPoints [i].x + vt * (polyPoints [i + 1].x - polyPoints [i].x)) ++cn; } } // 实际测试发现cn为0的情况即为宣雨松算法中存在的问题 // 所以在这里进行屏蔽直接返回false这样就可以让透明区域不再响应 if(cn == 0) return false; // 返回true表示在多边形外部否则表示在多边形内部 return cn % 2 == 0; } } 我们可以发现现在它可以正常工作啦！我们必须意识到的一点是，这个方法的空间复杂度为 O(n-1)，所以随着多边形顶点数目的增加，这个方法的执行效率会越来越低，如果对不规则精灵的边界没有十分苛刻的要求的话，我的建议是我们使用多边形碰撞器标记出一个相对模糊的边界即可，因为现在我们这个方法主要依靠数学计算，没有涉及到摄像机相关计算，所以宣雨松博客中有朋友指出他的方法仅仅适用于 Canvas 的模式为 Screen-Space Camera 这种情况，而我目前这个方法对除了 World Space 以外都是可以使用的，我最大的疑虑来自对鼠标位置进行转化的时候是否应该使用 Screen.width 和 Screen.height，因为我担心可能会出现屏幕适配这种需求。\n演示效果1\r精灵像素检测 精灵像素检测这个方案的灵感来自 Image 组件，我们在 MonoDevelop 或者 Visual Studio 中通过\u0026quot;转到定义\u0026quot;这个功能可以获得 Image 组件的内部细节。我们发现 uGUI 在处理控件是否被点击的时候，主要是根据 IsRaycastLocationValid 这个方法的返回值来进行判断的，而这个方法用到的基本原理则是判断指定点对应像素的 RGBA 数值中的 Alpha 是否大于某个指定临界值。例如，我们知道半透明通常是指 Alpha=0.5，而对一个.png 格式的图片来说半透明甚至完全透明的区域理论上不应该被响应的，所以根据这个原理我们只需要设定一个透明度的临界值然后对当前鼠标位置对应的像素进行判断就可以了，因此这种方法叫做精灵像素检测。\n下面我们来一起看这段 uGUI 的代码，这段代码通过 MonoDevelop 或者 Visual Studio 的\u0026quot;转到定义\u0026quot;功能可以找到，这里我做了简单的注释帮助大家理解代码：\npublic virtual bool IsRaycastLocationValid(Vector2 screenPoint, Camera eventCamera) { //当透明度\u0026gt;=1.0时，表示点击在可响应区域返回true if (this.m_EventAlphaThreshold \u0026gt;= 1f) { return true; } //当没有指定精灵时为什么要返回true? Sprite overrideSprite = this.overrideSprite; if (overrideSprite == null) { return true; } //坐标系转换\tVector2 local; RectTransformUtility.ScreenPointToLocalPointInRectangle(base.rectTransform, screenPoint, eventCamera, ref local); Rect pixelAdjustedRect = base.GetPixelAdjustedRect (); local.x += base.rectTransform.get_pivot ().x * pixelAdjustedRect.get_width (); local.y += base.rectTransform.get_pivot ().y * pixelAdjustedRect.get_height (); local = this.MapCoordinate(local, pixelAdjustedRect); Rect textureRect = overrideSprite.get_textureRect (); Vector2 vector = new Vector2(local.x / textureRect.get_width (), local.y / textureRect.get_height ()); //计算屏幕坐标对应的UV坐标 float num = Mathf.Lerp(textureRect.get_x (), textureRect.get_xMax (), vector.x) / (float)overrideSprite.get_texture().get_width(); float num2 = Mathf.Lerp(textureRect.get_y (), textureRect.get_yMax (), vector.y) / (float)overrideSprite.get_texture().get_height(); bool result; //核心方法：像素检测 try { result = (overrideSprite.get_texture().GetPixelBilinear(num, num2).a \u0026gt;= this.m_EventAlphaThreshold); } catch (UnityException ex) { Debug.LogError(\u0026#34;Using clickAlphaThreshold lower than 1 on Image whose sprite texture cannot be read. \u0026#34; + ex.Message + \u0026#34; Also make sure to disable sprite packing for this sprite.\u0026#34;, this); result = true; } //返回结果\treturn result; } 从这段代码中我们可以看出，这个方法核心在第 31 行代码，即传入一个 UV 坐标返回一个 RGBA 数值并将其和临界值相比较。可是在此之前，我们看到在引入 uGUI 及其专属组件 RectTransform 以后，现在 Unity 中的坐标系转换变得更加复杂了，我个人看到这部分代码是相当凌乱的，或许我应该找时间补习下矩阵变换了吧。所以现在我们就有思路啦，我们有两种方式，第一种基于这个思路重新定制一个 Image 组件;第二种直接修改 Image 组件的 eventAlphaThreshold 属性。考虑到坐标系转换这里非常复杂，显然第二种方式更容易接受，为什么这里可以直接修改 eventAlphaThreshold 属性呢，因为它在 Image 组件内部和代码中的 m_EventAlphaThreshold 相关联，这就是这篇文章的完整解释啦！\n圆形精灵图片\r好了，现在我们来一个简单的测试，我们这里准备一张圆形的精灵图片(如上图)，然后编写下面的代码：\n/* * 基于精灵像素检测实现的不规则按钮 * 作者：PayneQin * 日期：2016年7月9日 */ using UnityEngine; using System.Collections; using UnityEngine.UI; using UnityEngine.EventSystems; public class UnregularButtonWithPixel : MonoBehaviour,IPointerClickHandler { /// \u0026lt;summary\u0026gt; /// Image组件 /// \u0026lt;/summary\u0026gt; private Image image; /// \u0026lt;summary\u0026gt; /// 透明度临界值 /// \u0026lt;/summary\u0026gt; [Range(0.0f,0.5f)] public float Alpha; public void Start() { //获取Image组件 image = transform.GetComponent\u0026lt;Image\u0026gt;(); //设定透明度临界值 image.eventAlphaThreshold = Alpha; } public void OnPointerClick(PointerEventData eventData) { Debug.Log(\u0026#34;这是一个圆形!\u0026#34;); } } 这里我为了让大家在学(复)习(制)的时候更容易理解，我在 Click 事件的响应上，使用的是实现 IPointerClickHandler 接口这种方法，希望通过动态绑定这种方式添加事件响应的可以自己解决，我是不会为了满足你们的好(懒)奇(惰)而奉献出我的 EventTriggerListener 的代码的。好了，现在我们要做的就是为需要响应点击的不规则精灵附加该脚本，这样就可以解决不规则精灵响应的问题了。这种方法使用起来非常简单，需要注意的是：图片的类型必须是 Advance 且保证可读可写。因为我们在脚本中访问了像素，而简单伴随着的代价就是我们无法使用图集、该图片在内存中会复制一份，所以在项目性能上允许的情况下这种方法还是可以考虑使用的。\n演示效果2\r小结 本文通过对网络上两种比较通用的不规则按钮制作方案进行对比和研究，解决了基于多边形碰撞器实现不规则按钮这个过程中存在的问题，剖析了基于精灵像素检测实现不规则按钮 这个过程的内部原理，从易用性角度来讲，后者要优于前者，而这种方法的缺陷主要来自于它对图片类型的限制以及允许像素可读写这两个方面，它必须是 Advance 类型，所以普通的 Texture 或者 Sprite 拥有的特性在这里它都无法享受，比如我们无法为其做颜色渐变这类 Tween 动画、无法使用精灵特有的图集特性等等，于此同时它必须允许像素可读写，因此在实际使用中它会在内存中复制一份，在执行效率上可能会受到影响。而从技术性角度来讲，我个人更推推崇前者，因为在这个过程中我们学到了新的知识，明白了如何利用一个算法来解决实际的问题，而且它不会限制我们对精灵的使用，所有精灵拥有的特性在这里我们都可以使用，无非是在寻找算法、解决问题的过程中我们耗费了大量精力，可是这是值得的啊，不是吗？这就是我们做这件事情的意义所在。从昨天开始研究这两个问题到今天写完整篇文章，整个人是非常疲惫的，欢迎大家继续关注我的博客，今天的内容就是这样啦，谢谢大家！\n","date":"2016-07-08T21:58:39Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1190622881/","slug":"1190622881","tags":["Unity3D","uGUI","游戏开发"],"title":"Unity3D 游戏开发之在 uGUI 中使用不规则精灵制作按钮"},{"categories":["编程语言"],"content":"各位朋友大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。在我们这个 Web 服务器有了一个基本的门面以后，我们是时候来用它做点实际的事情了。还记得我们最早提到 HTTP 协议的用途是什么吗？它叫超文本传输协议啊，所以我们必须考虑让我们的服务器能够接收到客户端传来的数据。因为我们目前完成了大部分的工作，所以对数据传输这个问题我们这里选择以最简单的 GET 和 POST 为例来实现，这样我们今天的重点就落实在 Get 和 Post 的实现这个问题上来。而从原理上来讲，无论 Get 方式请求还是 Post 方式请求，我们都可以在请求报文中获得其请求参数，不同的是前者出现在请求行中，而后者出现在消息体中。例如我们传递的两个参数 num1 和 num2 对应的数值分别是 12 和 24，那么在具体的请求报文中我们都能找到类似“num1=12\u0026amp;num2=24”这样的字符结构，所以只要针对这个字符结构进行解析，就可以获得客户端传递给服务器的参数啦。\n实现 Get 请求 首先我们来实现 Get 请求，Get 是 HTTP 协议中默认的请求类型，我们平时访问网页、请求资源实际上都是通过 Get 方式实现的。Get 方式请求需要通过类似“?id=001\u0026amp;option=10”这样的形式附加在 URL 上，因此 Get 方式对浏览器来说是透明的，即用户可以通过浏览器地址栏知道，这个过程中传递了哪些参数以及这些参数的值分别是什么。而由于浏览器的限制，我们通过这种方式请求的时候能够传递的参数数目和长度都是有限的，而且当参数中存在中文数值的时候还需要对其进行编码。Get 方式请求相对简单，我们下面来看看它的请求报文：\nGET /?num1=23\u0026amp;num2=12 HTTP/1.1\rAccept: text/html, application/xhtml+xml, image/jxr, */*\rAccept-Language: zh-Hans-CN,zh-Hans;q=0.5\rUser-Agent: Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2486.0 Safari/537.36 Edge/13.10586\rAccept-Encoding: gzip, deflate\rHost: localhost:4040\rConnection: Keep-Alive\rCookie: _ga=GA1.1.1181222800.1463541781 此时我们可以注意到在请求报文第一行，即请求行中出现了“/?num1=23\u0026amp;num2=12”这样的字样，这就是客户端传递给服务器的参数，我们很容易想到只需要将这个字段串中的“键”和“值”都解析出来，服务器就可以对这些数据进行处理然后返回给客户端了。所以下面我们通过这样的方式来实现，我们为 HtttpRequest 类增加了一个 Parms 属性，它是一个键和值均为字符串类型的字典，我们使用这个字典来存储和管理客户端传递来的参数。\n//获取请求参数 if (this.Method == \u0026#34;GET\u0026#34; \u0026amp;\u0026amp; this.URL.Contains(\u0026#39;?\u0026#39;)) this.Params = GetRequestParams(lines[0].Split(\u0026#39; \u0026#39;)[1].Split(\u0026#39;?\u0026#39;)[1]); 显然我们首先需要判断请求类型是否为 GET 以及请求中是否带有参数，其方法是判断请求地址中是否含有“?\u0026ldquo;字符。这里的 lines 是指将报文信息按行分割以后的数组，显然请求地址在第一行，所以我们根据“?\u0026ldquo;分割该行数据以后就可以得到“num1=23\u0026amp;num2=12”这样的结果，这里我们使用一个方法 GetRequestParms 来返回参数字典，这样作做是为了复用方法，因为在处理 Post 请求的时候我们会继续使用这个方法。该方法定义如下：\n/// \u0026lt;summary\u0026gt; /// 从内容中解析请求参数并返回一个字典 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;content\u0026#34;\u0026gt;使用\u0026amp;连接的参数字符串\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;如果存在参数则返回参数否则返回null\u0026lt;/returns\u0026gt; protected Dictionary\u0026lt;string, string\u0026gt; GetRequestParams(string content) { //防御编程 if (string.IsNullOrEmpty(content)) return null; //按照\u0026amp;对字符进行分割 string[] reval = content.Split(\u0026#39;\u0026amp;\u0026#39;); if (reval.Length \u0026lt;= 0) return null; //将结果添加至字典 Dictionary\u0026lt;string, string\u0026gt; dict = new Dictionary\u0026lt;string, string\u0026gt;(); foreach (string val in reval) { string[] kv = val.Split(\u0026#39;=\u0026#39;); if (kv.Length \u0026lt;= 1) dict.Add(kv[0], \u0026#34;\u0026#34;); dict.Add(kv[0],kv[1]); } //返回字典 return dict; } 实现 Post 请求 Post 请求相对 Get 请求比较安全，因为它克服了 Get 请求参数长度的限制问题，而且由于它的参数是存放在消息体中的，所以在传递参数的时候对用户而言是不可见的，我们平时接触到的网站登录都是这种类型，而复杂点的网站会通过验证码、Cookie 等形式来避免爬虫程序模拟登录，在 Web 开发中 Post 请求可以由一个表单发起，可以由爬虫程序如 HttpWebRequest、WebClient 等发起，下面我们重点来分析它的请求报文：\nPOST / HTTP/1.1\rAccept: text/html, application/xhtml+xml, image/jxr, */*\rAccept-Language: zh-Hans-CN,zh-Hans;q=0.5\rUser-Agent: Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2486.0 Safari/537.36 Edge/13.10586\rAccept-Encoding: gzip, deflate\rHost: localhost:4040\rConnection: Keep-Alive\rCookie: _ga=GA1.1.1181222800.1463541781\rnum1=23\u0026amp;num2=12 我们可以注意到此时请求行的请求方法变成了 POST，而在报文结尾增加了一行内容，我们称其为“消息体”，这是一个可选的内容，请注意它前面有一个空行。所以，当我们处理一个 Posst 请求的时候，通过最后一行就可以解析出客户端传递过来的参数，和 Get 请求相同，我们这里继续使用 GetRequestParams 来完成解析。\nif (this.Method == \u0026#34;POST\u0026#34;) this.Params = GetRequestParams(lines[lines.Length-1]); 实例 现在我们来完成一个简单地实例，服务器自然由我们这里设计的这个服务器来完成咯，而客户端则由 Unity 来完成因为 Unity 有简单的 WWW 可以使用。首先来编写服务端，这个继承 HttpServer 就好了，我们主要来写这里的方法：\nusing System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Threading.Tasks; using HttpServerLib; using System.IO; namespace HttpServer { public class ExampleServer : HttpServerLib.HttpServer { /// \u0026lt;summary\u0026gt; /// 构造函数 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;ipAddress\u0026#34;\u0026gt;IP地址\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;port\u0026#34;\u0026gt;端口号\u0026lt;/param\u0026gt; public ExampleServer(string ipAddress, int port) : base(ipAddress, port) { } public override void OnPost(HttpRequest request) { //获取客户端传递的参数 int num1 = int.Parse(request.Params[\u0026#34;num1\u0026#34;]); int num2 = int.Parse(request.Params[\u0026#34;num2\u0026#34;]); //设置返回信息 string content = string.Format(\u0026#34;这是通过Post方式返回的数据:num1={0},num2={1}\u0026#34;,num1,num2); //构造响应报文 HttpResponse response = new HttpResponse(content, Encoding.UTF8); response.StatusCode = \u0026#34;200\u0026#34;; response.Content_Type = \u0026#34;text/html; charset=UTF-8\u0026#34;; response.Server = \u0026#34;ExampleServer\u0026#34;; //发送响应 ProcessResponse(request.Handler, response); } public override void OnGet(HttpRequest request) { //获取客户端传递的参数 int num1 = int.Parse(request.Params[\u0026#34;num1\u0026#34;]); int num2 = int.Parse(request.Params[\u0026#34;num2\u0026#34;]); //设置返回信息 string content = string.Format(\u0026#34;这是通过Get方式返回的数据:num1={0},num2={1}\u0026#34;,num1,num2); //构造响应报文 HttpResponse response = new HttpResponse(content, Encoding.UTF8); response.StatusCode = \u0026#34;200\u0026#34;; response.Content_Type = \u0026#34;text/html; charset=UTF-8\u0026#34;; response.Server = \u0026#34;ExampleServer\u0026#34;; //发送响应 ProcessResponse(request.Handler, response); } } } 因为这里需要对 Get 和 Post 进行响应，所以我们这里对 OnGet 和 OnPost 两个方法进行了重写，这里的处理方式非常简单，按照一定格式返回数据即可。下面我们来说说 Unity 作为客户端这边要做的工作。WWW 是 Unity3D 中提供的一个简单的 HTTP 协议的封装类，它和.NET 平台下的 WebClient、HttpWebRequest/HttpWebResponse 类似，都可以处理常见的 HTTP 请求如 Get 和 Post 这两种请求方式。\nWWW 的优势主要是简单易用和支持协程，尤其是 Unity3D 中的协程（Coroutine）这个特性，如果能够得到良好的使用，常常能够起到事倍功半的效果。因为 WWW 强调的是以 HTTP 短链接为主的易用性，所以相应地在超时、Cookie 等 HTTP 头部字段支持的完整性上无法和 WebClient、HttpWebRequest/HttpWebRespons 相提并论，当我们需要更复杂的 HTTP 协议支持的时候，选择在 WebClient、HttpWebRequest/HttpWebResponse 上进行深度定制将会是一个不错的选择。我们这里需要的是发起一个简单的 HTTP 请求，所以使用 WWW 完全可以满足我们的要求，首先我们来看在 Unity3D 中如何发起一个 Get 请求，这里给出一个简单的代码示例：\n//采用GET方式请求数据 IEnumerator Get() { WWW www = new WWW (\u0026#34;http://127.0.0.1:4040/?num1=12\u0026amp;num2=23\u0026#34;); yield return www; Debug.Log(www.text); } 现在我们是需要使用 StartCoroutine 调用这个方法就可以啦！同样地，对于 Post 请求，我们这里采用一个 WWWForm 来封装参数，而在网页开发中我们通常都是借助表单来向服务器传递参数的，这里给出同样简单的代码示例：\n//采用POST方式请求数据 IEnumerator Post() { WWWForm form = new WWWForm (); form.AddField (\u0026#34;num1\u0026#34;, 12); form.AddField (\u0026#34;num2\u0026#34;, 23); WWW www = new WWW (\u0026#34;http://127.0.0.1:4040/\u0026#34;, form); yield return www; Debug.Log (www.text); } 而运行这个实例，我们可以得到下面的结果：\n测试结果\r都是谁告诉你做服务器开发一定要用 Java 的啊，现在我们可以写出自己的服务器了，本篇结束，下期见！\n","date":"2016-06-11T15:01:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1700650235/","slug":"1700650235","tags":["HTTP","服务器","C#"],"title":"使用 C#开发 HTTP 服务器系列之实现 Get 和 Post"},{"categories":["编程语言"],"content":"各位朋友大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。到目前为止，我已经发布了 3 篇 HTTP 服务器开发的系列文章。对我个人而言，我非常享受这个从无到有的过程，或许我现在写的这个 Web 服务器有各种不完美的因素，可是当有一天我需要一个轻量级的服务器的时候，我在无形中是不是比别人多了一种选择呢？我们常常提到“不要重复造轮子”，可事实上这并不能成为我们“不造轮子”的理由，虽然我们有各种各样的服务器软件、有各种各样的服务端框架可以供我们选择，可是在动手写这个系列文章前，我对 Web 服务器的印象无非是因为我是用 LAWP(Linux + Apache + MySQL + PHP)搭建过 Wordpress 博客而已。虽然在对动态页面(如.aspx、.jsp、.php 等)的处理上，可能会和静态页面有所不同，但是我庆幸我了解了这个过程以及它的内部原理，这种跨语言、跨平台的设计思路是任何框架或者标准都无法告诉我的。或许有人会问我，为什么不在最开始的时候就选择更简单的实现方法，那么在这篇文章中你将会找到答案。\n从原理说起 我们知道 HTTP 服务器其实是一个“服务端循环监听客户端请求然后响应客户端请求”的请求/响应模型，在这个模型中请求通常是由浏览器来发起的，而服务端负责响应客户端的请求。这是我们通常意义上的认识，可是当我们了解到 HTTP 协议的实质以后就会明白，不管是客户端还是服务端，从本质上来讲都是 Socket 通信，只要我们能够发送符合 HTTP 协议规范的报文就可以啦。\n所以我们立刻就能够想到无论是 Unity 引擎中的 WWW 还是.NET 平台下的 WebClient，它们之所以能够向服务器发起请求，无一例外地是它们都遵循了 HTTP 协议的规范。从这个角度来讲，人类社会存在各种各样的问题，本质上都是存在游离于规范以外的不公平的现象。还记得我们在这个系列中提到的请求报文和响应报文的结构是什么样的吗？此时此刻我们发自内心地向创造 HTTP 协议的先驱们致敬，因为这个协议我们构建起了连接人与人的社交网络，可是同样因为这个协议我们和人越来越远、和手机越来越近。\nHTTP 协议是一种无状态的应用层协议，这个无状态该怎么理解呢？我这里想借助聊天机器人这个实例来解释这个问题，我们都知道聊天机器人是一种问答型的程序，程序每次都可以根据提问者的问题给出，一个从人类角度来看完全合理的答案。然而从目前我了解到的聊天机器人的技术现状来看，具备自然语言理解的机器人程序基本没有，所以在这样的大背景下，机器人程序实际上是没有上下文理解的能力的。\n好了，现在我们回到 HTTP 协议，首先聊天机器人的问答模式是不是和 HTTP 协议中的请求/响应模式非常相似呢？其次，我们在设计 HTTP 服务器的时候，每次在向客户端返回响应报文以后，我们就关闭了 Socket 连接，这意味着每次的请求和响应完全都是独立的，那么这样是不是就和聊天机器人不能理解上下文非常相似了呢？所以综合下来，我们理解的无状态其实就是说 HTTP 请求和响应完全独立，即在客户端中不会存储服务端的响应，在服务端中同样不会存储客户端的请求。\n这样难免引发一个问题，如果我需要在不同请求和响应中保持状态该怎么做呢？这个在不同的服务器软件中有不同的技术实现，这里我们说一种最通用的 Cookie。Cookie 是存储在客户端中的一个数据，在发起下一轮请求时这个参数会被加入到参数列表中然后传递给服务器，服务器会对客户端传递的参数进行验证，以此来判断本轮请求和上轮请求间是否存在上下文联系。\n两种不同的实现 到目前为止我们了解的 HTTP 服务器开发，实际上由两部分组成，即 Socket 通信和请求-响应模型。基于这两点考虑，我们这里提供两种快速实现 Web 服务器的具体思路，这是在我们理解了 HTTP 协议实质以后，从原理出发想到的解决方案，为什么我不建议在刚开始就学习这些东西呢？因为我觉得学习有时候其实就是一个不断开阔视野和思路的过程吧。好了，下面我们来说说这两种不同实现方式的具体思路吧！\n基于 TcpListener/TcpClienr 改进 Socket 如果说使用 Socket 从头开始编写 HTTP 服务器是一个“刀耕火种”时代的缩影，那么使用 TcpListener/TcpClient 则是让我们开始进入“青铜铸犁”的农耕时代。和 Sokcet 相比，TcpListener/TcpClient 是.NET 对 Socket 的进一步封装，在这个体系下，TcpListener 负责监听和接收传入的连接请求，在该类中仅需要传入一个网络终端信息就可以完成服务端的初始化，而无需设置网络通信协议等细节性的内容。调用 Start 方法后即可以开始监听，这里我们使用 AcceptTcpClient 方法来阻塞进程直到接受到一个客户端请求为止，该方法将返回一个 TcpClient 对象，我们可以借助它完成和客户端的通信。下面我们来一起看基本的代码实现：\npublic void Start() { if (isRunning) return; //创建TcpListener serverListener = new TcpListener(IPAddress.Parse(ServerIP), ServerPort); //开始监听 serverListener.Start(10); isRunning = true; //输出服务器状态 Console.WriteLine(\u0026#34;Sever is running at http://{0}:{1}/.\u0026#34;, ServerIP, ServerPort); while (isRunning) { //获取客户端连接 TcpClient acceptClient = serverListener.AcceptTcpClient(); //获取请求报文 NetworkStream netstream = acceptClient.GetStream(); //解析请求报文 byte[] bytes = new byte[1024]; int length = netstream.Read(bytes, 0, bytes.Length); string requestString = Encoding.UTF8.GetString(bytes, 0, length); //以下为响应报文(略) } } 我个人感觉这种形式和原生的 Socket 在实现上区别不是非常大，按照这种思路继续往下设计，我的 HttpRequest 和 HttpResponse 可能都需要进行改进，因为在我的设计中，我是在尽可能地隐藏 Socket 通信的细节，因为我不想让使用者觉察到他这是在使用 Socket 进行通信，这里细心的朋友可能会发现，这里的 TcpListener/TcpClient 都保留了常见的 Socket 用法如同步通信和异步通信的支持等，所以在使用 cpListener/TcpClient 其实没有必要纠结它的这套流程，如果你喜欢继续使用 Socket 通信的经验和方法就可以了。这里我们仅提供一种延伸思路。具体的代码实现大家顺着这个思路继续下去就好啦。\n基于 HttpListener 实现请求-响应模型 下面我们再来说说基于 HttpListener 实现请求-响应模型，它和改进 Socket 不同，它对我们编写一个 Web 服务器的意义主要体现在它提供了一个非常规范的接口，类似我这里的 HttpResponse 和 HttpRequest 以及 OnPost、OnGet 等接口这些设计。这个让我不喜欢的一点是它在设置服务器 IP 地址和端口的时候非常别扭，其思路和我的设计是非常相似的，下面我们来一起看代码：\npublic void Listen() { if (!HttpListener.IsSupported) throw new InvalidOperationException( \u0026#34;请确保使用WindowsXP以上版本的Windows!\u0026#34;); //初始化Http监听器 listener = new HttpListener(); //初始化服务器URL string[] prefixes = new string[] { address }; foreach (string prefix in prefixes) { listener.Prefixes.Add(prefix); } //开启服务器 listener.Start(); //监听服务器 while (isActive) { HttpListenerContext context = listener.GetContext(); HttpListenerRequest request = context.Request; HttpListenerResponse response = context.Response; if (request.HttpMethod == \u0026#34;GET\u0026#34;) { OnGetRequest(request, response); } else { OnPostRequest(request, response); } } } 好了，现在这个东西就非常简单了，因为我们只需要继承 HttpServerBase 这个类然后重写相关方法就可以了，而请求报文和响应报文中的相关属性都在 HttpListenerRequest 和 HttpListenerResponse 这两个类中封装好了，我们直接使用就好了。在没有写这个系列文章前，可能我会对这种方案充满好奇，可是当我了解到这一切的实质以后，我反而更加喜欢使用我设计的 HTTP 服务器了，因为这些东西在我看来区别真的可以忽略。\nOne More Thing 关于今天本文中提到的两种方案，我都是作为 HTTP 服务器开发延伸出来的内容来写出来给大家看,所以这块儿内容我都是点到为止不打算给出完整的实现，如果有兴趣的朋友可以顺着我这个思路区继续改进。这个系列文章中的示例代码主要来自我的项目HttpServer，大家到我的 GIthub 上去了解更多细节。到目前为止我觉得 HTTP 服务器快发这块儿我能写的内容都基本上写完了，因为是一边写代码一边写博客，所以有时候博客中如果有写得不好或者写的不明白的地方，希望大家能够谅解，同时希望大家在博客中给我积极留言，下一篇我想简单写一下 RESTful API 的相关问题，写完这一篇整个系列就结束了，我还是想说写文章真的很累啊，希望大家继续支持，下期见。\n","date":"2016-06-11T15:01:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3603924376/","slug":"3603924376","tags":["HTTP","服务器","C#"],"title":"使用 C#开发 HTTP 服务器系列之更简单的实现方式"},{"categories":["编程语言"],"content":"各位朋友大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。在这个系列文章的第一篇中，我们着重认识和了解了 HTTP 协议，并在此基础上实现了一个可交互的 Web 服务器，即当客户端访问该服务器的时候，服务器能够返回并输出一个简单的“Hello World”。现在这个服务器看起来非常简陋，为此我们需要在这个基础上继续开展工作。今天我们希望为这个服务器增加主页支持，即当我们访问这个服务器的时候，它可以向我们展示一个定制化的服务器主页。通常情况下网站的主页被定义为 index.html，而在动态网站技术中它可以被定义为 index.php。了解这些将有助于帮助我们认识 Web 技术的实质，为了方便我们这里的研究，我们以最简单的静态页面为例。\n大意失荆州 首先我们可以认识到的一点是，网站主页是一个网站默认展示给访问者的页面，所以对服务器而言，它需要知道两件事情，第一客户端当前请求的这个页面是不是主页，第二服务端应该返回什么内容给客户端。对这两个问题，我们在目前设计的这个 Web 服务器中都可以找到答案的。因为 HTTP 协议中默认的请求方法是 GET，所以根据 HttpRequest 的实例我们可以非常容易的知道，当前请求的方法类型以及请求地址。我们来看一个简单的客户端请求报文：\nGET / HTTP/1.1\rAccept: text/html, application/xhtml+xml, image/jxr, */*\rAccept-Language: zh-Hans-CN,zh-Hans;q=0.5\rUser-Agent: Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2486.0 Safari/537.36 Edge/13.10586\rAccept-Encoding: gzip, deflate\rHost: localhost:4040\rConnection: Keep-Alive 我们在这里可以非常清晰地看到，客户端当前发出的请求是 GET 类型，而其请求的地址是\u0026quot;/\u0026quot;，这表示请求页面为主页，而实际上我们将 Host 字段和这个地址组合起来，就能得到一个完整的地址，这正是我们在 HTML 结构中编写超链接的时候使用相对地址的原因。好了，在明白了这样两件事情具体的运作机理以后，下面我们来继续编写相关逻辑来实现如何向访问者展示一个网站主页。\npublic override void OnGet(HttpRequest request) { //判断请求类型和请求页面 if (request.Method == \u0026#34;GET\u0026#34; \u0026amp;\u0026amp; request.URL == \u0026#34;/\u0026#34;) { //构造响应报文 HttpResponse response; //判断主页文件是否存在，如存在则读取主页文件否则返回404 if (!File.Exists(ServerRoot + \u0026#34;index.html\u0026#34;)){ response = new HttpResponse(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;404 - Not Found\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;, Encoding.UTF8); response.StatusCode = \u0026#34;404\u0026#34;; response.Content_Type = \u0026#34;text/html\u0026#34;; response.Server = \u0026#34;ExampleServer\u0026#34;; } else { response = new HttpResponse(File.ReadAllBytes(ServerRoot + \u0026#34;index.html\u0026#34;), Encoding.UTF8); response.StatusCode = \u0026#34;200\u0026#34;; response.Content_Type = \u0026#34;text/html\u0026#34;; response.Server = \u0026#34;ExampleServer\u0026#34;; } //发送响应 ProcessResponse(request.Handler, response); } } 可以注意到在这里我们首先根据请求方法和请求地址来判断当前客户端是否在请求主页页面，然后我们判断在服务器目录下是否存在 index.html 文件，如果该文件存在就读取文件并返回给客户端，否则我们将返回给客户端一个 404 的状态，熟悉 Web 开发的朋友应该会知道这个状态码表示的是无法找到请求资源，类似地我们还可以想到的状态码有 200、501 等等，通常来讲，这些状态码的定义是这样的：\n1XX：指示信息-表示请求已接收，继续处理。 2XX：成功-表示请求已被成功接受、理解和处理。 3XX: 重定向-表示完成请求需要更进一步的操作。 4XX：客户端错误-表示请求错误或者无法实现。 5XX：服务端错误-表示服务器未提供正确的响应。 具体来讲，常见的状态代码描述如下：\n状态码 状态描述 200 OK 客户端请求成功 400 Bad Request 客户端请求错误且不能被服务器所理解 401 Unauthorized 请求未经授权需要使用 WWW-Authenticate 报头域 403 Forbidden 服务器收到请求但拒绝提供服务 404 Not Found 请求资源不存在 500 Internal Server Error 服务器发生不可预期的错误 503 Server Unavailable 服务器当前不能处理客户端的请求 为了简化需求，我们这里假设服务器目录下只有一个主页文件 index.html，实际上像 IIS、Apache 等大型的服务器软件都是支持多个主页文件的，而且同时支持静态页面和动态页面，所以这里就涉及到一个优先级的问题，无论是在 Apache 还是 IIS 中我们可以找到对主页优先级设置的选项。所谓优先级，其实就是对这些主页文件重要性的一种排序，在实际设计的过程中，会优先读取优先级较高的主页文件，如该文件不存在则退而求其次，以此类推。在读取主页文件的时候，我们需要注意的一点是编码类型，因为无论是客户端还是服务端在其各自的头部信息里都声明了它可以接受的编码类型，所以服务器端在响应请求的时候应该注意和客户端保持一致，这样可以避免“鸡同鸭讲”问题的发生，进而提高沟通效率。我们这里在说技术，可是人何尝不是这样啊，我感觉我们生活和工作中 90%的时间都被用来沟通了，可是这恰恰说明了沟通的重要性啊。好了，下面我们来测试下我们编写的主页：\n一个失败的尝试\r龙潜在渊 咦？这个页面显示的结果怎么和我们期望的不一样啊，看起来这是一个因为样式丢失而引发的错误啊，不仅如此我们发现页面中的图片同样丢失了。首先我们检查下静态页面是否有问题，这个怎么可能嘛？因为这是博主采用 Hexo 生成的静态页面，所以排除页面本身的问题后，我们不得不开始重新思考我们的设计。我们静下心来思考这样一个问题：在浏览器加载一个页面的过程中难道只有静态页面和服务器发生交互吗？这显然不是啊，因为傻子都知道一个网页最起码有 HTML、CSS 和 JavaScript 三部分组成，所以我们决定在 Chrome 中仔细看看浏览器在加载网页的过程中都发生了什么。按下\u0026quot;F12\u0026quot;打开开发者工具对网页进行监听：\n浏览器的小秘密\rWTF！感觉在这里懵逼了是不是？你没有想到服务器在这里会响应如此多的请求吧？所以我们自作聪明地认为只要响应静态页面的请求就好了，这完全就是在作死啊！这里我的理解是这样的，对页面来讲服务器在读取它以后会返回给客户端，所以对客户端而言这部分响应是完全可见的，而页面中关联的 CSS 样式和 JavaScript 脚本则可能是通过浏览器缓存下载到本地，然后再根据相对路径引用并应用到整个页面中来的，而为了区分这些不同类型的资源，我们需要在响应报文中的 Content-Type 字段中指明内容的类型，所以现在我们就清楚了，首先在请求页面的时候存在大量关联资源，这些资源必须通过响应报文反馈给客户端，其次这些资源由不同的类型具体体现在响应报文的 Content-Type 字段中。因此，我们在第一段代码的基础上进行修改和完善，最终编写出了下面的代码：\npublic override void OnGet(HttpRequest request) { if (request.Method == \u0026#34;GET\u0026#34;) { ///获取客户端请求地址 ///链接形式1:\u0026#34;http://localhost:4050/assets/styles/style.css\u0026#34;表示访问指定文件资源， ///此时读取服务器目录下的/assets/styles/style.css文件。 ///链接形式2:\u0026#34;http://localhost:4050/assets/styles/\u0026#34;表示访问指定页面资源， ///此时读取服务器目录下的/assets/styles/style.index文件。 //当文件不存在时应返回404状态码 string requestURL = request.URL; requestURL = requestURL.Replace(\u0026#34;/\u0026#34;, @\u0026#34;\\\u0026#34;).Replace(\u0026#34;\\\\..\u0026#34;, \u0026#34;\u0026#34;); //判断地址中是否存在扩展名 string extension = Path.GetExtension(requestURL); //根据有无扩展名按照两种不同链接进行处 string requestFile = string.Empty; if (extension != \u0026#34;\u0026#34;) { requestFile = ServerRoot + requestURL; } else { requestFile = ServerRoot + requestURL + \u0026#34;index.html\u0026#34;; } //构造HTTP响应 HttpResponse response = ResponseWithFile(requestFile); //发送响应 ProcessResponse(request.Handler, response); } } 注意到我在代码中写了两种不同形式的链接的分析：\n链接形式 1: \u0026ldquo;http://localhost:4050/assets/styles/style.css\u0026rdquo; 表示访问指定文件资源，此时读取服务器目录下的 /assets/styles/style.css 文件。\n链接形式 2: \u0026ldquo;http://localhost:4050/assets/styles/\u0026rdquo; 表示访问指定页面资源，此时读取服务器目录下的 /assets/styles/style.index 文件。\n首先我们判断这两种形式是根据扩展名来判断的，这样我们可以获得一个指向目标文件的地址 requestFile。这里提供一个辅助方法 ResponseWithFile，这是一个从文件中构造响应报文的方法，其返回类型是一个 HttpResponse，当文件不存在时将返回给客户端 404 的错误代码，我们一起来看它具体如何实现：\n/// \u0026lt;summary\u0026gt; /// 使用文件来提供HTTP响应 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;fileName\u0026#34;\u0026gt;文件名\u0026lt;/param\u0026gt; private HttpResponse ResponseWithFile(string fileName) { //准备HTTP响应报文 HttpResponse response; //获取文件扩展名以判断内容类型 string extension = Path.GetExtension(fileName); //获取当前内容类型 string contentType = GetContentType(extension); //如果文件不存在则返回404否则读取文件内容 if (!File.Exists(fileName)){ response = new HttpResponse(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;404 - Not Found\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;, Encoding.UTF8); response.StatusCode = \u0026#34;404\u0026#34;; response.Content_Type = \u0026#34;text/html\u0026#34;; response.Server = \u0026#34;ExampleServer\u0026#34;; } else { response = new HttpResponse(File.ReadAllBytes(fileName), Encoding.UTF8); response.StatusCode = \u0026#34;200\u0026#34;; response.Content_Type = contentType; response.Server = \u0026#34;ExampleServer\u0026#34;; } /返回数据 return response; } 同样的，因为在响应报文中我们需要指明资源的类型，所以这里使用一个叫做 GetContentType 的辅助方法，该方法定义如下，这里仅仅选择了常见的 Content-Type 类型来实现，有兴趣的朋友可以自行了解更多的内容并在此基础上进行扩展：\n/// \u0026lt;summary\u0026gt; /// 根据文件扩展名获取内容类型 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;extension\u0026#34;\u0026gt;文件扩展名\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; protected string GetContentType(string extension) { string reval = string.Empty; if (string.IsNullOrEmpty(extension)) return null; switch (extension) { case \u0026#34;.htm\u0026#34;: reval = \u0026#34;text/html\u0026#34;; break; case \u0026#34;.html\u0026#34;: reval = \u0026#34;text/html\u0026#34;; break; case \u0026#34;.txt\u0026#34;: reval = \u0026#34;text/plain\u0026#34;; break; case \u0026#34;.css\u0026#34;: reval = \u0026#34;text/css\u0026#34;; break; case \u0026#34;.png\u0026#34;: reval = \u0026#34;image/png\u0026#34;; break; case \u0026#34;.gif\u0026#34;: reval = \u0026#34;image/gif\u0026#34;; break; case \u0026#34;.jpg\u0026#34;: reval = \u0026#34;image/jpg\u0026#34;; break; case \u0026#34;.jpeg\u0026#34;: reval = \u0026#34;image/jgeg\u0026#34;; break; case \u0026#34;.zip\u0026#34;: reval = \u0026#34;application/zip\u0026#34;; break; } return reval; } 风雨过后终见彩虹 好啦，到目前为止，关于静态 Web 服务器的编写我们基本上告一段落啦！其实这篇文章写的不是特别顺利，因为我几乎是在不断否认自我的情况下，一边调试一边写这篇文章的。整篇文章总结下来其实就两个点，第一，Web 服务器在加载一个页面的时候会发起无数个请求报文，除了页面相关的请求报文以外大部分都是和资源相关的请求，所以 HTML 页面的优化实际上就是从资源加载这个地方入手的。第二，不同的资源有不同的类型，具体表现在响应报文的 Content-Type 字段上，构造正确的 Content-Type 能让客户端了解到这是一个什么资源。好了，现在我们可以气定神闲的验证我们的劳动成果啦，这里我以我本地的 Hexo 生成的静态博客为例演示我的 Web 服务器，假设我的博客是存放在\u0026quot;D:\\Hexo\\public\u0026quot;这个路径下，所以我可以直接在 Web 服务器中设置我的服务器目录：\nExampleServer server = new ExampleServer(\u0026#34;127.0.0.1\u0026#34;,4050); server.SetServerRoot(\u0026#34;D:\\\\Hexo\\\\public\u0026#34;); server.Start(); 现在打开浏览器就可以看到：\nWeb服务器运行效果\r如此激动人心的时候，让我们踏歌长行、梦想永续，下期见！\n","date":"2016-06-11T15:01:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3695777215/","slug":"3695777215","tags":["HTTP","服务器","C#"],"title":"使用 C#开发 HTTP 服务器系列之静态页面"},{"categories":["编程语言"],"content":" 各位朋友大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。到目前为止，“使用C#开发HTTP服务器”这个系列系列文章目前已经接近尾声了，虽然我们在服务器功能的完整性(如支持并发、缓存、异步、Htts等)上没有再继续深入下去，可是我们现在已经具备了一个基本的服务器框架啦，所以更多深层次的问题就需要大家根据自己的需要来扩展了，因为写博客更多的是一种“记录-输出-反馈”的一个过程，所以我更希望大家在看完我的博客后能对我有所反馈，因为抄博客上的代码实在是太无聊啦！好了，保持愉悦的心情我们下面来引出今天的话题：构建RESTful API。RESTful API，这个概念或许你曾经听说过，可能它和我们所熟悉的各种Web息息相关，甚至在某种意义上来讲它并不是一种新的技术，而这一切的一切归根到底都是在问一个问题，即网站真的是Web的唯一形态吗？\n什么是RESTful API 什么是RESTful API?首先，REST即REpresentational State Transfer，通常被翻译为“表述性状态传输”或者“表述性状态转移”，它最早出自Roy Fielding的《Archltectural Styles and the Design of Network-based Software Arcltechures》这篇论文，作者曾经参与HTTP协议和Apache Web Server的设计，所以REST实际上是一个和HTTP协议联系非常紧密的一种设计思想。而从这个题目中我们可以找到三个关键词：\n架构样式——(Archltectural Styles) 软件架构——(Software Arcltechures) 网络为基础——(Network-based) 所以从我个人角度来理解REST，我更倾向于将REST理解为一种以网络为基础的设计风格，因此REST从本质上来讲解决的是如何正确地使用Web标准的问题。\n以国内为例，当Google的Chrome浏览器选择以Chormium这种形式开源以后，国内厂商纷纷表示跟进以双核为主要特点进行了新一轮的互联网入口争夺战，虽然从技术角度来讲这让Chorme浏览器更加流行，可我们更应该注意到不同的厂商纷纷建立起自己的护城河，以牺牲Web的统一性和标准性来满足其商业竞争的需要，所以我们看到了即使在HTML5定稿以后，在不同浏览器对HTML5的支持区别依然非常大。微信带动了大量可以在朋友圈内流传的“H5”媒介，可是这个东西从来就不是HTML5，而微信内置的QQ浏览器内核更是以各种不兼容让开发者为此殚精竭虑，所以你问我REST是什么的时候，我会回答它是一种风格上统一的Web API，而根据百科中的描述REST通常被这样定义：\nREST是一组架构约束条件和原则，而满足这些约束条件和原则的应用程序就是RESTful。 REST的目标是构建可扩展的Web Service，它是一种更简单的SOAP协议以及以WSDL为基础的WebService的替代。 REST采用的是HTTP协议并通过HTTP中的GET、POST、PUT、DELETE等动词收发数据。 REST希望通过HTTP来完成对数据的元操作，即传统的CRUD(Create、Read、Update、Delete)分别对应GET、POST、PUT、DELETE，这样就统一了数据操作的接口，实现在不同平台上提供一套相同的服务。 REST是一种面向服务的、分布式的API设计风格。 从WebService看REST 在这里我们提到了SOAP、WSDL、RPC等概念，这是因为从某种意义上来讲，REST是这些概念的一种延伸。以我们熟悉的WebService为例，当我们需要从网络上获取天气预报信息时，我们可以采取两种思路，第一种是通过抓包分析相关天气预报网站来获取信息，第二种是通过调用互联网上提供的WebService来获得信息。虽然这两种方法在技术上具有相似性和可行性，可是我觉得对开发者来讲，除了技术层面的突破以外在道德层面的坚守更为重要，我们说\u0026quot;人无德不立，国无德不兴\u0026quot;正是如此，所以我们这里强烈推荐第二种思路。WebService能够让我们像调用一个方法一样获取信息，那么对我们来讲WebService到底是什么呢？\nWebService首先是一种服务，它不需要客户端提供额外的软件支持，只要客户端支持HTTP协议和XML这样两个特性就可以了。而对WebService自身来讲，它本身就是一种自我描述型的设计，所以服务端和客户端可以通过它来了解响应和请求的内容及格式，因为XML是一种平台无关、语言无关的文档结构，所以WebService是一种可以跨平台的Web API。WebService能够让客户端像调用本地代码一样调用服务端代码，所以WebService是一种分布式计算的Web应用程序组件。我们对WebService下了如此多的定义，其实核心是什么呢？核心是WebService是一种基于HTTP协议和XML的Web API。\n好了，现在我们再来说说什么是SOAP和WSDL。事实上，这些概念听起来都非常地学术，可是我保证这对我们理解REST会有所帮助。首先，SOAP即简单访问对象协议(Simple Object Access Protocol)，听起来感觉非常高大上吗？然而这是一个“唯一没有发明任何新技术的技术”。因为它是一个访问Web服务的协议，如同HTTP协议定义了访问Web的协议一样，SOAP在HTTP协议的基础上，采用XML定义了消息协议，所以SOAP本质上是使用XML进行通信的HTTP协议，这样听起来是不是非常熟悉啦，因为我们熟悉的AJAX同样是采用XML进行通信，所不同的是AJAX是运行在浏览器中的且其主要目的是实现页面的无刷新更新。需要说明的是，虽然SOAP的基础HTTP协议是基于TCP/IP协议的，可是SOAP是具有穿透防火墙的能力的，对此有兴趣的朋友可以自行了解，我们这里因为篇幅有限所以就不做详细说明啦！\n接下来，WSDL即Web服务描述语言(Web Service Description Language)，我对它的理解是提供了一个WebService的文档，因为从定义可以看出，它是一个基于XML的用于描述Web服务以及如何访问Web服务的语言，Web服务提供者通过它可以告知使用者当前Web服务访问的规范和说明，而Web使用者通过它可以在满足平台无关性和语言无关性的情况下快速进行开发，所以综合下来看，WebService和REST都能为我们提供类似地服务需求，关于两者或者说REST能为我们带来哪些不一样的体验，我们将在本文的第二部分说明。\n从WCF看REST 我觉得对技术而言，我们每个人都应该试图去发现技术背后真正美的东西，就像我们在了解了WebService，并发现它和REST从本质上来讲都是一个东西的时候，这个时候我们应该直接去了解REST给我们带来了哪些不一样的东西。可是事实上因为开发者使用的平台和语言的多样性，让开发者再这个过程中不得不去对平台或者语言造成依赖，而当每个厂商都试图建立一套自己的标准或者框架的时候，它对开发者造成的这种依赖感就越发地强烈。虽然我目前的工作是做.NET开发，可是事实上我最喜欢的只有微软的C#语言而已。这里我们简单介绍下WCF，WCF即Windows Communication Foundation是由微软发展的一组数据通信的应用程序开发接口，它是.NET框架的一部分，从.NET Framework 3.0开始引入，其设计目标是整合不同进程的通信、不同系统间的通信、C/S架构通信等等通信目标，所以对.NET开发者而言它是一个“全家桶”般的存在，我们到底需要“小而美”还是“大而全”，这是一个问题。\n回到我们关注本身，WCF整合了Web服务、.NET Remoting、消息队列和Enterprise Services的功能并将其整合在Visual Studio中，显然对我们而言，我们关注的核心依然在Web服务。首先，我们要明确的是WebService这个是行业标准，即WebService规范，这是一个和平台、和语言无关的标准，而微软的ASP.NET WebService是ASP.NET框架的组成部分，我不喜欢ASP.NET的一个原因就是我们常常认为网站是Web技术的核心而Web服务不是，更离谱的是我们认为开发一个Web服务器或者一个WebService一定要采用XXX框架，虽然使用Web框架、写业务代码都是技术能力的一种体现，可是不求甚解真的无法让我安心。那么WCF呢？其实WCF本质上是将ASP.NET WebService和微软的相关技术如Enterprise Services(COM+)、.NET Remoting、MSMQ消息队列等进行了整合，为什么要整合在一起呢？因为从宏观上来讲，跨进程、跨机器、跨网络都属于通信的范畴，所以我们现在回过头来看，这些东西玩来玩去有什么稀奇，归根到底还不是HTTP协议啊，我们追求新的技术并没有错，错误的是我们将希望寄托在技术本身，而不是我们自己。\n让REST理解起来简单点 我们从最初接触到REST的云里雾里，到翻来覆去地讲述WebService，其实我的目的只有一个，那就是告诉大家，Web技术发展到今天，从本质上来将变化并没有太大，可是为什么我们会看到前端领域每隔一段时间就会有新的框架产生呢？回答这个问题非常简单，所有的框架的提出都是因为某种业务的背景需要，而所有的业务无一不是因为人类增加了其复杂性，所以当你下来看待这一切的时候，你发现从WebService到REST其实变化都是非常细微的东西，与其在新技术里疲于奔命不如静下心来学习好HTML、CSS和JavaScript，虽然JavaScript是一个垃圾的语言，可是有时候它会让我们这些后端程序开发者都懵逼呢，哈哈，所以现在是时候给REST一个简单的定义：\nREST是一种使用URL来定位资源，使用HTTP请求描述操作的Web服务规范。\nREST的约束条件和原则 我们说REST本质上是Web服务的一种规范，一种思想，所以单独来说REST是没有意义的，这意味着，如果我们要深入了解REST，就需要了解它的约束条件和原则，下面我们就来说说这个问题。\n资源(Resources) 在REST中资源是整个架构或者说整个网络处理的核心，那么什么是资源呢？在我们传统的观念中，资源是指服务器上的一个文件，而在REST里资源则是指一个URL。URL即统一资源定位，而我们都知道通过URL可以访问互联网上的资源，所以在REST里这种对资源的指向性更加强烈，并且在这里资源的范畴会被无限放大而并非局限在文件本身，例如：\nhttp://api.qc.com/v1/feed 表示获取某人的最新Feed\rhttp://api.qc.com/v1/friends 表示获取某人的好友列表\rhttp://api.qc.com/v1/profile 表示获取某人的详细信息 由此我们注意到REST在形式上更加趋向API设计，而我们获取的资源则通过一定的形式进行统一而规范化的表达，因此REST实现了让不同的平台共享一套API这样的愿望，这是一件非常美好的事情，这个世界上的技术阵营举不胜数，而它们为了各自的利益建立一套封闭、臃肿的体系框架，很多时候当我们不需要这样的“全家桶”并且希望“跨平台”的时候，REST将会是一个不错的选择。\n表现形式(Representational) 在REST中表现形式作为我们对资源请求的一个结果的呈现，通过对HTTP协议的学习我们已经知道，服务器会给客户端返回什么形式的信息，这一点取决于服务器响应报文中相关头部字段，而对REST来讲，它通常会采用XML或者JSON来告诉请求者请求的结果，因为JSON相比XML所含的冗余信息较少，所以目前更加倾向于或者说流行使用JSON作为请求结果的表现形式。\n##状态变化(State Transfer) 虽然我们一再强调HTTP协议是无状态，这主要体现在HTTP请求与请求、HTTP响应与响应的上下文无关性上。在REST中，我们所说状态变化更多是指HTTP中的GET、POST、DELETE等动词实现。具体来讲，虽然这一点我们在前面有所提及我们来看下面的简单示例：\nGET http://someurl/tasks 表示获取全部的tasks\rPOST http://someurl/tasks 表示创建一个新的task\rGET http://someurl/tasks/{id} 表示获取一个指定id的task\rPET http://someurl/tasks/{id} 表示更新一个指定id的task\rDELETE http://someurl/tasks/{id} 表示删除一个指定id的task 除此之外，我们注意到REST基于HTTP协议，所以HTTP协议中的状态码对它来讲同样适用，例如最常用的200表示成功、500表示服务器内部错误、404表示无法找到请求资源等等。\n如何构建REST风格的API 如何构建REST风格的API?这是这篇文章的最后一个问题，相信大家在阅读这篇文章的时候会感到疲惫吧，我想说写作者的疲惫不一定会比阅读者的疲惫要轻，现在到了这篇文章里最难的部分啦，这可比我们花费大量篇幅来讲什么是REST要更有意义，这是真正的说起来容易做起来难，在正式开始实践以前，我们首先提出下面的最佳实践：\nURLRoot采用下面这样的结构： http://example.com/api/v1/\rhttp://api.example.com/v1/ API版本可以放在URL或者HTTP的Header里\nURL使用名词而非动词：\nhttp://example.com/api/v1/getProducts 这是一个糟糕的设计\rGET http://example.com/api/v1/products 这是一个优雅的设计 保证方法时安全的不会对资源状态有所改变。例如： GET http://example.com/api/v1/deleteProduct?id=1 这是一个危险的信号 资源的地址推荐使用嵌套结构 GET http://example.com/api/v1/friends/10375923/profile 使用正确的HTTP状态码表示访问状态\n返回含义明确的结果(这是我为什么推荐使用JSON的理由)\n好了，这篇文章我目前能够理解并输出给大家的只有这些啦，关于具体在Web开发中我们如何去实现RESTful API，这个我觉得并没有一个固定的方法吧，而且我现在编写的这个服务器只支持Get和Post两种类型，如果要实现一个完整的RESTful API架构，还需要很长的时间去探索，这篇文章写得我的确有些疲惫，所以有不周的地方希望大家谅解，后续更新关注我的项目HttpServer就好啦，谢谢大家！\n","date":"2016-06-11T15:01:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3637847962/","slug":"3637847962","tags":["HTTP","服务器","C#"],"title":"使用C#开发HTTP服务器系列之构建RESTful API"},{"categories":["编程语言"],"content":"各位朋友大家好，我是秦元培，欢迎大家关注我的博客。从今天起，我将开始撰写一组关于 HTTP 服务器开发的系列文章。我为什么会有这样的想法呢？因为人们对 Web 技术存在误解，认为网站开发是 Web 技术的全部。其实在今天这样一个时代，Web 技术可谓是无处不在，无论是传统软件开发还是移动应用开发都离不开 Web 技术，所以在我的认识中，任何使用了 HTTP 协议实现数据交互都可以认为是 Web 技术的一种体现，而且当我们提及服务器开发的时候，我们常常提及 Java 或者 PHP。可是这些重要吗？不，在我看来服务器开发和语言无关，和 IIS、Tomcat、Apache、Ngnix 等等我们熟知的服务器软件无关。Web 技术可以像一个网站一样通过浏览器来访问，同样可以像一个服务一样通过程序来调用，所以在接下来的时间里，我将和大家一起见证如何使用 C# 开发一个基本的 HTTP 服务器，希望通过这些能够让大家更好的认识 Web 技术。\n至繁至简的 HTTP 我们对 HTTP 协议最直观的认识应该是来自浏览器，因为在互联网时代我们都是通过浏览器这个入口来接触互联网的，而到了移动互联网时代我们开始思考新的互联网入口。在这个过程中我们有创新的模式不断涌现出来，同样有并购、捆绑、垄断等形式的恶性竞争此起彼伏，所谓“痛并快乐着”。我想说的是，HTTP 是一个简单与复杂并存的东西，那么什么是 HTTP 呢？我们在浏览器中输入 URL 的时候，早已任性地连“http”和“www”都省略了吧，所以我相信 HTTP 对人们来说依然是一个陌生的东西。\nHTTP 是超文本传输协议(HyperText Transfer Protocol)的简称，它建立在 C/S 架构的应用层协议，熟悉这部分内容的朋友应该清楚，TCP/IP 协议是协议层的内容，它定义了计算机间通信的基础协议，我们熟悉的 HTTP、FTP、Telnet 等协议都是建立在 TCP/IP 协议基础上的。在 HTTP 协议中，客户端负责发起一个 Request ，该 Request 中含有请求方法、URL、协议版本等信息，服务端在接受到该 Request 后会返回一个 Response，该 Response 中含有状态码、响应内容等信息，这一模型称为请求/响应模型。HTTP 协议迄今为止发展出 3 个版本：\n0.9 版本：已过时。该版本仅支持 GET 一种请求方法，不支持请求头。因为不支持 POST 方法，所以客户端无法向服务器传递太多信息。 HTTP/1.0 版本：这是第一个在通讯中指定版本号的 HTTP 协议版本，至今依然被广泛采用，特别是在代理服务器中。 HTTP/1.1 版本：目前采用的版本。持久连接被默认采用，并能很好地配合代理服务器工作。相对 1.0 版本，该版本在缓存处理、带宽优化及网络连接地使用、错误通知地管理、消息在网络中的发送等方面都有显著的区别。 HTTP 协议通信的核心是 HTTP 报文，根据报文发送者的不同，我们将其分为请求报文和响应报文。其中，由客户端发出的 HTTP 报文称为请求报文，由服务端发出的报文称为响应报文。下面我们来着重了解和认识这两种不同的报文：\n请求报文：请求报文通常由浏览器来发起，当我们访问一个网页或者请求一个资源的时候都会产生请求报文。请求报文通常由 HTTP 请求行、请求头、消息体(可选)三部分组成，服务端在接收到请求报文后根据请求报文请求返回数据给客户端，所以我们通常讲的服务端开发实际上是指在服务端接收到信息以后处理的这个阶段。下面是一个基本的请求报文示例： /* HTTP请求行 */ GET / HTTP/1.1 /* 请求头部 */ Accept: text/html, application/xhtml+xml, image/jxr, */* Accept-Encoding: gzip, deflate Accept-Language: zh-Hans-CN, zh-Hans; q=0.5 Connection: Keep-Alive Host: localhost:4000 User-Agent: Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko /* 消息体 */ 响应报文：响应报文是指在服务端接收并处理了客户端的请求信息以后，服务端发送给客户端的 HTTP 报文，服务端开发的重要工作就是处理来自客户端的请求，所以这是我们开发一个 HTTP 服务器的核心工作。和请求报文类似，响应报文由 HTTP 状态行、响应头、消息体(可选)三部分组成。例如我们通常熟悉的 200 和 404 分别表示连接正常和无法访问资源这两种响应状态。下面是一个基本的响应报文示例： /* HTTP状态行 */ HTTP/1.1 200 OK /* 响应头部 */ Content-Type: text/html;charset=utf-8 Connection: keep-alive Server: Microsoft-IIS/7.0 Date: Sun, 12 Jun 2016 11:00:42 GMT X-Powered-By: Hexo /* 消息体 */ 这里需要说明的是，实际的请求报文和响应报文会因为服务端设计的不同，和这里的报文示例略有不同，报文中头部信息参数种类比较多，我不打算在这里详细解释每个参数的含义，我们只需要对报文格式有一个基本的认识即可，想了解这些内容的朋友可以阅读这里。在请求报文中我们注意到第一行，即 HTTP 请求行指明当前请求的方法。所以下面我们来说说 HTTP 协议的基本请求方法。常见的方法有 GET、POST、HEAD、DELETE、OPTIONS、TRACE、CONNECT，我们这里选取最常用的两种方式，即 GET 和 PSOT 来讲解：\nGET：最为常见的一种请示方式。当客户端从服务器读取文档或者通过一个链接来访问页面的时候，都是采用 GET 方式来请求的。GET 请求的一个显著标志是其请求参数附加在 URL 后，例如\u0026quot;/index.jsp?id=100\u0026amp;option=bind\u0026quot;这种形式即为 GET 方式请求。GET 方式对用户而言，传递参数过程是透明的，因为用户可以通过浏览器地址栏直接看到参数，所以这种方式更适合用来设计 API，即在不需要验证身份或者对安全性要求不高的场合，需要注意的是 GET 方式请求对参数长度由一定限制。 POST：POST 克服了 GET 方式对参数长度存在限制的缺点，以键-值形式将参数封装在 HTTP 请求中，所以从理论上讲它对参数长度没有限制(实际上会因为浏览器和操作系统的限制而大打折扣)，而且对用户来讲参数传递过程是不可见的，所以它是一种相对安全的参数传递方式。通常用户登录都会采取这种方式，我们在编写爬虫的时候遇到需要登录的情况通常都需要使用 POST 方式进行模拟登录。 Socket 与 HTTP 的紧密联系 到目前为止，我们基本上搞清楚了 HTTP 是如何运作的，这恰恰符合普通人对技术的认知水平，或许在普通人看起来非常简单的东西，对技术人员来讲永远都是复杂而深奥的，所以从这个角度来讲，我觉的我们更应该向技术人员致敬，因为是技术人员让这些经过其简化以后的复杂流程以一种产品的形态走进了你我的生活，感谢有技术和技术人员的存在，让我们这个世界更加美好。好了，现在我们来思考这样一个问题，Socket 和 HTTP 有一种怎样的关联？这是因为我们目前所有对 HTTP 的理解都是一种形而上学上的理解，它现在仅仅是一种协议，可是协议离真正的应用很遥远不是吗？所以我们需要考虑如何去实现这样一种协议。我们注意到 HTTP 是建立在 TCP/IP 协议上的，所以 HTTP 的协议应该考虑用 TCP/IP 协议的实现来实现，考虑到 Socket 是 TCP/IP 协议的一种实现，所以我们非常容易地想到应该用 Socket 来构建一个 HTTP 服务器，由此我们找到了 Socket 和 HTTP 的紧密联系。\n在找到 Socket 和 HTTP 的紧密联系以后，我们现在就可以开始着手来设计一个 HTTP 服务器了。我们的思路是这样的，首先我们在服务端创建一个 Socket 来负责监听客户端连接。每次客户端发出请求后，我们根据请问报文来判断客户端的请求类型，然后根据不同的请求类型进行相应的处理，这样我们就设计了一个基本的 HTTP 服务器。\n从头开始设计 HTTP 服务器 好了，现在我们要开始从头设计一个 HTTP 服务器了，在此之前，我们首先来为整个项目设计下面的基本约束。我一直非常好奇为什么有的开发者会如此强烈地依赖框架。尤其是在 Web 开发领域，MVC 和 MVVM 基本上是耳熟能详到烂俗的词汇。我个人更加认同这是一种思想。什么是思想呢？思想是你知道其绝妙处而绝口不提，却在潜移默化中心领神会的运行它。可事实上是什么样呢？无数开发者被框架所禁锢，因为我们缺少了犯错的机会。所以我在这里不想再提及 Java、PHP、.NET 在 Web 开发领域里那些广为人知的框架，因为我认为忘掉这些框架可以帮助我们更好的理解框架，下面我就来用我的这种方法告诉大家什么叫做 MVC？\n什么叫做 MVC？我们都知道 MVC 由模型、视图、控制器三部分组成，可是它们的实质是什么呢？我想这个问题可能没有人想过，因为我们的时间都浪费在配置 XML 文档节点上。(我说的就是 Java 里的配置狂魔)\n首先，模型是什么呢？模型对程序员而言可以是一个实体类，亦可以是一张数据表，而这两种认知仅仅是因为我们看待问题的角度不同而已，为了让这两种认知模型统一，我们想到了ORM、想到了根据数据表生成实体类、想到了在实体类中使用各种语法糖，而这些在我看来非常无聊的东西，竟然可以让我们不厌其烦地制造出各种框架，对程序员而言我还是喜欢理解为实体类。\n其次，视图是什么呢？视图在我看来是一个函数，它返回的是一个 HTML 结构的文本，而它的参数是一个模型，一个经过我们实例化以后的对象，所以控制器所做的工作无非是从数据库中获取数据，然后将其转化为实体对象，再传递给视图进行绑定而已。这样听起来，我们对 MVC 的理解是不是就清晰了？而现在前端领域兴起的 Vue.js 和 React，从本质上来讲是在纠结控制器的这部分工作该有前端来完成还是该有后端来完成而已。\nMVC 中有一个路由的概念，这个概念我们可以和 HTTP 中请求行来对应起来，我们知道发出一个 HTTP 请求的时候，我们能够从请求报文中获得请求方法、请求地址、请求参数等一系列信息，服务器正是根据这些信息来处理客户端请求的。那么，路由到底是什么呢？路由就是这里的请求地址，它可以是实际的文件目录、可以是虚拟化的 Web API、可以是项目中的文件目录，而一切的一切都在于我们如何定义路由，例如我们定义的路由是\u0026quot;http://www.zhihu.com/people/vczh\u0026quot;，从某种意义上来讲，它和\u0026quot;http://www.zhihu.com/people/?id=vczh\u0026ldquo;是一样的，因为服务器总是能够一眼看出这些语法糖的区别。\n虽然我在竭尽全力地避免形成对框架的依赖，可是在设计一个项目的时候，我们依然需要做些宏观上的规划，我设计的一个原则就是简单、轻量，我不喜欢重度产品，我喜欢小而美的东西，就像我喜欢 C# 这门语言而不喜欢 ASP.NET 一样，因为我喜欢 Nancy 这个名字挺起来文艺而使用起来简单、开心的东西。我不会像某语言一样丧心病狂地使用接口和抽象类的，在我这里整体设计是非常简单的：\nIServer.cs：定义服务器接口，该接口定义了 OnGet()、OnPost()、OnDefault()、OnListFiles() 四个方法，分别用来响应 GET 请求、响应 POST 请求、响应默认请求、列取目录，我们这里的服务器类 HttpServer 需要实现该接口。 Request.cs：封装来自客户端的请求报文继承自 BaseHeader。 Response.cs：封装来自服务端的响应报文继承自 BaseHeader。 BaseHeader.cs: 封装通用头部和实体头部。 HttpServer.cs: HTTP 服务器基类需实现 IServer 接口。 因为我这里希望实现的是一种全局上由我来控制，细节上由你来决定的面向开发者的设计思路，这和通常的面向大众的产品思路是完全不同的。例如委托或者事件的一个重要意义就是，它可以让程序按照设计者的思路来运行，同时满足使用着在细节上的控制权。所以，在写完这个项目以后，我们就可以无需再关注客户端和服务端如何通信这些细节，而将更多的精力放在服务器接收到了什么、如何处理、怎样返回这样的问题上来，这和框架希望我们将精力放在业务上的初衷是一样的，可是事实上关注业务对开发者来讲是趋害的，对公司来讲则是趋利的。当你发现你因为熟悉了业务而逐渐沦落为框架填充者的时候，你有足够的理由来唤起内心想要控制一切的欲望。世界很大、人生很短，这本来就是一个矛盾的存在，当我们习惯在框架中填充代码的时候，你是否会想到人生本来没有这样的一个框架？\n好了，现在我们来开始编写这个 Web 服务器中通信的基础部分。首先我们需要创建一个服务端 Socket 来监听客户端的请求。如果你熟悉 Socket 开发，你将期望看到下面这样的代码：\n/// \u0026lt;summary\u0026gt; /// 开启服务器 /// \u0026lt;/summary\u0026gt; public void Start() { if(isRunning) return; //创建服务端Socket serverSocket = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp); serverSocket.Bind(new IPEndPoint(IPAddress.Parse(ServerIP), ServerPort)); serverSocket.Listen(10); isRunning = true; //输出服务器状态 Console.WriteLine(\u0026#34;Sever is running at http://{0}:{1}/.\u0026#34;, ServerIP, ServerPort); //连接客户端 while(isRunning) { Socket clientSocket = serverSocket.Accept(); Thread requestThread = new Thread(() =\u0026gt;{ ProcessRequest(clientSocket);}); requestThread.Start(); } } 这里我们使用 isRunning 来表示服务器是否运行，显然当服务器处在运行状态时，它应该返回。我们这里使用 ServerIP 和 ServerPort 分别表示服务端 IP 和端口，创建服务端 Socket 这里就不再赘述了，因为这是非常简单而基础的东西。当服务器处在运行状态时我们接受一个客户端请求，并使用一个独立的线程来处理请求，客户端请求的处理我们这里提供了一个叫做 ProcessRequest 的方法，它具体都做了什么工作呢？我们继续往下看：\n/// \u0026lt;summary\u0026gt; /// 处理客户端请求 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;handler\u0026#34;\u0026gt;客户端Socket\u0026lt;/param\u0026gt; private void ProcessRequest(Socket handler) { //构造请求报文 HttpRequest request = new HttpRequest(handler); //根据请求类型进行处理 if (request.Method == \u0026#34;GET\u0026#34;){ OnGet(request); } else if (request.Method == \u0026#34;POST\u0026#34;){ OnPost(request); } else { OnDefault(); } } 接下来我们可以注意到我们这里根据客户端 Soket 构造了一个请求报文，其实就是在请求报文的构造函数中通过解析客户端发来的消息，然后将其和我们这里定义的 HttpRequest 类对应起来。我们这里可以看到，根据请求方法的不同，我们这里分别采用 OnGet、OnPost 和 OnDefault 三个方法进行处理，而这些是定义在 IServer 接口中并在 HttpServer 类中声明为虚方法。严格来讲，这里应该有更多的请求方法类型，可是因为我这里写系列文章的关系，我想目前暂时就实现 Get 和 Post 两种方法，所以这里大家如果感兴趣的话可以做更深层次的研究。所以，现在我们就明白了，因为这些方法都被声明为虚方法，所以我们只需要 HttpServer 类的子类中重写这些方法就可以了嘛，这好像离我最初的设想越来越近了呢。关于请求报文的构造，大家可以到http://github.com/qinyuanpei/HttpServer/中来了解，实际的工作就是解析字符串而已，这些微小的工作实在不值得在这里单独来讲。\n我们今天的正事儿是什么呢？是 Hello World 啊，所以我们需要想办法让这个服务器给我们返回点什么啊，接下来我们继承 HttpServer 类来写一个具体的类 MyServer ，和期望的一样，我们仅仅需要重写相关方法就可以写一个基本的 Web 服务器，需要注意的是子类需要继承父类的构造函数。我们一起来看代码：\nusing System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Threading.Tasks; using System.IO; namespace HttpServerLib { public class MyServer : HttpServer { public MyServer(string ipAddress, int port) : base(ipAddress, port) { } public override void OnGet(HttpRequest request) { HttpResponse response = new HttpResponse(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hello World\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;, Encoding.UTF8); response.StatusCode = \u0026#34;200\u0026#34;; response.Server = \u0026#34;A Simple HTTP Server\u0026#34;; response.Content_Type = \u0026#34;text/html\u0026#34;; ProcessResponse(request.Handler, response); } } } 可以注意到我们这里构造了一个 HttpResponse ，这是我这里定义的 HTTP 响应报文，我们这里响应的内容是一段简单的 HTML 采用 UTF-8 编码。在构造完 HttpResponse 以后我们设定了它的相关状态，熟悉 Web 开发的朋友应该可以想到这是抓包工具抓包时得到的服务端报文信息，最近博主最喜欢的某个妹子写真集网站开始反爬虫了，因此博主以前写的 Python 脚本现在执行会被告知 403，这是一个禁止访问的状态码。解决方案其实非常简单地，将 HTTP 请求伪装成一个“浏览器”即可，思路就是在 HTTP 请求报文中增加相关字段，这样就可以“骗”过服务器，当然更深层次的“欺骗”就是 Cookie 和 Session 级别的伪装了，这个话题我们有时间再说。这里我们设定状态码为 200，这是一个正常的请求，其次 ContentType 等字段可以自行阅读 HTTP 协议中头部字段的相关资料，最后我们通过 ProcessResponse 这个方法来处理响应，其内部是一个使用 Socket 发送消息的基本实现，详细的设计细节大家可以看项目代码。\n现在让我们怀着无比激动的心情运行我们的服务器，此时服务器运行情况是：\n服务器运行情况\r这样是不是有一种恍若隔世的感觉啊，每次打开 Hexo 的时候看到它自带的本地服务器，感觉非常高大上啊，结果万万没想到有朝一日你就自己实现了它，这叫做“长大以后我就成了你吗”？哈哈，现在是见证奇迹的时刻：\n浏览器运行情况\r浏览器怀着对未来无限的憧憬，自豪地写下“Hello World”，正如很多年前诗人北岛在绝望中写下的《相信未来》一样，或许生活中眼前都是苟且，可是只要心中有诗和远方，我们就永远不会迷茫。好了，至此这个系列第一篇 Hello World 终于写完了，简直如释重负啊，第一篇需要理解和学习的东西实在太多了，本来打算在文章后附一份详细的 HTTP 头部字段说明，可是因为这些概念实在太枯燥，而使用 Markdown 编写表格时表格内容过多是写作者的无尽痛苦。关于这个问题，大家可以从这里找到答案。下期再见！\n","date":"2016-06-11T12:38:03Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3040357134/","slug":"3040357134","tags":["HTTP","服务器","C#"],"title":"使用 C# 开发 HTTP 服务器系列之 Hello World"},{"categories":["编程语言"],"content":"最近想尝试对一个 Ghost 博客主题进行移植，因为对一个后端程序员来说，进行前端方面的工作实在是个不小的挑战，而我对 CSS 更是有种与生俱来的恐惧感，所以我是非常喜欢Bootstrap和Materilize这种对后端程序员友好的前端框架。现在前端技术如火如荼，而前端技术作为最有可能实现跨平台技术的技术形态，相对原生技术有着更为灵活的适应性和扩展性，因此以响应式设计为代表的 Web 技术，能够让 Web 页面在不同尺寸屏幕上都有着相近的体验，因为目前软件开发基本都是在计算机设备上来完成的，这样我们在制作 Web 页面的时候就需要在不同的设备上进行调试，如果每次都将 Web 页面部署到远程服务器上，这样将浪费大量的时间而且容易将测试阶段的问题暴露给用户，因此本文将采用一种扫描二维码的方式来实现在移动设备上浏览响应式页面。\n工作原理 因为我们这里是在测试阶段在不同的移动设备上浏览响应式页面，所以这些 Web 页面实际上是部署在本地服务器上的，因此这个问题的实质就是如何让移动设备访问本地服务器，这个问题无论从原理上还是实现上来讲都不复杂，只要保证运行本地服务器的计算机和移动设备在同一个局域网内就可以了，考虑到移动设备的便携性，采用无线局域网的方式对移动设备更为友好。我们知道 Windows 系统，从 Windows7 版本以后就可以支持虚拟热点的创建，因此可以说是近水楼台啦！在这种情况下，理论上我们可以直接使用运行本地服务器的计算机的 IP 来访问本地服务器，可是因为不同的服务器软件配置不同以及不同的计算机设置不同等等外部性的因素，在实际操作的过程中依然存在各种问题，下面我们就来针对实际操作中需要注意和解决的问题，来说说具体的实现过程。\n实现过程 考虑到实际操作中的配置项目主要由计算机设置和服务器设置两部分组成，因此我们这里对这两部分各自进行详细说明。 ##计算机设置\n热点的创建 1、创建一个名称为 QRPager-WIFI 的无线网络，其密码为 88888888\nnetsh wlan set hostednetwork ssid=QRPager-WIFI key=88888888 2、开启网络热点确保其它设备可以访问这个热点\nnetsh wlan start hostednetwork 3、关闭网络热点\nnetsh wlan stop hostednetwork 防火墙设置 防火墙设置非常简单，因为关闭防火墙就好啦，这样可以保证其它设备能够正常访问本地服务器，在测试完页面后应该立即开启防火墙，这个世界可谓是充满了诱惑，有诱惑的地方就有危险，所以当我们通过互联网获取知识的同时，更为重要的一点是学会如何去甄别信息的真伪，魏则西事件让我们每一个人都感到痛心，可我们必须认识到，即使百度在你我的口诛笔伐中宣告破产，对这个世界的影响永远都是杯水车薪，所以无论是杀毒软件还是防火墙，任何形式的东西都不能代替你保护自我的意识，就像在地震、火灾这类破坏性灾难中，学会自救互救比等待公共救援更为有效。\n服务器设置 IIS 设置 IIS 设置起来非常简单，只要在网站“绑定”设置中设置本地服务器所在计算机的 IP 地址即可，这样做的目的是保证服务器可以使用除了 127.0.0.1 和 localhost 以外地址来访问，因为我们在手机上访问本地服务器的时候，需要指定本地服务器所在计算机的 IP。\nApache 设置 Apache 设置相对 Geek，因为我们都知道它需要去编辑 httpd.conf 这个文件来开启或者关闭特定功能。但在这里它并不会显得复杂，因为和 IIS 一样，我们要进行的修改是让 Apache 可以通过除 localhost 以外的地址进行访问。在该文件中找到下列片段：\n\u0026lt;Directory \u0026#34;D:/Program Files/WAMP/www/\u0026#34;\u0026gt; # # Possible values for the Options directive are \u0026#34;None\u0026#34;, \u0026#34;All\u0026#34;, # or any combination of: # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews # # Note that \u0026#34;MultiViews\u0026#34; must be named *explicitly* --- \u0026#34;Options All\u0026#34; # doesn\u0026#39;t give it to you. # # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.2/mod/core.html#options # for more information. # Options Indexes FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be \u0026#34;All\u0026#34;, \u0026#34;None\u0026#34;, or any combination of the keywords: # Options FileInfo AuthConfig Limit # AllowOverride all # # Controls who can get stuff from this server. # # onlineoffline tag - don\u0026#39;t remove Order Deny,Allow Allow from all #将这里由\u0026#34;Deny from all\u0026#34;修改为\u0026#34;Allow from all\u0026#34; Allow from 127.0.0.1 \u0026lt;/Directory\u0026gt; 将第 29 行代码由\u0026quot;Deny from all\u0026quot;修改为\u0026quot;Allow from all\u0026quot;即可，这样就可以给其它设备访问本地服务器的权限，至此，这个问题得以成功解决。\n总结 这篇文章无论从需求还是实现上来讲都是非常简单的，我之所以想写这篇文章，更多的是希望帮助大家克服这些“非技术性”的问题，因为有时候阻碍我们的可能并非问题本身，而是因为一个微不足道的部分，就像我们无法使用 Google 并非是因为 Google 使用起来有多么的困难，而是因为一道技术含量非常低的防火墙，在开发中除了我们使用的编程语言，任何开发工具、部署工具都可能出现这种问题，所以除了技术本身以外，关注这些“非技术性”因素同样是非常重要的一件事情。在这个设计中，有一个问题，即用户连接到无线网络是一个手动去完成的行为，换言之在扫描二维码前用户必须首先连接到无线网络。这样让我感觉显得不够优雅，我一直在思考有没有一种方法可以通过扫描二维码，直接建立移动设备到本地服务器的连接，可是二维码保存的是普通的文本信息，除非我能够通过二维码去调用 Android 系统中的 Intent，否则这个过程是没有办法实现自动化的，一个更好的想法是在 PC 端生成二维码图片，在手机端通过编写二维码应用来实现两者间的 Socket 通信，而通信的细节如 IP 地址、端口号等则可以通过二维码来进行加密和解密。\n","date":"2016-05-01T10:58:18Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2158696176/","slug":"2158696176","tags":["二维码","响应式","Web"],"title":"扫描二维码在移动设备上浏览响应式页面"},{"categories":["编程语言"],"content":"在使用 Mono 让.NET 程序跨平台运行这篇文章中，我们已经对 Mono 以及.NET 程序的运行机制有了初步的理解。今天我想来谈谈\u0026quot;使用 Mono 打造轻量级的.NET 运行时\u0026quot;这样一个话题。为什么我会有这样一种想法呢？因为 Mono 和.NET 都可以执行 IL 代码，所以我用 Mono 来作为.NET 程序的运行时是一个顺理成章的想法。由于.NET 程序需要.NET Framework 提供运行支持，所以当目标设备没有安装.NET Framework 或者.NET Framework 版本不对的时候，我们的程序都无法顺利运行。强迫用户安装.NET 框架无疑会影响用户体验，在 Windows XP 尚未停止服务前，国内软件厂商为了兼容这些用户，通常会选择 C++这类语言来编写原生应用，这就造成了国内.NET 技术长期不被重视的现状。\n考虑.NET 版本的兼容 在考虑使用 Mono 来作为.NET 应用程序的运行时前，首先我们来考虑.NET 版本的兼容问题。假设我们使用.NET Framework 3.5 版本生成了一个应用程序，那么这个应用程序将在安装了.NET Framework v3.5 的计算机上运行，如果目标计算机上没有安装.NET Framework v3.5，这个应用程序将无法正常运行。这个时候，我们可以有两种解决方案：第一种，强迫用户安装.NET Framework v3.5，无论是将该框架集成到安装包中，还是在安装软件的时候自动从网上下载安装，显然这种方式都会影响用户的使用体验让用户对应用程序的印象大打折扣；第二种，尝试让应用程序和.NET 版本兼容。我们知道 Android 程序有一个最低 API 版本的设置，这样能够保证应用程序在不低于该 API 版本的设备上运行。这里我们选择这种思路，在.NET 程序中，我们可以通过应用程序配置文件中的 supportedRuntime 节点来指定应用程序运行的.NET Framework 版本。例如下面的配置文件能够保证应用程序在.NET Framework v2.0 到 v3.5 间的版本上运行。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;startup\u0026gt; \u0026lt;supportedRuntime version=\u0026#34;v2.0.50727\u0026#34;/\u0026gt; \u0026lt;supportedRuntime version=\u0026#34;v3.0\u0026#34;/\u0026gt; \u0026lt;supportedRuntime version=\u0026#34;v3.5\u0026#34;/\u0026gt; \u0026lt;/startup\u0026gt; \u0026lt;/configuration\u0026gt; 虽然说这样能够保证应用程序的兼容性，可是你这个应用程序的命运却是掌握在.NET Framework 手里的，如果用户的计算机上没有安装.NET Framework 我们一样还是没辙儿，那么怎么办呢？我们来搭建 Mono 运行时。\nMono 运行时的搭建 我们在前面说过，Mono 主要由三部分组成，即 C#编译器(mcs.exe)、Mono 运行时(mono.exe)和基础类库。因为我们这里是为了让编写的.NET 应用程序运行在 Mono 运行时中，所以我们这里需要的是 Mono 运行时(mono.exe)和基础类库。我们建立如下的目录结构：\nMono运行时目录结构\r下面来说说这些目录各自的结构和功能：\nbin 目录：放置 Mono 运行时的目录，主要放置 mono.exe、mono-2.0.dll、libgio-2.0-0.dll、libglib-2.0-0.dll、libgthread-2.0-0.dll 共 5 个文件。 lib 目录：放置 Mono 依赖库的目录，主要放置.NET 库目录(此处以 4.0 为例)、Gac 库目录。其中 Gac 库目录下的 Accessibility、Mono.Posix、System、System.Drawing、System.Windows.Forms 共 5 个子目录是我们开发 WindowsForm 需要使用到的依赖库。 etc 目录：放置我们编写的程序及其相关文件，主程序的文件名为 Main.exe。 好了，现在我们就具备了一个非常轻量级的.NET 程序运行环境(其实整个环境的大小在 40M 左右)，注意以上文件都可以在安装 Mono 在其安装目录内找到。根据博主目前了解到的资料来看，通过 Mono 运行时来运行文件主要有命令行和一种被称为 Mono Embedding 的方案。特别地，第二种方案可以直接将运行时嵌入到程序内，我们熟悉的 Unity3D 引擎就是将整个脚本的运行时嵌入到了 C++程序中，但是这种方式比较复杂，暂时博主还没有弄清楚它的内部机制，所以我们这里选择第一种方案。可是它要用命令行啊，迫使普通用户来使用命令行工具是件痛苦的事情，就像我们常常被 Git 搞得晕头转向一样。那么，我们就用程序来模拟命令行好了！什么？用程序来模拟命令行？这个用 C#来写简直不能更简单了好吗？请注意我们这里是不能使用.NET Framework 里的功能的，因为它就是一个引导程序嘛，如果引导程序都需要依赖.NET，那我们这个程序怎么搞啊。\n好嘛，那就写 C++原生应用吧，它是无需任何依赖的。而在 C++中模拟命令行主要有 WinExec、ShellExecute 和 CreateProcess 三种方法，关于这三种方法的异同大家可以自行了解，这里我们选择最简单的 WinExec。代码如下：\n#include \u0026lt;Windows.h\u0026gt; int main(int agrc,char *args[]) { /* 执行命令 */ WinExec(\u0026#34;bin\\\\mono.exe etc\\\\Main.exe\u0026#34;,SW_NORMAL); return 0; } 我们将编译好的程序命名为 Launcher.exe，放置我们前面定义的 Mono 运行时目录结构的根目录下，这个文件将作为启动文件暴露给用户，当用户点击这个程序后就可以打开主文件 Main.exe。好了，现在我们来验证下我们的想法：\n运行在 Mono 运行时下的程序\r作为对比，我们给出正常情况下程序的运行截图：\n运行在 .NET 框架下的程序\r这样我们现在这个程序就基本实现了脱离.NET 框架运行，为什么说是基本呢？因为.NET 中的基础类库是作为.NET 框架中的一部分存在的，即它并非是 CLR 的内容。所以我们现在使用到的大部分的基础类库都是 Mono 重新实现的版本，如果我们使用的某一个库在 Mono 中没有相应的实现，那么我们就需要自己想办法来解决依赖问题了。现在这个方案每次运行的时候都会闪出命令行窗口，虽然不影响使用，但对一个追求完美的人来说就是瑕疵啦，怎么解决呢？答案就是 Mono Embedding。\n本文小结 本文通过 Mono 实现了一个轻量级的.NET 程序运行环境，从某种程度上来说，它间接地实现了.NET 程序脱离.NET Framework 运行。这个方案目前看起来存在的主要问题是库依赖的问题，我们现在这个环境有将近 40M 左右的体积，这是因为我们将常用的库都放在了 lib 目录中，可是在实际运行中，这些库并非完全都会用到，因此如何根据程序来生成合适的 lib 目录，是解决运行时环境体积的有效方法。如果靠手动来解决这个问题，这显得困难重重，因为在平时微软将这些库都给我们了，它就在我们的计算机上，所以我们从来没有关注过这个问题。现在当我们面对这个问题的时候，反射可能是种不错的想法，但这种想法的验证就要等到以后啦！\n","date":"2016-03-25T12:47:58Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/907824546/","slug":"907824546","tags":[".NET","Mono","跨平台","Linux"],"title":"使用 Mono 打造轻量级的.NET 程序运行时"},{"categories":["编程语言"],"content":"众所周知，Unity3D 引擎凭借着强大的跨平台能力而备受开发者的青睐，在跨平台应用开发渐渐成为主流的今天，具备跨平台开发能力对程序员来说就显得特别重要。传统的针对不同平台进行开发的方式常常让开发者顾此失彼，难以保证应用程序在不同的平台都有着相同的、出色的体验，这种情况下寻找到一种跨平台开发的方式将会为解决这个问题找到一种思路。从目前的开发环境来看，Web 应该是最有可能成为跨平台开发的神兵利器，可是长期以来 Web 开发中前端和后端都有各自不同的工作流，虽然现在出现了前端和后端逐渐融合的趋势，可在博主看来想让 Web 开发变得像传统开发这样简单还需要一定的过渡期。\n从 Mono 到 Xamarin 对 Unity3D 来说，Mono 是实现它跨平台的核心技术。Mono 是一个旨在使得.NET 在 Linux 上运行的开源项目。它通过内置的 C#语言编译器、CLR 运行时和各种类库，可以使.NET 应用程序运行在 Windows、Linux、FreeBSD 等不同的平台上。而在商业领域，Xamarin 则实现了用 C#编写 Android 和 iOS 应用的伟大创举。Windows10 发布的时候，微软提出了通用应用 UWP 的设想，在这种设想下开发者可以直接在最新的 Visual Studio 中使用 C#编写跨平台应用。最近微软收购了 Xamarin，这一举措能够保证 Xamarin 这样的商业项目可以和微软的产品融合地更好。虽然在传统 Web 开发中 Java 和 PHP 目前占据主要优势，可是虽然云计算技术的流行，服务器成本的降低或许会让 C#这样优秀的语言更加成熟。我一直坚信技术没有好坏的区别，一切技术问题的核心是人，所以接下来，我们打算追随着跨平台开发的先驱——Java，最早提出的“一次编写、到处运行”的伟大思想来探索 C#程序跨平台的可能性。\nMono 跨平台的原理 在提到 Mono 跨平台的时候，我们首先需要引入公共语言基础(Common Language Infrastructure，CLI)这个概念，CLI 是一套 ECMA 定义的标准，它定义了一个和语言无关的跨体系结构的运行环境，这使得开发者可以用规范定义内各种高级语言来开发软件，并且无需修正即可让软件运行在不同的计算机体系结构上。因此我们可以说跨平台的原理是因为我们定义了这样一个和语言无关的跨体系结构的运行环境规范，只要符合这个规范的应用程序都可以运行在不同的计算机体系结构上，即实现了跨平台。针对这个标准，微软实现了公共语言运行时（Common Language Runtime，CLR)，因此 CLR 是 CLI 的一个实现。我们熟悉的.NET 框架就是一个在 CLR 基础上采用系统虚拟机的编程平台，它为我们提供了支持多种编程语言如 C#、VB.NET、C++、Python 等。我们编写的 C#程序首先会被 C#编译器编译为公共中间语言即 CIL 或者是 MSIL(微软中间语言)，然后再由 CLR 转换为操作系统的原生代码（Native Code）。\n好了，现在我们来回答最开始的问题：Mono 为什么能够跨平台。我们回顾.NET 程序运行机制可以发现实现.NET 跨平台其实需要这三个关键：编译器、CLR 和基础类库。在.NET 下我们编写一个最简单的“Hello World”都需要 mscorlib.dll 这个动态链接库，因为.NET 框架已经为我们提供了这些，因为在我们的计算机上安装着.NET 框架，这是我们编写的应用程序能够在 Windows 下运行的原因。再回头来看 Mono，首先 Mono 和 CLR 一样，都是 CLI 这一标准的实现，所以我们可以理解为 Mono 实现了和微软提供给我们的类似的东西，因为微软的.NET 框架属于商业化闭源产品，所以 Mono 除了在实现 CLR 和编译器的同时实现了大量的基础库，而且在某种程度上 Mono 实现的版本与相同时期.NET 的版本有一定的差距，这点使用 Unity3D 开发游戏的朋友应该深有感触吧！这就决定了我们在将应用程序移植到目标平台时能否实现在目标平台上和当前平台上是否能够具有相同的体验。因为公共中间语言即 CIL 能够运行在所有实现了 CLI 标准的环境中，而 CLI 标准则是和具体的平台或者说 CPU 无关的，因此只要 Mono 运行时能够保证 CIL 的运行，就可以实现应用程序的跨平台。我们可以通过下面这张图来总结下这部分内容：\n开发第一个跨平台程序 下面我们来尝试开发第一个跨平台程序，我们使用 Visual Studio 或者 MonoDevelop 编写一个简单的控制台应用程序，为了减少这个程序对平台特性的依赖，我们这里选择 System 这个命名空间来实现最为基础的 Hello World，这意味着我们的应用程序没有使用任何除 mscorlib.dll 以外的库：\nusing System; namespace MonoApplication { class MainClass { public static void Main(string[] args) { Console.WriteLine(\u0026#34;Hello World!\u0026#34;); } } } 因为我们的计算机安装了.NET 框架，所以我们编写的这个程序会被 C#编译器编译为公共中间语言 CIL,然后再由 CLR 转换为 Native Code。通常情况下公共中间语言(CIL)会被存储到.il 文件中，可是在这里我们在编译的时候好像并没有看到这个文件的生成啊，这是因为这里生成的可执行文件(.exe)本质上是公共中间语言(CIL)形态的可执行文件。这一点我们可以通过 ildasm 这个工具来验证，该工具可以帮助我们查看 IL 代码，通常它位于 C:\\Program Files\\Microsoft SDKs\\Windows\\v7.0A\\bin 这个位置。下面是通过这个工具获得的 IL 代码：\n.method public hidebysig static void Main(string[] args) cil managed { .entrypoint // 代码大小 13 (0xd) .maxstack 8 IL_0000: nop IL_0001: ldstr \u0026#34;Hello World!\u0026#34; IL_0006: call void [mscorlib]System.Console::WriteLine(string) IL_000b: nop IL_000c: ret } // end of method MainClass::Main 可以看到这段代码和我们编写的程序中的 Main 方法完全对应，关于这段代码的含义，大家可以通过搜索引擎来了解 IL 代码的语法。因为我们这里想要说明的是，这里生成的可执行文件(.exe)从本质上来讲并非是一个可执行文件。因为它能否执行完全是取决于 CPU 的，这和我们直接用 C++编写的应用程序不同，我们知道不同的编译器如 Windows 下的 VC++和 Linux 下的 GCC 都是和硬件紧密相连的，所以我们编译的程序能够在各自的平台直接运行，即 CPU 是认识这些程序的。可是在.NET 这里就不一样了，因为我们通过 C#编译器即 csc.exe 编译出来的文件，其实是一个看起来像可执行文件，实际上却是一个和平台无关、和 CPU 无关的 IL 文件。\n那么我们就会感到迷茫了啊，平时我们编译完 C#程序双击就可以打开啊，哈哈，现在隆重请出.NET 程序的家长公共语言运行时(CLR)。公共语言运行时实际上是程序运行的监管者，程序运行的情况完全由运行时来决定。我们双击这个文件的时候，公共语言运行时会将其加载到内存中，然后由即时编译器(JIT)来识别 IL 文件，然后由 CPU 去完成相应的操作。\n所以我们可以这样理解.NET 程序跨平台，因为 IL 文件是一个和平台无关、和 CPU 无关的、跨平台的文件结构，所以我们只需要在不同的平台上实现这样一个公共语言运行时(CLR)就可以实现在不同的平台上运行同一个程序。但这个过程中，需要有一个 C#编译器负责将 C#代码转换为 IL 代码，然后需要有一个公共语言运行时(CLR)来解析 IL 代码。与此同时，我们在.NET 框架下使用了大量的基础类库，这些类库在 Windows 以外的平台是没有的，所以除了 C#编译器和公共语言运行时以外，我们还需要基础类库。现在大家是不是对 Mono 有了更清楚的认识了呢？没错，Mono 所做的事情其实就是我们在讨论的这些事情。这里博主想说说即时编译(JIT)和静态编译(AOT)，这两种编译方式我们可以按照\u0026quot;解释型\u0026quot;和\u0026quot;编译型\u0026quot;来理解,为什么 Unity3D 在 iOS 平台上做热更新的时候会出现问题呢？这是因为 iOS 平台考虑到安全性禁止使用 JIT 即时编译，所以像 C#这种需要编译的语言在这里就无计可施了。\n好了，既然我们有 Mono 这样的工具能够帮助我们实现跨平台开发。那么我们现在就来考虑将这个程序移植到 Linux 平台，这里以 Linux Deepin 为例，我们按照 C#程序编译的过程来完成这个移植过程：\n1、将 C#程序编译为 IL 文件：在.NET 下我们使用 csc.exe 这个程序来完成编译，在 Mono 下我们使用 mcs.exe 这个程序来完成编译，这个程序在安装完 Mono 以后在其安装目录内可以找到。我们在命令行下输入命令： mcs D:\\项目管理\\CSharp\\MonoApplication\\MonoApplication\\Main.cs 2、这样将生成 Main.exe 这样一个 IL 文件，现在我们需要一个运行时来解析它，在.NET 下我们使用 CLR 来完成这个步骤，在 Mono 下我们使用 mono.exe 这个文件来完成这个步骤。我们在命令行下输入下列命令： mono D:\\项目管理\\CSharp\\MonoApplication\\MonoApplication\\Main.exe 在Mono中运行.NET程序\r我们可以看到命令行下输出了我们期望的 Hello World，这意味着我们编写的程序现在运行在 Mono 中了，实际上在 Windows 下由 Mono 提供的 C#编译器 mcs.exe 编译的 IL 文件双击是可以直接运行的，因为我们的计算机上安装了 CLR，它作为.NET 的一部分内置在我们的计算机中。由此我们会发现一个问题，我们这里的跨平台实际上是编译器、运行时和基础类库这三部分的跨平台，这意味着我们在 Linux 下运行.NET 程序是需要 Mono 提供支持的。因为在这里我无法在 Linux 离线安装 Mono，所以 Linux 下运行.NET 程序的验证需要等博主以后有时间再来更新啦！可是我们可以想象到，通过 C#编译器编译得到的可执行文件在 Linux 下是无法正常运行的，因为通常情况下 Windows 程序在 Linux 下运行是需要虚拟机环境或者 Wine 这样的软件来支持的，显然让这样一个 Windows 程序运行在 Linux 环境下是因为我们在 Linux 下安装了 Mono。\n谈谈 Mono 跨平台以后 好了，到现在为止我们基本理清了 Mono 跨平台的原理。我们知道微软的技术体系在发展过程中因为某些历史遗留问题，.NET 程序在不同的 Windows 版本中的兼容性有时候会出现问题，虽然微软宣布 Windows XP 停止维护，我们编写 Windows 应用程序的时候可以忽略对 Windows XP 版本的支持，可是因为国内用户不喜欢在线更新补丁的这种普遍现状，所以假如让用户在安装程序的时候先去安装.NET 框架一定会降低用户体验，其次.NET 框架会增加应用程序安装包的大小，所以我们需要一种能够让我们开发的.NET 应用程序在脱离微软的这套技术体系时，同时能够安全、稳定的运行，所以我们这里考虑借助 Mono 让.NET 程序脱离.NET 框架运行。\n首先，我们来说说.NET 程序为什么能够脱离.NET 框架运行，我们注意到 Mono 提供了一个 Mono 运行时，所以我们可以借助这样一个运行时来运行编译器生成的 IL 代码。我们继续以 Hello World 为例，我们在使用 Mono 编译出 IL 代码以后需要使用 Mono 运行时来解析 IL 代码，所以假如我们可以编写一个程序来调用 Mono 运行时就可以解决这个问题。在这个问题中，其实精简应用程序安装包的大小从本质上来讲就是解决基础类库的依赖问题，因为 Mono 实现了.NET 框架中大部分的基础类库，所以移植.NET 应用程序的关键是基础类库的移植，比如 WinForm 在 Linux 下的解决方案是 GTK，这些细节在考虑跨平台的时候都是非常重要的问题。\n小结 本文从 Mono 跨平台的原理说起，探讨了.NET 应用程序跨平台的可能性和具体实现。跨平台是一个涉及到非常多内容的话题，我个人理解的跨平台是要编写跨平台的代码，这意味着我们在编写程序的时候需要考虑减少对平台特性的移植，比如说 Linq 是一个非常棒的特性，可是这个特性离开了 Windows、离开了.NET 就没有办法得到保证，所以如果要让使用了 Linq 的应用程序跨平台就会是一件非常麻烦的事情！在不同的平台间保持相同的体验很难，就像我们编写的 Web 程序在不同的浏览器间都有着不一样的表现，所以跨平台这个问题我们就抱着学习的态度来研究吧！\n","date":"2016-03-06T12:20:09Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1836680899/","slug":"1836680899","tags":["跨平台","Mono",".NET","Linux"],"title":"使用 Mono 让.NET 程序跨平台运行"},{"categories":["Unity3D"],"content":"各位朋友，大家好，欢迎大家关注我的博客，我是秦元培，我的博客地址是：http://qinyuanpei.com。今天博主想和大家分享的是在 Unity3D 中基于订阅者模式实现消息传递机制，我们知道 Unity3D 中默认提供了一种消息传递机制 SendMessage，虽然 SendMessage 使用起来的确非常简单，可是它的这种简单是建立在付出一定的代价的基础上的。经常有朋友提及不同的模块间如何进行通信的问题，可能答案最终会落到单例模式、委托和事件机制这些关键词上，在这种情况下本文所探讨的内容可能会帮助你找到最终的答案。\n从生活中得到的启示 我们知道通过在 Unity3D 中通过 GetComponent 就可以获得某个模块的实例，进而引用这个实例完成相关任务的调用。可是显然这种方法，就像我们随身带着现金去和不同的人进行交易，每次交易的时候都需要我们考虑现金的支入和支出问题，从安全性和耦合度两个方面进行考虑，这种方法在面对复杂的系统设计的时候，非常容易造成模块间的相互依赖，即会增加不同模块间的耦合度。为了解决这个问题，大家开始考虑单例模式，因为单例模式能够保证在全局内有一个唯一的实例，所以这种方式可以有效地降低模块间的直接引用。单例模式就像是我们在银行内办理了一个唯一的账户，这样我们在交易的时候只需要通过这个账户来进行控制资金的流向就可以了。单例模式确保了各个模块间的独立性，可是单例模式更多的是一种主动行为，即我们在需要的时候主动去调用这个模块，单例模式存在的问题是无法解决被调用方的反馈问题，除非被调用方主动地去调用调用方的模块实例。说到这里我们好像看到了一种新的模式，这就是我们下面要提到的事件机制。\n订阅者模式和事件机制 首先这里要提到一种称为“订阅者模式”的设计模式，这种设计模式在《大话设计模式》这本书中称为“观察者模式”或者“发布-订阅（Publish/Subscribe）模式”，我们这里暂且叫做“订阅者模式”吧！该模式定义了一种一对多的依赖关系，让多个观察者对象同时监听某一个主题对象。这个对象在状态发生变化时会通知所有观察者对象，使它们能够自动更新自己。针对这个模式，我们可以考虑事件机制的实现，事件机制可以理解为在一个事件中心（Subject）保存有对所有事件（Observer）的引用，事件中心负责对这些事件进行分发，这样每个事件就可以通过回调函数的方式进行更新，这样就实现了一个事件机制。下面给出基本的代码实现：\nusing System; using System.Collections; using System.Collections.Generic; using UnityEngine; namespace UniEventDispatcher { /// \u0026lt;summary\u0026gt; /// 定义事件分发委托 /// \u0026lt;/summary\u0026gt; public delegate void OnNotification(Notification notific); /// \u0026lt;summary\u0026gt; ///通知中心 /// \u0026lt;/summary\u0026gt; public class NotificationCenter { /// \u0026lt;summary\u0026gt; /// 通知中心单例 /// \u0026lt;/summary\u0026gt; private static NotificationCenter instance=null; public static NotificationCenter Get() { if(instance == null){ instance = new NotificationCenter(); return instance; } return instance; } /// \u0026lt;summary\u0026gt; /// 存储事件的字典 /// \u0026lt;/summary\u0026gt; private Dictionary\u0026lt;string,OnNotification\u0026gt; eventListeners = new Dictionary\u0026lt;string, OnNotification\u0026gt;(); /// \u0026lt;summary\u0026gt; /// 注册事件 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;eventKey\u0026#34;\u0026gt;事件Key\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;eventListener\u0026#34;\u0026gt;事件监听器\u0026lt;/param\u0026gt; public void AddEventListener(string eventKey,OnNotification eventListener) { if(!eventListeners.ContainsKey(eventKey)){ eventListeners.Add(eventKey,eventListener); } } /// \u0026lt;summary\u0026gt; /// 移除事件 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;eventKey\u0026#34;\u0026gt;事件Key\u0026lt;/param\u0026gt; public void RemoveEventListener(string eventKey) { if(!eventListeners.ContainsKey(eventKey)) return; eventListeners[eventKey] =null; eventListeners.Remove(eventKey); } /// \u0026lt;summary\u0026gt; /// 分发事件 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;eventKey\u0026#34;\u0026gt;事件Key\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;notific\u0026#34;\u0026gt;通知\u0026lt;/param\u0026gt; public void DispatchEvent(string eventKey,Notification notific) { if (!eventListeners.ContainsKey(eventKey)) return; eventListeners[eventKey](notific); } /// \u0026lt;summary\u0026gt; /// 分发事件 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;eventKey\u0026#34;\u0026gt;事件Key\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;sender\u0026#34;\u0026gt;发送者\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;param\u0026#34;\u0026gt;通知内容\u0026lt;/param\u0026gt; public void DispatchEvent(string eventKey, GameObject sender, object param) { if(!eventListeners.ContainsKey(eventKey)) return; eventListeners[eventKey](new Notification(sender,param)); } /// \u0026lt;summary\u0026gt; /// 分发事件 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;eventKey\u0026#34;\u0026gt;事件Key\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;param\u0026#34;\u0026gt;通知内容\u0026lt;/param\u0026gt; public void DispatchEvent(string eventKey,object param) { if(!eventListeners.ContainsKey(eventKey)) return; eventListeners[eventKey](new Notification(param)); } /// \u0026lt;summary\u0026gt; /// 是否存在指定事件的监听器 /// \u0026lt;/summary\u0026gt; public Boolean HasEventListener(string eventKey) { return eventListeners.ContainsKey(eventKey); } } } 注意到在这个“通知中心”中，我们首先实现了单例模式，这样我们可以通过 Get 方法来获取该“通知中心”的唯一实例，其次这里利用一个字典来存储对所有事件的引用，这样保证外部可以通过 AddEventListener 和 RemoveEventListener 这两个方法来进行事件的添加和移除，对于添加的事件引用我们可以通过 DispatchEvent 方法来分发一个事件，事件的回调函数采用委托来实现，注意到这个委托需要一个 Notification 类型，对该类型简单定义如下：\nusing System; using UnityEngine; namespace UniEventDispatcher { public class Notification { /// \u0026lt;summary\u0026gt; /// 通知发送者 /// \u0026lt;/summary\u0026gt; public GameObject sender; /// \u0026lt;summary\u0026gt; /// 通知内容 /// 备注：在发送消息时需要装箱、解析消息时需要拆箱 /// 所以这是一个糟糕的设计，需要注意。 /// \u0026lt;/summary\u0026gt; public object param; /// \u0026lt;summary\u0026gt; /// 构造函数 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;sender\u0026#34;\u0026gt;通知发送者\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;param\u0026#34;\u0026gt;通知内容\u0026lt;/param\u0026gt; public Notification(GameObject sender, object param) { this.sender = sender; this.param = param; } /// \u0026lt;summary\u0026gt; /// 构造函数 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;param\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; public Notification(object param) { this.sender = null; this.param = param; } /// \u0026lt;summary\u0026gt; /// 实现ToString方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; public override string ToString() { return string.Format(\u0026#34;sender={0},param={1}\u0026#34;, this.sender, this.param); } } } 对 Notification 的定义需要提供发送者和发送内容，这样可以保证所有的通知都按照这样的格式进行定义，如果有 Socket 开发经验的朋友可能会联想到通讯协议的定义，这里是比较相似啦，哈哈！\n使用事件机制的一个示例 这里以一个简单的示例来验证事件机制的可行性，我们在场景中有一个球体，默认这个球体的颜色为白色，通过调整界面中的 RGB 数值，可以改变球体的颜色，在这个示例中 UI 是事件发送者，负责 UI 中 Slider 控件的数值发生变化时向球体发送消息，传递的数据类型是 Color 类型；球体为事件接收者，负责注册事件及接收到消息后的处理。因为代码较为简单，所以这里写在一个脚本中：\nusing UnityEngine; using UnityEngine.UI; using System.Collections; using UniEventDispatcher; public class Example : MonoBehaviour { /// \u0026lt;summary\u0026gt; /// R数值的Slider /// \u0026lt;/summary\u0026gt; private Slider sliderR; /// \u0026lt;summary\u0026gt; /// G数值的Slider /// \u0026lt;/summary\u0026gt; private Slider sliderG; /// \u0026lt;summary\u0026gt; /// B数值的Slider /// \u0026lt;/summary\u0026gt; private Slider sliderB; void Start () { //在接收者中注册事件及其回调方法 NotificationCenter.Get().AddEventListener(\u0026#34;ChangeColor\u0026#34;, ChangeColor); //在发送者中分发事件，这里以UI逻辑为例 sliderR = GameObject.Find(\u0026#34;Canvas/SliderR\u0026#34;).GetComponent\u0026lt;Slider\u0026gt;(); sliderG = GameObject.Find(\u0026#34;Canvas/SliderG\u0026#34;).GetComponent\u0026lt;Slider\u0026gt;(); sliderB = GameObject.Find(\u0026#34;Canvas/SliderB\u0026#34;).GetComponent\u0026lt;Slider\u0026gt;(); //注册UI事件 sliderR.onValueChanged.AddListener(OnValueChanged); sliderG.onValueChanged.AddListener(OnValueChanged); sliderB.onValueChanged.AddListener(OnValueChanged); } public void OnValueChanged(float value) { //获得RGB数值 float r = sliderR.value; float g = sliderG.value; float b = sliderB.value; //分发事件,注意和接收者协议一致 NotificationCenter.Get().DispatchEvent(\u0026#34;ChangeColor\u0026#34;, new Color(r, g, b)); } /// \u0026lt;summary\u0026gt; /// 改变物体材质颜色 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;notific\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; public void ChangeColor(Notification notific) { Debug.Log(notific.ToString()); //设置颜色 renderer.material.color = (Color)notific.param; } } 该示例运行效果如下：\n事件机制的简单示例\r小结 虽然目前这个事件机制在实现和使用上没有什么问题，可是从扩展性和可优化性上来考虑，这个设计目前存在以下问题：\n字符型的键名使用起来方便，可是对通知者和接收者由 1 个以上的人力来维护的时候双方需要通过沟通来确定键名，可以考虑使用 GameObject 或者 Transform 来替代现在的键名设计，可是这种设计带来的新问题是会增加不同模块间的 GameObject 或者 Transform 的相互引用。 通知者和接收者在传递参数和接受参数的时候需要分别进行装箱和拆箱，所以这并非一个优秀的设计，同时需要双方保证传递的参数类型一致。解决方法是针对不同的类型对通知中心进行派生或者考虑对通知中心提供泛型约束，这样做的目的是使 Notification 中的通知内容变成具体的类型，这样就可以解决目前需要进行装箱和拆箱而带来的性能问题。 ","date":"2016-01-15T12:30:41Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/632291273/","slug":"632291273","tags":["设计模式","消息","事件"],"title":"在 Unity3D 中基于订阅者模式实现事件机制"},{"categories":["Unity3D"],"content":"最近在学习 Shader 时感觉 Shader 语言参数众多、语法诡异，如果每次都从头开始写 Shader 一定是一件痛苦的事情。如果可以在本地定义好一组标准的 Shader 模板，这样当我们需要实现某些效果类似的 Shader 时，就可以在这个 Shader 模板的基础上进行修改。因为 Shader 文件是一个文本文件，所以我们可以非常容易地创建这样一个模板，在这个模板中我们可以进一步完善相关的参数注释，这样就不用每次写 Shader 的时候都需要查文档了，从这个角度出发，就进入了这篇文章的正题：扩展 Unity3D 编辑器的脚本模板。\n按图索骥，模板在哪里 Unity3D 默认的脚本模版位于/Editor/Data/Resources/ScriptTemplates/目录下，注意该目录相对 Unity3D 的安装目录而言，在这个目录中我们可以找到 Unity3D 中脚本模板的某些蛛丝马迹，首先，脚本模板是一个简单的文本文件，这个文本文件中预先填充了内容，我们在编辑器中创建模脚本或者 Shader 的时候实际上是读取这些文件然后在写入项目中的指定路径的。其次，这些模板文件中#SCRIPTNAME#或者#NAME#这样的标记，当我们在编辑器中创建文件的时候，这个标记会被替换成指定的文件名。比如 Unity3D 中继承自 MonoBehaviour 的脚本，有一个非常重要的特性是文件名必须和类名保持一致，这固然是 Unity3D 引擎的一个设定，可是在这里亦可以找到一个可以称得上理由的理由。我们注意到这些模板的文件名中都有一个独一无二的数字，比如 C#脚本的模板中的数字是 81、Shader 模板中的数字是 83，这些数字是什么呢，博主这里将其称为来自星星的黑科技。\n来自星星的黑科技 作为一个经常捣鼓 Unity3D 编辑器的人，如果说你不知道 MenuItem、EditorWindow、ScriptableWizard 这些黑科技，那么说明你不是一个喜欢折腾和探索的人。从 Unity3D 的 API 文档中，我们知道 MenuItem 的原型为：\nMenuItem(string itemName,bool isValidateFunction,int priority) 我知道我们通常使用 MenuItem 常常使用的是它的第一个参数，即定义一个菜单项的名称，我们可以使用\u0026quot;/\u0026ldquo;这样的分隔符来表示菜单的层级，MenuItem 需要配合一个静态方法来使用，可以理解为当我们点击当前定义的菜单后就会去执行静态方法中的代码，因此 MenuItem 常常可以帮助我们做些编辑器扩展开发的工作。好了，第二个参数作为一个验证的标志，如果该标志为 true，意味着我们定义的静态方法是一个验证方法在执行静态方法前会首先对方法进行验证，这个我们暂且不管，因为今天我们这个来自星星的黑科技主要和第三个参数有关，第三个参数表示一个优先级，它表示菜单项在菜单栏中的展示顺序，优先级大的菜单项会展示在优先级小的菜单项下面，由此我们就明白了了模板文件名中的类似 81、83 这样的数字的真实含义，注意到模板文件的排列顺序和编辑器中的菜单项顺序是一样的，我们做一个尝试，编写下面的代码：\n[MenuItem(\u0026#34;Assets/Create/Lua Scripts\u0026#34;, false, 85)] static void CreateLuaScripts() { } [MenuItem(\u0026#34;Assets/Create/固定功能着色器\u0026#34;, false, 86)] static void CreateFixedFunctionShader() { } [MenuItem(\u0026#34;Assets/Create/表面着色器\u0026#34;, false, 87)] static void CreateSurfaceShader() { } [MenuItem(\u0026#34;Assets/Create/可编程着色器\u0026#34;, false, 88)] static void CreateVertexAndFragmentShader() { } 注意到我们按照已知的优先级继续写了四个方法，现在我们在编辑器中可以发现默认的菜单栏发生了变化：\n黑科技让菜单栏发生了变化\r我们可以看到我们编写的这四个菜单都生效了，虽然它们暂时什么都做不了，但顺着这个方向去探索，我们是可以实现最初的梦想的。现在我们来思考如何根据模板来创建文件，这个对我们来说简直太简单了，通过 StreamReader 来读取模板，然后再用 StreamWriter 来生成文件就可以了。可是这样创建的文件的文件名是固定的，在创建文件的时候我们没法修改，而且即使修改了文件内定义的名字并不会改变啊。所以我们需要一个更好的解决方案。Unity3D 提供了一个 UnityEditor.ProjectWindowCallback 的命名空间，在这个空间中提供了一个称为 EndNameEditAction 的类，我们只需要继承这个类就可以完成这个任务。这个类需要重写 Action 的方法，我们知道创建一个文件的完整步骤是创建文件然后使其高亮显示，因此这部分代码实现如下：\n/// \u0026lt;summary\u0026gt; /// 定义一个创建资源的Action类并实现其Action方法 /// \u0026lt;/summary\u0026gt; class CreateAssetAction : EndNameEditAction { public override void Action(int instanceId, string pathName, string resourceFile) { //创建资源 Object obj = CreateAssetFormTemplate(pathName, resourceFile); //高亮显示该资源 ProjectWindowUtil.ShowCreatedAsset(obj); } internal static Object CreateAssetFormTemplate(string pathName, string resourceFile) { //获取要创建资源的绝对路径 string fullName = Path.GetFullPath(pathName); //读取本地模版文件 StreamReader reader = new StreamReader(resourceFile); string content = reader.ReadToEnd(); reader.Close(); //获取资源的文件名 string fileName = Path.GetFileNameWithoutExtension(pathName); //替换默认的文件名 content = content.Replace(\u0026#34;#NAME\u0026#34;, fileName); //写入新文件 StreamWriter writer = new StreamWriter(fullName, false, System.Text.Encoding.UTF8); writer.Write(content); writer.Close(); //刷新本地资源 AssetDatabase.ImportAsset(pathName); AssetDatabase.Refresh(); return AssetDatabase.LoadAssetAtPath(pathName, typeof(Object)); } } 这部分代码相对来说比较简单，就是读取本地模板文件然后生成新文件，在生成新文件的时候会将#NAME 替换成实际的文件名，这样我们就完成了文件资源的创建。现在的问题是如何在创建文件的时候获取实际的路径，这部分代码实现如下：\nprivate static string GetSelectedPath() { //默认路径为Assets string selectedPath = \u0026#34;Assets\u0026#34;; //获取选中的资源 Object[] selection = Selection.GetFiltered(typeof(Object), SelectionMode.Assets); //遍历选中的资源以返回路径 foreach (Object obj in selection) { selectedPath = AssetDatabase.GetAssetPath(obj); if (!string.IsNullOrEmpty(selectedPath) \u0026amp;\u0026amp; File.Exists(selectedPath)) { selectedPath = Path.GetDirectoryName(selectedPath); break; } } return selectedPath; } 现在解决了创建资源的问题，我们接下来只要调用 ProjectWindowUtil 的 StartNameEditingIfProjectWindowExists 方法即可，该方法需要传入一个继承自 EndNameEditAction 的类的实例、目标文件路径和模板文件的路径。例如要创建一个 Lua 脚本可以这样实现：\n[MenuItem(\u0026#34;Assets/Create/Lua Scripts\u0026#34;, false, 85)] static void CreateLuaScripts() { ProjectWindowUtil.StartNameEditingIfProjectWindowExists(0, ScriptableObject.CreateInstance\u0026lt;CreateAssetAction\u0026gt;(), GetSelectedPath() + \u0026#34;/NewLuaScript.lua\u0026#34;, null, \u0026#34;Assets/Editor/Template/85-Lua-NewLuaScript.lua.txt\u0026#34;); } 通过编辑器扩展实现 Shader 文件的创建\r小结 现在有了这个黑科技以后，我们可以创建更多的模板来扩展编辑器的功能，比如对 Shader 而言，我们可以创建些基础性的 Shader 模板，然后每次需要写 Shader 的时候直接从模板库中选择一个功能类似的 Shader 然后在此基础上进行修改，这样比从头开始写一个新的 Shader 应该会轻松不少，这段时间学习 Shader，感觉进程缓慢离图形学高手遥遥无期，行了，这篇博客就是这样了。\n","date":"2016-01-08T13:58:44Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3653662258/","slug":"3653662258","tags":["Unity3D","编辑器","模板"],"title":"扩展 Unity3D 编辑器的脚本模板"},{"categories":["读书笔记"],"content":" 最近开始着手Shader语言的学习，因为Unity3D没有提供类似虚幻四引擎的材质编辑器功能，所以当在Unity3D中碰到需要提供引擎默认材质以外的效果的时候，就需要我们来编写Shader以实现各种特效，本文主要是结合《Cg Programming in Unity》这本书和浅墨博客中关于Shader的这部分内容来学习和整理，目的是帮助博主快速掌握Shader语言。\nUnity3D中的Shader概述 为Unity3D编写Sahder代码相对OpenGL和DirectX要简单。Unity3D没有刻意地区分Cg语言和HLSL语言，因为这两者是非常相似的，这意味着使用HLSL编写的代码可以直接在Cg中使用，更为深入地探索HLSL和Cg的渊源你会戏剧性地发现Cg是Microsoft和NVIDIA联手推出并试图从硬件和软件上和GLSL相抗衡的一种产物。\n其中Cg是Nvidia提供的一种Shader编写语言，HLSL是DirectX提供的一种Shader编写语言，这意味着大部分的Cg代码同样可以被HLSL支持。Unity3D中使用的Shader编写语言是ShaderLab，其本质是对Cg进行了封装，因此在Unity3D中编写Shader本质上在给DirectX或者OpenGL写Shader，因为我猜测在引擎内部存在HLSL和GLSL的相互转换使得Unity3D能够在不同的平台都有较好的图形表现。\nUnity3D中Shader程序的编写可以参考这里。我们知道计算机图形学的中渲染管线一般可以分为两种类型，即固定功能渲染管线和可编程渲染管线。因此从这个角度来看，Unity3D中主要有三种着色器，即固定功能着色器（Fixed Function Shader）、表面着色器（Surface Shader）和 顶点着色器\u0026amp;片段着色器 （Vertex Shader \u0026amp; Fragment Shader）。\nUnity3D中Shader的基本结构 首先，Unity3D中Shader的基本结构是：\nShader { //------【属性】------//\rProperties\r{\r} //------【子着色器】------//\rSubShaders\r{\r}\r//------【回滚】------//\rFallback\r} 对这个结构我们的理解是，Shader代码首先是一些属性定义，用来指定这段代码将有哪些输入。接下来是一个或者多个的子着色器，在实际运行中，哪一个子着色器被使用是由运行的平台所决定的。子着色器是代码的主体，每一个子着色器中包含一个或者多个的Pass。在计算着色时，平台先选择最优先可以使用的着色器，然后依次运行其中的Pass，然后得到输出的结果。最后指定一个Fallback，用来处理所有SubShader都不能运行的情况称为回滚。下面来分别介绍Shader基本结构中的各个部分：\nShader中的Properties Properties是由多条标签组成的Shader属性定义，这些属性能够在Unity3D中的编辑器中显示出来，以此来确定这段Shader代码由哪些输入。常见的标签定义有：\nname(\u0026#34;display name\u0026#34;, Range(min, max)) = number 定义一个在编辑器中可通过滑动条修改的浮点数属性\nname(\u0026#34;display name\u0026#34;, Color) = (number,number,number,number) 定义一个在编辑器中可通过拾色器来设置RGBA的颜色值属性\nname(\u0026#34;display name\u0026#34;, 2D) = \u0026#34;name\u0026#34; {options } 定义一个在编辑器中可编辑的2D纹理属性，其中options可选表示即纹理自动生成纹理坐标时的模式，通常是ObjectLinear、EyeLinear、SphereMap、 CubeReflect、CubeNormal其中之一。\nname(\u0026#34;display name\u0026#34;, Rect) = \u0026#34;name\u0026#34;{ options } 定义一个在编辑器中可编辑的非二次方2D纹理属性\nname(\u0026#34;display name\u0026#34;, Cube) = \u0026#34;name\u0026#34;{ options } 定义一个在编辑器中可编辑的立方贴图纹理属性\nname(\u0026#34;display name\u0026#34;, Float) = number 定义一个在编辑器中可通过输入框修改的浮点数值属性\nname(\u0026#34;display name\u0026#34;, Vector) =(number,number,number,number) 定义一个在编辑器中可通过输入框修改的Vector4属性\nShader中的SubShader SubShader，即子着色器。子着色器是代码的主体，每一个子着色器中包含一个或者多个的Pass。在计算着色时，平台先选择最优先可以使用的着色器，然后依次运行其中的Pass，然后得到输出的结果。子着色器的基本结构是：\nSubshader\r{ //------【Tags标签】------//\rTags{}\r//------【Pass通道】------//\rPass\r{\r}\r} Tags标签 在这里子着色器使用Tags标签来告诉渲染引擎期望何时和如何渲染对象，其语法是：\nTags { \u0026#34;TagName1\u0026#34; = \u0026#34;Value1\u0026#34; \u0026#34;TagName2\u0026#34; = \u0026#34;Value2\u0026#34; } 即采用一个键值对来表示标签的名称及其对应的值，通常由三种标签可以在这里使用：\n\u0026#34;Queue\u0026#34; = \u0026#34;Transparent\u0026#34; 表示决定渲染次序的队列标签，其取值定义如下：\nBackground在所有队列渲染之前被渲染，如天空盒等。 Geometry默认渲染大部分的对象，如不透明的几何体等。 Transparent在所有队列渲染之后被渲染采用由后到前的次序，如玻璃、粒子效果等。 Overlay主要实现叠加效果的渲染，如镜头光晕等。 Tags { \u0026#34;Queue\u0026#34; = \u0026#34;Geometry+1\u0026#34; } 表示自定义中间渲染队列，当默认的渲染队列不能满足要求时可选用当前渲染队列。在Unity实现中每一个队列都被一个整数的索引值所代表。Background为1000、Geometry为2000、Transparent为3000、Overlay为4000.\nTags { \u0026#34;IgnoreProjector\u0026#34; =\u0026#34;True\u0026#34; } 表示忽略投影标签，其值为True表示忽略投影反之表示受投影影响。\nPass通道 Pass通道块控制被渲染的对象的几何体。其结构定义如下：\nPass { //------【名称与标签】------//\r[Name and Tags] //------【渲染设置】------//\r[RenderSetup]\r//------【纹理设置】------//\r[TextureSetup] } 名称与标签 在通道中可以定义其名称和任意数目的标签，通过使用tags来告诉渲染引擎在什么时候该如何渲染他们所期望的效果，其语法和Tags标签完全相同，即采用键值对来定义标签的名称和其对应的值。常用的标签有：\nTags { \u0026#34;LightMode\u0026#34; = \u0026#34;Always\u0026#34; } 表示一个光照模式标签，该标签的取值可以是：\nAlways总是渲染。没有运用光照。 ForwardBase用于正向渲染,环境光、方向光和顶点光等 ForwardAdd用于正向渲染，用于设定附加的像素光，每个光照对应一个pass PrepassBase用于延迟光照，渲染法线/镜面光。 PrepassFinal用于延迟光照，通过结合纹理，光照和自发光渲染最终颜色 Vertex用于顶点光照渲染，当物体没有光照映射时，应用所有的顶点光照 VertexLMRGBM用于顶点光照渲染，当物体有光照映射的时候使用顶点光照渲染。在平台上光照映射是RGBM 编码 VertexLM用于顶点光照渲染，当物体有光照映射的时候使用顶点光照渲染。在平台上光照映射是double-LDR 编码（移动平台，及老式台式CPU） ShadowCaster使物体投射阴影。 ShadowCollector为正向渲染对象的路径，将对象的阴影收集到屏幕空间缓冲区中。 渲染设置 渲染设置设定显示硬件的各种状态，常用的命令如下：\nMaterial { Diffuse Color(R,G,B,A)\r//漫反射颜色构成，即对象的基本颜色。\rAmbient Color(R,G,B,A)\r//环境色颜色构成，即当对象被RenderSettings中设定的环境色所照射时对象所表现的颜色。\rSpecular Color(R,G,B,A)\r//对象反射高光的颜色。\r//(R,G,B,A)四个分量分别代表红绿蓝和Alpha，取值为0到1之间。\rShininess Number\r//加亮时的光泽度，在0和1之间。\rEmission Color\r//自发光颜色，即当不被任何光照所照到时对象的颜色。\r//(R,G,B,A)四个分量分别代表红绿蓝和Alpha，取值为0到1之间。\r//【备注】对象上的完整光照颜色最终是：\r//FinalColor = Ambient * RenderSettings ambientsetting + //(Light Color * Diffuse + Light Color *Specular) + Emission\r} 定义一个使用顶点光照管线的材质\nLighting On | Off 开启或关闭顶点光照\nCull Back | Front | Off 设置多边形剔除模式\nZTest (Less | Greater | LEqual | GEqual |Equal | NotEqual | Always) 设置深度测试模式\nZWrite On | Off 设置深度写模式\nFog { Fog Block } 设置雾参数\nAlphaTest (Less | Greater | LEqual | GEqual| Equal | NotEqual | Always) CutoffValue 开启alpha测试\nBlend SourceBlendMode | DestBlendMode 设置alpha混合模式\nColor Color value 设置当顶点光照关闭时所使用的颜色\nColorMask RGB | A | 0 | any combination of R, G, B, A 设置颜色写遮罩。设置为0将关闭所有颜色通道的渲染\nOffset OffsetFactor , OffsetUnits 设置深度偏移\nSeparateSpecular On | Off 开启或关闭顶点光照相关的平行高光颜色\nColorMaterial AmbientAndDiffuse | Emission 当计算顶点光照时使用每顶点的颜色\n纹理设置 纹理设置的作用是在完成渲染设定后指定一定数目的纹理及其混合模式：\nSetTexture [texture property]{ [Combineoptions] } Shader中的Fallback Fallback就像switch-case结构中的default，其作用是定义当处理所有SubShader都不能运行时采取的一个补救方案，这个主要是为了解决不同的显卡对Shader支持的差异问题。\nUnity3D中Shader的语法 Unity3D中Shader的语法主要针对Cg代码而言，Cg代码是可编程着色器和表面着色器中的核心内容，Cg代码从CGPROGRAM开始到ENDCG结束\nUnity3D中的三种着色器","date":"2015-12-25T12:29:20Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1670305415/","slug":"1670305415","tags":["Shader","CG","Unity","图形"],"title":"《Cg Programming in Unity》读书笔记"},{"categories":["Unity3D"],"content":"各位朋友大家好，欢迎大家关注我的博客，我是秦元培，我的博客地址是http://qinyuanpei.com。到现在为止，我们对 EasyAR 中的 ImageTarget 基本上可以说是驾轻就熟了，因此我们这个系列教程可以说是接近尾声了。博主第一次接触 AR 这个概念是在大学时候读到一本讲解计算机图形视觉的书籍里，相对 VR 技术目前华而不实的市场现状，AR 技术从实用性和成熟度都能得到较好的保证。可是大家都清楚这些技术背后都是建立在复杂而高深的图形学算法的基础上的，如果想学习 AR 技术请回归计算机图形学的本源，这就和学习游戏技术要追寻可编程渲染管线是一样的，所以这个系列完全是博主个人的兴趣使然，希望了解这个技术的可以进行更加深入的探索。这次我们来说说 VideoTarget 如何实现吧！\nEasyAR 中对视频的支持 目前 EasyAR 对视频的支持主要是通过 VideoPlayerBehaviour 这个类，这个类继承自一个基类 VideoPlayerBaseBehaviour。我们可以将其理解为一个视频播放器组件，只要我们将这个组件添加到一个 GameObject 上，然后简单填写下参数就可以了。可是这个组件博主在 32 位操作系统下并没有看到实际的效果，虽然说都到了 2015 年了 64 位操作系统相对来说更为普及了，可是我觉得支持不支持 32 位操作系统更多的体现的是一家公司做产品的态度。既然暂时没有办法看到这里的具体效果，我们本着学习的态度对这个组件有所了解就是了。下面是这个组件的一张截图：\nVideoPlayerBehaviour组件截图\r从图中我们可以看到这个组件相关参数的设置，这里选取的视频资源是 StreamingAssets 目录下的 video.mp4 这个文件，视频资源的 Stroge 同样支持 App、Assets、Absolute 这三种类型，和图片资源的 Stroge 是一样的，关于这三种类型的资源路径的问题，我这里不想再重复说了，这个看看文档就知道了。其次会涉及到视频播放方式和视频缩放的相关参数，这些基本上没什么理解上的难度，大家对照着文档反复尝试就知道各自的用途了。博主这里不太理解 EasyAR 为什么不采用 MovieTexture 或者 Unity3D 中针对视频播放提供的相关插件，因为 VideoTarget 本质上就是把三维模型换成了可以播放的视频而已，所以大家在前面文章的基础上创建一个 ImageTarget 然后再其下面放置一个附加了 VideoPlayerBehaviour 的的子物体就可以了。官方的示例项目中提供了两种方式的 VideoTarget 创建方式，即手动创建和动态创建。手动创建即我们这里提到的这种方式，而动态创建则是由程序在运行时期间创建。这两种方式本质上没有什么不同，需要注意的是 VideoPlayerBehaviour 有一个 EnableAutoPlay 的选项，该选项被选中后会启用自动播放，即当识别图被识别后自动播放视频、识别图未被识别则暂停播放视频。如果这个选项没有被选中，我们需要在 ITargetEventHandle 接口中动手来实现。\n增强 ImageTarget 这个增强 ImageTarget 是指在 ImageTarget 的基础上融入 VideoPlayerBehaviour 的功能，因为按照官方的示例来考虑，这两部分功能是独立的，博主希望让大家在制作识别图的时候完全忘记区别 ImageTarget 和 VideoTarget，这样我们可以更为专注地制作识别图，因为视频组件就只是设置参数这一件事情，完全可以一次性搞定，所以我们首先来定义一个 VideoTargetBaseBehaviour 类，一起来看代码：\nusing UnityEngine; using System.Collections; using EasyAR; public class VideoTargetBaseBehaviour : ImageTargetBehaviour,ITargetEventHandler { /// \u0026lt;summary\u0026gt; /// 视频播放模块 /// \u0026lt;/summary\u0026gt; private VideoPlayerBehaviour videoPlayer; /// \u0026lt;summary\u0026gt; /// 视频文件路径 /// \u0026lt;/summary\u0026gt; public string VideoPath; /// \u0026lt;summary\u0026gt; /// 是否自动播放视频 /// \u0026lt;/summary\u0026gt; public bool VideoEnableAutoPlay = true; /// \u0026lt;summary\u0026gt; /// 是否允许视频循环 /// \u0026lt;/summary\u0026gt; public bool VideoEnableLoop = true; /// \u0026lt;summary\u0026gt; /// 视频类型 /// \u0026lt;/summary\u0026gt; public VideoPlayer.VideoType VideoType = VideoPlayer.VideoType.TransparentSideBySide; /// \u0026lt;summary\u0026gt; /// 视频资源类型 /// \u0026lt;/summary\u0026gt; public StorageType VideoStorage = StorageType.Assets; /// \u0026lt;summary\u0026gt; /// 视频是否加载 /// \u0026lt;/summary\u0026gt; private bool isVideoLoaded; protected override void Start() { //在Start方法中加载视频、隐藏模型 base.Start(); LoadVideo(); HideObjects(transform); } /// \u0026lt;summary\u0026gt; /// 加载视频 /// \u0026lt;/summary\u0026gt; private void LoadVideo() { //创建子物体VideoObject并为其添加视频组件 GameObject VideoObject = new GameObject(\u0026#34;VideoObject\u0026#34;); videoPlayer = VideoObject.AddComponent\u0026lt;VideoPlayerBehaviour\u0026gt;(); VideoObject.transform.SetParent(transform); VideoObject.transform.localPosition = Vector3.zero; VideoObject.transform.localRotation = Quaternion.identity; VideoObject.transform.localScale = Vector3.one; //设置视频组件相关参数 videoPlayer.Storage = VideoStorage; videoPlayer.Path = VideoPath; videoPlayer.EnableAutoPlay = VideoEnableAutoPlay; videoPlayer.EnableLoop = VideoEnableLoop; videoPlayer.Type = VideoType; videoPlayer.VideoReadyEvent+=videoPlayer_VideoReadyEvent; videoPlayer.VideoReachEndEvent+=videoPlayer_VideoReachEndEvent; videoPlayer.VideoErrorEvent+=videoPlayer_VideoErrorEvent; videoPlayer.Open(); videoPlayer.Play(); } #region 视频组件相关事件定义 public virtual void videoPlayer_VideoErrorEvent(object sender, System.EventArgs e) { } public virtual void videoPlayer_VideoReachEndEvent(object sender, System.EventArgs e) { } public virtual void videoPlayer_VideoReadyEvent(object sender, System.EventArgs e) { } #endregion /// \u0026lt;summary\u0026gt; /// 隐藏模型的方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;trans\u0026#34;\u0026gt;要隐藏的Transform\u0026lt;/param\u0026gt; void HideObjects(Transform trans) { for (int i = 0; i \u0026lt; trans.childCount; ++i) HideObjects(trans.GetChild(i)); if (transform != trans) gameObject.SetActive(false); } /// \u0026lt;summary\u0026gt; /// 显示模型的方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;trans\u0026#34;\u0026gt;要显示的Transform\u0026lt;/param\u0026gt; public void ShowObjects(Transform trans) { for (int i = 0; i \u0026lt; trans.childCount; ++i) ShowObjects(trans.GetChild(i)); if (transform != trans) gameObject.SetActive(true); } /// \u0026lt;summary\u0026gt; /// 实现ITargetEventHandler接口中的OnTargetFound方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;target\u0026#34;\u0026gt;识别目标\u0026lt;/param\u0026gt; void ITargetEventHandler.OnTargetFound(Target target) { if (videoPlayer) videoPlayer.Play(); ShowObjects(transform); } /// \u0026lt;summary\u0026gt; /// 实现ITargetEventHandler接口中的OnTargetLost方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;target\u0026#34;\u0026gt;识别目标\u0026lt;/param\u0026gt; void ITargetEventHandler.OnTargetLost(Target target) { if (videoPlayer) videoPlayer.Pause(); HideObjects(transform); } /// \u0026lt;summary\u0026gt; /// 实现ITargetEventHandler接口中的OnTargetLoad方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;target\u0026#34;\u0026gt;识别目标\u0026lt;/param\u0026gt; void ITargetEventHandler.OnTargetLoad(Target target, bool status) { } /// \u0026lt;summary\u0026gt; /// 实现ITargetEventHandler接口中的OnTargetUnload方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;target\u0026#34;\u0026gt;识别目标\u0026lt;/param\u0026gt; void ITargetEventHandler.OnTargetUnload(Target target, bool status) { } } 在这段代码中博主采用了动态创建视频组件的方法，这样我们在制作 VideoTarget 的时候只需要按照以下步骤即可：\n在 Assets/EasyAR/Prefabs 目录下找到 EasyAR 这个预制体，添加 EasyARConfig 组件，然后填写 KEY。具体请参考系列教程第三篇EasyAR 尝鲜系列教程之 ImageTarget 千呼万唤始出来。 在 Assets/EasyAR/Prefabs 目录中找到 ImageTarget 这个预制体，然后使用 VideoTargetBaseBehaviour 组件替换默认的 ImageTargetBehaviour 组件。下面是博主这里的参数配置截图 我制作的VideoTarget\r这里博主继续选择 idback 这张图片，这种方法是博主喜欢的方法，大家可以按照个人喜欢的方式来实现，总而言之万变不离其宗，只需要掌握它的原理就好了。在文章中已经提到过这个组件在 32 位操作系统下无法正常工作，所以这篇文章就不给大家展示相关的截图了，本文暂时先写到这里等有时间测试成功了再来更新这篇文章。如果像博主这样对 Unity3D 比较熟悉的朋友，可以考虑使用 MovieTexture 或者其它的方式来替代官方目前的这个方案，好了，这篇文章就是这样了，希望大家喜欢!\n","date":"2015-12-09T08:40:22Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/316230277/","slug":"316230277","tags":["增强现实","EasyAR","Unity3D","教程"],"title":"EasyAR 尝鲜系列教程之视频播放功能的实现"},{"categories":["Unity3D"],"content":"各位朋友大家好，欢迎大家关注我的博客，我是秦元培，我的博客地址是http://qinyuanpei.com。最近 EasyAR 终于迎来了一次重大的版本更新：v1.10，真可谓是“千呼万唤始出来”啊，所以在官方文档和示例项目基本完善的情况下，博主决定将 EasyAR 尝鲜系列教程继续下去。本次教程主要以官方新发布的 Unity 示例项目为基础来进行讲解，关注 Androis/iOS 原生应用开发的朋友请自行针对官方示例项目进行研究。好了，今天主要的内容是通过 EasyAR SDK 来自行构建一个 ImageTarget 的实例，采用 Unity3D 4.6.4 版本进行开发。\nEasyAR SDK 的结构 将 EasyAR SDK 导入 Unity3D 后会在项目的 Assets 根目录下生成 EasyAR 和 Plugins 两个文件夹。其中 EasyAR 文件夹中提供了开发 AR 应用相关的标准接口、材质、Shader 和 Prefab，Plugins 文件夹中提供了针对各个平台的插件。好了，下面我们来介绍 EasyAR SDK 中提供的标准接口：\nARBuilder: 该类提供了 EasyAR 初始化的相关方法，我们在编写 EasyAR 配置类的时候会用到这个类，这是一个可以直接使用的类。 ImageTargetBehaviour: 该类是一个抽象类，我们需要对其进行 override，可以将这个类理解为 ImageTarget 生命周期相关的一个类，在实际使用中需要配合 ITargetEventHandle 这个接口来使用。 VideoPlayerBaseBehaviour: 该类是一个组件，我们可以使用这个组件来播放视频。其原理和 ImageTarget 类似，所不同的地方是 ImageTarget 在识别成功后会显示一个模型，而这里则是使用一个隐藏的物体来播放视频，VideoPlayerBaseBehaviour 负责控制视频的播放、暂停等工作。 ITargetEventHandle: 这是一个接口，通过该接口可以捕捉到识别过程中的 OnTargetFound、OnTargetLost、OnTargetLoad 和 OnTargetUnload 四个事件，对于一个基本的 AR 应用来说，我们通常需要关注的是 OnTargetFound、OnTargetLost 这两个方法。 构建第一个 ImageTarget 项目 好了，在了解了 EasyAR 中常用的标准接口以后，我们下面来着手构建第一个 ImageTarget 项目，和我们第一次接触 EasyAR 不同，这次我们会编写些简单地代码，打开场景填入应用程序密钥(Key)然后运行它，这种方式在这里会显得略 LOW。\nEasyAR 的初始化 首先我们在 Assets/EasyAR/Prefabs 目录下找到 EasyAR 这个预制体，然后将其拖放到场景中，这样我们就创建了基本的 EasyAR 应用场景，接下来我们要做的事情就是在这个场景中填入各种各样的识别物。为了让 EasyAR 正常工作，我们首先要编写一个初始化 EasyAR 的脚本：\nusing UnityEngine; using System.Collections; using EasyAR; public class EasyARConfig : MonoBehaviour { /// \u0026lt;summary\u0026gt; /// 应用程序密钥 /// \u0026lt;/summary\u0026gt; [TextArea(1,10)] public string Key; public void Awake() { //检查KEY是否存在 if(string.IsNullOrEmpty(Key)) Debug.Log(\u0026#34;请先输入应用程序密钥\u0026#34;); //初始化EasyAR ARBuilder.Instance.InitializeEasyAR(Key); ARBuilder.Instance.EasyBuild(); } } 我确信这个类简单到彻底，它需要开发者在编辑器中填入 KEY 然后再 Awake 方法中完成对 EasyAR 的初始化，就是这样简单，我们这里将这个脚本附加到 EasyAR 这个物体上去，这样我们就完成了引擎的初始化工作，下面我们就可以专注于 AR 内容的产生了。\n制作一个 ImageTarget 接下来我们在 Assets/EasyAR/Prefabs 目录中找到 ImageTarget 这个预制体，将其拖放到场景中，确保它在摄像机的视野范围内。我们注意到默认情况下它附加了一个 ImageTargetBehaviour 脚本，我们在前面已经说过，这个类是一个抽象类，抽象类通常是不做任何事情的，因此我们需要继承这个类来编写一个具体类，我们将这个具体类命名为 CustomImageTargetBehaviour。下面给出它的代码实现：\nusing UnityEngine; using System.Collections; using EasyAR; public class CustomImageTargetBehaviour :ImageTargetBehaviour,ITargetEventHandler { protected override void Start() { //在Start方法中隐藏模型 base.Start(); HideObjects(transform); } /// \u0026lt;summary\u0026gt; /// 隐藏模型的方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;trans\u0026#34;\u0026gt;要隐藏的Transform\u0026lt;/param\u0026gt; void HideObjects(Transform trans) { for (int i = 0; i \u0026lt; trans.childCount; ++i) HideObjects(trans.GetChild(i)); if (transform != trans) gameObject.SetActive(false); } /// \u0026lt;summary\u0026gt; /// 显示模型的方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;trans\u0026#34;\u0026gt;要显示的Transform\u0026lt;/param\u0026gt; void ShowObjects(Transform trans) { for (int i = 0; i \u0026lt; trans.childCount; ++i) ShowObjects(trans.GetChild(i)); if (transform != trans) gameObject.SetActive(true); } /// \u0026lt;summary\u0026gt; /// 实现ITargetEventHandler接口中的OnTargetFound方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;target\u0026#34;\u0026gt;识别目标\u0026lt;/param\u0026gt; void ITargetEventHandler.OnTargetFound(Target target) { ShowObjects(transform); } /// \u0026lt;summary\u0026gt; /// 实现ITargetEventHandler接口中的OnTargetLost方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;target\u0026#34;\u0026gt;识别目标\u0026lt;/param\u0026gt; void ITargetEventHandler.OnTargetLost(Target target) { HideObjects(transform); } /// \u0026lt;summary\u0026gt; /// 实现ITargetEventHandler接口中的OnTargetLoad方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;target\u0026#34;\u0026gt;识别目标\u0026lt;/param\u0026gt; void ITargetEventHandler.OnTargetLoad(Target target, bool status) { } /// \u0026lt;summary\u0026gt; /// 实现ITargetEventHandler接口中的OnTargetUnload方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;target\u0026#34;\u0026gt;识别目标\u0026lt;/param\u0026gt; void ITargetEventHandler.OnTargetUnload(Target target, bool status) { } } 可以注意到在这个类中我们主要做了两件事情：第一，定义了隐藏和显示识别模型的方法 HideObjects 和 ShowObjects，其作用是在没有识别到 Target 的时候隐藏物体，在识别到 Target 的时候显示物体；第二，实现了 ITargetEventHandler 接口并在 OnTargetFound 和 OnTargetLost 两个方法中实现我们第一步希望达到的目的。至此，我们完成了一个基本的 AR 识别组件，我们下面所有的 AR 识别物体都是通过这个组件来工作的，所以我们从场景中的 ImageTarget 物体上移除默认的 ImageTargetBehaviour 脚本然后为其添加我们定义的 CustomImageTargetBehaviour 脚本。\n编写完脚本以后我们就可以着手制作识别图和 Marker 了，EasyAR 最让人喜欢的一点就是你可以按照自己的意愿来制作识别图和 Marker。虽然 Vuforia 在识别效果上比 EasyAR 更好点，可是对程序员来说选择一个透明的产品方案比面对着黑箱子进行调试要明智得多。EasyAR 中的识别图相对来说比较简单，因为我们只需要选择一张图片然后为其创建一个材质，再将这个材质附加到 ImageTarget 物体上就可以了。此外还会涉及到某些参数的设置，我们下面会提到。好了，我们继续选择官方示例中的 idback 这张图片来作为我们的识别图，因为身份证每个人都有可以随时用来进行测试，而一般的图片则需要打印出来制成硬质卡片来使用。我们在 Assets 目录中创建一个 StreamingAssets 目录，将官方示例中 targets.json 和 idbcak.jpg 两个文件拷贝过来。创建材质就不再说了，这是 Unity3D 中非常非常基础的内容。我们将创建好的材质附加到 ImageTarget 物体上以后，可能在场景中并不会看到对应的识别图，这是因为我们没有为其配置参数。具体的参数配置如下图：\nImageTarget参数配置\r具体这些参数的定义请大家自己去看文档，因为我这里说得再明白如果大家不看等于我没有说。好了，下面我们来创建 Marker，这个就比较简单了，我们直接找一个模型缩放到合适的大小然后拖拽到 ImageTarget 这个物体下面就可以了。如图是博主参照官方示例制作的两个识别图及其 Marker：\n两个ImageTarget及其对应Maker\r走向成功的关键步骤 1、在 EasyAR 物体的 EasyARConfig 组件中填入从官网申请的 KEY。 2、在 BuildSetting 中填写 KEY 对应的 AppID。 3、安装 SDK 中附带的 VC++2015 运行库。 4、如要编译 Android 版本，请确保安装 Java 环境和 Android SDK 更多的问题请自行到官方文档中对照寻找解决办法。\n截图展示 截图展示\r","date":"2015-12-09T08:39:54Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3736599391/","slug":"3736599391","tags":["增强现实","EasyAR","AR","教程"],"title":"EasyAR 尝鲜系列教程之 ImageTarget 千呼万唤始出来"},{"categories":["编程语言"],"content":"各位朋友大家好，我是秦元培，欢迎大家关注我的博客。最近偶然接触到了 C#中的扩展方法，觉得这个语法特性是一个不错的特性，因此决定在这里系统地对 C#中的扩展方法相关内容进行下总结和整理，因为博主觉得学习这件事情本身就是一个积累的过程，所以博主有时候会对现在的线上培训和视频教程这种“在线教育”感到反感。试想《射雕英雄传》中江南七怪远赴大漠传授郭靖武艺苦历十八载，何以难及全真教丹阳子马钰传授内功两年的积累？这里固然有郭靖愚笨木讷的天性和江南七怪武功低微的因素，可是在博主看来更重要的是强调了一个积累。想郭靖一生受益自全真教的玄门内功终成一代“为国为民”的侠之大者，则我辈需更加努力方可在这世间行走奔波。\n什么是扩展方法 扩展方法从字面上理解是指扩展的方法，而对应到面向对象编程这个格局中则是指为一个类提供的扩展方法。按照我们通常的理解，我们首先需要获得某个类的源代码，然后在这个类代码中增加成员方法，这样就可以达到为一个类提供扩展方法的目的。可是不幸地是，这种方法在没有源代码的情况下就无法奏效了，而且我们人为地去改变源代码有可能会破坏整个代码的稳定性。那么有没有一种方法能在不改变源代码的前提下为某个类提供扩展方法呢？这就是我们今天要说的扩展方法，所以我们可以将扩展方法理解为在不改变源代码的前提下向外部提供扩展方法的一种方式。C#中的扩展方法实现起来是相对来说比较简单的，例如我们做在 Unity3D 游戏开发的时候，可能会用到 DOTween 这个插件。这个插件是 iTween 的作者重新编写一个动画插件，效率上比 iTween 有较大的提升。更为重要的一点是，它采用扩展方法这种实现方式，使得我们在调用这些 API 接口的时候难以感觉到我们是在使用一个插件，更像是在使用 Unity3D 的原生函数，所以当我们使用 DOTween + uGUI 这样的组合的时候，内心会感到无比的舒畅，一切都像是水到渠成一般。\n扩展方法有哪些特点 扩展方法在实现上和普通的面向对象编程是一样的，换句话说，我们只需要定义一个类，然后在里面添加并实现相应的方法即可。但是这里需要注意的地方有三点，第一，实现扩展方法的类必须是静态类且类的名称和实现扩展方法的类无关；第二、实现扩展方法的类方法必须是静态方法；第三、实现扩展方法的类方法的第一个参数必须是使用 this 关键字指明要实现扩展方法的类。例如，我们知道将一个合法字符串类型转换为整型，可以使用 int.parse()方法，假如我们希望为 string 类型扩展一个 ToInt 方法应该怎么办呢？我们一起来看下面的这段代码：\n/// \u0026lt;summary\u0026gt; /// 1、定义一个静态类 /// 2、静态类的名称和要实现扩展方法的具体类无关 /// \u0026lt;/summary\u0026gt; public static class SomeClass { /// \u0026lt;summary\u0026gt; /// 3、实现一个具体的静态方法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;str\u0026#34;\u0026gt;4、第一个参数必须使用this关键字指定要使用扩展方法的类型\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; public static int ToInt(this string str) { return int.Parse(str); } } 需要注意的是 C#支持扩展方法是从.NET3.5 版本开始，所以在编写扩展方法的时候请确保你的.NET 版本是否满足这一要求。提到版本问题，有很多朋友尤其是从 Unity5.0 以后开始学习 Unity3D 的朋友，常常会在我的博客中留言提到我的代码无法在新环境下运行等等类似地问题，我觉得这个世界上更新速度最快的当属 IT 技术了，大家使用新版本没有问题，可是有时候因为技术发展中的历史遗留问题例如 Python2.7 和 Python3、Unity4.X 和 Unity5.X，这个时候可能出现版本不兼容的问题，这个时候如果网络上的资源没有及时更新，建议大家还是及时查看官方的最新文档，因为在博主看来网络上的书籍或者相关文章都是用来参考的，古话说：尽信书不如无书，只有客观、冷静地判断知识的正确与否，我们方能学到真正有用的知识。\n好了，现在我们编写完这个扩展方法以后，就可以像下面这样使用扩展方法了：\nstring str = \u0026#34;1234\u0026#34;; int val = str.ToInt(); 这个示例向大家展示了如何编写一个无参数的扩展方法，那么当我们需要在扩展方法中传入参数的时候该怎么做呢？我们只需要在第一个参数后继续加入参数的声明就好了。例如我们在 Unity3D 中常常需要给一个 3D 物体设置坐标，通常我们可以通过下面的代码来实现：\ntransform.position = new Vector3(1,1,1); 这个代码到目前为止是比较简洁的，可是我们知道在 Unity3D 中除了 position 属性以外还有 localPosition 属性，如果我们的代码中再涉及坐标计算的话，我相信这个代码一定会变得非常的长。更有甚者，有时候我们只想改变三维坐标中的一个维度，可是我们必须给 transform.position 一个三维坐标，毫无意外地此时的代码会变得更长。为了解决这个问题，我们可以扩展出三个方法 SetPositionX、SetPositionY、SetPositionZ 来分别为 x、y、z 三个坐标分量进行赋值，我们继续在 SomeClass 这个类中添加方法：\n/// \u0026lt;summary\u0026gt; /// 设置Tranform的X坐标 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;tran\u0026#34;\u0026gt;当前Transform\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;x\u0026#34;\u0026gt;X坐标\u0026lt;/param\u0026gt; public static void SetPositionX(this Transform tran, float x) { tran.position = new Vector3(x, tran.position.y, tran.position.z); } /// \u0026lt;summary\u0026gt; /// 设置Tranform的Y坐标 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;tran\u0026#34;\u0026gt;当前Transform\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;x\u0026#34;\u0026gt;Y坐标\u0026lt;/param\u0026gt; public static void SetPositionY(this Transform tran, float y) { tran.position = new Vector3(tran.position.x, y, tran.position.z); } /// \u0026lt;summary\u0026gt; /// 设置Tranform的Z坐标 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;tran\u0026#34;\u0026gt;当前Transform\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;x\u0026#34;\u0026gt;Z坐标\u0026lt;/param\u0026gt; public static void SetPositionZ(this Transform tran, float z) { tran.position = new Vector3(tran.position.x, tran.position.y, z); } 同样的，我们现在可以直接为一个三维物体的坐标进行赋值：\ntransform.SetPositionX(1.0f); transform.SetPositionY(1.0f); transform.SetPositionZ(1.0f); 使用扩展方法的利弊 扩展方法使用起来得心应手，所以我们这里来讨论下使用扩展方法的利弊。好处当然是自由而任性地使用扩展方法对类进行扩展，而且扩展方法在 Visual Studio 中的智能提示会以蓝色向下箭头进行标识。扩展方法的坏处则是要看设计扩展方法的人能否较好的驾驭这个特性啦，其实所有的技术都是一样的，我常常在游戏群里听到人鄙视 Unity3D 引擎，以 UnReal Engine4 为游戏引擎世界里的泰山北斗，我承认 UE4 的画面效果好，可是能真正用好这个引擎的人有多少呢？扩展方法在使用的时候应该遵守就近原则，即是在最小的范围内使用扩展方法，对具体类而非抽象类实现扩展方法。我们使用扩展方法无非是因为它在逻辑层需要这样的功能，所以我们没有必要去改变抽象层的逻辑，因为这样会“污染”整个代码。举一个简单的例子，我们知道.NET 中的基类是 object，如果我们对这个类进行扩展，毫无疑问它会影响所有继承自 object 的类，这样就会造成“污染”，显然是不可取的。\n小结 在 C#中实现扩展方法的类必须是静态类且类的名称和实现扩展方法的类无关 实现扩展方法的类方法必须是静态方法 实现扩展方法的类方法的第一个参数必须是使用 this 关键字指明要实现扩展方法的类 实现扩展方法应遵守就近原则，在最小的范围内使用扩展方法以避免造成“污染” ","date":"2015-12-05T12:01:02Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/305484621/","slug":"305484621","tags":["CSharp","扩展方法","Unity3D","技巧"],"title":"C# 中的扩展方法学习总结"},{"categories":["生活感悟"],"content":" 匆匆时光总是把我们这些过客留在感慨和叹息中，而它却如风之旅人一般渐行远去。转眼这大半年的时间里，每天我都在努力让时间发挥它的意义，可是有时候这种努力却像枷锁一样让我有些莫名的压抑。从毕业那天起，我就决定这辈子不会再靠我的本科专业生活，因为它从来没有和我的内心发生过强烈的共鸣，所以当我毕业以后就意味着我再没有回头的路可以走。曾经因为怯懦而将自己封闭在这座小城市，其结果就是我在我人生中的第一家公司的项目在拖延和等待中慢慢地死亡。\n从刚进公司时的踌躇满志到此时此刻心灰意冷，大概就像冬日里的懒洋洋的太阳，一个季节的凋落需要的可能只是一片枯萎的叶子而已。我每天都坚持早起，因为我不想被这种安逸到近似麻木的生活拔去梦想的翅膀，虽然我在乎的事情没有人懂、更没有人在乎，可我就是不愿做一只温水中的青蛙，因为当危险临近的时候更加不会有人来救我。每天穿梭在来来往往的人群中、听着年轻的男男女女们讨论被生活剥夺去的纷纷扰扰，我曾经为了让自己和别人不同而努力过，此时此刻却要为了和别人一样而满腹忧愁。我不甘心让我的生活变成电视连续剧，一切都在编剧和观众的期望之中。我想要一个独特的故事，虽然狗血可它却是我在这世界上来过的真实写照。\n古语说：父母在，不远游，游必有方。我固然不愿意离开年迈的父母，我固然不愿意离开生养我的土地，可是对我来说这一切都没有可以选择的余地，每次看到回家看到父母不停地辛劳，我意识到他们渐渐地老了，他们的身体不再像以前那般硬朗，他们的头发一天比一天白，我不愿意他们再为我辛劳下去，留在这座小城市里除了可以经常看到他们以外，对我而言并不会有更好的理由。虽然在这座小城市里我可以勉强混得下去，可对我付出过的精力和时间来说，它更像是一种灵魂上的亵渎。我还要照顾她、和她在一起生活，看看此时此刻的我有什么资格这样说呢？我身边的同龄人有的人已经买好了房子、有的人已经在准备结婚成家，可我什么都没有啊，我没有资格让每天的日子都这样平淡而安稳地过下去，大概这就是我一直在纠结的原因吧！\n将近七个月的时间，可对我来说真正为第一家公司工作的时间只有三个月。我自问每天都在认真的做事情，可是因为领导层和整个公司的问题现在这个项目变成这个样子，我真的感到失望而寒心。时光一过不再有，这过去的大半年时间就让它过去吧，在接下来的2016年里，我希望我可以更加勇敢、更加努力，我要更好地把握我的人生，我要变得更加成熟，我要做最初的、最好的自己！2016，加油！\n","date":"2015-12-01T19:24:18Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1394521917/","slug":"1394521917","tags":["成长","感悟","生活"],"title":"青黄未接的2015"},{"categories":["Unity3D"],"content":"各位朋友大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。虽然 Unity3D 引擎依靠强大的跨平台能力睥睨高手林立的游戏引擎世界，我们在使用 Unity3D 游戏引擎的时候基本上不会去接触底层的东西，可是有时候面对某些奇葩的要求的时候，我们就不得不考虑使用 C++这样的语言来为其编写相关的插件。你如果问我是什么样的奇葩要求，比如接入蓝牙手柄来控制游戏、接入类似街机的设备来控制游戏、接入同一个游戏到两个不同的设备上并响应不同的控制……诸如此类的种种问题，可能目前在 Unity3D 引擎中找不到解决方案，这个时候写 C++插件就变成了一种刚性需求，这就是我们今天要来一起探讨的问题。\nUnity3D 主要使用 C#进行开发，所以为 Unity3D 编写插件本质上就是让 C#调用 C++代码。目前主要有 C++ CLR 和 C++ Native 两种实现方法，其中 C++ CLR 可以理解为运行在.Net CLR 即公共语言运行库上的 C++代码，这种代码是托管的 C++代码，目前并没有被 C++标准承认，因为它更像是 C++和 C#两种语言的混合代码，这种代码的优势是可以像普通的.NET 库一样被 C#调用，考虑到 Unity3D 建立在和.Net 类似的 Mono 上，因此这种方式应该是我们的最佳实践方案；C++ Native 则是指传统的 C++ 动态链接库，通过 DllImport 在 C#中进行包装后在 C#中进行调用，相对地这种方式调用的是非托管的 C++代码，这种方式相信接触过 Windows 开发的朋友应该不会感到陌生啦，它是一种更为普遍的方法，例如我们要接入苹果官方 SDK 的时候，需要对 Object C 的代码进行封装后交给 C#去调用，而这里使用的方法就是 DllImport 了。\n好了，下面我们来看看两种方式各自是如何实现的吧！这里博主使用的开发环境是 Windows 8.1 32bit 和 Visual Studio 2012，Unity3D 的版本为 4.6 版本。\nC++ CLR 创建一个 C++ CLR 类库项目 首先我们按照下图中的步骤创建一个 C++ CLR 项目：\n截图是件讨厌的事情，虽然懒惰的人们都喜欢\r请注意.Net 版本问题，重要的事情说三遍，不认真看这里的人出现问题就不要到我这里来评论了，我最讨厌连文章都没有看明白就来和你纠缠不清的人，谢谢。创建好项目后请打开项目属性窗口设置【公共语言运行时支持】节点的值为【安全 MSIL 公共语言运行时支持(/clr:safe)】好了，下面我们找到 CLR4Unity.h 文件，添加 ExampleClass 声明：\n/// \u0026lt;summary\u0026gt; /// 一个简单的托管C++示例类 /// \u0026lt;/summary\u0026gt; public ref class ExampleClass { public: /// \u0026lt;summary\u0026gt; /// 产生一个介于min和max之间的整型随机数 /// \u0026lt;returns\u0026gt;整型随机数\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;min\u0026#34;\u0026gt;最小值\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;max\u0026#34;\u0026gt;最大值\u0026lt;/param\u0026gt; /// \u0026lt;/summary\u0026gt; static int Random(int min,int max) { //注意在托管的C++中使用gcnew来代替new //我承认C++写CLR代码略显奇葩像是C++和C#语法的混合 return (gcnew System::Random)-\u0026gt;Next(min,max); } /// \u0026lt;summary\u0026gt; /// 计算一个整数的平方 /// \u0026lt;returns\u0026gt;整型数值\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;a\u0026#34;\u0026gt;需要平方的数值\u0026lt;/param\u0026gt; /// \u0026lt;/summary\u0026gt; static int Square(int a) { return a * a; } /// \u0026lt;summary\u0026gt; /// 返回两个数中的最大值 /// \u0026lt;returns\u0026gt;整型数值\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;a\u0026#34;\u0026gt;参数1\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;b\u0026#34;\u0026gt;参数2\u0026lt;/param\u0026gt; /// \u0026lt;/summary\u0026gt; static int Max(int a,int b) { if(a\u0026lt;=b){ return b; }else{ return a; } } }; 显然我们这里定义了三个简单的方法，注意到第一个方法 Random 依赖于 System.Rnadom 类，而在托管的 C++中是使用 gcnew 来代替 new 这个关键字的，所以请尽情感受 C#和 C++的混搭语法风格吧！这样我们就可以编译得到 CLR4Unity.dll 这个类库，将这个文件复制到 Unity3D 项目中的 Plugins 目录下下，然后将其加入项目引用列表。如果你以为引用就是：\nusing CLR4Unity; 呵呵，我严重怀疑你对.Net 的熟悉程度。你没有添加对 CLR4Unity.dll 的引用，你到底在 using 什么啊？\n先添加引用然后using\r如果你对.NET 熟悉到足以无视这里的一切，请闭上眼接着往下看，哈哈！\n在 C#中添加引用及方法调用 接下来我们在 Unity3D 中创建一个脚本 PluginTest.cs，然后在 OnGUI 方法增加下列代码。可是你要以为这些代码就应该写在 OnGUI 方法中，抱歉请你先去了解 MonoBehaviour 这个类。什么？添加了这些代码报错？没有 using 的请自行面壁：\n//调用C++ CLR中的方法 if(GUILayout.Button(\u0026#34;调用C++ CLR中的方法\u0026#34;, GUILayout.Height (30))) { Debug.Log(\u0026#34;调用C++ CLR中的方法Random(0,10):\u0026#34; + ExampleClass.Random(0,10)); Debug.Log(\u0026#34;调用C++ CLR中的方法Max(5,10):\u0026#34; + ExampleClass.Max(5,10)); Debug.Log(\u0026#34;调用C++ CLR中的方法Square(5):\u0026#34; + ExampleClass.Square(5)); } C++ Native 创建一个 C++动态链接库项目 首先我们按照下图中的步骤来创建一个 C++ Win32 项目：\n不要问我从哪里来\r我的故乡在远方\r好了，接下来我们找到 Native4Unity.cpp 写入下列代码：\n// Native4Unity.cpp : 定义 DLL 应用程序的导出函数。 // #include \u0026#34;stdafx.h\u0026#34; //为了使用rand()函数引入C++标准库 #include \u0026#34;stdlib.h\u0026#34; /// \u0026lt;summary\u0026gt; /// 产生一个介于min和max之间的整型随机数 /// \u0026lt;returns\u0026gt;整型随机数\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;min\u0026#34;\u0026gt;最小值\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;max\u0026#34;\u0026gt;最大值\u0026lt;/param\u0026gt; /// \u0026lt;/summary\u0026gt; extern \u0026#34;C\u0026#34; __declspec(dllexport) int Random(int min,int max) { return rand() % (max - min + 1) + min; } /// \u0026lt;summary\u0026gt; /// 返回两个数中的最大值 /// \u0026lt;returns\u0026gt;整型数值\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;a\u0026#34;\u0026gt;参数1\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;b\u0026#34;\u0026gt;参数2\u0026lt;/param\u0026gt; /// \u0026lt;/summary\u0026gt; extern \u0026#34;C\u0026#34; __declspec(dllexport) int Max(int a ,int b) { if(a\u0026lt;=b){ return b; }else{ return a; } } /// \u0026lt;summary\u0026gt; /// 计算一个整数的平方 /// \u0026lt;returns\u0026gt;整型数值\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;a\u0026#34;\u0026gt;需要平方的数值\u0026lt;/param\u0026gt; /// \u0026lt;/summary\u0026gt; extern \u0026#34;C\u0026#34; __declspec(dllexport) int Square(int a) { return a * a; } 和 C++ CLR 类似，我们使用标准的 C++语言来实现同样的功能。注意到 rand()这个函数是 C++标准库里的内容，所以我们在文件开头增加了对 stdlib.h 这个头文件的引用。这里需要注意的一点是：所有希望使用 DllImport 引入 C#的 C++方法都应该在方法声明中增加__declspec(dllexport)关键字，除非它在.def 文件中对这些方法进行显示声明。关于.def 文件的相关定义大家可以到 MSDN 上检索，这些都是属于 C++编译器的内容，这里不再详细说了。\n在 C#中使用 DllImport 封装方法 将编译好的 Native4Unity.dll 复制到 Plugins 目录中后，下面我们要做的事情就是在 C#里对这些方法进行封装或者说是声明：\n[DllImport(\u0026#34;Native4Unity\u0026#34;)] private extern static int Random(int min, int max); [DllImport(\u0026#34;Native4Unity\u0026#34;)] private extern static int Max(int a, int b); [DllImport(\u0026#34;Native4Unity\u0026#34;)] private extern static int Square(int a); 然后就是简单地调用啦：\n//调用C++ Native中的方法 if(GUILayout.Button(\u0026#34;调用C++ Native中的方法\u0026#34;, GUILayout.Height (30))) { Debug.Log(\u0026#34;调用C++ Native中的方法Random(0,10):\u0026#34; + Random(0, 10)); Debug.Log(\u0026#34;调用C++ Native的方法Max(5,10):\u0026#34; + Max(5, 10)); Debug.Log(\u0026#34;调用C++ Native中的方法Square(5):\u0026#34; + Square(5)); } 最终程序的运行效果如图：\n这个结果来之不易请大家珍惜\r","date":"2015-11-21T14:47:26Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2527231326/","slug":"2527231326","tags":["Unity3D","C++","插件"],"title":"Unity3D 游戏开发之 C++ 插件接入"},{"categories":["独立博客"],"content":"各位朋友，大家好，欢迎大家关注我的博客，我是秦元培，我的博客地址是http://qinyuanpei.com。今天想和大家说说博客文章版权这件事情。每当提到版权的时候，我知道大家内心深处都是对此不以为然的，因为国内版权意识薄弱，所以版权在我们的眼中就变成了这样一件可有可无的东西，可是事实真的是这样的吗？首先我们必须承认一件事情，即你从互联网上获得的知识都是有价值的，即使这些知识的创造者并未因此而获得利益。\n相对其它的行业，因为程序员这个职业本身需要其通过不断地学习新知识来适应新变化，因此程序员这个群体更喜欢在互联网上分享知识和经验，而这些知识的受众面窄、技术门槛高则决定了程序员们无法像普通的博客作者一样有更多的机会来获得收入。大部分的程序员都是从分享知识、记录学习和技术交流这样的角度来撰写博客的。那么这样就会造成一个问题，在国内版权意识薄弱和技术博客变现困难的双重夹缝中，博客作者该如何寻求新的突破呢？\n为什么要说博客文章版权这件事儿 首先，我们为什么要说博客文章版权这件事情呢？因为博客是文字创造的产物，是知识共享的一种形式。当我们无视版权的时候，这意味着知识是没有价值的，创造是无关痛痒的，脑力劳动和创意活动的价值在现实中被无情地扼杀。当你引用一个人的观点却不加以注明的时候，当你盗用一个人的创意却不给予报酬的时候，试问谁还会愿意为这个社会贡献创意和想法呢？引用许锡良的观点，尊重版权更深层次的含义在于对人权的尊重：\n从人权的角度看，人与动物的区别在于知识与思想，因此，人类最根本的财富也应该是知识与思想上的财富，一切社会财富都来源于知识与思想，这是从自然界获得生活资料，以及改造社会的根本。没有知识与思想的人类将与动物世界没有什么区别。尊重人权，首先就要学会尊重人的劳动成果，特别是创造性劳动成果，只有人，才能创造出来的知识产权与发明专利。尊重人权，首先就要学会尊重一个人的思想与创意。只有人，才能创造出来的知识产权与发明专利。因为每个人都是一个独立的个体，每个人头脑里的想法都是独一无二的，无人能替代的。在没有说出来，或者写出来之前，也无人能盗取的，这种思想就是个人独有，就是只属于个人的专利。\n我常常遇到在博客评论中和我直接要源代码的人，我不明白从什么时候起对知识的分享变成了某些人懒惰的借口。我选择将我知道的某个分享出来，当然我同样有权利可以选择沉默。你不能因为习惯了做伸手党就认为我应该理所当然地把源代码给你，这是对我的不尊重。我写博客的目的在于和别人交流技术、互相学习，如果你根本没有看懂我博客写了什么，只是希望可以找到可以“抄”来就能用的代码。抱歉!这违背了我的初衷我更没有道理要将源代码分享给你,况且如果我的代码都清清楚楚（命名规范、注释清晰）地写在博客里，如果这样的代码你都不能看懂，就算把完整的工程分享给你又有什么用呢？我本来不情愿论坛来转载我的文章，因为论坛盖楼的这种互动模式实在难以产生较为良好的互动效果，可是人家来诚心诚意地询问你的态度，如此拒绝难免有点却之不恭吧！当然最让人讨厌的是网络爬虫和网站编辑，这种让人讨厌是因为它\u0026quot;简单粗暴\u0026quot;，完全不考虑博客作者的感受，文章的原始链接被删除、文章的作者署名被删除。或许大家都觉得一个署名、一个原始链接都是无足轻重的东西，可是在我看来这恰恰是体现责任的地方。作者的责任在于对文章的真实性和客观性负责，转发者的责任在于帮助读者找到作者当他们之间需要某种交流的时候，这就是我为什么强调署名和原始链接的理由！\n谈谈如何保护博客文章版权 在保护博客文章版权这个问题上，我们可以采取的方式固然很多，但是这件事情的根本原因在于人们普遍不重视知识产权的保护，所以我们这里提到的这些方式都是外家功夫，真正要根除这等沉疴痼疾需要人们不断提升自我、勤修内功。\n第一种方式，我们称为通过技术方式提醒，比如通过编写 JavaScript 脚本，实现当对方复制你的博客内容的时候，程序可以自动在这段复制的内容中增加署名和连接。当然我们可以通过修改文章的模板来在文章中加入版权信息，这个我们留到最后会说，因为实际上这个是我们今天要重点研究的内容，博主是个程序员，我们当然要用技术的方式来实现了，前面的这些大家看完心里有个数就是了，哈哈！ 第二种方式，我们称为增加文章内链的方式，就是在文章中尽可能地使用指向这个博客的连接，这样可以保证在文章转载后为博客带来一定的反向访客。 第三种方式，在图片上增加水印，这样读者在看到图片的时候就可以很容易地找到原始出处，可是如果你不能保证拥有所引用的图片得版权，建议不要轻易地使用这种方式。 第四种方式，逐渐形成个性化的写作风格，这样当读者读到这些文字的时候，可以通过文章的风格知道文章的作者和出处。 第五种方式，努力提高自己博客的文章质量，让更多的读者从中受益，形成有独立风格的博客品牌。一个博客有了高质量的内容和品牌，那么在搜索引擎中的权重就会很高，网民通过搜索引擎进行查找的时候，博客原文都会排在第一位。 博主之所以要使用 http://qinyuanpei.com 这个独立博客的原因正是基于这个原因。博主目前采用的知识共享许可是署名(BY)-非商业性使用(NC)-相同方式共享(SA)，请各位在转载文章的时候注意保留作者署名和文章出处，谢谢！\n保护博客文章版权，独立博客在行动 我的博客是采用 Hexo 这个博客系统来搭建的，说到底程序员是天生爱折腾的命吧，都有对掌控事物的欲望，不喜欢受到条件制约。CSDN 的博客虽然还不错，可是“限制因素 + 服务器奔溃”这样的强效组合实在让我很难有继续坚持下去的动力，所以果断就自己搭了博客买了域名，老老实实地开始管理起独立博客。好了，废话少说，放码过来，我们下面来看看怎么在 Hexo 中的文章中增加一个展示版权信息的模块，这里以 Jacman 主题为例，我们首先定位到该主题文件夹下的 \\layout_partial\\post\\article.ejs 文件：\n\u0026lt;div id=\u0026#34;main\u0026#34; class=\u0026#34;\u0026lt;%= item.layout %\u0026gt;\u0026#34; itemscope itemprop=\u0026#34;blogPost\u0026#34;\u0026gt; \u0026lt;% if (page.layout==\u0026#39;photo\u0026#39; \u0026amp;\u0026amp; item.photos \u0026amp;\u0026amp; item.photos.length){ %\u0026gt; \u0026lt;%- partial(\u0026#39;gallery\u0026#39;) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;article itemprop=\u0026#34;articleBody\u0026#34;\u0026gt; \u0026lt;%- partial(\u0026#39;header\u0026#39;) %\u0026gt; \u0026lt;div class=\u0026#34;article-content\u0026#34;\u0026gt; \u0026lt;% if( table\u0026amp;\u0026amp;(item.toc !== false) \u0026amp;\u0026amp; theme.toc.article){ %\u0026gt; \u0026lt;div id=\u0026#34;toc\u0026#34; class=\u0026#34;toc-article\u0026#34;\u0026gt; \u0026lt;strong class=\u0026#34;toc-title\u0026#34;\u0026gt;\u0026lt;%= __(\u0026#39;contents\u0026#39;) %\u0026gt;\u0026lt;/strong\u0026gt; \u0026lt;% if(item.list_number == false) {%\u0026gt; \u0026lt;%- toc(item.content,{list_number:false}) %\u0026gt; \u0026lt;% }else{ %\u0026gt; \u0026lt;%- toc(item.content) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;%- item.content %\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;%- partial(\u0026#39;footer\u0026#39;) %\u0026gt; \u0026lt;/article\u0026gt; \u0026lt;%- partial(\u0026#39;pagination\u0026#39;) %\u0026gt; \u0026lt;%- partial(\u0026#39;comment\u0026#39;) %\u0026gt; \u0026lt;/div\u0026gt; 我们可以注意到文章的内容是在 \u0026lt;%- item.content %\u0026gt; 这个标签里，因此我们如果要在文章中增加内容，只需要在 \u0026lt;%- item.content %\u0026gt; 的后面引入一个 ejs 模板文件即可，所以我们接下在 article.ejs 的同级目录下创建一个 declare.ejs 文件：\n\u0026lt;pre\u0026gt; \u0026lt;code\u0026gt; \u0026lt;b\u0026gt;版权声明\u0026lt;/b\u0026gt;:本文由 \u0026lt;b\u0026gt; \u0026lt;a href=\u0026#34;\u0026lt;%= config.root %\u0026gt;about\u0026#34; target=\u0026#34;_blank\u0026#34; title=\u0026#34;\u0026lt;%= config.author %\u0026gt;\u0026#34;\u0026gt; \u0026lt;%=c onfig.author %\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/b\u0026gt;创作和发表,采用 \u0026lt;b\u0026gt;署名(BY)\u0026lt;/b\u0026gt;- \u0026lt;b\u0026gt;非商业性使用(NC)\u0026lt;/b\u0026gt;- \u0026lt;b\u0026gt;相同方式共享(SA)\u0026lt;/b\u0026gt;国际许可协议进行许可,转载请注明作者及出处,本文作者为 \u0026lt;b\u0026gt; \u0026lt;a href=\u0026#34;\u0026lt;%= config.root %\u0026gt;about\u0026#34; target=\u0026#34;_blank\u0026#34; title=\u0026#34;\u0026lt;%= config.author %\u0026gt;\u0026#34;\u0026gt; \u0026lt;%=c onfig.author %\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/b\u0026gt;,本文标题为 \u0026lt;b\u0026gt; \u0026lt;a href=\u0026#34;\u0026lt;%- config.root %\u0026gt;\u0026lt;%- item.path %\u0026gt;\u0026#34; target=\u0026#34;_blank\u0026#34; title=\u0026#34;\u0026lt;%= item.title %\u0026gt;\u0026#34;\u0026gt; \u0026lt;%=i tem.title %\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/b\u0026gt;,本文链接为 \u0026lt;b\u0026gt; \u0026lt;a href=\u0026#34;\u0026lt;%- config.root %\u0026gt;\u0026lt;%- item.path %\u0026gt;\u0026#34; target=\u0026#34;_blank\u0026#34; title=\u0026#34;\u0026lt;%= item.title %\u0026gt;\u0026#34;\u0026gt; \u0026lt;%- config.url %\u0026gt;/ \u0026lt;%- item.path %\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/b\u0026gt;.\u0026lt;/code\u0026gt; \u0026lt;/pre\u0026gt; 大家可以看到这里就是一段 HTML 代码，因为我们要引入的这个模板和 article.ejs 在同一个页面中，所以我们可以直接在这里调用 item 这个变量，而 item 这个变量里是封装了当前文章的标题和链接的，因此我们可以顺利成章的构造这样一段 HTML 代码，因为博主不会写 CSS 样式，所以使用了一个默认的代码样式来完成这个工作，如果大家懂 CSS，请自行发挥你的创意将它做得更好。好了，下面我们要做的工作就是将这个模版引用到 article.ejs 文件中，类似地我们可以使用\u0026lt;%- partial(\u0026lsquo;footer\u0026rsquo;) %\u0026gt;这样的结构来引入这个模板，这里给出完整的 article.ejs 文件内容：\n\u0026lt;div id=\u0026#34;main\u0026#34; class=\u0026#34;\u0026lt;%= item.layout %\u0026gt;\u0026#34; itemscope itemprop=\u0026#34;blogPost\u0026#34;\u0026gt; \u0026lt;% if (page.layout==\u0026#39;photo\u0026#39; \u0026amp;\u0026amp; item.photos \u0026amp;\u0026amp; item.photos.length){ %\u0026gt; \u0026lt;%- partial(\u0026#39;gallery\u0026#39;) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;article itemprop=\u0026#34;articleBody\u0026#34;\u0026gt; \u0026lt;%- partial(\u0026#39;header\u0026#39;) %\u0026gt; \u0026lt;div class=\u0026#34;article-content\u0026#34;\u0026gt; \u0026lt;% if(theme.show_declare) { %\u0026gt; \u0026lt;%- partial(\u0026#39;declare\u0026#39;) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;% if( table\u0026amp;\u0026amp;(item.toc !== false) \u0026amp;\u0026amp; theme.toc.article){ %\u0026gt; \u0026lt;div id=\u0026#34;toc\u0026#34; class=\u0026#34;toc-article\u0026#34;\u0026gt; \u0026lt;strong class=\u0026#34;toc-title\u0026#34;\u0026gt;\u0026lt;%= __(\u0026#39;contents\u0026#39;) %\u0026gt;\u0026lt;/strong\u0026gt; \u0026lt;% if(item.list_number == false) {%\u0026gt; \u0026lt;%- toc(item.content,{list_number:false}) %\u0026gt; \u0026lt;% }else{ %\u0026gt; \u0026lt;%- toc(item.content) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;%- item.content %\u0026gt; \u0026lt;% if(theme.show_declare) { %\u0026gt; \u0026lt;%- partial(\u0026#39;declare\u0026#39;) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;%- partial(\u0026#39;footer\u0026#39;) %\u0026gt; \u0026lt;/article\u0026gt; \u0026lt;%- partial(\u0026#39;pagination\u0026#39;) %\u0026gt; \u0026lt;%- partial(\u0026#39;comment\u0026#39;) %\u0026gt; \u0026lt;/div\u0026gt; 这里博主在文章的开头和结尾处插入了这个模板，同时在主题文件夹中设置了一个是否显示版权声明的开关变量，这样我们就可以在主题中设置是否开启版权声明模块了。好啦，相信你在看到这边文章的时候你已经看到了它的版权声明了，这就是我们今天的内容啦，谢谢大家！\n","date":"2015-11-15T13:12:22Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2950334112/","slug":"2950334112","tags":["Hexo","版权","知识共享"],"title":"在 Hexo 中为文章自动添加版权信息声明模块"},{"categories":["游戏开发"],"content":"各位朋友，大家好，欢迎大家关注我的博客，我是秦元培，我的博客地址是http://qinyuanpei.com。今天想和大家交流的是解析 obj 模型并将其加载到 Unity3D 场景中，虽然我们知道 Unity3D 是可以直接导入 OBJ 模型的，可是有时候我们并不能保证我们目标客户知道如何使用 Unity3D 的这套制作流程，可能对方最终提供给我们的就是一个模型文件而已，所以这个在这里做这个尝试想想还是蛮有趣的呢，既然如此，我们就选择在所有 3D 模型格式中最为简单的 OBJ 模型来一起探讨这个问题吧！\n关于 OBJ 模型 OBJ 格式是一种 3D 模型文件格式，是由 Alias|Wavefront 公司为 3D 建模和动画软件 “Advanced Visualizer”开发的一种标准，适合用于 3D 软件模型之间的互相转换。和 FBX、Max 这种内部私有格式不同，OBJ 模型文件是一种文本文件，我们可以直接使用记事本等软件打开进行编辑和查看，因此我们这里选择 OBJ 模型主要是基于它开放和标准这两个特点。需要说明的是，OBJ 文件是一种 3D 模型文件，它主要支持多边形模型（三个点以上的面）。OBJ 模型支持法线和贴图坐标，可是因为它本身并不记录动画、材质特性、贴图路径、动力学及粒子等信息，所以我们在游戏开发中基本看不到这种模型格式的，所以我们这里做下简单研究就好。\nOBJ 模型解读 因为 OBJ 模型文件是一个文本文件，所以我们可以使用记事本等软件打开它来对它的文件结构进行下了解。首先 OBJ 文件没有头文件，如果你曾经尝试解析过 mp3 文件的 ID3v1/ID3v2 标签就应该知道它是根据 mp3 文件的开头或者末尾的若干字节来判断这些标签信息的，而在 OBJ 文件中是没有类似这样的头文件的。OBJ 文件是由一行行由关键字、空格和文本字符组成的文本文件，通过关键字我们就可以知道这一行的文本表示的是什么数据。例如：\n# Blender v2.76 (sub 0) OBJ File: \u0026#39;\u0026#39; **#**关键字表示一个注释行，通过这个注释信息我们可以知道这个 OBJ 模型是由 Blender2.76 版本导出的。再比如：\nmtllib liumengli.mtl mtllib关键字则表示当前模型对应的材质库(.mtl)文件名称，每个 OBJ 模型文件都会有这样一个对应和它同名的.mtl 文件，在这个文件中记录了材质相关的信息，稍后我们说到材质的时候会详细说说这个文件的格式，因为它和 OBJ 文件一样是一个文件文件。再比如：\nusemtl Material__33 usemtl关键字则表示从当前行到下一个 usemtl 关键字所在行间的全部网格结构都使用其对应的材质，通过这个材质名称我们可以在.obj 文件对应的.mtl 文件中找到它的材质定义，这个我们在讲到材质部分的时候会详细说。\n好了，目前我们要做的工作室解析.obj 文件然后创建网格进而可以使其显示在 Unity3D 场景中，在这里我们要重点关注的关键字有：\nv 即 Vertex，表示一个顶点的局部坐标系中的坐标，通常有三个分量，因为这里讨论的是三角面。例如： v 1.5202 14.9252 -1.1004 vn 即 Vertex Normal，表示法线，注意到这些向量都是单位向量，所以我们可以认为三维软件在导出模型的时候已经做好了相关的标准化工作。 vn 0.8361 -0.0976 0.5399 vt 即 Vertex Texture，表示纹理坐标，就是我们熟悉的 UV 坐标啦，显然 UV 是个 2D 坐标，有两个分量。 vt -0.5623 0.4822 1.0000 f 即 face，这是一个真正描述面的关键字，通常它后面有三个索引结构，每个索引结构由顶点索引、法线索引和纹理坐标索引三部分构成。例如： f 256/303/637 257/304/638 258/305/639 以上这些关键字对我们解析.obj 文件来说已经完全足够了，如果大家想对这些细节有更为深入的了解，可以参考这里这里。\nOBJ 模型的读取 OBJ 模型的读取涉及到网格部分的读取和材质部分的读取两个部分，其中网格部分的读取难点在于当模型存在多个材质的时候，需要将模型分为若干个子物体，然后分别为这些子物体添加材质。可是不幸的是到目前为止，博主并没有找到一种行之有效的方法来对这些网格进行分类，所以这里我们假定模型是一个整体且共享同一种材质和一张贴图。如果大家找到了更好的解决方案，请记得告诉我，再次谢谢大家！\n网格部分 在网格读取这部分，因为我们已经假设所有的面构成一个物体，因此我们可以先将 OBJ 文件内的文本按照换行符来进行分割，然后再按照关键字去判断每一行的数据类型并进行相应的处理就可以了。读取 OBJ 模型的基本流程是：\n读取顶点、法线、UV 以及三角面 将三角面合并为四边面 根据索引重新计算顶点、法线、UV 数组 读取顶点、法线、UV 以及三角面 首先我们来看第一步的代码实现：\n/// \u0026lt;summary\u0026gt; /// 从一个文本化后的.obj文件中加载模型 /// \u0026lt;/summary\u0026gt; public ObjMesh LoadFromObj(string objText) { if(objText.Length \u0026lt;= 0) return null; //v这一行前面是两个空格后面是一个空格 objText=objText.Replace(\u0026#34; \u0026#34;, \u0026#34; \u0026#34;); //将文本化后的obj文件内容按行分割 string[] allLines = objText.Split(\u0026#39;\\n\u0026#39;); foreach(string line in allLines) { //将每一行按空格分割 string[] chars = line.Split(\u0026#39; \u0026#39;); //根据第一个字符来判断数据的类型 switch(chars[0]) { case \u0026#34;v\u0026#34;: //处理顶点 this.vertexArrayList.Add(new Vector3( ConvertToFloat(chars[1]), ConvertToFloat(chars[2]), ConvertToFloat(chars[3])) ); break; case \u0026#34;vn\u0026#34;: //处理法线 this.normalArrayList.Add(new Vector3( ConvertToFloat(chars[1]), ConvertToFloat(chars[2]), ConvertToFloat(chars[3])) ); break; case \u0026#34;vt\u0026#34;: //处理UV this.uvArrayList.Add(new Vector3( ConvertToFloat(chars[1]), ConvertToFloat(chars[2])) ); break; case \u0026#34;f\u0026#34;: //处理面 GetTriangleList(chars); break; } } 在这段代码中，我们首先将文本化的.obj 文件按照换行符分割成字符串数组 allLines，然后再对每一行按照空格分隔成字符串数组 chars，这样我们就可以通过该数组的第一个元素 chars[0]来判断当前行中的数据类型。这样我们将每一行的文本读取完后，所有的数据都被存储到了其相对应的列表中。其中，vertexArrayList 存储顶点信息、normalArrayList 存储法线信息、uvArrayList 存储 UV 坐标。至此，我们完成第一部分中的顶点、法线和 UV 的读取。\n这里可以注意到我们在开始对文本化的.obj 文件的内容有 1 次替换操作，这是因为在 3dsMax 中导出的.obj 文件关键字v这一行中 v 后面的第一处空格位置是有 2 个空格，而我们在处理的时候是按照空格来分割每一行的内容的，这样 chars[1]就会变成一个空字符串，显然这不符合我们的初衷，所以这里就需要对字符串进行这样一个操作，希望大家在解析的过程中注意，好吧，我承认我想吐槽 3dsMax 了，我不明白同一家公司的 3dsMax 和 Maya 为什么不能互相转换，我不明白 3dsMax 导出.obj 文件的时候要做这样奇葩的设定，我更不明白为什么有开源、免费、轻巧的 Blender 都不去用非要每次都去安装容量动辄上 G 的盗版软件和不知道会不会变成下一个 GhostXXXX 的注册机，我更加不能容忍的是封闭的 FBX 格式和用起来就如同自虐的 FBX SDK。\n好了，吐槽结束，我们接下来来看看三角面是如何读取的。三角面的读取定义在 GetTriangleList()方法中，因此三角面的读取实际上首先需要将每一行文本按照空格进行分割，然后再将每一个元素按照/分割，这样就可以依次得到顶点索引、法线索引和 UV 索引。在某些情况下法线索引可能不存在，所以在处理的过程中需要对其进行处理。\n/// \u0026lt;summary\u0026gt; /// 获取面列表. /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;chars\u0026#34;\u0026gt;Chars.\u0026lt;/param\u0026gt; private void GetTriangleList(string[] chars) { List\u0026lt;Vector3\u0026gt; indexVectorList = new List\u0026lt;Vector3\u0026gt;(); List\u0026lt;Vector3\u0026gt; triangleList = new List\u0026lt;Vector3\u0026gt;(); for(int i = 1; i \u0026lt; chars.Length;++i ) { //将每一行按照空格分割后从第一个元素开始 //按照/继续分割可依次获得顶点索引、法线索引和UV索引 string[] indexs = chars[i].Split(\u0026#39;/\u0026#39;); Vector3 indexVector = new Vector3(0, 0); //顶点索引 indexVector.x = ConvertToInt(indexs[0]); //法线索引 if(indexs.Length \u0026gt; 1){ if(indexs[1] != \u0026#34;\u0026#34;) indexVector.y = ConvertToInt(indexs[1]); } //UV索引 if(indexs.Length \u0026gt; 2){ if(indexs[2] != \u0026#34;\u0026#34;) indexVector.z = ConvertToInt(indexs[2]); } //将索引向量加入列表中 indexVectorList.Add(indexVector); } //这里需要研究研究 for(int j = 1; j \u0026lt; indexVectorList.Count - 1; ++j) { //按照0,1,2这样的方式来组成面 triangleList.Add(indexVectorList[0]); triangleList.Add(indexVectorList[j]); triangleList.Add(indexVectorList[j + 1]); } //添加到索引列表 foreach(Vector3 item in triangleList) { faceVertexNormalUV.Add(item); } } 在这里，我们首先使用一个索引向量列表 indexVectorList 存储每一行的索引向量。这里的索引向量是指由顶点索引、法线索引和 UV 索引分别构成 Vector3 的三个分量，这样做的好处是我们可以节省重新去定义数据机构的时间。好了，我们把所有的索引向量读取完后，按照 0、1、2 这样的方式组成三角面，这里可能是.obj 文件本身定义的一种方式，我们暂且按照这样的方式来处理。最后，全部的三角面会被读取到 faceVertexNormalUV 列表中，它表示的是每个三角面的顶点、法线和 UV 的索引向量，是一个 List类型的变量。\n将三角面合并为四边面 现在我们读取到的是三角面，接下来我们需要将它们合并成四边面，合并的原理是判断它们是否在同一个面上。如果两个点的顶点索引相同则表明它们是同一个点，如果两个点的法线索引相同则表明它们在同一个面上。好了，我们来看定义的一个方法 Combine():\n/// \u0026lt;summary\u0026gt; /// 合并三角面 /// \u0026lt;/summary\u0026gt; private void Combine() { //使用一个字典来存储要合并的索引信息 Dictionary\u0026lt;int, ArrayList\u0026gt; toCambineList = new Dictionary\u0026lt;int,ArrayList\u0026gt;(); for(int i = 0; i \u0026lt; faceVertexNormalUV.Count; i++) { if(faceVertexNormalUV[i] != Vector3.zero) { //相同索引的列表 ArrayList SameIndexList = new ArrayList(); SameIndexList.Add(i); for(int j = 0; j \u0026lt; faceVertexNormalUV.Count; j++) { if(faceVertexNormalUV[j]!=Vector3.zero) { if(i != j) { //如果顶点索引和法线索引相同，说明它们在一个面上 Vector3 iTemp = (Vector3)faceVertexNormalUV[i]; Vector3 jTemp = (Vector3)faceVertexNormalUV[j]; if(iTemp.x == jTemp.x \u0026amp;\u0026amp; iTemp.y == jTemp.y) { //将索引相同索引列表然后将其重置为零向量 //PS:这是个危险的地方，如果某个索引信息为Vector3.Zero //就会被忽略过去，可是貌似到目前为止没有发现为Vector3.Zero的情况 SameIndexList.Add(j); faceVertexNormalUV[j]=Vector3.zero; } } } } //用一个索引来作为字典的键名，这样它可以代替对应列表内所有索引 toCambineList.Add(i, SameIndexList); } } } 在这里我们使用了一个字典来存储合并后的四边面，这个字典的键名为这一组三角面共同的索引，因为大家都是用同一个索引，因此它可以代替那些被合并的三角面的索引，这样合并以后的四边面列表中元素的个数就是实际的网格中的面数个数，因为如果采用三角面的话，这个面数会比现在的面数还要多，这意味着这样会带来更多的性能上的消耗。这里可能不大好理解，大家可以将博主这里的表达方式换成自己能够理解的方式。佛曰不可说，遇到这种博主自己都说不明白的地方，博主就只能请大家多多担待了。好了，接下来要做的是重新计算顶点、法线和 UV 数组。可能大家会比较疑惑，这部分内容我们在第一步不是就已经读取出来了嘛，怎么这里又要重新计算了呢？哈哈，且听我慢慢道来！\n根据索引重新计算顶点、法线、UV 数组 虽然我们在第一步就读取到了这些坐标数据，可是当我们合并三角面以后，就会出现大量的无用的点，为什么无用呢，因为它被合并到四边面里了，这样我们原来读取的这些坐标数据就变得不适用了。那怎么办呢？在第三步中我们合并四边面的时候已经用一个字典保存了合并后的索引信息，这就相当于我们已经知道哪些是合并前的索引，哪些是合并后的索引，这个时候我们只要根据索引重新为数组赋值即可：\n//初始化各个数组 this.VertexArray = new Vector3[toCambineList.Count]; this.UVArray = new Vector2[toCambineList.Count]; this.NormalArray = new Vector3[toCambineList.Count]; this.TriangleArray = new int[faceVertexNormalUV.Count]; //定义遍历字典的计数器 int count = 0; //遍历词典 foreach(KeyValuePair\u0026lt;int,ArrayList\u0026gt; IndexTtem in toCambineList) { //根据索引给面数组赋值 foreach(int item in IndexTtem.Value) { TriangleArray[item] = count; } //当前的顶点、UV、法线索引信息 Vector3 VectorTemp = (Vector3)faceVertexNormalUV[IndexTtem.Key]; //给顶点数组赋值 VertexArray[count] = (Vector3)vertexArrayList[(int)VectorTemp.x - 1]; //给UV数组赋值 if(uvArrayList.Count \u0026gt; 0) { Vector3 tVec =(Vector3)uvArrayList[(int)VectorTemp.y - 1]; UVArray[count] = new Vector2(tVec.x, tVec.y); } //给法线数组赋值 if(normalArrayList.Count \u0026gt; 0) { NormalArray[count] = (Vector3)normalArrayList[(int)VectorTemp.z - 1]; } count++; } 这样我们就读取到了合并后的坐标信息，通过顶点、法线、UV、面等信息我们现在就可以生成网格了。这部分我们暂且不着急，因为这基本上属于最后整合到 Unity3D 中步骤了。好了，为了方便大家理解，我已经完整的项目上传到 Github，大家可以通过这里了解完整的项目。\n材质部分 材质这块儿的解析主要集中在.mtl 文件中，和.obj 文件类似，它同样是一个文本文件、同样采用关键字、空格、文本字符这样的结构来表示数据，因此我们可以借鉴.obj 文件的读取。例如：\nnewmtl Material newmtl关键字表示从当前行到下一个 newmtl 关键字所在行间都表示该关键字所对应的材质，这里的 Material 即表示材质的名称，它和.obj 文件中的usemtl关键字相对应，因此我们给模型添加材质的过程本质上是从.obj 文件中读取网格，然后找到其对应的材质名称，然后在.mtl 文件中找到对应的材质定义，并根据定义来生成材质。目前已知的关键字有：\nKa 0.5880 0.5880 0.5880 Ka关键字表示环境反射的 RGB 数值。\nKd 0.640000 0.640000 0.640000 Kd关键字表示漫反射的 RGB 数值。\nKs 0.500000 0.500000 0.500000 Ks关键字表示镜面反射的 RGB 数值。\nmap_Ka E:\\学习资料\\Unity3D技术\\Unity3D素材\\柳梦璃\\Texture\\1df2eaa0.dds map_Ka关键字表示环境反射的纹理贴图，注意到这里使用的是绝对路径，显然我们在读取模型的时候不会将贴图放在这样一个固定的路径，因此我们这里初步的想法读取贴图的文件名而非贴图的完整路径，考虑到我们在 Unity3D 中一般使用 PNG 格式的贴图，因此这里需要对路径进行处理。\nmap_Kd E:\\学习资料\\Unity3D技术\\Unity3D素材\\柳梦璃\\Texture\\1df2eaa0.dds map_Kd关键字表示漫反射的纹理贴图，和环境反射的纹理贴图是类似地，这里就不再说了。此外还有其它的关键字，初步可以推断出的结论是它和 3dsMax 中材质编辑器里的定义特别地相似，感兴趣的朋友可以进一步去研究。可是现在就有一个新的问题了，怎样将这些参数和 Unity3D 里的材质关联起来呢？我们知道 Unity3D 里的材质是是由着色器和贴图两部分组成的，博主对 Shader 并不是很熟悉，因此这里确实有些说不清楚了。博主感觉对 OBJ 文件来说，其实使用 Diffuse 就完全足够了，所以这里对材质部分的研究我们点到为止，不打算做代码上的实现。如果不考虑这些参数的话，我们要做的就是通过 WWW 或者 Resource 将贴图加载进来，然后赋值给我们通过代码创建的 Shader 即可。而对于.obj 文件来说，无论是通过 Resource、WWW 或者是 IO 流，只要我们拿到了这个文件中的内容就可以使用本文中的方式加载进来，因为我们假定的是读取只有一种材质的模型。有朋友可能要问，那如果有多种材质怎么办呢？答案是在.mtl 问价中获取到所有贴图的名称，然后再到程序指定的路径去读取贴图，分别为其创建不同的材质，可是这些材质要怎么附加到它对应的物体上呢？这个目前博主没有找到解决的方法，所以此事暂且作罢吧！\n在 Unity3D 中加载 obj 模型 下面我们以一个简单的例子来展示今天研究的成果，我们将从.obj 文件中读取出一个简单的模型并将其加载到场景中。好了，我们一起来看代码：\nif(!File.Exists(\u0026#34;D:\\\\cube.obj\u0026#34;)) Debug.Log(\u0026#34;请确认obj模型文件是否存在!\u0026#34;); StreamReader reader = new StreamReader(\u0026#34;D:\\\\cube.obj\u0026#34;,Encoding.Default); string content = reader.ReadToEnd(); reader.Close(); ObjMesh objInstace = new ObjMesh(); objInstace = objInstace.LoadFromObj(content); Mesh mesh = new Mesh(); mesh.vertices = objInstace.VertexArray; mesh.triangles = objInstace.TriangleArray; if(objInstace.UVArray.Length \u0026gt; 0) mesh.uv = objInstace.UVArray; if(objInstace.NormalArray.Length\u0026gt;0) mesh.normals = objInstace.NormalArray; mesh.RecalculateBounds(); GameObject go = new GameObject(); MeshFilter meshFilter = go.AddComponent\u0026lt;MeshFilter\u0026gt;(); meshFilter.mesh = mesh; MeshRenderer meshRenderer = go.AddComponent\u0026lt;MeshRenderer\u0026gt;(); 这里没有处理材质，所以读取出来就是这个样子的，哈哈！\n最终效果，这是一个悲伤的故事\r材质大家可以尝试用代码去创建一个材质，然后在给一张贴图，这个玩玩就好，哈哈！好了，今天的内容就是这样子了，希望大家喜欢，为了写这篇文章我都怀疑我是不是有拖延症啊！\n","date":"2015-11-15T13:07:57Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1124152964/","slug":"1124152964","tags":["OBJ","Unity3D","格式"],"title":"解析 OBJ 模型并将其加载到 Unity3D 场景中"},{"categories":["Unity3D"],"content":"各位朋友大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是：http://qinyuanpei.com。今天想和大家分享的是 uGUI 中分页效果的实现，我们知道相对 NGUI 来说 uGUI 在功能覆盖上来讲，它并没有像 NGUI 那样提供较为丰富和炫酷的组件，可是因为 uGUI 有着较好的扩展性，因此我们可以通过编写脚本来扩展它的功能。虽然在移动开发时代以开发速度论成败，可是这并不是我们“不求甚解”的正当理由。每次看到 NGUI 各种\u0026quot;丰富\u0026quot;的组件在脑海中打转的时候，每次看到编译项目时弹出各种 Warming 的时候，我内心是如此地期望有这样一个简单高效的 UI 系统啊，直到有一天我遇上了 uGUI。\n好了，博主这里并没有想要表达厚此薄彼的观点啦，博主真正想要表达的是我们在开发中应该摒弃“唯语言论”、“唯平台论”的狭隘观点，努力去掌握和语言无关、平台无关的“通用型技能”。这样，当我们面对全新的任务的时候我们可以更快地适应新的环境。言归正传，我们这里来接着说 uGUI 中的分页，分页通常是指将内容分散到不同的页面上来显示的一种手段，这种手段我们在传统的 Web 开发中可以经常看到。到了移动互联网以后分页被我们更为熟知的“下拉更新”所替代，这种方式我们就更为熟悉啦。好了，我们回到分页，为什么要要分页呢？这里有两个关键点：第一，内容在一页内无法展示完全；第二，对内容的数量无法进行估计。\n例如，我们在 uGUI 中可以使用 ScrollRect 组件 + GridLayout 组件 + Mask 组件实现一个滚动列表，具体的案例可以参考：这里。这里我们可以注意到一件事情就是这个滚动列表，它可以滚动的范围是由 Mask 组件来决定的，因此这个滚动列表是无法无限滚动的，虽然我们知道在游戏设计中不会出现这种无限滚动的列表，可是我们这里是为了探讨这个问题，所以我们假设这个情况是允许发生的。那么面对这个问题，我们有什么好的解决方案呢？博主尝试过一种思路，即借鉴 Android 中的 ListView 控件，这个控件的特点是可以对列表中的项目进行回收。相信说到这里，大家都明白我想做什么了吧，大概的思路就是制作一个高度大于屏幕高度的列表，然后让所有的列表项在这个列表中循环显示。可是新的问题就来了，第一，频繁地生成和销毁物体是 Unity3D 大忌。虽然我们可以缓存池来解决这个问题，可是因为博主并没有具体这样实践过，所以这里目前是存疑的。**第二，GridLayout 这个组件内的元素排序是根据子元素添加的顺序来决定的，因此每次列表更新以后都需要将所有的子元素更新一遍。**曾经因为这样需求和策划发生过争执，最终妥协的一个结果就是采用分页来解决这个问题。分页首先解决了无限滚动的需求，因为它是一种“以不变应万变”的策略，不论列表内元素有多少它都可以显示出来。其次，在分页的过程是将数据模板化的过程，它改变是界面的外观和行为，UI 结构是相对稳定的，这样可以避免频繁地生成和销毁物体。\n下面我们以一个简单的案例来探讨分页效果在 uGUI 中的实现，首先我们使用 GridLayoutGroup 来制作一个简单的网格布局，请用心感受下图中萌萌哒十二生肖：\n请不要伤害一个程序员的艺术细胞，虽然我知道它比较难看\r这里我们不再对这个布局的制作方法进行详细的说明，因为我们今天的重点不在这里。我们注意到这里有 12 个元素，当我们每次对页面进行切换的时候，实际上这 12 个元素是基本不会发生变化的，真正变化的是这些元素的外观（如这里的精灵图片和名称）以及其对应的 UI 事件，在这个案例中我们利用匿名函数实现了一个简单的 Click 事件的监听。好了，前面我们说到分页的一个目的是可以解决列表内元素数目不确定的问题，因此我们这里利用一个 12 生肖的数组来随机生成元素数目不同的列表，代码实现如下：\n/// \u0026lt;summary\u0026gt; /// 初始化元素 /// \u0026lt;/summary\u0026gt; private void InitItems() { //准备一个存储着12生肖信息的数组 GridItem[] items = new GridItem[] { new GridItem(\u0026#34;鼠\u0026#34;,\u0026#34;Mouse\u0026#34;), new GridItem(\u0026#34;牛\u0026#34;,\u0026#34;Ox\u0026#34;), new GridItem(\u0026#34;虎\u0026#34;,\u0026#34;Tiger\u0026#34;), new GridItem(\u0026#34;兔\u0026#34;,\u0026#34;Rabbit\u0026#34;), new GridItem(\u0026#34;龙\u0026#34;,\u0026#34;Dragon\u0026#34;), new GridItem(\u0026#34;蛇\u0026#34;,\u0026#34;Snake\u0026#34;), new GridItem(\u0026#34;马\u0026#34;,\u0026#34;Horse\u0026#34;), new GridItem(\u0026#34;羊\u0026#34;,\u0026#34;Goat\u0026#34;), new GridItem(\u0026#34;猴\u0026#34;,\u0026#34;Monkey\u0026#34;), new GridItem(\u0026#34;鸡\u0026#34;,\u0026#34;Rooster\u0026#34;), new GridItem(\u0026#34;狗\u0026#34;,\u0026#34;Dog\u0026#34;), new GridItem(\u0026#34;猪\u0026#34;,\u0026#34;Pig\u0026#34;) }; //利用12生肖数组来随机生成列表 m_ItemsList = new List\u0026lt;GridItem\u0026gt;(); for(int i = 0; i \u0026lt; Random.Range(1,1000); i++) { m_ItemsList.Add(items[Random.Range(0,items.Length)]); } //计算元素总个数 m_ItemsCount = m_ItemsList.Count; //计算总页数 m_PageCount = (m_ItemsCount % 12) == 0 ? m_ItemsCount / 12 : (m_ItemsCount / 12) + 1; BindPage(m_PageIndex); //更新界面页数 m_PanelText.text = string.Format(\u0026#34;{0}/{1}\u0026#34;, m_PageIndex.ToString(), m_PageCount.ToString()); } 在这段代码中，m_ItemList 表示我们要展示的元素列表，m_ItemsCount 表示元素列表中元素的个数，m_PageCount 表示这些元素可以分成的总页数，m_PageIndex 表示页数的索引默认从 1 开始。其中 GridItem 是一个简单的类，它有 ItemName 和 ItemSprite 两个属性，这里不再具体说明了。好了，现在我们来思考如何将这些元素和 UI 对应起来，因为列表中元素的数目不确定，因此我们可以分成两种情况来讨论：\n页面总数为 1，即 m_PageCount=1，此时列表内的元素个数的范围是 1~12，因此我们可以利用循环判断哪些元素是要展示的？哪些元素是不需要的？因为如果此时列表内的元素为 10，则意味着前面 10 个元素是要展示给用户，而剩下的 2 个元素是不需要的。在这里我们简单地使用 SetActive 来让这些元素隐藏起来。\n页面总数大于 1，即 m_PageCount\u0026gt;1，此时前面的 m_PageCount-1 个页面都是显示完全的，它相当于元素总个数中被 12 整除的部分。而第 m_PageCount 个页面此时的情况和页数总数为 1 的情况类似，我们可以采取和页面总数为 1 类似的方法来处理。\n好了，下面我们来看这部分代码的具体实现：\n/// \u0026lt;summary\u0026gt; /// 绑定指定索引处的页面元素 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;index\u0026#34;\u0026gt;页面索引\u0026lt;/param\u0026gt; private void BindPage(int index) { //列表处理 if (m_ItemsList == null || m_ItemsCount \u0026lt;= 0) return; //索引处理 if (index \u0026lt; 0 || index \u0026gt; m_ItemsCount) return; //按照元素个数可以分为1页和1页以上两种情况 if (m_PageCount == 1) { int canDisplay = 0; for (int i = 12; i \u0026gt; 0; i--) { if (canDisplay \u0026lt; 12){ BindGridItem(transform.GetChild(canDisplay), m_ItemsList[12 - i]); transform.GetChild(canDisplay).gameObject.SetActive(true); } else { //对超过canDispaly的物体实施隐藏 transform.GetChild(canDisplay).gameObject.SetActive(false); } canDisplay += 1; } } else if (m_PageCount \u0026gt; 1) { //1页以上需要特别处理的是最后1页 //和1页时的情况类似判断最后一页剩下的元素数目 //第1页时显然剩下的为12所以不用处理 if(index == m_PageCount){ int canDisplay = 0; for(int i = 12; i \u0026gt; 0; i--) { //最后一页剩下的元素数目为 m_ItemsCount - 12 * (index-1) if (canDisplay \u0026lt; m_ItemsCount - 12 * (index-1)){ BindGridItem(transform.GetChild(canDisplay), m_ItemsList[12 * index-i]); transform.GetChild(canDisplay).gameObject.SetActive(true); } else { //对超过canDispaly的物体实施隐藏 transform.GetChild(canDisplay).gameObject.SetActive(false); } canDisplay += 1; } } else { for (int i = 12; i \u0026gt; 0; i--) { BindGridItem(transform.GetChild(12 - i), m_ItemsList[12 * index - i]); transform.GetChild(12 - i).gameObject.SetActive(true); } } } } 好了，在这里完成 BindPage 方法的定义以后，我们就可以指定程序显示对应页面的元素，此时上一页和下一页的工作基本上就是改变索引的一个过程了。这部分我们不再说了，大家可以去看最终给出的完整代码，我们这里来看看实际的效果吧！\n简单的分页效果展示\r其实这里核心的内容是分页的处理，在处理里只有 1 页时的元素个数和超过 1 页的最后 1 页时我们可以采取两个循环处理的方法，即先从 0 循环到 m_ItemsCount - 12 * (index-1))设置要显示的元素，然后再从 m_ItemsCount - 12 * (index-1))循环到 12 设置要隐藏的元素，可是这样的方式我不太喜欢，所以在文章中就没有采取这样的方式。这篇文章是根据一个项目当时的经历写的，因为时间过得比较久，所以如果文章中不当之处希望大家指出并批评。现在这个方案感觉还可以在特效上进行改进，因为现在感觉切换的时候画面比较突兀，这一点请大家注意。好了，下面给出完整的代码和场景布局截图。再次强调：请在看懂文章的基础上“抄”代码，每次看到别人问我把 XXX 脚本挂到场景中不起作用这类的问题，我就觉得整个世界充满了深深地罪恶感。别人愿意分享技术文章，更多的是希望可以和别人交流学习相互促进，如果你只是希望拿到一个随便抄来就能用地代码，抱歉！这违背了我的初衷所以我很难做到！\n/* * 一个基于uGUI的分页功能的实现 * 作者：秦元培 * 时间：2015年11月11日 * 博客：http://qinyuanpei.com */ using UnityEngine; using System.Collections; using System.Collections.Generic; using UnityEngine.UI; public class PaginationPanel : MonoBehaviour { /// \u0026lt;summary\u0026gt; /// 当前页面索引 /// \u0026lt;/summary\u0026gt; private int m_PageIndex = 1; /// \u0026lt;summary\u0026gt; /// 总页数 /// \u0026lt;/summary\u0026gt; private int m_PageCount = 0; /// \u0026lt;summary\u0026gt; /// 元素总个数 /// \u0026lt;/summary\u0026gt; private int m_ItemsCount = 0; /// \u0026lt;summary\u0026gt; /// 元素列表 /// \u0026lt;/summary\u0026gt; private List\u0026lt;GridItem\u0026gt; m_ItemsList; /// \u0026lt;summary\u0026gt; /// 上一页 /// \u0026lt;/summary\u0026gt; private Button m_BtnPrevious; /// \u0026lt;summary\u0026gt; /// 下一页 /// \u0026lt;/summary\u0026gt; private Button m_BtnNext; /// \u0026lt;summary\u0026gt; /// 显示当前页数的标签 /// \u0026lt;/summary\u0026gt; private Text m_PanelText; void Start() { InitGUI(); InitItems(); } /// \u0026lt;summary\u0026gt; /// 初始化GUI /// \u0026lt;/summary\u0026gt; private void InitGUI() { m_BtnNext = GameObject.Find(\u0026#34;Canvas/Panel/BtnNext\u0026#34;).GetComponent\u0026lt;Button\u0026gt;(); m_BtnPrevious = GameObject.Find(\u0026#34;Canvas/Panel/BtnPrevious\u0026#34;).GetComponent\u0026lt;Button\u0026gt;(); m_PanelText = GameObject.Find(\u0026#34;Canvas/Panel/Text\u0026#34;).GetComponent\u0026lt;Text\u0026gt;(); //为上一页和下一页添加事件 m_BtnNext.onClick.AddListener(() =\u0026gt; { Next(); }); m_BtnPrevious.onClick.AddListener(() =\u0026gt; { Previous(); }); } /// \u0026lt;summary\u0026gt; /// 初始化元素 /// \u0026lt;/summary\u0026gt; private void InitItems() { //准备一个存储着12生肖信息的数组 GridItem[] items = new GridItem[] { new GridItem(\u0026#34;鼠\u0026#34;,\u0026#34;Mouse\u0026#34;), new GridItem(\u0026#34;牛\u0026#34;,\u0026#34;Ox\u0026#34;), new GridItem(\u0026#34;虎\u0026#34;,\u0026#34;Tiger\u0026#34;), new GridItem(\u0026#34;兔\u0026#34;,\u0026#34;Rabbit\u0026#34;), new GridItem(\u0026#34;龙\u0026#34;,\u0026#34;Dragon\u0026#34;), new GridItem(\u0026#34;蛇\u0026#34;,\u0026#34;Snake\u0026#34;), new GridItem(\u0026#34;马\u0026#34;,\u0026#34;Horse\u0026#34;), new GridItem(\u0026#34;羊\u0026#34;,\u0026#34;Goat\u0026#34;), new GridItem(\u0026#34;猴\u0026#34;,\u0026#34;Monkey\u0026#34;), new GridItem(\u0026#34;鸡\u0026#34;,\u0026#34;Rooster\u0026#34;), new GridItem(\u0026#34;狗\u0026#34;,\u0026#34;Dog\u0026#34;), new GridItem(\u0026#34;猪\u0026#34;,\u0026#34;Pig\u0026#34;) }; //利用12生肖数组来随机生成列表 m_ItemsList = new List\u0026lt;GridItem\u0026gt;(); for (int i = 0; i \u0026lt; Random.Range(1,1000); i++) { m_ItemsList.Add(items[Random.Range(0,items.Length)]); } //计算元素总个数 m_ItemsCount = m_ItemsList.Count; //计算总页数 m_PageCount = (m_ItemsCount % 12) == 0 ? m_ItemsCount / 12 : (m_ItemsCount / 12) + 1; BindPage(m_PageIndex); //更新界面页数 m_PanelText.text = string.Format(\u0026#34;{0}/{1}\u0026#34;, m_PageIndex.ToString(), m_PageCount.ToString() ); } /// \u0026lt;summary\u0026gt; /// 下一页 /// \u0026lt;/summary\u0026gt; public void Next() { if(m_PageCount \u0026lt;= 0) return; //最后一页禁止向后翻页 if(m_PageIndex \u0026gt;= m_PageCount) return; m_PageIndex += 1; if (m_PageIndex \u0026gt;= m_PageCount) m_PageIndex = m_PageCount; BindPage(m_PageIndex); //更新界面页数 m_PanelText.text = string.Format(\u0026#34;{0}/{1}\u0026#34;, m_PageIndex.ToString(), m_PageCount.ToString()); } /// \u0026lt;summary\u0026gt; /// 上一页 /// \u0026lt;/summary\u0026gt; public void Previous() { if (m_PageCount \u0026lt;= 0) return; //第一页时禁止向前翻页 if (m_PageIndex \u0026lt;= 1) return; m_PageIndex -= 1; if (m_PageIndex \u0026lt; 1) m_PageIndex = 1; BindPage(m_PageIndex); //更新界面页数 m_PanelText.text = string.Format(\u0026#34;{0}/{1}\u0026#34;, m_PageIndex.ToString(), m_PageCount.ToString()); } /// \u0026lt;summary\u0026gt; /// 绑定指定索引处的页面元素 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;index\u0026#34;\u0026gt;页面索引\u0026lt;/param\u0026gt; private void BindPage(int index) { //列表处理 if (m_ItemsList == null || m_ItemsCount \u0026lt;= 0) return; //索引处理 if (index \u0026lt; 0 || index \u0026gt; m_ItemsCount) return; //按照元素个数可以分为1页和1页以上两种情况 if (m_PageCount == 1) { int canDisplay = 0; for (int i = 12; i \u0026gt; 0; i--) { if(canDisplay \u0026lt; 12){ BindGridItem(transform.GetChild(canDisplay), m_ItemsList[12 - i]); transform.GetChild(canDisplay).gameObject.SetActive(true); }else{ //对超过canDispaly的物体实施隐藏 transform.GetChild(canDisplay).gameObject.SetActive(false); } canDisplay += 1; } } else if (m_PageCount \u0026gt; 1){ //1页以上需要特别处理的是最后1页 //和1页时的情况类似判断最后一页剩下的元素数目 //第1页时显然剩下的为12所以不用处理 if (index == m_PageCount){ int canDisplay = 0; for (int i = 12; i \u0026gt; 0; i--) { //最后一页剩下的元素数目为 m_ItemsCount - 12 * (index-1) if (canDisplay \u0026lt; m_ItemsCount - 12 * (index-1)){ BindGridItem(transform.GetChild(canDisplay), m_ItemsList[12 * index-i]); transform.GetChild(canDisplay). gameObject.SetActive(true); } else { //对超过canDispaly的物体实施隐藏 transform.GetChild(canDisplay). gameObject.SetActive(false); } canDisplay += 1; } } else { for (int i = 12; i \u0026gt; 0; i--) { BindGridItem(transform.GetChild(12 - i), m_ItemsList[12 * index - i]); transform.GetChild(12 - i).gameObject.SetActive(true); } } } } /// \u0026lt;summary\u0026gt; /// 加载一个Sprite /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;assetName\u0026#34;\u0026gt;资源名称\u0026lt;/param\u0026gt; private Sprite LoadSprite(string assetName) { Texture texture = (Texture)Resources.Load(assetName); Sprite sprite = Sprite.Create((Texture2D)texture, new Rect(0, 0, texture.width, texture.height), new Vector2(0.5f, 0.5f)); return sprite; } /// \u0026lt;summary\u0026gt; /// 将一个GridItem实例绑定到指定的Transform上 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;trans\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;gridItem\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; private void BindGridItem(Transform trans,GridItem gridItem) { trans.GetComponent\u0026lt;Image\u0026gt;().sprite = LoadSprite(gridItem.ItemSprite); trans.Find(\u0026#34;Item/Name\u0026#34;).GetComponent\u0026lt;Text\u0026gt;().text = gridItem.ItemName; trans.GetComponent\u0026lt;Button\u0026gt;().onClick.AddListener(()=\u0026gt; { Debug.Log(\u0026#34;当前点击的元素名称为:\u0026#34; + gridItem.ItemName); }); } } 好了，今天的内容就是这样啦，欢迎大家继续关注我的博客，谢谢大家！\n2016 年 1 月 10 日更新： 经过博客中一位朋友指出，这篇文章中实现 BindPage 这个方法时可以在代码上再精简些，主要是考虑这个代码中有部分功能是重合的，因此这里对这个方法进行重写，分页从本质上来讲是编写这样一个函数：输入数据集合 data、每页显示的元素个数 pageSize 以及当前页数 page，然后返回一个新的数据集合。为了考虑扩展性我们这里编写一个分页的泛型方法，代码实现如下：\nList\u0026lt;T\u0026gt; Pagination(List\u0026lt;T\u0026gt; data,int size,int page) { //要返回的结果 List\u0026lt;T\u0026gt; output = new List\u0026lt;T\u0026gt;(); //计算最大页数 int PageCount = (data.Count % size) == 0 ? (data.Count / size) : (data.Count / size) + 1; //判断输入页数的合法性 if(page \u0026lt; 1 || page \u0026gt; PageCount) return null; //计算第page页第一个元素的索引 int startIndex = (page - 1) * size; //除了尾页所有的页面中元素个数都是size个 if(page \u0026lt; PageCount) { for (int i = 0; i \u0026lt; size; i++) { output.Add(data[startIndex + i]); } } else { for (int i = startIndex; i \u0026lt; data.Count; i++) { output.Add(data[i]); } } return output; } 这里我们只需要考虑传入的页数是不是尾页即可，因为在所有的页面中除了尾页以外都有 size 个元素，所以我们只需要计算出第 page 页的第一个元素的索引，然后以此递增即可，而尾页显然是从 startIndex 到 data.Count-1 个。现在回过头来看，写个分页函数确实是简单至极，这篇博客显然是小题大做了。\n","date":"2015-11-10T20:46:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/166983157/","slug":"166983157","tags":["游戏开发","uGUI","Unity3D"],"title":"Unity3D 游戏开发之分页效果在 uGUI 中的实现"},{"categories":["Unity3D"],"content":"各位朋友大家好，欢迎大家关注我的博客，我是秦元培，我的博客地址是：http://blog.yuanpei.me。通过本系列第一篇文章，我们初步了解了 EasyAR 这个增强现实引擎，这次我们来尝试自己定义一个 Marker，这样我们就可以用自己喜欢的图片来作为 Marker。因为目前 EasyAR 文档并不完善，所以下面的这些内容可能更多的是我个人的尝试和探索。如果大家对此感兴趣的话继续往下看否则就不要往下看了，因为我担心在官方正式文档出来以后大家可能会骂我啊。好了，对这个话题感兴趣的朋友就请继续往下看吧！\nEasyAR 的基本流程 首先我们来看看官方给出的一张 EasyAR 的基本流程示意图：\nEasyAR基本流程示意图\r在这张流程图，当中作为开发者的我们此刻需要关注的 Target 这一条线和 Frame 这条线。前者对应的是如何将普通的图片，例如 .jpg、.png 等配合 JSON 文件转化为系统可以识别的 Target，后者对应的是我们在识别到 Target 后要去处理哪些逻辑。在官方文档中我们可以找到这样一段话：\n创建相机设备、图像追踪器和增强对象（Create CameraDevice and ImageTracker and Augmenter objects）. 打开相机设备（Open CameraDevice）. 给相机设备附加图像追踪器（Attach ImageTracker to CameraDevice）. 开始执行相机设备和图形追踪器的相关逻辑（Start CameraDevice and ImageTracker）. 获得从图像追踪器增强后的帧画面（New frame using Augmenter from ImageTracker）. 绘制视频和其它的内容（Draw video background and other stuffs）. 这段话基本上就是 EasyAR 流程示意图的全面解读了，所以我们学习 EasyAR 可以从这个基本流程来入手，了解这个流程能帮助我们更快地理解 API 接口，虽然现在官方的 API 文档依然处在预览状态下，想到这里简直各种不开心啊！\n创建自定义 Marker 在了解了 EasyAR 的基本流程后，我们就来说说如何在 EasyAR 中创建自定义 Marker 吧！相信使用过 Vuforia 的人都知道要创建一个自定义的 Marker 需要到开发者后台去创建然后下载资源包，这种方式虽然高效、直接，可是因为没有人为地干预过程，所以我们对 AR 引擎内部究竟做了怎样的处理基本上是一无所知的，换句话说我们大部分的工作都是在做黑箱测试。到了 EasyAR 这里，一切就变得特别简单，这一点要给 EasyAR 点个赞。首先在 EasyAR 中配置 Marker 是通过 StreamingAssets 目录下的 dataset.json 这个文件来实现的：\n{ \u0026#34;images\u0026#34; : [ { \u0026#34;image\u0026#34; : \u0026#34;mousepad.jpg\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;mousepad\u0026#34; }, { \u0026#34;image\u0026#34; : \u0026#34;idback.jpg\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;idback\u0026#34;, \u0026#34;size\u0026#34; : [8.56, 5.4], \u0026#34;uid\u0026#34; : \u0026#34;todo=uid-string\u0026#34; } ] } 从这个文件中我们可以发现每一张图片都具有某些不同的属性，从目前博主掌握的资料来看，每张图片最重要的两个属性是 image 和 name。其中 image 是指图片的相对路径，该路径相对于 StreamingAssets 目录，因为我们做 Unity3D 游戏开发的时候都知道这个目录下的资源在编译的时候不会被压缩，当导出 APK 安装包的时候它会被完整的保留到根目录下的 assets 目录中。同样地，name 是指图片的名称即 ID，EasyAR 正是通过这个 ID 来和图片资源关联起来的。比如在默认的 SDK 项目中身份证背面这张图片是和 idback 这个 ID 对应的，如图所示，在这里 Easy 提供了四种存储方案即 Assets、App、Absolute、Json。和官方的人交流的时候说可以支持路径和 Json 字符串两种形式，但是对更加具体的这四种存储方案上的区别和优缺点目前并没有一个确切的说法，所以在这里我们就继续沿用 Assets 这种存储方案吧！我们可以注意到 idback 这张图片和 mousepad 这张图片相比增加了两个属性，即 uid 和 size。size 目前基本可以了解为 Unity3D 中的缩放，因为这个值表示的是在物理空间里的范围大小，单位是米，而我们知道 Unity3D 里默认的单位就是米，所以这个数值可以暂时理解为 Unity3D 里的缩放，它对应到下图里的 Size，我已经用红色字体标示出来。对于 uid 这个属性嘛，既然配置文件里都有 todo 标识出来了，那么我们就姑且认为这是一个暂时没有启用的属性值吧！\n配置文件和ImagTarget的对应关系\r好了，下面我们来具体看看如何创建一个自定义 Marker：\n首先我们在 StreamingAssets 目录中添加一张图片 ziying.jpg，然后在 dataset.json 文件中增加该图片的信息。此时 ziying.jpg 的位置是在 StreamingAssets 根目录下。如果我们希望把它放在一个自定义的文件夹中，如 StreamingAssets/ziying 目录下，则需要将 ziying 的 image 属性值改为 ziying/ziying.jpg，以此类推。 { \u0026#34;images\u0026#34; : [ { \u0026#34;image\u0026#34; : \u0026#34;mousepad.jpg\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;mousepad\u0026#34; }, { \u0026#34;image\u0026#34; : \u0026#34;ziying.jpg\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;ziying\u0026#34; }, { \u0026#34;image\u0026#34; : \u0026#34;idback.jpg\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;idback\u0026#34;, \u0026#34;size\u0026#34; : [8.56, 5.4], \u0026#34;uid\u0026#34; : \u0026#34;todo=uid-string\u0026#34; } ] } 在 Materials 目录下新建一个材质，然后找到 ziying.jpg 将其作为该材质的纹理贴图。 在场景中找到 ImageTargetDataSet-idback 节点，修改其附加的 SimpleImageTargetBehaviour 脚本下的 Name 属性，将其修改为 ziying，同时将第二步创建的材质赋给 ImageTargetDataSet-idback 节点。此时场景效果如图所示，这意味着我们使用手机摄像头来扫描这张图片就可以看到场景中的这个模型啦！ 自定义Markder效果\r好了，现在编译这个项目并部署到手机上可以得到我们期望的结果，哈哈，慕容紫英站在桌面上和我一起玩对一个仙剑迷来说是不是特别有趣呢？ 站在手机上的慕容紫英\r总结 到目前为止，EasyAR 官方还没有给出一个完整的 API 文档，所以我们目前能做的研究依然十分有限，在本文中涉及到的部分没有解决的问题，博主会在官方给出文档后第一时间给予解决，希望大家继续关注我的博客！我们现在使用的都是 SDK 中现成的脚本，如果我们希望自己来设计脚本来满足自己的要求实现某些定制的功能或者是想用原生代码来减少 Unity3D 这类游戏引擎带来的性能上的损耗以及实现播放视频的功能等等。这些内容博主在稍后会陆续写出来，好了，今天的内容就是这个样子啦！希望大家喜欢。\n","date":"2015-11-03T10:23:14Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1156673678/","slug":"1156673678","tags":["增强现实","AR","Unity3D","教程"],"title":"EasyAR 尝鲜系列教程之自定义 Marker 的实现"},{"categories":["Unity3D"],"content":"各位朋友，大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。从今天起博主将为大家带来EasyAR尝鲜系列教程，本教程适用的对象是增强现实应用开发者和Unity3D游戏开发者，在阅读本教程前请确保具备增强现实应用开发及Unity3D游戏开发的相关基础知识。在本节及后续内容中，博主将以国产增强现实引擎EasyAR为主要开发平台来带领大家一起走进增强现实应用开发的世界，希望大家能够喜欢！\n什么是增强现实？ 为了让更多的人了解增强现实，所以在开始本文教程前，我们首先来了解下什么是增强现实。增强现实(Augmented Reality，简称 AR)，它是一种将真实世界信息和虚拟世界信息进行融合和集成的新技术，这种技术的目标是在屏幕上把虚拟世界和现实世界进行叠加并在此基础上进行互动。增强现实是真实世界和虚拟世界的信息集成，具有实时交互性，是在三维尺度空间中增添定位虚拟物体。增强现实技术可广泛应用到军事、医疗、建筑、教育、工程、影视、娱乐等领域。增强现实是新型的人机交互和三维仿真工具，目前已发挥出了重要的作用，具有巨大的应用潜力。\n增强现实概念图\r增强现实应用现状 目前，增强现实在国内尚处在起步阶段。2012年4月Google发布的Google Class是全球唯一一款真正意义上实现增强现实技术的硬件设备。随着移动设备的普及和相关技术的成熟，增强现实开始逐渐地走进人们的生活。如国内首款聚合了目前移动互联最新增强现实技术的智能手机应用《城市镜头》以及中视典数字科技研发的VRP系统等。AR技术在人工智能、CAD、图形仿真、虚拟通讯、遥感、娱乐、模拟训练等许多领域带来了革命性的变化。 目前增强现实相关技术主要有开源社区的ARToolkit、面向商业化解决方案的Metaio和Vuforia等。\n国产增强现实引擎EasyAR EasyAR(Easy Augmented Reality)是视辰信息科技（上海）有限公司的增强现实解决方案系列的子品牌，其含义是希望让增强现实变得简单易实施。EasyAR提供了诸如手机APP互动营销、户外大屏幕互动活动、网络营销互动等形式在内的增强现实互动营销技术和解决方案。著作权归作者所有。EasyAR无需授权、无水印、无识别次数的限制，开放后可免费下载，无需任何费用，是一款完全免费的AR引擎。EasyAR具有强大的跨平台特性可支持Windows、 Mac OS、 Android和iOS等主流平台。从目前的情况来看，EasyAR的SDK是目前市场上同类产品中最为简单易用的，唯一的不足是产品刚发布不久尚未能提供完整的技术文档。\nHello EasyAR 好了，下面我们以EasyAR提供的Unity3D版本SDK为例来学习EasySDK的使用。在开始前请确保你的计算机上正确安装了以下开发工具或者硬件：\nUnity3D(必选)：主要的开发环境 JDK相关工具(必选)：编译Android应用所需环境 Android SDK(必选)：编译Android应用所需环境 摄像头(可选)：如使用手机进行调试则不需要 在完成以上准备工作后：\n打开EasyAR官网并登录官网，我们将在登陆后创建应用以获得开发所需的密钥以及SDK。如果尚未注册可以在注册后完成这一步骤。 创建应用\r点击创建应用，并在这里填入应用的名称和包的名称，此处以“EasyAR测试”和“com.easyar.first”为例，在创建完应用后可以在应用列表中找到当前创建的应用，点击显示可以查看当前应用对应的密钥。 点击“下载EasyAR SDK v1.0.1”完成SDK的下载。 下载SDK\r解压下载的SDK压缩包，找到vc_redist目录安装对应平台的VC++运行库。请注意，即使在你的计算机上安装了VC++运行库，这里依然需要安装。Win8及Win8.1请先使用磁盘清理工具清理系统垃圾，否则可能会出现无法安装的问题。建议使用64位操作系统且安装x86和x64的VC++运行库。 找到SDK压缩包内的package/unity目录下的EasyAR.unitypackage文件并将其导入到Unity3D中。 在Unity3D中找到Scenes目录下的easyar场景并打开该场景，然后找到EasyAR节点名称，在右侧属性窗口中填入应用对应的密钥。 填入应用程序密钥\r打开BuildSetting-\u0026gt;PlayerSetting在右侧属性窗口中填入应用对应的包名。 填入应用程序包名\rSDK默认提供了三张识别图片，我们选择每个人都有的身份证照片作为识别目标，在场景中找到ImageTargetDataSet-idback这个物体，找到它的子节点Cube。这意味着如果我们识别到了身份证照片，那么就会在身份证照片上显示一个Cube。如果大家手头上有自己喜欢的模型，可以将Cube隐藏，然后将模型添加进来，并为其添加VideoPlayerBehaviour.cs脚本。如手头上没有模型，这一步可以忽略。如图是我现在的场景效果： 加入自定义模型后的效果\r好了，现在编译程序，将其导出为APK安装包，这样我们就可以在手机上测试EasyAR的效果啦。假如一切顺利的话，在手机上将会看到这样的画面。下面放点运行情况截图供大家参考：\n截图1\r截图2\r问题汇总 作为一款国产的增强现实引擎，目前EasyAR的表现我还是比较满意的，虽然在识别的准确度上无法和国外的同类产品相比，但是它的简单易用确实是做得不错。作为一个程序员尝鲜更像是吃螃蟹，目前发现的问题及解决方案有：\n编辑器提示DllNotFoundException错误，请安装SDK中对应的VC++运行库。 视频导入失败，Unity3D导入视频需要依赖苹果公司的QuickTime播放器，所以请安装最新版的QuickTime后重试。 在64位计算机上编译的Android应用可以正常运行，在32位计算机上编译的Android应用无法正常运行。具体表现如图 32位计算机下的问题\r好了，作为整个系列的第一篇文章，我们至此对EasyAR有了一个较为直观的印象。在接下来的内容中，我们将对SDK中的内容进行更加深入的了解，因此希望大家继续关注我的博客，谢谢大家！\n","date":"2015-10-30T09:44:18Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3120185261/","slug":"3120185261","tags":["增强现实","AR","Unity3D","教程"],"title":"EasyAR尝鲜系列教程之Hello EasyAR"},{"categories":["Unity3D"],"content":"各位朋友，大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。最近因为项目需要决定尝试自己来实现一个虚拟摇杆，所以在今天的文章中我们的目标是使用 uGUI 来制作一个可以在移动平台稳定运行的虚拟摇杆(请不要问我为什么不使用 NGUI 来实现，你说我做个虚拟摇杆有必要在项目里导入那么多的资源嘛 23333)。关于使用第三方插件来实现虚拟摇杆，请大家参照我以前写的文章Unity3D 游戏开发之使用 EasyTouch 虚拟摇杆控制人物移动，在这里就不再赘述了。\n虚拟摇杆这种输入方式相信大家在手机游戏平台上已经相当的熟悉了，首先我们来简单了解下虚拟摇杆的设计原理。虚拟摇杆有一张固定的 2D 贴图(背景层)和一张可拖动的 2D 贴图(控制层)构成，默认情况下控制层在背景层的中心，我们称这个位置为初始位置，当移动控制层后移动层的位置会发生变化，此时控制层的当前位置和初始位置两点间可以计算出一个 2D 向量，通过这个向量我们就可以判断虚拟摇杆的移动方向。在经典的八方向摇杆导航中摇杆中可移动方向被分成了上、左上、右上、下、左下、右下、左、右共 8 个方向。我们知道根据三角函数可以非常容易地计算出这个 2D 向量的角度并由此判定摇杆是在向着这 8 个方向中的哪一个方向移动。在今天的文章中，我们不需要考虑这 8 个方向，因为我们可以向任何一个方向进行移动。\n好了，首先在场景中创建两个 Image 组件和一个空的游戏体，然后将这两个 Image 组件拖拽到这个空的游戏体下使它们称为其子节点。这里需要注意的是这两个 Image 的层级关系。现在我们来编写脚本，这个脚本将被添加到控制层物体上：\n/* * uGUI虚拟摇杆 * 作者：秦元培 * 博客：http://qinyuanpei.com * 时间：2015年10月24日 */ using UnityEngine; using System.Collections; using UnityEngine.UI; using UnityEngine.EventSystems; public class JoyStick : MonoBehaviour,IPointerDownHandler, IPointerUpHandler, IDragHandler { /// \u0026lt;summary\u0026gt; /// 摇杆最大半径 /// 以像素为单位 /// \u0026lt;/summary\u0026gt; public float JoyStickRadius = 50; /// \u0026lt;summary\u0026gt; /// 摇杆重置所诉 /// \u0026lt;/summary\u0026gt; public float JoyStickResetSpeed = 5.0f; /// \u0026lt;summary\u0026gt; /// 当前物体的Transform组件 /// \u0026lt;/summary\u0026gt; private RectTransform selfTransform; /// \u0026lt;summary\u0026gt; /// 是否触摸了虚拟摇杆 /// \u0026lt;/summary\u0026gt; private bool isTouched = false; /// \u0026lt;summary\u0026gt; /// 虚拟摇杆的默认位置 /// \u0026lt;/summary\u0026gt; private Vector2 originPosition; /// \u0026lt;summary\u0026gt; /// 虚拟摇杆的移动方向 /// \u0026lt;/summary\u0026gt; private Vector2 touchedAxis; public Vector2 TouchedAxis { get { if(touchedAxis.magnitude \u0026lt; JoyStickRadius) return touchedAxis.normalized / JoyStickRadius; return touchedAxis.normalized; } } /// \u0026lt;summary\u0026gt; /// 定义触摸开始事件委托 /// \u0026lt;/summary\u0026gt; public delegate void JoyStickTouchBegin(Vector2 vec); /// \u0026lt;summary\u0026gt; /// 定义触摸过程事件委托 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;vec\u0026#34;\u0026gt;虚拟摇杆的移动方向\u0026lt;/param\u0026gt; public delegate void JoyStickTouchMove(Vector2 vec); /// \u0026lt;summary\u0026gt; /// 定义触摸结束事件委托 /// \u0026lt;/summary\u0026gt; public delegate void JoyStickTouchEnd(); /// \u0026lt;summary\u0026gt; /// 注册触摸开始事件 /// \u0026lt;/summary\u0026gt; public event JoyStickTouchBegin OnJoyStickTouchBegin; /// \u0026lt;summary\u0026gt; /// 注册触摸过程事件 /// \u0026lt;/summary\u0026gt; public event JoyStickTouchMove OnJoyStickTouchMove; /// \u0026lt;summary\u0026gt; /// 注册触摸结束事件 /// \u0026lt;/summary\u0026gt; public event JoyStickTouchEnd OnJoyStickTouchEnd; void Start () { //初始化虚拟摇杆的默认方向 selfTransform = this.GetComponent\u0026lt;RectTransform\u0026gt;(); originPosition = selfTransform.anchoredPosition; } public void OnPointerDown(PointerEventData eventData) { isTouched = true; touchedAxis = GetJoyStickAxis(eventData); if(this.OnJoyStickTouchBegin != null) this.OnJoyStickTouchBegin(TouchedAxis); } public void OnPointerUp(PointerEventData eventData) { isTouched = false; selfTransform.anchoredPosition = originPosition; touchedAxis = Vector2.zero; if(this.OnJoyStickTouchEnd != null) this.OnJoyStickTouchEnd(); } public void OnDrag(PointerEventData eventData) { touchedAxis = GetJoyStickAxis(eventData); if(this.OnJoyStickTouchMove != null) this.OnJoyStickTouchMove(TouchedAxis); } void Update() { //当虚拟摇杆移动到最大半径时摇杆无法拖动 //为了确保被控制物体可以继续移动 //在这里手动触发OnJoyStickTouchMove事件 if(isTouched \u0026amp;\u0026amp; touchedAxis.magnitude\u0026gt;=JoyStickRadius) { if(this.OnJoyStickTouchMove != null) this.OnJoyStickTouchMove(TouchedAxis); } //松开虚拟摇杆后让虚拟摇杆回到默认位置 if(selfTransform.anchoredPosition.magnitude \u0026gt; originPosition.magnitude) selfTransform.anchoredPosition -= TouchedAxis * Time.deltaTime * JoyStickResetSpeed; } /// \u0026lt;summary\u0026gt; /// 返回虚拟摇杆的偏移量 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The joy stick axis.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;eventData\u0026#34;\u0026gt;Event data.\u0026lt;/param\u0026gt; private Vector2 GetJoyStickAxis(PointerEventData eventData) { //获取手指位置的世界坐标 Vector3 worldPosition; if (RectTransformUtility.ScreenPointToWorldPointInRectangle (selfTransform, eventData.position, eventData.pressEventCamera, out worldPosition)) selfTransform.position = worldPosition; //获取摇杆的偏移量 Vector2 touchAxis = selfTransform.anchoredPosition-originPosition; //摇杆偏移量限制 if(touchAxis.magnitude \u0026gt;= JoyStickRadius) { touchAxis = touchAxis.normalized * JoyStickRadius; selfTransform.anchoredPosition = touchAxis; } return touchAxis; } } 在这段脚本中，我们实现了 OnPointerDown、OnPointerUp 和 OnDrag 三个 uGUI 事件接口，然后注册了相关的事件委托，这里借鉴了 EasyTouch 的设计，可以使得虚拟摇杆的逻辑和角色控制逻辑相互分离。这里的核心方法是 GetJoyStickAxis()方法，通过这个方法我们可以获得一个 Vector2 类型的值，它表示的是未标准化过的虚拟摇杆的偏移量。这里的 RectTransformUtility.ScreenPointToWorldPointInRectangle()方法表示将一个屏幕坐标转化为对应 RectTransform 的世界坐标，RectTransform 的 anchoredPosition 属性表示的是当前元素在场景中的屏幕坐标。我们知道屏幕坐标是以像素为单位的，因此这里使用屏幕坐标可以计算出虚拟摇杆在水平方向和垂直方向上移动了多少个像素，我们以此来作为虚拟摇杆的偏移量衡量指标。TouchedAxis 是经过标准化以后的偏移量，我们将把这个值传递到事件委托中以提供给外部来调用。好了，要说的就这些了，没有说到的大家可以看看代码里的注释或者是在博客中给我留言，就是这样啦。\n接下来，我们在场景中添加一个角色模型来测试我们编写的虚拟摇杆，因为在 JoyStick 中我们已经定义了事件委托，所以在这里就是简单的调用啦。好了，我们一起来看看代码吧！\n/* * Joystick3D.cs * 3D模式下的虚拟摇杆测试 * 作者：秦元培 * 博客：http://qinyuanpei.com * 时间：2015年10月30日 */ using UnityEngine; using System.Collections; public class JoyStick3D : MonoBehaviour { private JoyStick js; void Start () { js = GameObject.FindObjectOfType\u0026lt;JoyStick\u0026gt; (); js.OnJoyStickTouchBegin += OnJoyStickBegin; js.OnJoyStickTouchMove += OnJoyStickMove; js.OnJoyStickTouchEnd += OnJoyStickEnd; } void OnJoyStickBegin(Vector2 vec) { Debug.Log(\u0026#34;开始触摸虚拟摇杆\u0026#34;); } void OnJoyStickMove (Vector2 vec) { Debug.Log(\u0026#34;正在移动虚拟摇杆\u0026#34;); //设置角色朝向 Quaternion q = Quaternion.LookRotation (new Vector3 (vec.x, 0, vec.y)); transform.rotation = q; //移动角色并播放奔跑动画 transform.Translate(Vector3.forward * 75f * Time.deltaTime); animation.CrossFade(\u0026#34;Run\u0026#34;); } void OnJoyStickEnd () { Debug.Log(\u0026#34;触摸移动摇杆结束\u0026#34;); //播放默认待机动画 animation.CrossFade(\u0026#34;idle\u0026#34;); } void OnGUI() { GUI.Label(new Rect(30,30,200,30),\u0026#34;3D模式下的虚拟摇杆测试\u0026#34;); } } 最终程序的运行效果如下图所示，我们编写的这个虚拟摇杆可以在手机上完美的运行，欢饮大家来一起测试和吐槽！\n2D模式演示\r3D模式演示\r好了，今天的内容就是这样啦！欢迎大家继续关注我的博客，希望大家喜欢！\n","date":"2015-10-30T09:44:18Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/331752533/","slug":"331752533","tags":["游戏开发","虚拟摇杆","uGUI"],"title":"教你一步步实现一个虚拟摇杆"},{"categories":["游戏开发"],"content":"今天博主想和大家分享的是 Unity3D 场景编辑器的扩展开发，相关的话题我们在Unity3D 游戏开发之编辑器扩展程序开发实例这篇文章中我们已经有所涉及，今天博主想特别针对场景编辑器的扩展开发来进行下深入研究。对于一个场景编辑器来说，它主要的作用是 3D 场景视图中实时显示、输入反馈和相关信息的更新。在 Unity3D 中提供了 Editor、EditorWindow、GUILayout、EditorGUILayout、GUIUtility、EditorGUIUtility、Handles、Event 等来完成这些工作。其中基于 EditorWindow 的这种扩展方式我们已经研究过了，这种扩展方式拥有自己的独立窗口使用 OnGUI 方法进行界面的绘制。\n今天我们想说的是基于 Editor 的这种扩展方式，这种扩展方式只能针对脚本，从脚本内容在 Inspector 里的显示布局到变量在 Scene 视图的可视化编辑，它都可以完全胜任。这里特别想说的是 Handles 和 Event 这两个类，这两个类分别提供了 3D 显示和输入反馈的功能，我们下面就来学习如何使用这些类来扩展 Unity3D 的场景编辑器。\n创建一个扩展的 Transform 组件 Transform 是 Unity3D 中一个基本的组件，下面我们来创建一个扩展的 Transform 组件，该组件可以对游戏体的坐标、旋转、缩放进行重置。首先，我们创建一个 ExtendTransform 的类，该类继承自 Editor 类：\nusing UnityEngine; using System.Collections; using UnityEditor; [CustomEditor(typeof(Transform),true)] public class ExtendTransform : Editor { /// \u0026lt;summary\u0026gt; /// Position属性 /// \u0026lt;/summary\u0026gt; private SerializedProperty mPos; /// \u0026lt;summary\u0026gt; /// Scale属性 /// \u0026lt;/summary\u0026gt; private SerializedProperty mScale; void OnEnable() { mPos = serializedObject.FindProperty(\u0026#34;m_LocalPosition\u0026#34;); mScale = serializedObject.FindProperty(\u0026#34;m_LocalScale\u0026#34;) ; } /// \u0026lt;summary\u0026gt; /// Inspector相关GUI函数 /// \u0026lt;/summary\u0026gt; public override void OnInspectorGUI() { EditorGUIUtility.labelWidth = 15; //获取最新的可序列化对象 serializedObject.Update(); //绘制物体的坐标、旋转和缩放 DrawPosition(); DrawRotate(); DrawScale(); //更新可序列化对象的属性 serializedObject.ApplyModifiedProperties(); } /// \u0026lt;summary\u0026gt; /// 绘制位置 /// \u0026lt;/summary\u0026gt; private void DrawPosition() { GUILayout.BeginHorizontal(); { bool Reset = GUILayout.Button(\u0026#34;P\u0026#34;, GUILayout.Width(20f)); EditorGUILayout.LabelField(\u0026#34;Position\u0026#34;); EditorGUILayout.PropertyField(mPos.FindPropertyRelative(\u0026#34;x\u0026#34;)); EditorGUILayout.PropertyField(mPos.FindPropertyRelative(\u0026#34;y\u0026#34;)); EditorGUILayout.PropertyField(mPos.FindPropertyRelative(\u0026#34;z\u0026#34;)); if(Reset) mPos.vector3Value = Vector3.zero; } GUILayout.EndHorizontal(); } /// \u0026lt;summary\u0026gt; /// 绘制旋转 /// \u0026lt;/summary\u0026gt; private void DrawRotate() { Vector3 eulerAngles = ((Transform)target).eulerAngles; GUILayout.BeginHorizontal(); { bool Reset = GUILayout.Button(\u0026#34;R\u0026#34;, GUILayout.Width(20f)); EditorGUILayout.LabelField(\u0026#34;Rotation\u0026#34;, GUILayout.Width(70f)); EditorGUILayout.LabelField(\u0026#34;X\u0026#34;, GUILayout.Width(13f)); float angleX=EditorGUILayout.FloatField(eulerAngles.x, GUILayout.Width(56f)); EditorGUILayout.LabelField(\u0026#34;Y\u0026#34;, GUILayout.Width(13f)); float angleY = EditorGUILayout.FloatField(eulerAngles.y, GUILayout.Width(56f)); EditorGUILayout.LabelField(\u0026#34;Z\u0026#34;, GUILayout.Width(13f)); float angleZ = EditorGUILayout.FloatField(eulerAngles.z, GUILayout.Width(56f)); ((Transform)target).eulerAngles = new Vector3(angleX, angleY, angleZ); if(Reset) { eulerAngles = Vector3.zero; ((Transform)target).eulerAngles = Vector3.zero; } } GUILayout.EndHorizontal(); } /// \u0026lt;summary\u0026gt; /// 绘制缩放 /// \u0026lt;/summary\u0026gt; private void DrawScale() { GUILayout.BeginHorizontal(); { bool Reset = GUILayout.Button(\u0026#34;S\u0026#34;, GUILayout.Width(20f)); EditorGUILayout.LabelField(\u0026#34;Scale\u0026#34;); EditorGUILayout.PropertyField(mScale.FindPropertyRelative(\u0026#34;x\u0026#34;)); EditorGUILayout.PropertyField(mScale.FindPropertyRelative(\u0026#34;y\u0026#34;)); EditorGUILayout.PropertyField(mScale.FindPropertyRelative(\u0026#34;z\u0026#34;)); if (Reset) mScale.vector3Value = Vector3.one; } GUILayout.EndHorizontal(); } } 首先我们注意到 ExtendTransform 继承自 Editor，这是我们开发这类编辑器扩展的第一个前提。其次我们注意到在该类的声明位置有这样一个标记:\n[CustomEditor(typeof(Transform),true)] 该标记表明我们这个编辑器扩展是针对 Transform 组件进行扩展的，即当物体存在 Tranform 组件时会在编辑器中响应这个编辑器扩展程序。我们在这个编辑器扩展程序中都做了哪些事情呢？第一，我们实现了 OnEnable()方法，该方法相当于一个初始化的方法；第二，我们重写了 OnOnInspectorGUI()方法，该方法将覆盖默认的 Inspector 窗口外观。\n扩展后的Transform\r好了，现在我们点击场景中默认的相机 MainCamera 可以发现默认的 Transform 会变成具有重置功能的扩展型 Transform。下面我们来介绍这段程序中较为重要的核心内容：\nUnity3D 中的可序列化对象 通常我们所说的序列化是指将一个对象的实例转化为字符串的过程，而在 Unity3D 中可序列化对象更像是一种智能对象，它可以将脚本中的属性显示在 Inspector 窗口中，当场景发生变化时这些属性值将自动被更新。例如我们可以定义这样一个简单的脚本：\n/// \u0026lt;summary\u0026gt; /// 定义一个可序列化类 /// \u0026lt;/summary\u0026gt; [System.Serializable] public class ExampleClass { [SerializeField] public int ID; [SerializeField] public string Name; [SerializeField] public Vector3[] Points; private bool editable = false; } /// \u0026lt;summary\u0026gt; /// 定义一个简单的脚本 /// \u0026lt;/summary\u0026gt; public class ExampleScript : MonoBehaviour { public ExampleClass Example; } 此时如果我们给场景中的某个物体附加上该脚本，则我们在 Inspector 窗口可以看到 Example 类的实例 Example 将被序列化到编辑器面板中，同时我们可以注意到私有的 editable 字段并没有被序列化出来，这是因为在 Unity3D 中，公有的字段默认支持序列化，私有的字段除非显式的增加[SerializeField]标记，否则都不会被序列化，这一点希望大家注意。好了，那么我们为什么要讲这部分内容呢，这是因为它和我们下面要讲的Editor 基类中的属性和方法有着十分密切的关联。\nUnity3D中的可序列化对象\rEditor 基类中的属性和方法 Editor 基类中有两个重要的属性，即 target 和 serializedObject。target 表示当前受检查的物体我们可以通过它获得当前物体；而 serializedObject 表示当前物体的全部可序列化信息，我们可以通过它获得指定的序列化字段及其数值。Editor 基类中重要的方法有：\nOnInspectorGUI():该方法可对 Inspector 窗口面板进行扩展或者重写，比如我们可以通过 DrawDefaultInspector()方法来绘制默认 Inspector 窗口面板然后在此基础上使用 GUILayout 或者 EditorGUILayout 等辅助类进行自定义的绘制。在这个示例中我们对整个面板进行了重写，值得注意的是为了让 Inspector 窗口面板正常工作，如果要重绘该窗口请确保对该方法进行覆盖。 OnSceneGUI():该方法可对场景视图进行绘制，在实际的使用中可以配合 Handles 类和 Event 类来进行网格编辑、地形绘制或高级 Gizmos 等方面的工作。在本文的第二个示例中，我们将利用这一特性来编写一个用于 NPC 寻路的路径节点编辑工具。 对第一个示例的总结 在第一个示例中，可以注意到我们使用了 FindProperty()方法来获取一个可序列化物体的属性(字段)，然后我们在 EditorGUILayout.PropertyField()方法来绘制了各种属性框，这种方式可以实现属性的自动更新。注意到 DrawRotate()方法与 DrawPositin()及 DrawScale()方法在实现方式上略有不同，这是因为 Transform 组件的 Rotation 属性是一个 Quaternion 即四元数的结构，四元数是利用 x、y、z、w 四个数值来表示物体的三维旋转，这不仅和我们平时习惯的欧拉角相违背而且更为关键的是貌似目前我还没有发现可以直接绘制四元数的 API 接口，如果有的话希望大家可以告诉我，所以这里我们用了变通的一种方法，即通过 Transform 的 eulerAngles 来实现，但是这种方式绘制的属性框大小和 EditorGUILayout.PropertyField()方法绘制的属性框大小并不一致，同时我们需要自己去完成属性值的更新。好了，暂时先总结到这里更多的细节大家可以通过代码来了解。\n创建一个 NPC 寻路节点编辑工具 创建这样一个工具的想法来自我实际的工作体验，当我 Unity3D 中使用的 Tween 动画库从 iTween 变成 Dotween 后，我在使用 Dotween 的过程中一直没有找到类似于 iTweenPath 的路径节点编辑工具。作为一个有节操的程序员，去寻找破解版的 Dotween Pro 这样的事情我是能不干就不干啦，因为我觉得自己有能力做这样一个类似的小工具，所以在一边准备这篇文章的时候，一边开始设计这样一个路径节点编辑工具。相信经过第一个示例的铺垫和相关知识的储备，大家都了解了这些内容，所以这里直接给出代码啦，因为实在是没有多少内容，嘿嘿：\nusing UnityEngine; using System.Collections; using UnityEditor; [CustomEditor(typeof(PatrolNPC))] public class PatrolPathEditor : Editor { /// \u0026lt;summary\u0026gt; /// 寻路节点 /// \u0026lt;/summary\u0026gt; private Vector3[] paths; /// \u0026lt;summary\u0026gt; /// 显示寻路信息的GUI /// \u0026lt;/summary\u0026gt; private GUIStyle style=new GUIStyle(); /// \u0026lt;summary\u0026gt; /// 初始化 /// \u0026lt;/summary\u0026gt; void OnEnable() { //获取当前NPC的寻路路径 paths = ((PatrolNPC)target).Paths; //初始化GUIStyle style.fontStyle = FontStyle.Normal; style.fontSize = 15; } void OnSceneGUI() { //获取当前NPC的寻路路径 paths = ((PatrolNPC)serializedObject.targetObject).Paths; //设置节点的颜色为红色 Handles.color = Color.red; if(paths.Length \u0026lt;= 0 || paths.Length\u0026lt;2) return; //在场景中绘制每一个寻路节点 //可以在场景中编辑节点并将更新至对应的NPC for (int i = 0; i \u0026lt; paths.Length; i++) { paths[i] = Handles.PositionHandle(paths[i], Quaternion.identity); Handles.SphereCap(i, paths[i], Quaternion.identity, 0.25f); Handles.Label(paths[i], \u0026#34;PathPoint\u0026#34; + i, style); if (i \u0026lt; paths.Length \u0026amp;\u0026amp; i + 1 \u0026lt; paths.Length) { Handles.DrawLine(paths[i], paths[i + 1]); } } } } 这里的 PatrolNPC 是一个可寻路 NPC 类，基本和这篇文章的内容无关，大家只要知道那个 Paths 字段是一个 Vector3[]就好啦，这样当我们在场景中编辑这些路径节点的时候，对应 NPC 的路径节点信息就会同步发生更新，这样我们就可以随心所欲地规划 NPC 的移动路径啦，哈哈。好了，今天的内容就是这样啦，写完熬到这个点真心不容易啊，大家晚安，这是这个小工具在场景编辑器中的效果，嘻嘻，感觉还是蛮不错的吧，反正我是很喜欢就对啦！\n路径节点编辑工具演示\r","date":"2015-10-13T12:59:01Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3019914405/","slug":"3019914405","tags":["编辑器","扩展","Unity3D"],"title":"Unity3D 游戏开发之 Unity3D 场景编辑器扩展开发"},{"categories":["Unity3D"],"content":"各位朋友大家好，我是秦元培，欢迎大家关注我的博客。最近在做项目的过程中遇到这样的一个需求：玩家可以在游戏过程中进行实时存档，在存档过程中会保存当前游戏进度，同时会截取当前游戏画面并加载到游戏存档界面中。当下一次进入游戏的时候，将读取本地存档图片并加载到游戏界面中。这在单机游戏中是特别常见的一种功能，这里主要有两个关键点。首先是截取游戏画面，这个问题大家可以在《Unity3D 游戏开发之截屏保存精彩瞬间》这篇文章中找到答案。其次是从本地加载图片，因为这里要保证可读可写，因此传统的 Resources.Load()方式和 AssetBundle 方式均无法实现这样的功能。那么怎样从外部加载图片到游戏中，这就是我们今天要讨论的内容啦。好了，这里介绍两种方法来实现这一目的。\n喜闻乐见的 WWW 方式 喜闻乐见的 WWW 方式之所以喜闻乐见，这是因为这是我们最为熟悉的一种，我们都知道通过 WWW 可以从网络上加载文本、图片、音频等形式的内容，那么通过 WWW 能否加载本地外部（相对于应用程序）资源呢？答案是肯定的，这是因为 WWW 可以支持 http 和 file 两种协议。我们通常接触到的 WWW 默认都是指 http 协议，现在我们来说说 file 协议，该协议可以用来访问本地资源（绝对路径）。例如我们希望加载文件 D:\\TestFile\\pic001.png 这个文件，则此时对应的 C#脚本为：\n//请求WWW WWW www = new WWW(\u0026#34;file://D:\\\\TestFile\\\\pic001.png); yield return www; if(www != null \u0026amp;\u0026amp; string.IsNullOrEmpty(www.error)) { //获取Texture Texture texture=www.texture; //更多操作... } 注意到这里出现了 yield return 结构，这表示这里使用到了协程，因此我们需要付出的代价就是需要在项目中使用 StartCoroutine 等协程相关的方法来调用这些协程。虽然在 Unity3D 中使用协程是件简单的事情，可是如果我们随随便便地使用协程而不注意去维护这些协程，那么这些让我们引以为傲的简单代码可能就会变成我们痛苦不堪的无尽深渊。\n亘古不变的传统 IO 方式 好了，下面我们隆重推出亘古不变的传统 IO 方式，这种方式相信大家都没有接触过，所以这里将这种方法和大家分享。既然是传统的 IO 方式，那么无非就是各种 IO 流的处理啦。好，我们一起来看下面这段代码：\n//创建文件读取流 FileStream fileStream = new FileStream(screen, FileMode.Open, FileAccess.Read); fileStream.Seek(0, SeekOrigin.Begin); //创建文件长度缓冲区 byte[] bytes = new byte[fileStream.Length]; //读取文件 fileStream.Read(bytes, 0, (int)fileStream.Length); //释放文件读取流 fileStream.Close(); fileStream.Dispose(); fileStream = null; //创建Texture int width=800; int height=640; Texture2D texture = new Texture2D(width, height); texture.LoadImage(bytes); 可以看到在使用这种方式读取图片文件的时候主要是将图片文件转化为 byte[]数组，再利用 Texture2D 的 LoadImage 方法转化为 Unity3D 中的 Texture2D。这种方法需要在创建过程中传入图片的大小，在这里我们创建了一张 800X640 的图片。经过博主的研究发现，这种方式加载外部图片相对于使用 WWW 加载外部图片效率更高，所以如果大家遇到类似的需求，博主个人推荐大家使用这种方式进行加载。\n到目前为止我们解决了如何从外部加载图片到 Unity3D 中，现在我们回到最开始的问题，我们从外部读取到这些图片以后需要将它们加载到游戏界面中。比如当我们使用 UGUI 的时候，UGUI 中的 Image 控件需要一个 Sprite 来作为它的填充内容，那么此时我们就需要将 Texture 转化为 Sprite.号了，下面我们给出一个简单的例子：\nusing UnityEngine; using System.Collections; using UnityEngine.UI; using System.IO; public class TestLoading : MonoBehaviour { /// \u0026lt;summary\u0026gt; /// Image控件 /// \u0026lt;/summary\u0026gt; private Image image; void Start () { image = this.transform.Find(\u0026#34;Image\u0026#34;).GetComponent\u0026lt;Image\u0026gt;(); //为不同的按钮绑定不同的事件 this.transform.Find(\u0026#34;LoadByWWW\u0026#34;).GetComponent\u0026lt;Button\u0026gt;().onClick.AddListener ( delegate(){LoadByWWW();} ); this.transform.Find(\u0026#34;LoadByIO\u0026#34;).GetComponent\u0026lt;Button\u0026gt;().onClick.AddListener ( delegate(){LoadByIO();} ); } /// \u0026lt;summary\u0026gt; /// 以IO方式进行加载 /// \u0026lt;/summary\u0026gt; private void LoadByIO() { double startTime = (double)Time.time; //创建文件读取流 FileStream fileStream = new FileStream(\u0026#34;D:\\\\test.jpg\u0026#34;, FileMode.Open, FileAccess.Read); fileStream.Seek(0, SeekOrigin.Begin); //创建文件长度缓冲区 byte[] bytes = new byte[fileStream.Length]; //读取文件 fileStream.Read(bytes, 0, (int)fileStream.Length); //释放文件读取流 fileStream.Close(); fileStream.Dispose(); fileStream = null; //创建Texture int width = 300; int height = 372; Texture2D texture = new Texture2D(width, height); texture.LoadImage(bytes); //创建Sprite Sprite sprite = Sprite.Create(texture, new Rect(0, 0, texture.width, texture.height), new Vector2(0.5f, 0.5f)); image.sprite = sprite; startTime=(double)Time.time-startTime; Debug.Log(\u0026#34;IO加载用时:\u0026#34; + startTime); } /// \u0026lt;summary\u0026gt; /// 以WWW方式进行加载 /// \u0026lt;/summary\u0026gt; private void LoadByWWW() { StartCoroutine(Load()); } IEnumerator Load() { double startTime = (double)Time.time; //请求WWW WWW www = new WWW(\u0026#34;file://D:\\\\test.jpg\u0026#34;); yield return www; if(www != null \u0026amp;\u0026amp; string.IsNullOrEmpty(www.error)) { //获取Texture Texture2D texture=www.texture; //创建Sprite Sprite sprite = Sprite.Create(texture, new Rect(0, 0, texture.width, texture.height), new Vector2(0.5f, 0.5f)); image.sprite = sprite; startTime = (double)Time.time - startTime; Debug.Log(\u0026#34;WWW加载用时:\u0026#34; + startTime); } } } 现在我们运行程序可以发现两种方式均可以让图片加载进来，为了对比两种方式在执行效率上的高低，我们在脚本中加入了相关代码，通过对比可以发现使用 IO 方式加载一张 227k 的图片需要的时间为 0s，而使用 WWW 方式加载需要 0.0185s，因此传统的 IO 方式具有更高的效率，建议大家在遇到这类问题时尽可能地使用这种方式。好了，今天的内容就是这样啦，欢迎大家在我的博客中留言、欢迎大家关注和支持我的博客，谢谢大家！\n2016 年 6 月 12 日更新： 针对有朋友指出 WWW 加载和传统 IO 加载方式在效率上的差异，我们这里重新做一个效率测试。\n","date":"2015-10-08T15:03:01Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/821259985/","slug":"821259985","tags":["Unity3D","游戏开发","uGUI"],"title":"在 Unity3D 中加载外部图片的两种方法"},{"categories":["生活感悟"],"content":"在中秋节这样一个万家团圆的日子，我却再度因为工作的问题和家人发生争执。发生争执的原因简单到习以为常，家人喜欢稳定、安逸的生活，而我却喜欢有挑战、梦想的生活。我不知道梦想对一个二十三岁的人是不是一种奢侈品，我只知道当我住在狭小、拥挤的出租房里的时候，我想努力去拥有一个温暖的家，我不想靠着一张嘴去哗众取宠，我不想刻意地迎合和奉承这个世界，我只想靠我平凡而微薄的努力让我的生活一天天地温暖起来。从小我被告诉要做一个正直、善良的人，可是随着我慢慢地长大，在我的耳边总会听到“要去适应这个社会”这样的话，然而最讽刺的是这样的话常常出自同一人的口中。\n虽然我知道当今中国的社会是一个人情社会、关系社会，可我就是不愿意把时间浪费在交际应酬这样的事情上，因为我知道人的这一生的时间是非常宝贵的，从小我就看到身边熟悉的人因为各种各样的原因突然离开这个世界，我们每一个人都会面临死亡，所以当我懒得理会这些无聊的事情的时候，我更希望在我喜欢或者关注的事情上投入精力。每次当家人说我应该应该怎么样的时候，我常常假设自己如果按照这样的规划来度过这一生，那么当我衰老直至死亡的那一刻我心里又会想些什么？我讨厌政治和宗教，因为这是由人类自己为人类制造的精神枷锁，在它们涉足的领域常常伴随各种无可争辩的假象或者谎言，这恰恰是我这样一个正直的本性中极度厌恶的部分。长辈们或多或少地喜欢给我这样的年轻人冠以“愤青”这样的荣誉称号，可我做错了什么呢？我无非就是像《皇帝的新装》里的那个小孩，突然说出了一个大家都习以为常的秘密而已，我们从小到大的成长过程其实就是小孩从见到了就要说出来，变成现在见到了习以为常、看在眼里说在心里。人们常常把这种转变当作成熟，可是事实上人们只是变得更加麻木而已。此时此刻我假装麻木想要摆脱这些掩耳盗铃的秘密，长辈们却再度摆出“社会就是这样，你必须要去适应”这样的架势，大概是嫌弃我假装麻木难以入戏需要变得更加虚伪。\n我天生就是一个不会表演的人，从小时候排练舞蹈学习动作到此时此刻需要我去逢场作戏的各种场合，我不会说除了让人高兴还是高兴的话，我不会让喜怒哀乐像变脸、像翻书一样快。长辈们一直希望我变成一个圆滑世故、胸有城府的人，可我听惯了许嵩的《城府》、《别咬我》、《秋千坠》对这些东西天生排斥，所以在长辈们的世界观里，我就变成了一个冥顽不灵、图样图森破的年轻人。长辈们固然是从自己的经验出发，想让作为年轻人的我走上一条平坦舒适的道路，可是这个世界早已在不知不觉中发生着天翻地覆的变化，长辈们的经验获取可以让你顺利通关人生这场游戏，然而缺少了自我探索的旅程未免显得平凡而无趣。我有幸在小学三年级的时候接触计算机，在初中的时候接触互联网，在高中的时候接触编程，然而在这短短的若干年间互联网行业风起云涌、起起伏伏却并非我们的父辈可以理解和掌握。我走进大学的时候社交网站(SNS)开始兴起，以 Facebook、Twitter、人人网、新浪微博、腾讯微博等等为代表的社会化平台迅速地占领了整个互联网行业的制高点。或许和 70 后、80 后相比，我们这一代人在这个变化剧烈的时代显得有点生不逢时，可是机遇和挑战总是并存的，当我们无法和前辈们一起成为时代的弄潮儿的时候，我们只有努力去追赶这个时代忙碌的脚步。短短大学四年，我感受到了互联网每天天翻地覆的变化，从 SNS 到云计算、移动互联网、大数据、物联网再到互联网金融、O2O，这个行业慢慢地渗透到我们的生活中来。曾经我的长辈认为如果依靠政治力量毁灭了百度，则我们完全可以借由政治力量重新创造出来一个百度，可是同样是由政治力量领导的人民网、同样是由政治力量推到台前的邓亚萍，人民网最终依然在这场搜索引擎大战中以失败告终。我们无法访问国外网站并非是我们拥有世界山最先进的互联网技术，而是我们依靠政治力量用流氓一样的手段在全球一体化的今天实施信息领域的闭关锁国。我一直认为互联网行业是政治干预较为稀疏的一个行业，所以在这个行业当中我不会遭遇那些让我厌恶的政治因素，虽然有人聚集的地方就会有政治产生，但是作为互联网基础要素之一的技术是一个相对纯粹的领域，它依靠最为简单的 0 和 1 构成了今天丰富多彩的世界，它讲道理、守规矩让我觉得这个领域简单而纯粹。长辈们不理解我为什么会对计算机有这样独特的情结，因为在普通人眼中它就是一个可以娱乐和办公的机器，然而在我眼中它像是我的一位朋友默默地支持着我去解决各种问题。从我高中的时候起我就认定这个行业将会成为我一辈子的一种寄托，我相信技术可以让我们这个世界变得更加美好，这是我永恒的信仰，所以我不会把政治和宗教当成我一生的信仰。\n大学四年里它每天和我如影随形，让我去思考、去创造、去解决，我喜欢这样的一个过程。长辈们认为大学学习什么样的专业并不重要，因为当你从事实际的工作以后注定要去从头学习新的东西不是吗？可是这样的思路通常适用于那些对未来没有目标、随遇而安的人，显然我并不是这样的人，我一直都清楚地知道自己想要做什么，从始至终这个目标由大到小，但是从未有所改变。我的长辈们对互联网、对计算机技术基本都没有过深入的了解，他们从来不愿意去尝试这些新的东西，却习惯于去指责我做出了这样一个他们并不期望的选择。我是一名程序员，可是我从来没有觉得我的工作低人一等，我每天的付出和老师给学生授课、销售员给顾客售货、银行柜员给消费者办理业务、公务员为人民服务……并没有什么不同，我靠自己掌握的技能去解决工作中的问题，我靠自己掌握的知识去帮助更多的人，我并没有觉得我选择了一条错误的道路，难道在安逸中渐渐迷失了自我会让你从此与众不同？曾经和老师一起做艾依河的毕业设计，当时觉得对整个艾依河了如指掌，然而每天上下班从宝湖经过的时候却突然发现自己的渺小，我是一个普通人，我想做的事情就是努力让自己变得强大去拥有一个温暖的家，能够让因衰老而疲惫的心有个归宿，不至于在满是迷雾的现实中丢失本心，我就想一直这样简单地生活下去，做正直、正确的事情，做一个温暖、善良的人，做最初的自己。\n","date":"2015-09-30T10:19:26Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/786195243/","slug":"786195243","tags":["生活","梦想","人生"],"title":"做最初的自己"},{"categories":["游戏开发"],"content":"各位朋友大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。最近开始研究 Unity3D 游戏场景优化，每次提及游戏优化这个话题的时候，我的脑海中都会浮现出《仙剑奇侠传六》这个让四路泰坦都光荣陨落的神奇游戏，作为一个使用 Unity3D 引擎进行游戏开发的仙剑玩家，我曾经天真的以为，这款使用 Unity3D 引擎打造的仙剑二十周年献礼之作，会让我对《仙剑奇侠传》这个系列游戏的未来充满更多期待，然而当游戏真正呈现在我眼前的时候，我感受到了在历代仙剑游戏中从未有过的尴尬和失望，我尴尬的是 Unity3D 这样一个比较强大的游戏引擎硬生生地被北软玩成了这个鬼样子，我失望的是这部游戏除了剧情和跳跳乐以外并没有什么让人看到希望的东西。\n仙剑奇侠传六\r不到20帧的优化\r我知道我这样说会有一堆仙剑玩家指责我说，仙剑本来就是玩剧情的嘛，所以只要剧情好其它的都可以原谅啦。然而我们每一个人都清楚《仙剑奇侠传》是一个 RPG 游戏，它不是每隔三年出一次新番的 GAL 动漫、不是每隔三年更新一次的言情小说、更不是每隔三年播放一次的偶像电影。两年前的今天我可以耐着性子玩通关《仙剑奇侠传五》，但是这一次我真的玩不下去了。当一个游戏因为优化问题而获得《仙剑奇侠传六：泰坦陨落》称号的时候，作为一个玩家我真的不想再为这个游戏洗白什么，虽然我曾经深爱过这个游戏。所以言归正传，作为一个程序员，我们还是来做点程序员该做的事情，那么我们今天说什么呢，我们来说说 Unity3D 里的批处理！\n一、什么是批处理 我们知道 Unity3D 在屏幕上绘制一个图形本质上调用OpneGL或者DirectX这样的 API，因此在这个过程中会产生一定程度上的性能消耗。DrawCall 是 OpenGL 中描述绘制次数的一个量，例如一个基本的 OpenGL 绘制流程是设置颜色-\u0026gt;绘图方式-\u0026gt;顶点坐标-\u0026gt;绘制-\u0026gt;结束，在绘制的过程中每帧都会重复这个过程，这就是一次 DrawCall，所以当游戏中的绘制过程变得复杂的时候，就会带来 DrawCall 的急剧增加，进而带来游戏的性能问题，反映到游戏表现上就变成了优化问题。那么在 Unity3D 中采取了什么样的措施来降低 DrawCall 呢？这就是我们今天要说的批处理，换句话说 Unity3D 使用了批处理来达到降低 DrawCall 的目的，批处理希望通过对物体网格的重组来获得更高的绘制效率，试想以下如果将多个物体合并为一个物体，那么在绘制的时候只需要绘制一次就够了，因此从这个角度上来讲这样做肯定是可以降低 DrawCall 的，更深刻的一种理解是这里体现了一种资源循环调用的思想，接触过 Android 开发的朋友们一定知道 ListView 控件可以对其元素进行“缓存”从而提高效率，因为我们可以发现其实 ListView 是对列表项进行某种程度上的“复用”从而提高了效率，在 Unity3D 这里同样遵循了这个原理。在 Unity3D 中进行批处理的一个前提是相同材质的物体可以被合并，如果这些物体使用不同的材质，那么当我们把这些材质对应的纹理打成“图集”以后可以对其进行合并，并且在合并的时候应该是用Renderer.sharedMaterial 而非 Renderer.material以保证材质是可以共享的。关于 DrawCall 的相关细节大家从这里来了解,博主并未对图形学领域有过深入的研究，因此就不在这里班门弄斧了啊，哈哈！\n二、Unity3D 中批处理的两种方式 在 Unity3D 中有静态批处理和动态批处理两种方式，下面我们就来分别说说这两种不同的批处理方式！\n静态批处理 静态批处理其实大家都是知道的。为什么这样说呢？因为我们在使用 Unity3D 的过程中无形中培养了这样一个习惯，那就是将场景中相对来说“静态”的物体都勾选 Static 选项，这在 Unity3D 中称为Static GameObjects，并且因为这一特性和Lightmapping、Navigation、Off-meshLinks、ReflectionProbe、Occluder and Occludee等内容均有着密切的联系，因此说静态批处理大家都是知道的其实一点都为过，和场景优化相关的内容博主会在后续的博客中涉及，希望大家能及时关注我的博客更新。静态批处理允许游戏引擎尽可能多的去降低绘制任意大小的物体所产生的 DrawCall，它会占用更多的内存资源和更少的 CPU 资源，因为它需要额外的内存资源来存储合并后的几何结构，如果在静态批处理之前，如果有几个对象共享相同的几何结构，那么将为每个对象创建一个几何图形，无论是在编辑器还是在运行时。这看起来是个艰难的选择，你需要在内存性能和渲染性能间做出最为正确的选择。在内部，静态批处理是通过将静态对象转换为世界空间，并为它们构建一个大的顶点+索引缓冲区。然后，在同一批中，一系列的“便宜”画调用，一系列的“便宜”，几乎没有任何状态变化之间的。所以在技术上它并不保存“三维的调用”，但它可以节省它们之间的状态变化（这是昂贵的部分）。使用静态批处理非常简单啦，只要勾选物体的 Static 选项即可！\n动态批处理 相对静态批处理而言，动态批处理的要求更为严格一些，它要求批处理的动态对象具有一定的顶点，所以动态批处理只适用于包含小于 900 个顶点属性的网格。如果你的着色器使用顶点位置，法线和单光，然后你可以批处理 300 个顶点的动态对象；而如果你的着色器使用顶点位置，法线，uv0，UV1 和切线，那么只能处理 180 个顶点的动态对象。接下来最为重要的一点，**如果动态对象使用的是不同的材质，那么即使进行了动态批处理从效率上来讲并不会有太大的提升。**如果动态对象采用的是多维子材质，那么批处理是无效的。如果动态对象接收实时光影，同样批处理是无效的。下面展示的是一个将多个物体合并为一个物体的脚本示例：\n[MenuItem(\u0026#34;ModelTools/将多个物体合并为一个物体\u0026#34;)] static void CombineMeshs2() { //在编辑器下选中的所有物体 object[] objs = Selection.gameObjects; if(objs.Length \u0026lt;= 0) return; //网格信息数组 MeshFilter[] meshFilters =new MeshFilter[objs.Length]; //渲染器数组 MeshRenderer[] meshRenderers = new MeshRenderer[objs.Length]; //合并实例数组 CombineInstance[] combines = new CombineInstance[objs.Length]; //材质数组 Material[] mats = new Material[objs.Length]; for (int i = 0; i \u0026lt; objs.Length; i++) { //获取网格信息 meshFilters[i] = ((GameObject)objs[i]).GetComponent\u0026lt;MeshFilter\u0026gt;(); //获取渲染器 meshRenderers[i] = ((GameObject)objs[i]).GetComponent\u0026lt;MeshRenderer\u0026gt;(); //获取材质 mats[i] = meshRenderers[i].sharedMaterial; //合并实例 combines[i].mesh = meshFilters[i].sharedMesh; combines[i].transform = meshFilters[i].transform.localToWorldMatrix; } //创建新物体 GameObject go = new GameObject(); go.name = \u0026#34;CombinedMesh_\u0026#34; + ((GameObject)objs[0]).name; //设置网格信息 MeshFilter filter = go.transform.GetComponent\u0026lt;MeshFilter\u0026gt;(); if (filter == null) filter = go.AddComponent\u0026lt;MeshFilter\u0026gt;(); filter.sharedMesh = new Mesh(); filter.sharedMesh.CombineMeshes(combines,false); //设置渲染器 MeshRenderer render = go.transform.GetComponent\u0026lt;MeshRenderer\u0026gt;(); if (render == null) render = go.AddComponent\u0026lt;MeshRenderer\u0026gt;(); //设置材质 render.sharedMaterials = mats; } 这段脚本的核心是 CombineMeshes()方法，该方法有三个参数，第一个参数是合并实例的数组，第二个参数是是否对子物体的网格进行合并，第三个参数是是否共享材质，如果希望物体共享材质则第三个参数为 true，否则为 false。在我测试的过程中发现，如果选择了对子物体的网格进行合并，那么每个子物体都不能再使用单独的材质，默认会以第一个材质作为合并后物体的材质，下面演示的是合并前的多个物体和合并后的一个物体的对比：\n合并前\r合并后\r三、批处理效率分析 那么批处理对游戏效率提升究竟有怎样的作用呢？我们来看下面几组测试对比：\n1、三个不同的物体使用同一种材质，不做静态批处理，不做动态批处理：DrawCall 为 4、面数为 584、顶点数为 641\n2、三个不同的物体使用同一种材质，只做静态批处理，不做动态批处理：DrawCall 为 2、面数为 584、顶点数为 641\n3、三个不同的物体使用不同的材质，不做静态批处理，不做动态批处理：DrawCall 为 4、面数为 584、顶点数为 641\n4、三个不同的物体使用不同的材质，只做静态批处理，不做动态批处理：DrawCall 为 4、面数为 584、顶点数为 641\n5、三个不同的物体使用不同的材质，不做静态批处理，只做动态批处理：DrawCall 为 4、面数为 584、顶点数为 641\n6、三个不同的物体使用不同的材质，做静态批处理，做动态批处理：DrawCall 为 4、面数为 584、顶点数为 641\n7、三个不同的物体使用同一种材质，不做静态批处理，只做动态批处理：：DrawCall 为 4、面数为 584、顶点数为 641\n大家可以注意到各组测试结果中，只有第二组的 DrawCall 降低，这说明只有当不同的物体使用同一种材质时通过批处理可以从一定程度上降低 DrawCall，即我们在文章开始提到的尽可能地保证材质共享。昨天下午兴冲冲地将游戏场景里的某些物体进行了动态批处理，但是实际测试的过程中发现 DrawCall 非常地不稳定，但是在场景中的某些地方 DrawCall 却可以降得非常低，如果静态批处理和动态批处理都不能对场景产生较好的优化，那么 Unity3D 游戏场景的优化究竟要从哪里抓起呢？我觉得这是我们每一个人都该用心去探索的地方，毕竟游戏做出来首先要保证能让玩家流畅的玩下去吧，一味的强调引擎、强调画面，却时常忽略引擎使用者的主观能动性，希望把一切问题都交给引擎去解决，这样的思路是错误而落后的，仙剑六的问题完全是用不用心的问题，我常常看到有人在公开场合说仙剑以后要换虚幻三，其实按照北软现在这样的状态，给他们一个虚幻四也不过是然并卵。我在知乎上看到了号称 15 岁就开发次时代游戏的高中生妹子，做出个能称为 DEMO 的游戏就觉得自己可以搞引擎了，更有甚者随便用 DirectX 或者 OpenGL 封装若干函数就敢说自己会做游戏引擎了，呵呵，你确定你的游戏能在别人的电脑或者手机上运行起来吗？优化的重要性可见一斑。\n四、小结 好了，通过今天这篇文章，我们可以整理出以下观点： 1、如果不同的物体间共享材质，则可以直接通过静态批处理降低 DrawCall 2、动态批处理并不能降低 DrawCall、面数和顶点数（我不会告诉你我昨天傻呵呵地合并了好多场景中的模型，结果面数和顶点数并没有降下来，23333） 3、不管是静态批处理还是动态批处理都会影响 Culiing，这同样是涉及到场景优化的一个概念，好吧，为了让场景的 DrawCall 降下来我最近可能要研究好多涉及的优化的内容…… 那么今天的内容就是这样子了，希望对大家学习 Unity3D 有所帮助，欢迎大家和我交流这些问题来相互促进，毕竟这才是我写博客最初的目的嘛，哈哈！\n","date":"2015-09-07T10:59:13Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/927393529/","slug":"927393529","tags":["游戏","Unity3D","优化"],"title":"Unity3D 游戏场景优化系列之批处理"},{"categories":["游戏开发"],"content":"大家好，我是秦元培，欢迎大家关注我的博客。近期博客的更新频率基本直降到冰点，因为这段时间实在是忙得没有时间来写博客了。今天想和大家分享的内容是 RPG 游戏中游戏存档的实现，因为最近在做一个 RPG 游戏的项目，所以遇到这个问题就随时记录下来，在对知识进行总结的同时可以将这种思路或者想法分享给大家，这是一件快乐而幸运的事情。我讨厌写按部就班的技术教程，因为我觉得学习是一种自我的探索行为，如果一切都告诉你了，探索的过程便会变得没有意义了。\n游戏存档是一种在单机游戏中特别常见的机制，这种机制是你在玩网络游戏的时候无法体验到的，你知道每次玩完一款单机游戏都会把游戏存档保存起来是一种怎样的感觉吗？它就像是一个征战沙场的将军将陪伴自己一生金戈铁马的宝剑静静地收入剑匣，然而每一次打开它的时候都会不由自主的热泪盈眶。人的本性其实就是游戏，我们每一天发生的故事何尝不是一个游戏？有时候让我们怀念的可能并不是游戏本身，而只是搁浅在时光里的那时的我们。好了，游戏存档是我们在游戏世界里雪泥鸿爪，它代表了我们曾经来到过这个世界。以 RPG 游戏为例，一个一般化的游戏存档应该囊括以下内容：\n角色信息：指一切表征虚拟角色成长路线的信息，如生命值、魔法值、经验值等等。 道具信息：指一切表征虚拟道具数量或者作用的信息，如药品、道具、装备等等。 场景信息：指一切和游戏场景相关的信息，如场景名称、角色在当前场景中的位置坐标等等。 事件信息：指一切和游戏事件相关的信息，如主线任务、支线任务、触发性事件等等。 从以上信息划分的层次来看，我们可以发现在游戏存档中要储存的信息相对是比较复杂的，那么我们这里不得不说说 Unity3D 中的数据持久化方案 PlayerPrefs。该方案采用的是一种键值型的数据存储方案，支持 int、string、float 三种基本数据类型，通过键名来获取相对应的数值，当值不存在时将返回一个默认值。这种数据存储方案本质上是将数据写入到一个 Xml 文件。这种方案如果用来存储简单的信息是没有问题的，可是如果用它来存储游戏存档这样负责的数据结构就显得力不从心了。一个更为重要的问题是在数据持久化的过程中我们希望得到是一个结构化的【游戏存档】实例，显然此时松散的 PlayerPrefs 是不能满足我们的要求的。因此我们想到了将游戏数据序列化的思路，常见的数据序列化思路主要有 Xml 和 JSON 两种形式，在使用 Xml 的数据序列化方案的时候通常有两种思路，即手动建立数据实体和数据字符间的对应关系和基于 XmlSerializer 的数据序列化。其中基于 XmlSerializer 的数据序列化是利用了[Serializable]这样的语法特性来帮助.NET 完成数据实体和数据字符间的对应关系，两种思路本质上一样的。可是我们知道 Xml 的优点是可读性强，缺点是冗余信息多，因此在权衡了两种方案的利弊后，我决定采用 JSON 来作为数据序列化的方案，而且 JSON 在数据实体和数据字符间的对应关系上有着天然的优势，JSON 所做的事情不就是将数据实体转化为字符串和从一个字符串中解析出数据实体吗？所以整个方案基本一气呵成。好了，下面我们来看具体的代码实现过程吧！\nJSON 的序列化和反序列化 这里我使用的是 Newtonsoft.Json 这个类库，相信大家都是知道的了！因此，序列化和反序列化特别简单。\n/// \u0026lt;summary\u0026gt; /// 将一个对象序列化为字符串 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The object.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;pObject\u0026#34;\u0026gt;对象\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;pType\u0026#34;\u0026gt;对象类型\u0026lt;/param\u0026gt; private static string SerializeObject(object pObject) { //序列化后的字符串 string serializedString = string.Empty; //使用Json.Net进行序列化 serializedString = JsonConvert.SerializeObject(pObject); return serializedString; } /// \u0026lt;summary\u0026gt; /// 将一个字符串反序列化为对象 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The object.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;pString\u0026#34;\u0026gt;字符串\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;pType\u0026#34;\u0026gt;对象类型\u0026lt;/param\u0026gt; private static object DeserializeObject(string pString,Type pType) { //反序列化后的对象 object deserializedObject = null; //使用Json.Net进行反序列化 deserializedObject=JsonConvert.DeserializeObject(pString,pType); return deserializedObject; } Rijandel加密/解密算法 因为我们这里要做的是一个游戏存档的方案设计，因为考虑到存档数据的安全性，我们可以考虑采用相关的加密/解密算法来实现对序列化后的明文数据进行加密，这样可以从一定程度上保证游戏存档数据的安全性。因为博主并没有深入地研究过加密/解密方面的内容，所以这里仅仅提供一个从 MSDN 上获取的 Rijandel 算法，大家感兴趣的话可以自行去研究。\n/// \u0026lt;summary\u0026gt; /// Rijndael加密算法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;pString\u0026#34;\u0026gt;待加密的明文\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;pKey\u0026#34;\u0026gt;密钥,长度可以为:64位(byte[8]),128位(byte[16]),192位(byte[24]),256位(byte[32])\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;iv\u0026#34;\u0026gt;iv向量,长度为128（byte[16])\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; private static string RijndaelEncrypt(string pString, string pKey) { //密钥 byte[] keyArray = UTF8Encoding.UTF8.GetBytes(pKey); //待加密明文数组 byte[] toEncryptArray = UTF8Encoding.UTF8.GetBytes(pString); //Rijndael解密算法 RijndaelManaged rDel = new RijndaelManaged(); rDel.Key = keyArray; rDel.Mode = CipherMode.ECB; rDel.Padding = PaddingMode.PKCS7; ICryptoTransform cTransform = rDel.CreateEncryptor(); //返回加密后的密文 byte[] resultArray = cTransform.TransformFinalBlock(toEncryptArray, 0, toEncryptArray.Length); return Convert.ToBase64String(resultArray, 0, resultArray.Length); } /// \u0026lt;summary\u0026gt; /// ijndael解密算法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;pString\u0026#34;\u0026gt;待解密的密文\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;pKey\u0026#34;\u0026gt;密钥,长度可以为:64位(byte[8]),128位(byte[16]),192位(byte[24]),256位(byte[32])\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;iv\u0026#34;\u0026gt;iv向量,长度为128（byte[16])\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; private static String RijndaelDecrypt(string pString, string pKey) { //解密密钥 byte[] keyArray = UTF8Encoding.UTF8.GetBytes(pKey); //待解密密文数组 byte[] toEncryptArray = Convert.FromBase64String(pString); //Rijndael解密算法 RijndaelManaged rDel = new RijndaelManaged(); rDel.Key = keyArray; rDel.Mode = CipherMode.ECB; rDel.Padding = PaddingMode.PKCS7; ICryptoTransform cTransform = rDel.CreateDecryptor(); //返回解密后的明文 byte[] resultArray = cTransform.TransformFinalBlock(toEncryptArray, 0, toEncryptArray.Length); return UTF8Encoding.UTF8.GetString(resultArray); } 完整代码 好了，下面给出完整代码，我们这里提供了两个公开的方法 GetData()和 SetData()以及 IO 相关的辅助方法，我们在实际使用的时候只需要关注这些方法就可以了！\n/** * Unity3D数据持久化辅助类 * 作者:秦元培 * 时间:2015年8月14日 **/ using UnityEngine; using System.Collections; using System; using System.IO; using System.Text; using System.Security.Cryptography; using Newtonsoft.Json; public static class IOHelper { /// \u0026lt;summary\u0026gt; /// 判断文件是否存在 /// \u0026lt;/summary\u0026gt; public static bool IsFileExists(string fileName) { return File.Exists(fileName); } /// \u0026lt;summary\u0026gt; /// 判断文件夹是否存在 /// \u0026lt;/summary\u0026gt; public static bool IsDirectoryExists(string fileName) { return Directory.Exists(fileName); } /// \u0026lt;summary\u0026gt; /// 创建一个文本文件\t/// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;fileName\u0026#34;\u0026gt;文件路径\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;content\u0026#34;\u0026gt;文件内容\u0026lt;/param\u0026gt; public static void CreateFile(string fileName,string content) { StreamWriter streamWriter = File.CreateText(fileName); streamWriter.Write(content); streamWriter.Close(); } /// \u0026lt;summary\u0026gt; /// 创建一个文件夹 /// \u0026lt;/summary\u0026gt; public static void CreateDirectory(string fileName) { //文件夹存在则返回 if(IsDirectoryExists (fileName)) return; Directory.CreateDirectory(fileName); } public static void SetData(string fileName,object pObject) { //将对象序列化为字符串 string toSave = SerializeObject(pObject); //对字符串进行加密,32位加密密钥 toSave = RijndaelEncrypt(toSave, \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;); StreamWriter streamWriter = File.CreateText(fileName); streamWriter.Write(toSave); streamWriter.Close(); } public static object GetData(string fileName,Type pType) { StreamReader streamReader = File.OpenText(fileName); string data = streamReader.ReadToEnd(); //对数据进行解密，32位解密密钥 data = RijndaelDecrypt(data, \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;); streamReader.Close(); return DeserializeObject(data,pType); } /// \u0026lt;summary\u0026gt; /// Rijndael加密算法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;pString\u0026#34;\u0026gt;待加密的明文\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;pKey\u0026#34;\u0026gt;密钥,长度可以为:64位(byte[8]),128位(byte[16]),192位(byte[24]),256位(byte[32])\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;iv\u0026#34;\u0026gt;iv向量,长度为128（byte[16])\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; private static string RijndaelEncrypt(string pString, string pKey) { //密钥 byte[] keyArray = UTF8Encoding.UTF8.GetBytes(pKey); //待加密明文数组 byte[] toEncryptArray = UTF8Encoding.UTF8.GetBytes(pString); //Rijndael解密算法 RijndaelManaged rDel = new RijndaelManaged(); rDel.Key = keyArray; rDel.Mode = CipherMode.ECB; rDel.Padding = PaddingMode.PKCS7; ICryptoTransform cTransform = rDel.CreateEncryptor(); //返回加密后的密文 byte[] resultArray = cTransform.TransformFinalBlock(toEncryptArray, 0, toEncryptArray.Length); return Convert.ToBase64String(resultArray, 0, resultArray.Length); } /// \u0026lt;summary\u0026gt; /// ijndael解密算法 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;pString\u0026#34;\u0026gt;待解密的密文\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;pKey\u0026#34;\u0026gt;密钥,长度可以为:64位(byte[8]),128位(byte[16]),192位(byte[24]),256位(byte[32])\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;iv\u0026#34;\u0026gt;iv向量,长度为128（byte[16])\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; private static String RijndaelDecrypt(string pString, string pKey) { //解密密钥 byte[] keyArray = UTF8Encoding.UTF8.GetBytes(pKey); //待解密密文数组 byte[] toEncryptArray = Convert.FromBase64String(pString); //Rijndael解密算法 RijndaelManaged rDel = new RijndaelManaged(); rDel.Key = keyArray; rDel.Mode = CipherMode.ECB; rDel.Padding = PaddingMode.PKCS7; ICryptoTransform cTransform = rDel.CreateDecryptor(); //返回解密后的明文 byte[] resultArray = cTransform.TransformFinalBlock(toEncryptArray, 0, toEncryptArray.Length); return UTF8Encoding.UTF8.GetString(resultArray); } /// \u0026lt;summary\u0026gt; /// 将一个对象序列化为字符串 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The object.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;pObject\u0026#34;\u0026gt;对象\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;pType\u0026#34;\u0026gt;对象类型\u0026lt;/param\u0026gt; private static string SerializeObject(object pObject) { //序列化后的字符串 string serializedString = string.Empty; //使用Json.Net进行序列化 serializedString = JsonConvert.SerializeObject(pObject); return serializedString; } /// \u0026lt;summary\u0026gt; /// 将一个字符串反序列化为对象 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The object.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;pString\u0026#34;\u0026gt;字符串\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;pType\u0026#34;\u0026gt;对象类型\u0026lt;/param\u0026gt; private static object DeserializeObject(string pString,Type pType) { //反序列化后的对象 object deserializedObject = null; //使用Json.Net进行反序列化 deserializedObject=JsonConvert.DeserializeObject(pString,pType); return deserializedObject; } } 这里我们的密钥是直接写在代码中的，这样做其实是有风险的，因为一旦我们的项目被反编译，我们这里的密钥就变得很不安全了。这里有两种方法，一种是把密钥暴露给外部方法，即在读取数据和写入数据的时候使用同一个密钥即可，而密钥可以采取由机器 MAC 值生成的方法，这样每台机器上的密钥都是不同的可以防止数据被破解；其次可以采用 DLL 混淆的方法让反编译者无法看到代码中的内容，这样就无法获得正确的密钥从而无法获得存档里的内容了。\n最终效果 好了，最后我们来写一个简单的测试脚本：\nusing UnityEngine; using System.Collections; using System.Collections.Generic; public class TestSave : MonoBehaviour { /// \u0026lt;summary\u0026gt; /// 定义一个测试类 /// \u0026lt;/summary\u0026gt; public class TestClass { public string Name = \u0026#34;张三\u0026#34;; public float Age = 23.0f; public int Sex = 1; public List\u0026lt;int\u0026gt; Ints = new List\u0026lt;int\u0026gt; () { 1, 2, 3 }; } void Start () { //定义存档路径 string dirpath = Application.persistentDataPath + \u0026#34;/Save\u0026#34;; //创建存档文件夹 IOHelper.CreateDirectory (dirpath); //定义存档文件路径 string filename = dirpath + \u0026#34;/GameData.sav\u0026#34;; TestClass t = new TestClass (); //保存数据 IOHelper.SetData (filename,t); //读取数据 TestClass t1 = (TestClass)IOHelper.GetData(filename,typeof(TestClass)); Debug.Log(t1.Name); Debug.Log(t1.Age); Debug.Log(t1.Ints); } } 脚本执行结果：\np1\r加密后游戏存档：\np2\r好了，这就是今天的内容了，希望大家能够喜欢，有什么问题可以给我留言，谢谢！ 感谢风宇冲Unity3D 教程宝典之两步实现超实用的 XML 存档一文提供相关思路！\n喜欢我的博客请记住我的名字：秦元培，我的博客地址是：http://qinyuanpei.com 转载请注明出处，本文作者：秦元培， 本文出处：http://blog.csdn.net/qinyuanpei/article/details/39717795\n","date":"2015-08-20T08:57:10Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/887585917/","slug":"887585917","tags":["游戏开发","JSON","加密"],"title":"Unity3D 游戏开发游戏读/存档在 Unity3D 中的实现"},{"categories":["游戏开发"],"content":"各位朋友，大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是：http://qinyuanpei.com。话题紧接上回，在上回我们讲到了 SDL 的下载、安装和配置并对 SDL 游戏有了初步的了解。我们知道游戏开发中最为基础的内容是图形的绘制，因此在我们学习 SDL 游戏开发的过程中我们同样要从最简单的图形绘制开始学习。在 2D 游戏开发中，精灵（Sprite）是一个基础而核心的内容，具体来讲精灵首先是一张 2D 图片，精灵的绘制从本质上是图片的绘制，所以这是一个基础的内容。因为精灵在 2D 游戏中承担着 GameObject 的重要角色，所以一个图形引擎对精灵的支持好坏会决定游戏设计的最终效果。今天这篇文章主要是通过使用 SDL 中的 SDL_LoadBMP()、SDL_CreateTextureFromSurface()和 SDL_RenderCopy()这三个方法来实现在 SDL 中基本图形的绘制，从整体上尚属较为简单的内容。可是从学习 SDL 游戏开发的角度来看，一切都值得我们深入地去研究。好了，这就开始吧！\n使用 SDL_loadBMP 加载位图 从 SDL_LoadBMP()这个方法的名称，我们就可以看出这是一个读取 BMP 位图的方法。BMP 是 Windows 操作系统中最早的图形格式，这种图形格式的容量较大，经常出现在 Win32 API 中。好了，言归正传，我们下面来看看整个绘制过程：\n1、首先我们使用 SDL_LoadBMP()方法来加载一张 BMP 位图：\n//读取一张BMP位图\rSDL_Surface* m_pSurface=SDL_LoadBMP(\u0026#34;background.bmp\u0026#34;); 2、接下来我们使用 SDL_CreateTextureFromSurface()方法将 SDL_Surface 类型转化为 SDL_Texture 类型\n//获取SDL纹理\rSDL_Texture* m_pTexture=SDL_CreateTextureFromSurface(g_pRenderer,m_pSurface);\r//释放m_pBackgroundSurface\rSDL_FreeSurface(m_pSurface); 注意到在这里 m_pSurface 扮演了一个临时演员的角色。当我们获得了 SDL 纹理后，它的演员生涯便就此结束了，因此我们需要使用 SDL_FreeSurface()方法来释放它的内存。\n3、接下来是关键性的一个步骤，我们首先来关注 SDL_RenderCopy()的方法定义: SDL_RenderCopy(SDL_Renderer * renderer,SDL_Texture * texture,const SDL_Rect * srcrect,const SDL_Rect * dstrect); 如你所见，该方法的第一个参数和第二个参数我们已经相当熟悉了，即 SDL 渲染器和 SDL 纹理。这里想说的是第三个参数 srcrect 和第四个参数 dstrect，这两个参数都是 SDL_Rect 类型，表示一个矩形范围，它有四个参数，即矩形左上角横坐标、矩形左上角纵坐标、矩形宽度、矩形高度。那么该如何理解这两个参数呢？\nSDL 绘图中的精灵裁剪 这里我是这样理解的：第一个参数 srcrect 表示一个裁剪范围，即我们希望绘制图形的一个范围。例如我们现在有一张大小为 640*480 的图片，当我们使用(0,0,640,480)这样一个矩形对图片进行裁剪时，我们将获得整张图片；当我们使用(320,240,320,240)这个矩形对图片进行裁剪的时，我们将获得整张图片右下角 1/4 的部分。依次类推。相反地，dstrect 则更加类似于一个画布（Canvas）的概念，即我们可以在一个多大的矩形范围内去绘制这样一张图片。\n在这里，我们可以联想到 2D 图形绘制中的 SpriteSheet，即“雪碧图”这个概念。在游戏开发中我们常常会使用 TexturePacker 这样的工具来将零散的小图打包成一张大图，因为这样可以提高游戏运行的效率，该工具最终导出的文件由.plist 文件和合成的大图两部分组成，其中的.plist 文件中记录了每张小图的位置信息，因此将这个概念引申到这里来，你就会理解这里提到的精灵裁剪，即 srcrect 这个矩形的作用是选择“大图”中的“小图”，而 dstrect 这个矩形的作用是决定将选出的“小图”绘制在一个多大的范围内。\n一个经典的例子是我们现在一个有一张 1124x676 的图片，我们希望将其绘制到一个 800x640 的窗口作为背景图片，那么我们的代码可以这样写：\n/* 添加对SDL的引用 */ #include\u0026lt;SDL.h\u0026gt; /* 声明SDL窗口 */ SDL_Window *g_pWindow; /* 声明SDL渲染器 */ SDL_Renderer *g_pRenderer; /* 声明程序入口函数main */ int main(int agrc,char *args[]) { //初始化SDL int SDLInit=SDL_Init(SDL_INIT_EVERYTHING); if(SDLInit\u0026gt;=0) { //创建一个SDL窗口 g_pWindow=SDL_CreateWindow(\u0026#34;SDL Game Development-02\u0026#34;, SDL_WINDOWPOS_CENTERED,SDL_WINDOWPOS_CENTERED, 800,640, SDL_WINDOW_SHOWN); if(g_pWindow!=0){ //创建SDL渲染器 g_pRenderer=SDL_CreateRenderer(g_pWindow,-1,0); }\t} //设置背景色 SDL_SetRenderDrawColor(g_pRenderer,255,255,255,255); //渲染器清空 SDL_RenderClear(g_pRenderer); //读取一张BMP位图 SDL_Surface* m_pSurface=SDL_LoadBMP(\u0026#34;background.bmp\u0026#34;); //获取SDL纹理 SDL_Texture* m_pTexture=SDL_CreateTextureFromSurface(g_pRenderer,m_pSurface); //释放m_pBackgroundSurface SDL_FreeSurface(m_pSurface); //构造SDL矩形 SDL_Rect* m_pSrcRect=new SDL_Rect(); m_pSrcRect-\u0026gt;x=0; m_pSrcRect-\u0026gt;y=0; m_pSrcRect-\u0026gt;w=1124; m_pSrcRect-\u0026gt;h=676; SDL_Rect* m_pTargetRect=new SDL_Rect(); m_pTargetRect-\u0026gt;x=0; m_pTargetRect-\u0026gt;y=0; m_pTargetRect-\u0026gt;w=800; m_pTargetRect-\u0026gt;h=640; //绘制SDL纹理 SDL_RenderCopy(g_pRenderer,m_pTexture,m_pSrcRect,m_pTargetRect); //显示绘制结果 SDL_RenderPresent(g_pRenderer); //注意这里增加秒的延迟是为了看到渲染的结果 //在实际的开发中不应该出现这样的代码因为在运行期间会导致窗口的卡顿 //正确的做法是使用循环来处理这样一个渲染的过程 SDL_Delay(5000); //退出 SDL_Quit(); return 0; } 好了，现在运行这段代码，在运行这段代码前请确保完成了 SDL 的配置、在 Debug 目录中存放有一张名为 background.bmp 的位图文件以及 SDL2.dll。如果你准确无误地完成以上注意事项，那么你将毫无意外地看到这样一个画面：\nSDL游戏开发\r工程示例 现在让我们为这个示例增加点有趣的东西，我们知道在游戏设计中一般背景图片的大小是和游戏设计的窗口大小保持一致的，因为这样能够避免图片拉伸的问题。假定我们目前使用的精灵图片素材都是单个精灵的素材，那么我们可以设计这样一个方法来更加自由地绘制图片：\n/* 实现绘制BMP位图的方法 */ void DrawBMP(SDL_Renderer* renderer,const char* fileName, int positionX,int positionY,int textureWidth,int textureHeight) { //读取一张BMP位图 SDL_Surface* m_pSurface=SDL_LoadBMP(fileName); //获取SDL纹理 SDL_Texture* m_pTexture=SDL_CreateTextureFromSurface(renderer,m_pSurface); //释放m_pBackgroundSurface SDL_FreeSurface(m_pSurface); //构造SDL矩形 SDL_Rect* m_pSrcRect=new SDL_Rect(); m_pSrcRect-\u0026gt;x=0; m_pSrcRect-\u0026gt;y=0; m_pSrcRect-\u0026gt;w=textureWidth; m_pSrcRect-\u0026gt;h=textureHeight; SDL_Rect* m_pTargetRect=new SDL_Rect(); m_pTargetRect-\u0026gt;x=positionX; m_pTargetRect-\u0026gt;y=positionY; m_pTargetRect-\u0026gt;w=textureWidth; m_pTargetRect-\u0026gt;h=textureHeight; //绘制SDL纹理 SDL_RenderCopy(renderer,m_pTexture,m_pSrcRect,m_pTargetRect); 在认为背景图片大小和窗口大小一致的前提下，我们修改下代码：\n/* 添加对SDL的引用 */ #include\u0026lt;SDL.h\u0026gt; /* 声明SDL窗口 */ SDL_Window *g_pWindow; /* 声明SDL渲染器 */ SDL_Renderer *g_pRenderer; /* 声明相关方法 */ void DrawBMP(SDL_Renderer* renderer,const char* fileName ,int positionX,int positionY,int textureWidth,int textureHeight); /* 声明程序入口函数main */ int main(int agrc,char *args[]) { //初始化SDL int SDLInit=SDL_Init(SDL_INIT_EVERYTHING); if(SDLInit\u0026gt;=0) { //创建一个SDL窗口 g_pWindow=SDL_CreateWindow(\u0026#34;SDL Game Development-02\u0026#34;, SDL_WINDOWPOS_CENTERED,SDL_WINDOWPOS_CENTERED, 1124,676, SDL_WINDOW_SHOWN); if(g_pWindow!=0){ //创建SDL渲染器 g_pRenderer=SDL_CreateRenderer(g_pWindow,-1,0); }\t} //设置背景色 SDL_SetRenderDrawColor(g_pRenderer,255,255,255,255); //渲染器清空 SDL_RenderClear(g_pRenderer); //在绘制背景图片时因为我们已通过画图软件获得了该图片的大小为1124*676 //并且保证图片的大小和窗口大小一致因此我们可以直接构造一个(0,0,1024,676)的矩形来绘制 DrawBMP(g_pRenderer,\u0026#34;background.bmp\u0026#34;,0,0,1124,676); //接下来我们在窗口中心绘制一个大小为161*400的美少女 DrawBMP(g_pRenderer,\u0026#34;girl.bmp\u0026#34;,1124/2-161/2,676/2-400/2,161,400); //显示绘制结果 SDL_RenderPresent(g_pRenderer); SDL_Delay(10000); //退出 SDL_Quit(); return 0; } /* 实现绘制BMP位图的方法 */ void DrawBMP(SDL_Renderer* renderer,const char* fileName, int positionX,int positionY,int textureWidth,int textureHeight) { //读取一张BMP位图 SDL_Surface* m_pSurface=SDL_LoadBMP(fileName); //获取SDL纹理 SDL_Texture* m_pTexture=SDL_CreateTextureFromSurface(renderer,m_pSurface); //释放m_pBackgroundSurface SDL_FreeSurface(m_pSurface); //构造SDL矩形 SDL_Rect* m_pSrcRect=new SDL_Rect(); m_pSrcRect-\u0026gt;x=0; m_pSrcRect-\u0026gt;y=0; m_pSrcRect-\u0026gt;w=textureWidth; m_pSrcRect-\u0026gt;h=textureHeight; SDL_Rect* m_pTargetRect=new SDL_Rect(); m_pTargetRect-\u0026gt;x=positionX; m_pTargetRect-\u0026gt;y=positionY; m_pTargetRect-\u0026gt;w=textureWidth; m_pTargetRect-\u0026gt;h=textureHeight; //绘制SDL纹理 SDL_RenderCopy(renderer,m_pTexture,m_pSrcRect,m_pTargetRect); } SDL游戏开发\r现在我们再来运行程序，可以发现在背景图片上绘制了一个美少女，并且这个美少女处于窗口的中心。好了，通过今天的这部分内容我们可以实现在屏幕任意位置绘制图片，这里要注意一个前提，即图片表示的是单个精灵，在绘制过程中不存在裁切和缩放的问题。作为一个有节操的程序员，我们怎么能为了目前的这点成果而止步不前呢？注意到窗口标题上出现了未响应的字样，这是因为我们这里使用了 SDL_Delay()这个方法的缘故，该方法会造成程序在运行过程中的卡顿。那么怎么解决这个问题呢？这里就需要涉及到 SDL 中的事件机制，可能这里大家会有点迷茫，可是我们暂时只需要用到 SDL_PollEvent 这个方法，这个方法可以帮助我们判断是否触发了某个事件，比如我们需要判断用户是否点击了窗口右上角的关闭按钮：\nSDL_Event m_event; if(SDL_PollEvent(\u0026amp;m_event)) { if(m_event.type==SDL_QUIT) SDL_Quit(); } 考虑到游戏渲染是一个循环的过程，因此我们只需要在工程示例中增加事件处理的相关代码，就可以解决因为使用 SDL_Delay 方法而带来的卡顿问题。好了，今天的内容暂时就研究到这里，我们注意到这里的图片都是静态的缺乏某种交互感，而且窗口中心绘制的美少女的有白色背景的，如果我们希望这里透明该怎么做呢？欲知后事如何，且听下回分解，敬请期待 SDL 游戏开发系列第三话：说说 SDL 中的扩展库。\n","date":"2015-07-27T08:48:59Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3789971938/","slug":"3789971938","tags":["SDL","游戏","游戏引擎","图形","教程"],"title":"SDL 游戏开发系列第二话：基本图形的绘制"},{"categories":["游戏开发"],"content":"各位读者朋友大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。从今天起博主将带领大家一起走进 SDL 游戏开发的世界，如果说此前的 Unity3D 游戏开发系列文章让大家感受到的是游戏引擎工具化开发的方便与快捷，那么这一次就让我们以 SDL 库为基础，通过了解游戏开发中的底层图形渲染、输入事件响应等内容来全面认识游戏引擎，博主为 SDL 游戏开发系列文章建立了专栏，大家可以通过这里获取所有的系列文章，希望大家能够喜欢！好了，作为 SDL 游戏开发系列的第一篇文章，按照技术性文章写作的国际惯例这将是一篇介绍 SDL 入门内容的文章，因此这篇文章叫做：Hello SDL。\n什么是 SDL SDL（Simple DirectMedia Layer）是一套开放源代码的跨平台多媒体开发库，使用 C 语言写成。SDL 提供了数种控制图像、声音、输出入的函数，让开发者只要用相同或是相似的代码就可以开发出跨多个平台如 Linux、Windows、Mac OS X 等的应用软件。目前 SDL 可用于游戏、模拟器、媒体播放器等多媒体应用领域的开发，SDL 最为著名的案例是曾赢得 Linux 组游戏开发大奖的游戏《文明：权利的召唤》。或许大家对这个游戏会感到陌生吧，可是如果我提到一个 Linux 下经典单机游戏《仙剑奇侠传》的开源实现SDLPal相信大家就没有不知道的了吧？这款经典的单机游戏所以能够移植到 Linux 平台下就是因为使用 SDL 。好了，在对 SDL 有了大概的认识后，我们来看看 SDL 有哪些值得我们去研究的优良特性吧！\nSDL 提供了从图像、视频、音频、事件、线程、计时器的 API，功能特别强大。 SDL 具有良好的跨平台性能，支持 Windows、Linux 及 Android 和 IOS，是开发跨平台多媒体应用的神兵利器。 SDL 内置了 OpenGL 相关函数，使 SDL 开发 3D 应用成为可能，因此 SDL 是一个同时支持 2D 和 3D 开发的强力工具。 通过使用 SDL_image、SDL_ttf、SDL_mixer、SDL_net 等外部扩展库，可以轻松实现 JPG、PNG、TIFF 图像的加载使用，TrueType 字体的使用，MP3 文件的使用、网络相关的使用等。 SDL 支持 C++、C#、Java、 Lisp、Lua、Objective C、Pascal、Perl、 PHP、Python、Ruby 等近 20 种编程语言。 SDL 是 GNU LGPL 2 开源协议下发布的开源软件，该协议允许用户将 SDL 以动态链接库的形式免费地用于商业游戏软件的开发。 SDL的下载、安装和配置 SDL 开发相关的资源都可以从 http://www.libsdl.org/ 来获取。目前 SDL 存在 1.2 和 2.0 两个版本，从效率上来说 SDL 2.0 支持硬件加速效率较 SDL 1.2 有了较好的提升，从稳定性上来讲 SDL2.0 尚处于发展阶段，因此可能其中的 Bug 较 SDL1.2 可能会多些。博主这里选择的 SDL 2.0，下面是相关的下载链接：\nSDL 源代码——下载 SDL 二进制库——Win_x86、Win_x64、Mac SDL 开发包——VC++、GCC、Mac 博主选择的开发环境是 Visual Studio 2012，因此下载 VC++ 的 SDL 开发包。我们将下载得到的 SDL 开发包解压到本地，可以发现 SDL 开发包中已经为我们准备好了相关的 include 文件夹和 lib 文件夹。其中 include 文件夹下存放的是 SDL 的各种头文件，lib 文件夹下存放的是编译好的动态链接库（.dll）和依赖库（.lib），如果读者朋友有能力或是希望自行编译 SDL 源代码的，请先去编译源代码。这里我们为了节省时间，就直接使用编译好的文件了,请大家不要鄙视我啊，哈哈。好了，下面我们来以一个 VC++ 项目为例来讲解 SDL 的配置：\n1、使用 Visual Studio 创建一个空的 VC++ 项目 2、右键单击项目【属性】打开项目属性页找到【配置属性】-\u0026gt;【VC++目录】然后将包含目录和库目录分别定位到 SDL 开发包中的 include 目录和 lib 目（x86 和 x64 视系统情况而定） 3、在【配置属性】-\u0026gt;【链接器】-\u0026gt;【输入】-\u0026gt;【附加依赖项】中增加 SDL2.lib 和 SDL2main.lib 4、将【配置属性】-\u0026gt;【链接器】-\u0026gt;【系统】-\u0026gt;【子系统】设置为窗口 (/SUBSYSTEM:WINDOWS) 5、将 SDL2.dll 复制到项目的 Debug 目录中 SDL 游戏开发的基本流程 SDL 游戏开发的一般流程是：\n1、使用 SDL_Init() 方法对 SDL 进行初始化。其中该初始化方法的参数类型为 int 类型，可以从 SDL_INIT_HAPTIC、SDL_INIT_AUDIO、SDL_INIT_VIDEO、SDL_INIT_TIMER、SDL_INIT_JOYSTICK、SDL_INIT_EVERYTHING、SDL_INIT_NOPARACHUTE 七个类型中选择，分别表示力反馈子系统、音频子系统、视频子系统、计时器子系统、摇杆子系统、全部和忽略致命信号。 2、在 SDL 初始化成功后使用 SDL_CreateWindow() 方法创建一个 SDL 窗口（SDL_Window）。在这里我们可以设置窗口的名称、对齐方式、窗口宽度和窗口高度。 3、在 SDL 窗口创建成功后使用 SDL_CreateRenderer() 方法创建一个 SDL 渲染器（SDL_Renderer）。其中 SDL 渲染器有 SDL_RENDERER_SOFTWARE、SDL_RENDERER_ACCELERATED、SDL_RENDERER_PRESENTVSYNC、SDL_RENDERER_TARGETTEXTURE 四种类型分别表示软件渲染、硬件加速、屏幕同步刷新渲染和支持渲染纹理。 4、使用 SDL_RenderClear() 方法清空 SDL 渲染器、使用 SDL_RenderPresent() 方法将渲染的结果显示出来 工程示例 下面以一个简单的示例来向大家演示 SDL 游戏开发的一般流程：\n/* 添加对SDL的引用*/ #include\u0026lt;SDL.h\u0026gt; /* 声明SDL_Window */ SDL_Window *g_pWindow; /* 声明SDL_Renderer */ SDL_Renderer *g_pRenderer; /* 定义入口函数main */ int main(int argc,char *args[]) { /* SDL三部曲——1:初始化SDL */ int sdlInit=SDL_Init(SDL_INIT_EVERYTHING); if(sdlInit\u0026gt;=0){ /* 当SDL初始化完成后创建一个标题为\u0026#34;SDL Game Development——01\u0026#34; */ /* 窗口对齐方式为居中对齐，窗口大小为640*480的窗口 */ g_pWindow=SDL_CreateWindow(\u0026#34;SDL Game Development——01\u0026#34;, SDL_WINDOWPOS_CENTERED,SDL_WINDOWPOS_CENTERED, 640,480,SDL_WINDOW_SHOWN); /* SDL三部曲——2:初始化SDL渲染 */ if(g_pWindow!=0){ g_pRenderer=SDL_CreateRenderer(g_pWindow,-1,0); } } /* SDL三部曲——3:绘制窗口 */ SDL_SetRenderDrawColor(g_pRenderer,0,0,0,255); SDL_RenderClear(g_pRenderer); SDL_RenderPresent(g_pRenderer); SDL_Quit(); return 0; } 在以上代码中我们基本遵循了 SDL 游戏开发的一般流程，即首先对 SDL 进行初始化，当 SDL 初始化完成后，我们创建一个标题为\u0026quot;SDL学习示例1\u0026quot;,窗口对齐方式为居中对齐，窗口大小为 640 * 480 的窗口，然后创建了模式为软件渲染的 SDL 渲染器，并设置渲染器的背景色为黑色。作为第一个项目，它简单到纯粹，当我们运行项目，会发现一个黑色的窗口一闪而过，这是因为我们这里在渲染了一次后就使用 SDL_Quit() 方法退出了，第一篇文章并不会有太复杂的内容，因为它的意义在于让我们对 SDL 游戏开发有个基本的认识和了解。关于 SDL 绘制图片、文字以及处理渲染循环等问题我们放到后面的文章中去讲，这篇文章的内容就是这样啦，谢谢大家！\n","date":"2015-07-25T15:19:01Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/183718218/","slug":"183718218","tags":["SDL","游戏","图形","引擎","教程"],"title":"SDL 游戏开发系列第一话：Hello SDL"},{"categories":["单机游戏"],"content":" 目前游戏主线剧情进行到 50%左右，在游戏尚未通关前，我对于这一部游戏的感觉始终是一种说不清道不明的情感，作为仙剑系列中唯一一部，从项目立项到宣传曝光再到游戏上市整个过程中持续关注的游戏，它可以说是承载了无数玩家的期待和祝福。和大部分玩家一样，在游戏曝光的第一时刻我们曾经热火朝天地讨论过各种各样可能的设定、曾经为这部游戏的系统玩家想过各种各样的尝试，然而当我面对这款游戏的时候，我的内心平静得像一潭死水。我今天 23 岁，刚刚从大学毕业的我本应该还没有被这个社会完全改变，可我不知道是我变了还是仙剑变了，这一次打开仙剑的时候，我总有一种恍若隔世的恍惚感。\n引子 曾经，仙剑一的游戏开始界面是简单到不能再简单的竹简、酒葫芦、剑，这些元素组合起来就仗剑江湖的行侠仗义、白云苍狗的醉梦人生和徐徐道来的温暖故事；曾经，仙剑三的《御剑江湖》伴随着云山雾绕的蜀山像一幅遗留在历史深处的卷轴缓缓地打开让人不由得心头一阵惊艳；曾经，仙三外传开头蜀山掌门大战狼妖，无数道剑气凝成的剑柱从天而降可以让你感受到那种仙家道法的玄妙和奇幻；曾经，仙剑四的《回梦游仙》在耳畔响起的时候卷云台像朵清新脱俗的莲花静静地盛开却在最后一刻明白这朵莲花是一切悲剧的开始；曾经，仙剑五前传的明州码头在夕阳和晚霞的交相辉映中，瑾轩和瑕妹依偎在一起看着落霞与孤鹜齐飞是永远的温馨画面……\n可是仙剑六让我看到了什么呢？我看到了一片经过高斯模糊的绿油油的草地，没错！它真的是一片绿油油的草地，以至于当我打开这个游戏的时候我不得不在心里问自己：这真的是一个古风的仙侠/武侠游戏吗？在仙剑奇侠传六的宣传阶段，我在我的 QQ 群里、我的博客中不断向大家输送着这样一个概念：仙剑六是仙剑奇侠传系列二十年的突破之作，它一定不会让大家失望的，甚至我不遗余力地和游戏圈子里使用 Unity3D 引擎开发游戏的朋友们说，看，仙剑六是使用 Unity3D 引擎开发的，它的画面是历代游戏中最好的，这一次它终于要尝试即时战斗了。然而当我打开这个游戏的时候，我知道自己会被打脸，尤其是当我面对接受了我的这些观点的朋友的时候。\n对于此次仙剑六游戏优化的问题，我不想做详细的说明，我更不想为北软洗白，当我们觉得仙剑奇侠传六之泰坦陨落变成一个笑话的时候，我觉得我们可以说说 Unity3D 这个游戏引擎了。我承认，Unity3D 是个手机游戏引擎；我承认，Unity3D 在画面表现上无法和 UE、CE 这些顶级的游戏引擎相提并论；我承认，现在国内各种各样的 Unity3D 教程满天飞；我承认，Unity3D 入门快、成本低、跨平台性能强……可是这些都不是你们不用心做仙剑六的理由好吗？在我看来，技术从来都没有优劣之分，真正让技术体现出差异的是使用工具的人。Unity3D 本质上并非是一个差劲的游戏引擎，所以以引擎来论仙剑六的成败是不恰当而且不应该的，因为使用 Unity3D 开发的游戏目前已经相当的多了，比如《蒸汽之城》、《太空冒险》、《新仙剑 OL》等等以及无数的手机游戏。可是能将这个引擎用到如此地步的恐怕只有仙剑六吧！目前 Unity3D 的授权方式是收入超过 3 万美元即 18 万人民币左右需要按照一定的比例向 Unity3D 官方支付费用，我不知道这次的优化问题是否会对仙剑六的销量产生影响，可是我觉得恐怕官方都会认为这款游戏存在影响 Unity3D 引擎声誉的问题吧！\n画面 首先我们来说说游戏画面，我不知道有多少玩家可以在这款游戏中保证特效全开，总之在我的电脑上看起来整个画面有一种糊糊的感觉，远景看起来比较唯美壮丽，然而当我将镜头拉近的时候我觉得我还是不要计较仙剑奇侠传这个游戏的画面了吧！可是我真的不理解，作为仙剑奇侠传系列的好兄弟轩辕剑系列在使用 Unity3D 制作了两款游戏这样的背景下，北软为什么就不肯向 DOMO 小组学习哪怕借鉴相关的经验呢？虽然《轩辕剑六》恶名在外、《穹之扉》销量不佳，可是最起码人家的游戏的画面做得相对仙剑六要好很多好不好，况且人家在处理人物和场景时刻意加强了人物面部的特征，通过渲染景深和整体的光照使得画面透露出一种唯美的风格。可是仙剑六呢？仙剑六中做得最好看的永远都是人物的脸，我不知道北软是不是在有意告诉大家：这是一个看脸、靠脸的时代，所以当我们面对这个游戏的时候我们就发现整个游戏除了脸比较漂亮以外基本没有什么优点，可是事实上不同的人物在不同的场合、年龄他的面部应该都是不一样的啊，北软你把所有人的脸都做的这么漂亮，是想告诉玩家都不用去努力工作、只要拼脸就可以了吗？我不知道使用国外的 3A 级别的游戏引擎如 UE、CE 等来表现这种中国古典美的场景是否合适，因为这种类型的引擎更适合写实的渲染，而无疑中国的古风游戏需要的是一种意蕴上的美感的渲染。然而《古剑奇谭》和《轩辕剑》在表现这种场景时处理的相对来说是比较好的，这次的所有场景中我比较喜欢的是与青山，因为这个场景的色彩运用可以让玩家很明显的区分开场景中的不同的区域，反观忘尘寰、归墟、天晴之海、饮马河等场景因为使用的色彩较为接近，因为在玩家探索迷宫的时候常常搞晕，再辅以本次游戏中那个神奇的相机视角，探索迷宫的乐趣真是大大的增强啊！我不理解为什么北软连全局光照这种只需要简单设置下参数的东西都不愿意用，却要花大力气在角色的头顶上放置点光源，你告诉我，放置点光源就是为了让角色的头顶亮一下、脸白一点吗？更不要说启魂邪教总坛里那些支持实时反射的水晶石了？难道你宁可要这种华而不实的效果，都要让玩家的计算机耗费资源去支持它吗？景安正武盟门前的那条河的果冻绿材质就不能让美术想办法替换下吗？难道怕玩家不知道你是使用的 Unity3D 里的标准材质？\n下面是游戏中主角及配角的面部截图，颜值爆表啊！\n果然还是看脸的\r下面是《古剑奇谭二》、《穹之扉》、《仙剑六》三部游戏在特效全开的情况下的画面表现，相信高下立判了吧！\n古剑奇谭2最高画质\r穹之扉最高画质\r仙剑六最高画质\r建模 好了，下面说说建模的问题，我使用 disunity 对仙剑六的部分.unity3d 文件进行了解包，然后发现每个模型文件的包大概在 10M 左右，像太空步、循环动画、穿模这种问题我就不说了，反正每次说了你们又不打算改。我就来说说这个游戏里的模型吧？对三个模型进行了解包，然后发现这三个模型的单位都是不一样的，就是说在 scale 为 1、1、1 的时候三个模型的大小是不一样的；其次模型的角度需要手动改为-90,180,0，我觉得建模的时候难道不应该制定相关的规范吗？我觉得从 Max、Maya 里导出到 FBX 到 Unity3D 的时候难道不应该规范单位、角度和中心点位置吗？我从来不认为游戏引擎就是美术把模型做好了给程序用就行了，我觉得美术在建模的时候更应该去关注模型在这个引擎下的渲染效果，如怎么调整材质、怎么打灯光等等的问题，这些问题不应该推给程序而且不能推给程序。从模型贴图来看，美术想到了诸如法线贴图等等的次世代特性，可是到了实际使用的时候，我看到的结果的是整个游戏里基本清一色的使用了 Diffuse 着色器，那请问这样做这些贴图有什么意义？既然你根本用不到为什么还要放到游戏里？而且我在模型文件中经常看到诸如 Object01 或者 A_toufa、B_yifu 这样的命名，我是一个程序员，对命名比较敏感，我觉得出现汉语拼音式的命名，说明建模的人是特别不专业的。然后我想说的是这次整体美术风格的问题，难道大家不觉得天晴之海的建筑风格偏欧式了吗？这是一个中国的古风游戏啊！难道大家不觉得盈辉堡的道路和房子都是一样的颜色吗？我在地图里转了半天才找到路啊！此次的配角如赢旭危和朔漩的建模普遍要比主角团好看多了，难道你们要开始学《古剑奇谭二》在游戏中潜伏隐藏主角团吗？我不知道一个 2015 年的游戏出现 NPC 配音时嘴巴不动是出于什么考虑？NPC 不重要吗？NPC 戏份没有主角团多可以忍、长得没有主角团帅可以忍，可是你剥夺人家说话的权利是什么鬼？还有骆驼移动的时候没有移动动画直接悬空移动又是想干什么？一个骆驼值得你使用刚体这样的移动方式去移动吗？关于游戏读条慢的情况，我自己测试了下、同时找了相关的资料去查阅，Unity3D 场景的异步加载的确有坑存在，可是我相信只要运用合适的方法是可以规避这个问题的，因为目前仅仅解了部分 AssetBundle 包的内容，所以对程序内部的一些东西还有待确定，等确定后会继续更新到这里。\n朔璇模型\r赢旭危模型\r剧情 剧情、配音、配乐这里放到一起说，因为这是仙剑六引以为豪的地方，此次的剧情主线有两条，即双越身世之谜和洛家双生子早逝之谜，将这两条线交织在一起的是横道众和柷敔间的矛盾冲突，这样的设定明显是继承了仙五前的多线程叙事方式，这样的叙事方式应该是值得肯定的。但是我不能理解整个游戏到底是以谁为叙事中心的，正如仙剑五是以主角姜云凡为叙事中心的，他所看到的一切推进着整个剧情的深入，再入仙剑五前传是以主角夏侯瑾轩为叙事中心的，围绕着为姜承洗刷冤屈、为瑕妹治病两条主线将所有相关的人或事联系了起来。可是仙剑六我真没看出来是以谁为中心的，整个主角团是仙剑史上最冷漠、最分裂的团队，将大家联系到一起的唯一理由就是存在感爆表的神农九泉，然而这并没有什么卵用，大家都是站在自己的立场上做着自己关心的事情。\n比如越今朝是霸道总裁“只有我一个人可以叫你祈”。一路上不是摸头就是捏脸，可惜手压根没有放到脸上去；再比如越祈是天真傻“我听今朝的”。一路上吃面吃得我都饿了，可是那碗鸡蛋面就是一张贴图啊，吃半天空气最后居然吃完了，我要向仙剑六的四位程序员致敬；再比如闲卿是典型的双标狗，一面要讨好老婆洛昭言和世侄小绣儿，一面还要做出一副闲适淡泊的样子，我都忍不住要为你的演技点赞；再比如耳光绣明绣，我觉得要么是美术和策划有仇，故意将这样一个凶狠的角色画成甜美可人的女神范儿，要么就是编剧经常看琼瑶剧比较热衷于打人耳光，一个武侠游戏有什么不满直接亮兵器不就好了，要是当年月如被逍遥在扬州城外欺负了直接打李逍遥一个耳光，我觉得这个角色恐怕要失去不少忠实粉丝吧;再比如说技术宅居士方这货总是一副“你们都是对的，怪我咯”的态度，我至今都想不明白他有什么不对的地方，既然大家都不拿你当朋友，你凭什么要为这样一群人牺牲豆包啊。我一直喜欢仙剑营造的那种朋友间比较温暖的情感，比如仙剑四里小紫英一句“承君此诺必守一生”就会让人觉得温暖，即使以后大家分开了彼此的心中还可以相互牵挂。可是仙剑六呢，那晚大家做一起赏月喝酒本来应该是彼此相互了解和认识的机会，结果大家都忙着去约会了，留下居十方一个人在哪里喝闷酒，甚至他喝醉了酒吐露心事主角团中竟然无一人听见，我严重怀疑编剧每次和同学聚会的时候都是那个抢着麦克风嘶吼却从来不会有人去安慰他的那个人，编剧啊，己所不欲勿施于人啊。我一直认为一个 RPG 游戏的核心在于代入感，就是说你要让玩家觉得他就是游戏中的主角。比如我们玩仙剑一的时候就感觉自己是李逍遥，仙五前谢叔单挑姜世离的时候我们就感觉自己是谢叔，这就是代入感。\n可是仙剑六呢，居然巧妙的避开了这一点，搞得从头到尾都像在看电影，不，应该是叫做在看幻灯片。我不知道仙六是不是借鉴了《古剑奇谭二》的叙事方式，整个叙述视角更像是以上帝俯视人间的视角在讲整个故事，如果说《古剑奇谭二》成就了流月城，那么仙剑六便成就了衡道众，而且编剧觉得为了和《古剑奇谭二》拉开差距，刻意让站在对立面的衡道众认识到自己的错误并对主角一行人提供了补偿。我承认，这让仙剑六在立意上有了深度，可是我接下来要说的就是你们的不对了。我们玩仙剑一的时候比武招亲、蜀山剑法、林家绝学、苗疆蛊术、五灵仙术我们从来不会觉得存在违和感，因为这些东西都是东方文化中已有或者说可以找到起源追朔的东西，可是仙剑六的编剧你告诉我整个仙剑六除了鲲鹏能够在庄子的《逍遥游》中找到记载以外，其他的这些是中国传统文化存在的吗？是，时空穿越早就有了，可是回魂仙梦和血濡回魂都无法改变已经发生的事情；是，在天上飞早就有了，可是蜀山仙剑派御剑飞行早在武侠小说、志怪小说中有记载，所以蜀山的御剑术不会存在丝毫的违和感，可是你搞个二十一世纪都未必有的飞行器是什么鬼，古时候尝试上天的人最多是在一个椅子上捆满火药，希望通过反冲力飞到天上去，结果为科学事业献身了，编剧你告诉我这是什么鬼。我真傻，我单单知道黑科技会在仙剑剧里出现，却不知道有一天会在仙剑游戏里出现，你告诉我御界枢的人都是外星人吗？我们使用智能手机、平板电脑不过四五年的样子，编剧你告诉我衡道众里的人是怎么做到的，他们是从未来穿越过去的嘛？好了，下面请允许我替历代仙剑中因为剧情需要而牺牲的各位男主角、女主角、男配角、女配角、小怪以及 Boss 说句公道话，为什么六代的人可以通过交换实现“不死”的愿望，而六代以前的就只能领便当？我知道编剧一定会说，因为这次我们采用了全新的以神农为中心的世界观，可是编剧好像忘了神农和女娲差不多是同时在宇宙中产生的吧？我觉得五代的 Boss 魔翳比较冤枉，冒着做坏人的危险、拼着命为魔界找来了水源，结果你说九泉之一的热海同样可以产生水源，我原本只要伤害洛埋名一人就可以取得水源，结果就因为你这奇怪的设定，五代造就了仙剑史上最大的牺牲，编剧啊编剧原来你是真正的幕后黑手，神马黑包子各种连携技全都弱爆了好吗？你把仙剑六的故事设定到仙五前的五六十年里难道不担心这个世界的变化跟不上你的节奏吗？编剧你一句话就让蜀山派这样的神权天授、依靠盘古之心存在于世间的正派组织荡然无存啊，你告诉我御剑术都在江湖上失传了，这是摆明了以后不会再出现蜀山或者御剑术的节奏吗？我乐意看到仙剑六在世界观上的变化，可是这个新的世界观应该是原来以女娲为中心的世界观的一种补充而非推翻啊，你提出了神农九泉的概念，我觉得这个设定可以让仙剑的题材变得新颖些，然并卵这一次就把九泉的故事差不多都讲完了，是想等下一部游戏立项的时候再次推翻这次的设定，编剧啊，你到底是来挖坑的还是扩展仙剑的游戏世界观的啊！\n仙剑六黑科技\r游戏性 下面我们来重点说说游戏性。你问我为什么要说游戏性啊？一个游戏、一个商业游戏不提游戏性你觉得提什么呢？首先我想强调一个观点，认为仙剑六只要剧情好就行了的朋友请向姚仙建议将仙剑做成一个动漫或者电影，这样大家连自动战斗都不用点了对吧！仙剑六的突破挺多的，可惜注重了量而不注重质，这样平均下来仙剑六的突破其实很少很少。首先，我们来说说开放地图的问题，因为地图开放了玩家可以自由探索的地方就多了，可是你要真的想做好开放地图，就应该认真的去设计空气墙而不是等玩家掉坑里出来的时候打开游戏菜单重新回到原点。因为你们在设置空气墙的时候不用心，在过饮马河和去落日部的路上，比如祈妹的隔空移物和今朝的凌空飞剑，我不会告诉你我是直接从两边的石头上跳过去的。抓猫是挺好玩的，可是你告诉我玩家站在树顶纹丝不动、走绳子如履平地是什么鬼，在没有对 Unity3D 内部集成的 Physic 物理引擎进行完全充分的了解的情况下，贸然使用这样的技术你确定你能驾驭得好吗？浮金堂我跳了一个下午没有跳过去，然后跳出各种 Bug，我终于明白这次为什么有人能玩到 70 个小时以上啦，恭喜北软你们终于知道了怎样延长一个游戏的时间。这次的开机关让居十方都觉得郁闷，因为每次需要开机关的时候都会提示“开机关这种事情还是让十方去做吧!”，当我终于庆幸有用得着十方的时候，这下轮到我郁闷了，难道小游戏就不能给点提示吗？我总得知道自己要做什么吧！划船我再转了不知多久以后才明白过来怎么控制船的移动方向和角度，这种小问题难道每次都要让大家说吗？这次的迷宫设计有了层次感和立体感，比如启魂邪教总坛的迷宫和机关设计得都不错，天晴之海得迷宫设计得比较好，然而我最喜欢的是与青山这个场景！\n我就放个图，不说话！\n浮金堂跳跃Bug\r好了，说完这些小游戏，我们来说说这次仙剑六的战斗系统。我想知道，究竟是什么样的一种考量让你们选择去模仿 FF13 的战斗模式，难道是为了刻意和《古剑奇谭二》有所不同？然并卵，这次的战斗系统糟糕透了。首先，我是希望仙剑的战斗系统慢慢地向着即时制的方向发展的，因为这是现在的大势所趋吧，尤其是《古剑奇谭》、《雨血》和《御天降魔传》这类游戏正在引领着大家的兴趣往即时方向转变，在这样的背景下仙剑积极地向即时制转变从某种程度上来讲是一种不得不采取的防御性措施。可是我有教过你用一个伪即时制的战斗系统来欺骗大家的感情吗？以前大家对排站好在每个回合里我们可以依次控制多名角色，然后依次释放技能，通过不同角色间策略的调整来将游戏进行下去；现在大家迈着太空步，在每个回合里我们可以控制一名角色，每次可以发动多次行动，其他角色由 AI 控制，然后场景中各种粒子特效乱飞。当我看到粒子特效贴图的矩形边框时，我的内心是奔溃的。你告诉我这样的战斗系统和回合制有什么区别？当初主企划说为了让大家更好的观赏战斗画面特意将 UI 做到了右下角，可是你告诉我在一个即时制的游戏里用眼角的余光扫视右下角然后用滚动条从一堆物品中选择需要的物品该有多蛋疼，等你选好了，队友或者玩家可能已经死了。所以我们的数值策划为明绣配置逆天的治疗数值，这样一来大家就不用吃药了。呵呵，是你们该吃药了吧，你告诉我一个游戏玩到现在我都没记住几个技能的名称，以后问起来大家提到万剑诀、天剑、酒神、真元护体、天罡战气、五灵归宗、乾坤一掷、气疗术、气指剑、万物归烬、仙风云体、千方残光剑等等经典招式的时候，我希望你们不要说我们厚此薄彼就好。我相信有好多妹子已经习惯开着自动战斗直接看剧情的习惯了吧，如果这样仙剑还不如买小说或者拍动漫呢，正好这次所有的过场动画都是 2D 动画的形式，可是你告诉我 2D 动画是一种风格、3D 建模是一种风格、小剧情表情是一种风格，一个游戏里三种风格，你是打算同时照顾动漫和游戏两个不同群体的玩家吗？然并卵，你这样做了不见得人家会领情，人家会说你抄袭、撞梗，你说你又是何苦呢？我给仙剑六战斗系统提点意见吧，希望可以支持玩家自定义快捷键，比如玩家可以挑选自己喜欢的技能和常用的物品，每次使用都会消耗行动点数，行动点数目消耗完了就触发技能动画，这样至少可以让点鼠标变得高端些，就像英雄联盟说白了就是 Q、W、E 三个键各种按，可是你同样可以装 X 地说这里面涉及到走位和意识。对了，灵脉系统界面能不能点击了以后不要放大，你觉得那样真的好看吗？再说三种培养方式我非得一条路走到黑？\n战斗系统截图1\r战斗系统截图2\r好了，熬夜到凌晨三点写完这篇文章，我对仙剑绝对是真爱，我知道一定会有许多人来吐槽我写的这篇吐槽，可我想说的是：你要真的爱它就别总是惯着它，真正的爱从来都不是溺爱！在官方放出第三版补丁后，整个游戏的优化得到了较好的提升，从感官上像个游戏了，如果有朋友还在徘徊不定，不妨在这个时候尝试下吧！以上观点，一家之言，不足为据！\n","date":"2015-07-24T09:21:20Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1118169753/","slug":"1118169753","tags":["仙剑奇侠传","RPG","Unity3D","游戏"],"title":"《仙剑奇侠传六》游戏感言"},{"categories":["游戏开发"],"content":"各位朋友大家好，欢迎大家关注我的博客，我是秦元培，我是博客地址是http://blog.csdn.net/qinyuanpei。在经历了一段时间的忙碌后，博主终于有时间来研究新的东西啦，今天博客向和大家一起交流的内容是在 Unity3D 游戏开发中使用 SQLite 进行数据库开发，坦白来讲，在我的技术体系中 Web 和数据库是相对薄弱的两个部分，因此正好这段时间项目需要和服务器、数据库进行交互，因此在接下来的文章中博主可能会更加倾向于讲解这方面的内容，希望大家能够喜欢啊！\n什么是 SQLite？ SQLite是一款轻型的数据库，是遵守 ACID 的关系型数据库管理系统，它包含在一个相对小的 C 库中，以嵌入式作为它的设计目标，它占用资源非常的低，因此适合在嵌入式设备如 Android、Ruby on Rails 等中使用。它能够支持 Windows/Linux/Unix 等等主流的操作系统，同时能够跟和 C、C++、Ruby、Python、C#、PHP、Java 等编程语言相结合。SQLite 是一个以文件形式存在的关系型数据库，尽管无法实现分布式和横向扩展，可是作为一个轻量级的嵌入式数据库，它不需要系统提供服务支持，通过 SDK 直接操作文件避免了对数据库维护的相关事务，从这个角度来讲它是一个出色的数据库。\n为什么要选择 SQLite 好了，在了解了 SQLite 后，我们来了解下 SQLite 有哪些让我们心动的特性，或者说我们为什么要选择 SQLite，因为在这个世界上我们有太多的数据库可以选择，诸如 Oracle、MySQL、SQLServer、DB2、NoSQL、MongoDB 等等：\nACID 事务 零配置 – 无需安装和管理配置 储存在单一磁盘文件中的一个完整的数据库 数据库文件可以在不同字节顺序的机器间自由的共享 支持数据库大小至 2TB 足够小, 大致 13 万行 C 代码, 4.43M 比一些流行的数据库在大部分普通数据库操作要快\u0026mdash;SQLite 读写效率如此之高，会使用其他数据库的理由是？ 简单, 轻松的 API 包含 TCL 绑定, 同时通过 Wrapper 支持其他语言的绑定 良好注释的源代码, 并且有着90%以上的测试覆盖率 独立: 没有额外依赖 源码完全的开源, 你可以用于任何用途, 包括出售它 支持多种开发语言，C, C++, PHP, Perl, Java, C#,Python, Ruby 等 Unity3D 中的 SQLite 在 Unity3D 中使用 SQLite，我们首先要明白这样一件事情，即我们这里的使用的 SQLite 并非是通常意义上的 SQLite.NET,而是经过移植后的 Mono.Data.Sqlite。因为 Unity3D 基于 Mono，因此使用移植后的 Mono.Data.Sqlite 能够减少我们的项目在不同平台上出现各种各样的问题。在 Unity3D 中使用的 SQLite 以 Mono.Data.Sqlite.dll 即动态链接库的形式给出，因此我们需要将这个文件放置在项目目录下的 Plugins 文件夹中，此外我们需要 System.Data.dll 或者 Mono.Data.dll 这两个文件添加到 Plugins 目录中，因为我们需要的部分数据相关的 API 或者类都定义在这两个文件当中，这些文件可以从这里直接下载。\nPS：博主注意到在网上有使用 Mono.Data.SQLiteClient.dll 这个库实现在 Unity3D 操作 SQLite 数据库的相关文章，博主大概看了下，感觉和使用 Mono.Data.Sqlite.dll 这个库大同小异，大家喜欢哪个就用哪个吧！哈哈！博主在开源社区找到一个版本库，据说可以同时支持.NET 和 Mono，如果大家感兴趣欢迎大家去测试啊，哈哈!\n在正式开始写代码前，我们首先来回顾下通常情况下数据库读写的基本流程吧！\n定义数据库连接字符串(ConnectionString)完成数据库连接的构造，建立或者打开一个数据库。 定义相关的 SQL 命令(Command)通过这些命令实现对数据库的增加、删除、更新、读取四种基本功能。 在完成各种数据库操作后及时关闭数据库连接，解除对数据库的连接和引用。 SQLite 作为一款优秀的数据库，在为其编写数据库相关代码时同样遵循这样的流程，考虑到对数据库的增加、删除、更新、读取四种操作具有类似性和统一性，因此在动手写 Unity3D 脚本前，首先让我们来编写一个 SQLite 的辅助类 SQLiteHelper.cs。该类代码定义如下：\nusing UnityEngine; using System.Collections; using Mono.Data.Sqlite; using System; public class SQLiteHelper { /// \u0026lt;summary\u0026gt; /// 数据库连接定义 /// \u0026lt;/summary\u0026gt; private SqliteConnection dbConnection; /// \u0026lt;summary\u0026gt; /// SQL命令定义 /// \u0026lt;/summary\u0026gt; private SqliteCommand dbCommand; /// \u0026lt;summary\u0026gt; /// 数据读取定义 /// \u0026lt;/summary\u0026gt; private SqliteDataReader dataReader; /// \u0026lt;summary\u0026gt; /// 构造函数\t/// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;connectionString\u0026#34;\u0026gt;数据库连接字符串\u0026lt;/param\u0026gt; public SQLiteHelper(string connectionString) { try{ //构造数据库连接 dbConnection=new SqliteConnection(connectionString); //打开数据库 dbConnection.Open(); }catch(Exception e) { Debug.Log(e.Message); } } /// \u0026lt;summary\u0026gt; /// 执行SQL命令 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The query.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;queryString\u0026#34;\u0026gt;SQL命令字符串\u0026lt;/param\u0026gt; public SqliteDataReader ExecuteQuery(string queryString) { dbCommand = dbConnection.CreateCommand(); dbCommand.CommandText = queryString; dataReader = dbCommand.ExecuteReader(); return dataReader; } /// \u0026lt;summary\u0026gt; /// 关闭数据库连接 /// \u0026lt;/summary\u0026gt; public void CloseConnection() { //销毁Command if(dbCommand != null){ dbCommand.Cancel(); } dbCommand = null; //销毁Reader if(dataReader != null){ dataReader.Close(); } dataReader = null; //销毁Connection if(dbConnection != null){ dbConnection.Close(); } dbConnection = null; } /// \u0026lt;summary\u0026gt; /// 读取整张数据表 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The full table.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;tableName\u0026#34;\u0026gt;数据表名称\u0026lt;/param\u0026gt; public SqliteDataReader ReadFullTable(string tableName) { string queryString = \u0026#34;SELECT * FROM \u0026#34; + tableName; return ExecuteQuery (queryString); } /// \u0026lt;summary\u0026gt; /// 向指定数据表中插入数据 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The values.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;tableName\u0026#34;\u0026gt;数据表名称\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;values\u0026#34;\u0026gt;插入的数值\u0026lt;/param\u0026gt; public SqliteDataReader InsertValues(string tableName,string[] values) { //获取数据表中字段数目 int fieldCount=ReadFullTable(tableName).FieldCount; //当插入的数据长度不等于字段数目时引发异常 if(values.Length!=fieldCount){ throw new SqliteException(\u0026#34;values.Length!=fieldCount\u0026#34;); } string queryString = \u0026#34;INSERT INTO \u0026#34; + tableName + \u0026#34; VALUES (\u0026#34; + values[0]; for(int i=1; i\u0026lt;values.Length; i++) { queryString+=\u0026#34;, \u0026#34; + values[i]; } queryString += \u0026#34; )\u0026#34;; return ExecuteQuery(queryString); } /// \u0026lt;summary\u0026gt; /// 更新指定数据表内的数据 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The values.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;tableName\u0026#34;\u0026gt;数据表名称\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;colNames\u0026#34;\u0026gt;字段名\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;colValues\u0026#34;\u0026gt;字段名对应的数据\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;key\u0026#34;\u0026gt;关键字\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;value\u0026#34;\u0026gt;关键字对应的值\u0026lt;/param\u0026gt; public SqliteDataReader UpdateValues(string tableName,string[] colNames,string[] colValues,string key,string operation,string value) { //当字段名称和字段数值不对应时引发异常 if(colNames.Length!=colValues.Length) { throw new SqliteException(\u0026#34;colNames.Length!=colValues.Length\u0026#34;); } string queryString = \u0026#34;UPDATE \u0026#34; + tableName + \u0026#34; SET \u0026#34; + colNames[0] + \u0026#34;=\u0026#34; + colValues[0]; for(int i=1; i\u0026lt;colValues.Length; i++) { queryString+=\u0026#34;, \u0026#34; + colNames[i] + \u0026#34;=\u0026#34; + colValues[i]; } queryString += \u0026#34; WHERE \u0026#34; + key + operation + value; return ExecuteQuery(queryString); } /// \u0026lt;summary\u0026gt; /// 删除指定数据表内的数据 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The values.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;tableName\u0026#34;\u0026gt;数据表名称\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;colNames\u0026#34;\u0026gt;字段名\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;colValues\u0026#34;\u0026gt;字段名对应的数据\u0026lt;/param\u0026gt; public SqliteDataReader DeleteValuesOR(string tableName,string[] colNames,string[] operations,string[] colValues) { //当字段名称和字段数值不对应时引发异常 if(colNames.Length!=colValues.Length || operations.Length!=colNames.Length || operations.Length!=colValues.Length) { throw new SqliteException(\u0026#34;colNames.Length!=colValues.Length || operations.Length!=colNames.Length || operations.Length!=colValues.Length\u0026#34;); } string queryString = \u0026#34;DELETE FROM \u0026#34; + tableName + \u0026#34; WHERE \u0026#34; + colNames[0] + operations[0] + colValues[0]; for(int i=1; i\u0026lt;colValues.Length; i++) { queryString+=\u0026#34;OR \u0026#34; + colNames[i] + operations[0] + colValues[i]; } return ExecuteQuery(queryString); } /// \u0026lt;summary\u0026gt; /// 删除指定数据表内的数据 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The values.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;tableName\u0026#34;\u0026gt;数据表名称\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;colNames\u0026#34;\u0026gt;字段名\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;colValues\u0026#34;\u0026gt;字段名对应的数据\u0026lt;/param\u0026gt; public SqliteDataReader DeleteValuesAND(string tableName,string[] colNames,string[] operations,string[] colValues) { //当字段名称和字段数值不对应时引发异常 if(colNames.Length!=colValues.Length || operations.Length!=colNames.Length || operations.Length!=colValues.Length) { throw new SqliteException(\u0026#34;colNames.Length!=colValues.Length || operations.Length!=colNames.Length || operations.Length!=colValues.Length\u0026#34;); } string queryString = \u0026#34;DELETE FROM \u0026#34; + tableName + \u0026#34; WHERE \u0026#34; + colNames[0] + operations[0] + colValues[0]; for(int i=1; i\u0026lt;colValues.Length; i++) { queryString+=\u0026#34; AND \u0026#34; + colNames[i] + operations[i] + colValues[i]; } return ExecuteQuery(queryString); } /// \u0026lt;summary\u0026gt; /// 创建数据表 /// \u0026lt;/summary\u0026gt; + /// \u0026lt;returns\u0026gt;The table.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;tableName\u0026#34;\u0026gt;数据表名\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;colNames\u0026#34;\u0026gt;字段名\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;colTypes\u0026#34;\u0026gt;字段名类型\u0026lt;/param\u0026gt; public SqliteDataReader CreateTable(string tableName,string[] colNames,string[] colTypes) { string queryString = \u0026#34;CREATE TABLE \u0026#34; + tableName + \u0026#34;( \u0026#34; + colNames [0] + \u0026#34; \u0026#34; + colTypes [0]; for (int i=1; i\u0026lt;colNames.Length; i++) { queryString+=\u0026#34;, \u0026#34; + colNames[i] + \u0026#34; \u0026#34; + colTypes[i]; } queryString+= \u0026#34; ) \u0026#34;; return ExecuteQuery(queryString); } /// \u0026lt;summary\u0026gt; /// Reads the table. /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The table.\u0026lt;/returns\u0026gt; /// \u0026lt;param name=\u0026#34;tableName\u0026#34;\u0026gt;Table name.\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;items\u0026#34;\u0026gt;Items.\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;colNames\u0026#34;\u0026gt;Col names.\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;operations\u0026#34;\u0026gt;Operations.\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;colValues\u0026#34;\u0026gt;Col values.\u0026lt;/param\u0026gt; public SqliteDataReader ReadTable(string tableName,string[] items,string[] colNames,string[] operations, string[] colValues) { string queryString = \u0026#34;SELECT \u0026#34; + items [0]; for (int i=1; i\u0026lt;items.Length; i++) { queryString+=\u0026#34;, \u0026#34; + items[i]; } queryString += \u0026#34; FROM \u0026#34; + tableName + \u0026#34; WHERE \u0026#34; + colNames[0] + \u0026#34; \u0026#34; + operations[0] + \u0026#34; \u0026#34; + colValues[0]; for (int i=0; i\u0026lt;colNames.Length; i++) { queryString+=\u0026#34; AND \u0026#34; + colNames[i] + \u0026#34; \u0026#34; + operations[i] + \u0026#34; \u0026#34; + colValues[0] + \u0026#34; \u0026#34;; } return ExecuteQuery(queryString); } } SQLiteHelper 类主要实现了数据库、数据表的创建以及数据表中记录的增加、删除、更新、读取四种基本功能。该类最初由国外的 Unity3D 开发者发布在Unity3D 官方论坛,后来经宣雨松使用C#进行重写，我在此基础上进行了完善，再此对两位大神的无私付出表示感谢。这里要说明的有三点：\n一、在 Unity3D 编辑器下生成数据库文件(.db)默认位于和 Assets 目录同级的位置，即项目的工程文件夹中。我们可以通过修改路径在改变数据库文件的存储位置，具体来讲： Windows 平台：data source=Application.dataPath/数据库名称.db IOS 平台：data source=Application.persistentDataPath/数据库名称.db Android 平台：URL=file:Application.persistentDataPath/数据库名称.db(我想说 Android 平台就是个奇葩，搞什么特殊化嘛)\n二、确保 Unity3D 编辑器中的.NET 版本和 MonoDevelop 中的.NET 版本都为 2.0 版本，在 Unity3D 中打包导出的程序可能不会保留数据库文件，因此需要手动将数据库文件拷贝到相应的位置，当然更加合理的方案是将数据库文件存放到 StreamingAssets 文件夹下，然后在第一次加载游戏的时候将数据库文件复制到对应平台上的存放位置。\n三、在使用 InsertValues 方法时请参考 SQLite 中字段类型与 C#中数据类型的对应关系，博主目前测试了 int 类型和 string 类型都没有什么问题，更多类型的数据请大家自行测试然后告诉博主测试的结果，如果大家有兴趣扩展这个辅助类的话可以自行去扩展哦，嘿嘿！\n好了，千呼万唤始出来的时候到了，下面我们以一个实例来完成今天的项目讲解，因为我们已经定义好了 SQLite 的辅助类，因此我们可以快速地编写出下面的脚本代码：\nusing UnityEngine; using System.Collections; using System.IO; using Mono.Data.Sqlite; public class SQLiteDemo : MonoBehaviour { /// \u0026lt;summary\u0026gt; /// SQLite数据库辅助类 /// \u0026lt;/summary\u0026gt; private SQLiteHelper sql; void Start () { //创建名为sqlite4unity的数据库 sql = new SQLiteHelper(\u0026#34;data source=sqlite4unity.db\u0026#34;); //创建名为table1的数据表 sql.CreateTable(\u0026#34;table1\u0026#34;,new string[]{\u0026#34;ID\u0026#34;,\u0026#34;Name\u0026#34;,\u0026#34;Age\u0026#34;,\u0026#34;Email\u0026#34;},new string[]{\u0026#34;INTEGER\u0026#34;,\u0026#34;TEXT\u0026#34;,\u0026#34;INTEGER\u0026#34;,\u0026#34;TEXT\u0026#34;}); //插入两条数据 sql.InsertValues(\u0026#34;table1\u0026#34;,new string[]{\u0026#34;\u0026#39;1\u0026#39;\u0026#34;,\u0026#34;\u0026#39;张三\u0026#39;\u0026#34;,\u0026#34;\u0026#39;22\u0026#39;\u0026#34;,\u0026#34;\u0026#39;Zhang3@163.com\u0026#39;\u0026#34;}); sql.InsertValues(\u0026#34;table1\u0026#34;,new string[]{\u0026#34;\u0026#39;2\u0026#39;\u0026#34;,\u0026#34;\u0026#39;李四\u0026#39;\u0026#34;,\u0026#34;\u0026#39;25\u0026#39;\u0026#34;,\u0026#34;\u0026#39;Li4@163.com\u0026#39;\u0026#34;}); //更新数据，将Name=\u0026#34;张三\u0026#34;的记录中的Name改为\u0026#34;Zhang3\u0026#34; sql.UpdateValues(\u0026#34;table1\u0026#34;, new string[]{\u0026#34;Name\u0026#34;}, new string[]{\u0026#34;\u0026#39;Zhang3\u0026#39;\u0026#34;}, \u0026#34;Name\u0026#34;, \u0026#34;=\u0026#34;, \u0026#34;\u0026#39;张三\u0026#39;\u0026#34;); //插入3条数据 sql.InsertValues(\u0026#34;table1\u0026#34;,new string[]{\u0026#34;3\u0026#34;,\u0026#34;\u0026#39;王五\u0026#39;\u0026#34;,\u0026#34;25\u0026#34;,\u0026#34;\u0026#39;Wang5@163.com\u0026#39;\u0026#34;}); sql.InsertValues(\u0026#34;table1\u0026#34;,new string[]{\u0026#34;4\u0026#34;,\u0026#34;\u0026#39;王五\u0026#39;\u0026#34;,\u0026#34;26\u0026#34;,\u0026#34;\u0026#39;Wang5@163.com\u0026#39;\u0026#34;}); sql.InsertValues(\u0026#34;table1\u0026#34;,new string[]{\u0026#34;5\u0026#34;,\u0026#34;\u0026#39;王五\u0026#39;\u0026#34;,\u0026#34;27\u0026#34;,\u0026#34;\u0026#39;Wang5@163.com\u0026#39;\u0026#34;}); //删除Name=\u0026#34;王五\u0026#34;且Age=26的记录,DeleteValuesOR方法类似 sql.DeleteValuesAND(\u0026#34;table1\u0026#34;, new string[]{\u0026#34;Name\u0026#34;,\u0026#34;Age\u0026#34;}, new string[]{\u0026#34;=\u0026#34;,\u0026#34;=\u0026#34;}, new string[]{\u0026#34;\u0026#39;王五\u0026#39;\u0026#34;,\u0026#34;\u0026#39;26\u0026#39;\u0026#34;}); //读取整张表 SqliteDataReader reader = sql.ReadFullTable (\u0026#34;table1\u0026#34;); while(reader.Read()) { //读取ID Debug.Log(reader.GetInt32(reader.GetOrdinal(\u0026#34;ID\u0026#34;))); //读取Name Debug.Log(reader.GetString(reader.GetOrdinal(\u0026#34;Name\u0026#34;))); //读取Age Debug.Log(reader.GetInt32(reader.GetOrdinal(\u0026#34;Age\u0026#34;))); //读取Email Debug.Log(reader.GetString(reader.GetOrdinal(\u0026#34;Email\u0026#34;))); } //读取数据表中Age\u0026gt;=25的所有记录的ID和Name reader = sql.ReadTable (\u0026#34;table1\u0026#34;, new string[]{\u0026#34;ID\u0026#34;,\u0026#34;Name\u0026#34;}, new string[]{\u0026#34;Age\u0026#34;}, new string[]{\u0026#34;\u0026gt;=\u0026#34;}, new string[]{\u0026#34;\u0026#39;25\u0026#39;\u0026#34;}); while(reader.Read()) { //读取ID Debug.Log(reader.GetInt32(reader.GetOrdinal(\u0026#34;ID\u0026#34;))); //读取Name Debug.Log(reader.GetString(reader.GetOrdinal(\u0026#34;Name\u0026#34;))); } //自定义SQL,删除数据表中所有Name=\u0026#34;王五\u0026#34;的记录 sql.ExecuteQuery(\u0026#34;DELETE FROM table1 WHERE NAME=\u0026#39;王五\u0026#39;\u0026#34;); //关闭数据库连接 sql.CloseConnection(); } } 在上面的代码中我们是在 Start 方法中创建了数据库和数据表，然而在实际使用中我们需要判断数据库和数据表是否存在，因此如果你使用这段脚本提示错误信息，请确保数据库和数据表是否已经存在。好了，下面的截图展示了程序运行的结果：\n数据库效果演示\rUnity3D效果展示\r作为一个强大的数据库怎么能没有图形化的数据库管理工具呢？所以这里博主向大家推荐一个免安装的小工具 SqliteStudio，使用这个工具可以帮助我们方便地管理 Sqlite 数据库里的数据，这样是不是比较方便呢？哈哈！这个工具可以从这里下载哦！\nSQLiteStudio界面演示\r好了，今天的内容就是这样了，为了写这篇文章花了三个晚上准备，希望大家喜欢啊！如果大家觉得这篇文章有用，请继续关注我的博客，我是秦元培，我的博客地址是http://blog.csdn.net/qinyuanpei。\n2015 年 11 月 3 日更新内容如下：不同平台上的数据库存储路径\n//各平台下数据库存储的绝对路径(通用) //PC：sql = new SQLiteHelper(\u0026#34;data source=\u0026#34; + Application.dataPath + \u0026#34;/sqlite4unity.db\u0026#34;); //Mac：sql = new SQLiteHelper(\u0026#34;data source=\u0026#34; + Application.dataPath + \u0026#34;/sqlite4unity.db\u0026#34;); //Android：sql = new SQLiteHelper(\u0026#34;URI=file:\u0026#34; + Application.persistentDataPath + \u0026#34;/sqlite4unity.db\u0026#34;); //iOS：sql = new SQLiteHelper(\u0026#34;data source=\u0026#34; + Application.persistentDataPath + \u0026#34;/sqlite4unity.db\u0026#34;); //PC平台下的相对路径 //sql = new SQLiteHelper(\u0026#34;data source=\u0026#34;sqlite4unity.db\u0026#34;); //编辑器：Assets/sqlite4unity.db //编译后：和AppName.exe同级的目录下，这里比较奇葩 //当然可以用更随意的方式sql = new SQLiteHelper(\u0026#34;data source=\u0026#34;D://SQLite//sqlite4unity.db\u0026#34;); //确保路径存在即可否则会发生错误 //如果是事先创建了一份数据库 //可以将这个数据库放置在StreamingAssets目录下然后再拷贝到 //Application.persistentDataPath + \u0026#34;/sqlite4unity.db\u0026#34;路径即可 ","date":"2015-07-09T09:47:06Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/582264328/","slug":"582264328","tags":["Unity3D","SQLite","数据库"],"title":"Unity3D 游戏开发之 SQLite 让数据库开发更简单"},{"categories":["Unity3D"],"content":"各位朋友，大家好，欢迎大家关注我的博客，我是秦元培，我的独立博客地址是：http://blog.yuanpei.me、CSDN 博客地址是：http://blog.csdn.net/qinyuanpei。今天我想和大家聊聊 Unity3D 游戏项目的版本控制。\n为什么要进行版本控制 当我一个人写代码的时候，在我的脑海中是不存在版本控制这个概念的，因为我对整个项目的代码如数家珍。可是当我和一群人在一起写代码的时候，我可能并不会清楚团队中有谁修改了哪一行代码，即使是一个变量的名称或者是一个函数的名称，在我毫不知情的情况下，可能这样的修改会使得程序无法运行，这个时候我需要版本控制；尽管 Unity3D 是一个适合小团队开发的游戏引擎，可是即使再小的团队同样会有不同的分工，当大家需要将各自的工作合并到一个完整的项目中的时候，这个时候我需要版本控制；当我需要了解团队成员实际的编程能力的时候，最好的方法是让他们参与到一个项目的开发中，这样我可以从他提交代码的情况了解他的工作能力，这个时候我需要版本控制；当我希望时时刻刻对项目进行备份，并在某一个关键的时刻将项目恢复到一个正确的状态的时候，复制、黏贴不会让这个工作变得简单，这个时候我需要版本控制。\n怎样在 Unity3D 中进行版本控制 在 Unity3D 中进行版本控制主要针对 Assets 和 ProjectSetting 这两个文件夹，因为除此以外的文件和文件夹都是 Unity3D 在运行过程中产生的临时文件，这些文件会在使用 Unity3D 打开项目后重新生成，因此无需对这些文件或文件夹进行版本控制。好了，在了解了 Unity3D 版本控制中需要关注的主要内容后，接下来我们要关注的是怎样让版本控制的软件对我们提交的内容进行差异化识别，我们知道版本控制的一个核心任务就是将服务器上的文件和本地的文件进行比对，找出哪些文件是最新生成的、哪些文件是被修改过的等等。因此为了方便版本控制软件对文件进行比对，常常需要项目变动的这些因素转化为文本形式，如果熟悉 Github 的朋友应该知道，Github 中判断两个文件的差异就是根据文本(代码)来比较的，因此在 Unity3D 中使用版本控制同样需要遵循这个原则，好在 Unity3D 在管理 Unity3D 项目时已经考虑到了这一点，通常在对 Unity3D 项目进行版本控制的时候，我们需要做这样的事情：\n通过 Edit-\u0026gt;Project Settings-\u0026gt;Editor 菜单打开编辑器设置选项，将 Version Control 选项下的 Mode 设为 Visual Meta Files，这样 Unity3D 将为项目中的每个文件或者每个文件夹生成对应的.Meta 文件。该文件是一个文本文件，记录了对应文件的相关信息，版本控制软件可以以此来对文件版本进行对比和合并操作。\nUnity3D 中的资源默认是以二进制的形式进行组织的，这种组织方式对版本控制来说是不合适的，因此需要通过通过 Edit-\u0026gt;Project Settings-\u0026gt;Editor 菜单打开编辑器设置选项，将 Asset Serialization 下的 Mode 设为 Force Text。\n通过 Edit-\u0026gt;Prefences-\u0026gt;External Tools 找到 Revision Control Diff/Merge 选项，在安装了版本控制软件后可以在这里找到相关的选项，以博主为例，博主使用的是 TortoiseSVN，这里的选项是 TortoiseMegre。目前 Unity3D 支持的版本控制软件有 SourceGear DiffMerge、TKDiff、P4Megre、TortoiseMegre、WinMegre、PlasticSCM Megre。\n编辑器设置\r编辑器设置\r好了，在完成以上准备工作后，我们就可以开始进行 Unity3D 项目的版本控制了，目前在 Unity3D 中我们主要有以下三种方式来对 Unity3D 项目进行版本控制：\n使用 Asset Server 进行版本控制 Unity3D 的Asset Server是一个 Unity3D 内部集成的版本控制软件，它和我们熟知的 SVN 类似，适合在小团队内进行版本控制，这是一个收费软件，尽管在某些方面它甚至比 SVN 还要方便，不过在实际的项目中使用这个的还是比较少的，所以如果大家对这个感兴趣，可以从这里了解它的具体情况，这里我们不打算介绍这个软件的使用。\nUnity3D 游戏制作（四）——Asset Server 搭建\n【教程】Asset Server（联合开发）\n使用 Github 进行版本控制 使用 Github 进行版本控制时可以在 Git 仓库中添加一个.gitignore 文件来对项目中需要同步的文件进行过滤，在文章开始我们已经知道 Unity3D 项目的版本控制主要针对 Assets 和 ProjectSetting 这两个文件，因此.gitignore 的内容可以这样填写:\nLibrary/ Temp/ *.sln *.csproj *.sln *.userprefs *.unityproj *.DS_Store 这样每次提交文件的时候 Github 将忽略这些文件的更改。关于 Github 的使用及其相关命令可以查看这里：\n总结自己的 Git 常用命令\nGit 远程操作详解\nGithub 中每个仓库的容量限制为 1G，适合小项目的版本控制，对于大型项目的版本控制应该考虑使用 SVN。\n使用 SVN 进行版本控制 使用 SVN 进行版本控制时可以通过右键菜单将某些文件和文件夹添加到忽略的文件列表中，这样 SVN 在每次提交文件的时候将忽略这些文件的更改。这块儿其实和 Github 的.gitignore 是相同的。SVN 常用的软件组合是 TortoiseSVN(客户端)+VisualSVN Server(服务端)，具体内容请参考这 2 篇文章：SVN 使用教程总结和客户端 TortoiseSVN 的安装及使用方法\n小结 不管使用什么版本控制软件，建立相关的代码提交规范和流程控制规范都是必要的，因此在团队中应该有一个人负责对团队成员提交的代码进行审核和规范化，这样可以减少因为因为代码提交而产生的各种问题。好了，今天这篇文章先写到这里了，希望大家喜欢！\n","date":"2015-07-02T09:35:42Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1320325685/","slug":"1320325685","tags":["版本控制","Unity3D","SVN","Github"],"title":"Unity3D 游戏开发之从 Unity3D 项目版本控制说起"},{"categories":["生活感悟"],"content":"不知不觉已经在公司上班一个月了，在这一个月里每一天发生的事情是我平凡而普通的生活。作为一名有节操的程序员，当我大学的同学开始称我为程序员的时候，我知道我即将在这条路上踏下一个属于开始的足迹。和我大学的同学相比，可能我会显得幸运而孤独吧！我不用像他们一样到各种工厂里采样、监测，可是与此同时我会因为离大家越来越远而感到孤独。每天下班做公交车回到住处，简单地料理着我一个人的生活，不紧不慢却永远是一个人在摸黑赶路，这是我自己选择的路，我从来不曾后悔，即使在这段时间和美术各种闹别扭，我相信这些都会是暂时的，以后总会变得越来越好。\n第一份工作没有想象中的高大上，这是一个融合了 3D 漫游、Web 和电子商务的综合项目，可我想说我在努力地做好这件事情。当互联网+的概念被人们所熟知以后，传统行业和互联网的结合让人们对未来的生活充满了遐想，因为在这个过程中不断涌现出想要迫切进入互联网+时代的传统行业。可是当传统行业试图进入互联网行业的时候，我不知道传统行业经营者心中到底对互联网行业了解多少。我的第一份工作在家人的口中被演绎出了三个不同的版本，我想这就是传统行业对互联网行业认识的一种缩影吧，那就是传统行业并不了解互联网行业，当他们想要做互联网+的时候可能更多的是脑海中一闪而过的热情吧！\n我的一位朋友告诉我，当你处在传统行业和互联网行业的十字路口的时候，你首先要虚心地了解和掌握传统行业的运作模式，然后再尝试将其和互联网结合起来。可是更为实际的情况是当这些传统行业的经营者有了进入互联网行业的想法以后，可能并不会从互联网的行业来看待这个想法，甚至片面的认为这个项目已然成竹在胸我们需要的仅仅是两三个懂技术的人就好了。这种想法其实是特别可怕的，以公司为例，从我进公司以来，公司从未对即将要做的项目进行过技术上的评估和立项讨论，公司的大部分美术甚至都不知道有这样一个项目存在、更不知道做好的模型要运用到一个怎样的技术上去以及最终会以什么样的方式呈现给用户。我所看到的情况就是公司里没日没夜的做模型。我想这就是领导脑袋一热的结果吧，大概知道要做一个什么样的东西，可是对具体怎么实施这个项目、实施这个项目需要哪些资源却没有详细的思考。我进公司这么长时间，基本没有看到过成文的策划或者是方案，更多的时候是大家在一块儿做，然后做的过程中发现有什么问题再返回去改，领导的态度从来没有准，觉得什么东西可以借鉴过来就要求程序和美术去实现，计划朝令夕改内心深处就不知道自己想做什么。我在公司从法律上来讲应该是一名普通员工，可是在很多时候我不得不担当项目管理者的角色。或许在这样的情况下，我可能会收获比普通员工更为丰富的除技术以外的经验，可是从长远发展的角度来看，只会让我内心更加厌倦目前的生活，希望早一天离开这家公司！\n1、项目该谁说了算 在一个没有策划的团队里，美术和程序就像水火不容的两股势力此消彼长。虽然说作为一名有节操的程序员，我的内心是拒绝让策划来领导程序的。因为在游戏网游化的今天，在国内基本是找不到多少对历史、人文、宗教等领域都有研究的策划的。在过去开发一款游戏，可能在游戏的世界观的构建上都需要花费很长的时间去研究相关的资料，可是在策划办公软件化的今天，策划关注的重点早已不再是游戏的世界观这些深层次的内容了，大家的关注点在什么地方呢？可能都在关注游戏的盈利和各种游戏系统数值的设计上吧，这一点我不想做太多的说明，因为大家都明白是怎么回事啦！好了，那么现在的问题是我们处在一个没有策划的团队里，如果程序按照美术的思路去做，可能程序会在修改了若干次项目以后对美术的要求失去信心，因为相对于程序解决问题而言，作为美术的普通人提出需求的难度显然更低。可是如果按程序的思路去做，可能美术不大会接受程序的审美，因为从我自己的角度来讲，程序更喜欢纯粹而简洁的东西、更看重能否解决问题，好不好看通常都是在考虑了这些问题后再去考虑的。\n我进公司以后，基本经历了这样两种做事方式的洗礼，刚开始技术这边和我说了大概思路，然后我做出了第一个原型(1.0 版本),结果这个思路和公司的思路完全是两个东西，因此 1.0 版本就在这样被扼杀在襁褓中。接下来，美术提出了先做 UI,然后我们在等待她们做 UI 的过程中重新审视了这个项目，那段时间天天往隔壁办公室跑，搞得那个办公室里的妹子每次看到我进去都要抬起头看一下。每天跑来跑去做什么呢？答案是沟通，和领导沟通、和美术沟通，目的是在相互沟通的基础上加深对项目需求的理解。等到美术的 UI 做出来以后，我们就准备做 UI 了，结果做到一半的时候，领导说 UI 设计不合格，被打回去重新做，然后我们花了一周时间开会讨论，我从一开始没有资格参与公司会议变成了每次会议都要参加，我不知道这对我是好事还是坏事，说好事吧是因为我终于有发言权了，说坏事吧是因为经常和美术争得面红耳赤，总之每次开完会我都忍不住要吐槽下。\n那么好了，各位看官，说到这里我无非是想告诉大家一个简单到不能再简单的道理：凡事预则立，不预则废。这就是说我们在做一件事情前一定要做好规划，游戏开发是一个特别考验团队合作的工作，如果在这个过程中我们没有在项目立项前做好充足的准备，就会很容易出现上面的问题。当我了解到仙剑项目立项就需要三个月的时候，我深深地感受到了这些传统行业经营者们的脑门一拍的决定是多么的不靠谱啊。在知乎上曾经看到过说\u0026quot;项目万事俱备，再差个程序员就好了\u0026quot;的类似言论，其实说这句话的往往就是这些自命不凡的传统行业经营者们，当你觉得一个项目仅仅需要若干个程序员就够了的时候，恰恰说明你还不够懂互联网行业！\n2、猪一样的队友 我身边许多玩 LOL 的人都在吐槽打匹配的时候遇到的都是猪一样的队友，这种情况在项目开发中则更为常见。我不知道美术出身的领导怎么会认为程序员越多项目进度就越能赶上。做项目不是大家一块儿做模型，每个人分给几个然后用着破解版的 3DsMax 就搞定了。程序在我看来更应该在保证人员配备合理的基础上保证质量。\n首先第一条，人员配备合理就是说程序员的数量要合理，其次大家的层次差别应该不会太大。因为人多了的话，对项目代码的影响可能更大，尤其是当大家编程的风格和技术水平存在差异的时候，体现在项目中就是各种未知的 Bug。为什么要求大家的层次差别不大呢，因为层次差别太大，首先团队内沟通就是问题，以我为例，我手下的两个人都是培训班培训出来的，基本上就是老师给一套视频然后照着视频做出一款游戏就结束了，我一直反感用视频的方式来学习游戏开发，因为你是在学习一个游戏引擎而不是在学习一个工具软件，虽然 Unity3D 提供了可视化编辑器，可是在我眼里它始终都是一个游戏引擎，而非一个类似 Office 或者是 3D 软件的东西。那么我想说的是什么呢？我想说的是不要把编程当作一种固定的套路，经常有人直接抄我博客里的代码直接运行项目，然后出了各种问题再来问我怎么回事？碰到这种情况我首先问的第一句话是你能不能明白这个代码是干什么的？如果对方不理解，我一般会先让它搞懂这些代码的意义。\n我们公司里的美术都不愿意碰 Unity3D，因为他们觉得这个游戏引擎会增加他们学习软件的各种成本，可是事实是这个游戏引擎比我见过的 Max、Maya、Blender 等软件都简单啊，而且 Unity3D 免费版的就可以开发简单地游戏，比之美术口中各种不择手段的盗版、破解软件不知道要干净了多少？归根到底一句话，美术不愿意尝试新的东西，美术总认为 Max 里的模型导出到 Unity3D 后材质啊、灯光啊会丢，美术总认为 Max 渲染的效果要比 Unity3D 好许多，可是既然你选择了这个引擎来做项目，我觉得美术是有责任来了解这个引擎的，你让程序员帮你拼 UI 我可以接受，可是你让程序员帮你打灯光、修改材质、摆场景，这是程序员该做的事情嘛?我说虚幻四这样的引擎都是由策划来编辑关卡的，为什么你们美术就不能尝试了解下这个引擎呢？得到的答案是我们要做模型，显然当美术的眼睛只盯着手头的那几样工具软件的时候，你和他们间的差距已经拉开，如果有能力、有时间的话，不妨尝试下将编程以外的能力整合到自身的体系中，未来是属于全能型人才的！\n3、怎样让项目流程化 我觉得像游戏这样负责的软件工程，在立项之初就应该明确美术、策划、程序各自的责任。我的想法是美术来制作素材、程序来编写相关逻辑和外部工具、策划使用外部工具来编辑关卡。\n在我来公司前，曾看过一位前辈写过的关于这个项目的一个 Demo，当初这个 Demo 里只有两个场景，我最初是对这位前辈颇为敬重的，因为感觉这个 Demo 的表现还不错，甚至觉得如果能够得到这个前辈指点一二，实乃三生有幸啊。可是当我和这位前辈聊过以后以及看过他写的代码，我对他的敬重慢慢地变成了鄙夷。这是为什么呢？因为他向领导提议使用硬代码来编写项目，通过研究他写的项目，我发现他的项目确实使用硬代码写成的，你能想象在一个脚本中并列 7 个 if 仅仅是因为它们的 tag 不同嘛，你能想象在一个脚本中的命名都是汉语拼音的变量定义嘛。\n抛开他写的项目不说，从规模和负责程度上目前这个项都比他的 Demo 有难度，首先我们大概需要制作 35 个场景涉及到上千种模型和贴图而非 Demo 中的两个场景，其次我们最终的发布平台是 Web 平台而非 Demo 中的 PC 平台。写硬代码意味着放弃复用和扩展性，顾及目前而不考虑以后。可是我们这个项目肯定是需要扩整规模的，难道每次添加一个新的场景都需要把代码重新写一遍，因此这个方案在和他交流的时候我当着他的面就给 Pass 了，然后他说我们先做个 Demo 看看，因为在前面我们已经积累了部分代码，所以在这部分代码的基础上我们迅速地完成了一个较为灵活的框架。整个框架是将模型单独打包后和贴图一起存放在服务器上，因为模型和贴图对不同的户型来说都是通用的，因为使用配置文件设计了一个类似数据库的结构，这样当我们在程序中需要某些模型和贴图的时候只需要下载就可以了，因为模型和贴图都被存放在服务器上，本地仅仅存放相关的户型模型和配置文件，因此项目的体积被大大地压缩，从而可以解决 Web 平台浏览器的压力，因为所有的场景都是使用配置文件来定义，因此当需要更新项目的时候，只需要更新服务器上的模型和贴图以及配置文件即可，提高了项目更新得速度。总体来讲，我对我设计的这个架构表示满意，因为它让硬代码的优越感荡然无存。同时为了减少人工编写配置文件、打包等过程的工作量，通过为 Unity3D 编写插件的方式实现了整个过程的半自动化。为什么是半自动化啊？因为人在做事情的时候没有统一、规范的习惯或者说难以统一和规范。我一直强调统一和规范，可是美术总认为程序的要求过于苛刻，可是事实上懂得编程的人都明白计算机程序不过是对某个过程的一种模拟，而且这个过程是有限状态的，因此当美术说需要 XXX 功能的时候，程序员的内心其实是拒绝的，因为为了这点需求，他可能需要写十几行重复的代码，为了满足用户的懒惰和弱智，领导让我们将户型内的物体尽量全部实现动态化，要给用户最大的自由，结果却是剥夺了程序员的自由写了若干个 if 或者是重复调用相同的方法，这简直是恶魔啊！\n好了，写了这么多，大家可能觉得这不符合我作为一个有节操的程序员的风格，说好的每周一篇技术博客呢？其实技术运用的好坏，完全取决于运用技术的人，所以我们不能仅仅关注技术的高低，更要关注怎样让整个团队高效率、高沟通率的执行下去，因为千里之堤，毁于蚁穴啊，虽然团队间沟通这些东西看似都是些政治或者是形式的东西，可是实际上会占到整个项目开发中相当大的一部分，所以希望大家在看了今天的博客后能够有所启发吧，好了，睡觉，哈哈！今天居然写到了这个时候！\n","date":"2015-06-24T07:42:48Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1059499448/","slug":"1059499448","tags":["生活","工作","Unity3D"],"title":"Unity3D 游戏开发之路：一月工作总结"},{"categories":["游戏开发"],"content":"各位朋友，大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://qinyuanpei.com。 今天我想和大家聊聊在 Unity3D 中关于场景的动态加载的问题。众所周知在 Unity3D 游戏开发过程中，因为受到游戏容量、平台性能和热更新等诸多因素的限制，我们可能无法将所有的游戏场景打包到项目中然后相对\u0026quot;静态\u0026quot;地加载，那么这个时候就需要我们使用动态加载的方式来将游戏场景加载到场景中。博主在研究了 Unity3D 动态加载的相关资料后发现，目前 Unity3D 中实现动态加载场景的方式主要有以下两种方式：\n使用 BuildStreamedSceneAssetBundle()方法将场景打包为 AssetBundle：这种方法将生成一个流式的.unity3d 文件，从而实现按需下载和加载，因此这种方式特别适合 Web 环境下游戏场景的加载，因为在 Web 环境下我们可以希望的是玩家可以在玩游戏的同时加载游戏。可是因为这种打包方式仅仅是保证了场景中的 GameObject 与本地资源的引用关系而非是将本地资源打包，因此从减少游戏容量的角度来说并不是十分实用，而且当我们使用 WWW 下载完 AssetBundle 后，需要使用 Application.Load()方法来加载场景，我们知道在 Unity3D 中加载一个关卡(场景)是需要在 BuildSetting 中注册关卡的，因此在使用这种方式动态加载的时候请注意到这一点。\n将场景内的所有物体打包为 AssetBundle 配合相关配置文件动态生成场景：这种方法的思路是使用一个配置文件来记录下当前场景中所有物体的位置、旋转和缩放信息，然后再根据配置文件使用 Instantiate 方法逐个生成即可。这种思路是考虑到需要在一个场景中动态替换 GameObject 或者是动态生成 GameObject 的情形，使用这种方法首先要满足一个条件，即：场景内所有的物体都是预制件(Prefab)。这是由 Unity3D 的机制决定的，因为 Prefab 是一个模板，当你需要动态生成一个物体的时候就需要为其提供一个模板(Prefab)。\n如果你对这两种方式没有什么疑问的话，那么我觉得我们可以正式开始今天的内容了。既然今天的题目已然告诉大家是使用 AssetBundle 和 Xml 文件实现场景的动态加载，我相信大家已经明白我要使用那种方式了。好了，下面我们正式开始吧！\n准备工作 在实现场景的动态加载前，我们首先要在本地准备好一个游戏场景，然后做两件事情：\n将场景内的所有 GameObject 打包为 AssetBundle 将场景内所有的 GameObject 的信息导出为 Xml 文件 做这两件事情的时候，相当于我们是在准备食材和菜谱，有了食材和菜谱我们就可以烹制出美味佳肴了。可是在做着两件事情前，我们还有一件更为重要的事情要做，那就是我们需要将场景中使用到的 GameObject 制作成预制体(Prefab)。因为在博主的印象中，Unity3D 打包的最小粒度应该是 Prefab，所以为了保险起见，我还是建议大家将场景中使用到的 GameObject 制作成预制体(Prefab)。那么问题来了，当我们将这些 Prefab 打包成 AssetBundle 后是否还需要本地的 Prefab 文件？这里博主一直迷惑，因为理论上当我们将这些 Prefab 打包成 AssetBundle 后，我们实例化一个物体的时候实际上是在使用 AssetBundle 的 Load 方法来获取该物体的一个模板，这个模板应该是存储在 AssetBundle 中的啊！因为我的笔记本使用的是免费版的 Unity3D 无法对此进行测试，所以如果想知道这个问题结果的朋友可以等我下周到公司以后测试了再做讨论(我不会告诉你公司无耻地使用了破解版)，当然如果有知道这个问题的答案的朋友欢迎给我留言啊，哈哈！这里就是想告诉大家要准备好场景中物体的预设体(Prefab),重要的事情说三遍!!! 将场景内物体打包为 AssetBundle Unity3D 打包的相关内容这里就不展开说了，因为在官方 API 文档中都能找到详细的说明，虽然说 Unity5.0 中 AssetBundle 打包的方式发生了变化，不过考虑到大家都还在使用 4.X 的版本，所以等以后我用上了 Unity5.0 再说吧，哈哈！好了，下面直接给出代码：\n[MenuItem(\u0026#34;Export/ExportTotal----对物体整体打包\u0026#34;)] static void ExportAll() { //获取保存路径 string savePath=EditorUtility.SaveFilePanel(\u0026#34;输出为AssetBundle\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;New Resource\u0026#34;,\u0026#34;unity3d\u0026#34;); if (string.IsNullOrEmpty(savePath)) return; //获取选择的物体 Object[] objs=Selection.GetFiltered(typeof(Object),SelectionMode.DeepAssets); if (objs.Length \u0026lt; 0) return; //打包 BuildPipeline.BuildAssetBundle( null, objs, savePath, BuildAssetBundleOptions.CollectDependencies|BuildAssetBundleOptions.CompleteAssets ); AssetDatabase.Refresh(); } 将场景内物体信息导出为 Xml 文件 导出场景内物体信息需要遍历场景中的每个游戏物体，因为我们在制作场景的时候通常会用一个空的 GameObject 作为父物体来组织场景中的各种物体，因此我们在导出 Xml 文件的时候仅仅考虑导出这些父物体，因为如果考虑子物体的话，可能会涉及到递归，整个问题将变得特别复杂。为了简化问题，我们这里仅仅考虑场景中的父物体。好了，开始写代码：\n[MenuItem(\u0026#34;Export/ExportScene----将当前场景导出为Xml\u0026#34;)] static void ExportGameObjects() { //获取当前场景完整路径 string scenePath=EditorApplication.currentScene; //获取当前场景名称 string sceneName=scenePath.Substring(scenePath.LastIndexOf(\u0026#34;/\u0026#34;)+1,scenePath.Length-scenePath.LastIndexOf(\u0026#34;/\u0026#34;)-1); sceneName=sceneName.Substring(0,sceneName.LastIndexOf(\u0026#34;.\u0026#34;)); //获取保存路径 string savePath=EditorUtility.SaveFilePanel(\u0026#34;输出场景内物体\u0026#34;,\u0026#34;\u0026#34;,sceneName,\u0026#34;xml\u0026#34;); //创建Xml文件 XmlDocument xmlDoc=new XmlDocument(); //创建根节点 XmlElement scene=xmlDoc.CreateElement(\u0026#34;Scene\u0026#34;); scene.SetAttribute(\u0026#34;Name\u0026#34;,sceneName); scene.SetAttribute(\u0026#34;Asset\u0026#34;,scenePath); xmlDoc.AppendChild(scene); //遍历场景中的所有物体 foreach(GameObject go in Object.FindObjectsOfType(typeof(GameObject))) { //仅导出场景中的父物体 if (go.transform.parent==null) { //创建每个物体 XmlElement gameObject=xmlDoc.CreateElement(\u0026#34;GameObject\u0026#34;); gameObject.SetAttribute(\u0026#34;Name\u0026#34;,go.name); gameObject.SetAttribute(\u0026#34;Asset\u0026#34;,\u0026#34;Prefabs/\u0026#34;+ go.name + \u0026#34;.prefab\u0026#34;); //创建Transform XmlElement transform=xmlDoc.CreateElement(\u0026#34;Transform\u0026#34;); transform.SetAttribute(\u0026#34;x\u0026#34;,go.transform.position.x.ToString()); transform.SetAttribute(\u0026#34;y\u0026#34;,go.transform.position.y.ToString()); transform.SetAttribute(\u0026#34;z\u0026#34;,go.transform.position.z.ToString()); gameObject.AppendChild(transform); //创建Rotation XmlElement rotation=xmlDoc.CreateElement(\u0026#34;Rotation\u0026#34;); rotation.SetAttribute(\u0026#34;x\u0026#34;,go.transform.eulerAngles.x.ToString()); rotation.SetAttribute(\u0026#34;y\u0026#34;,go.transform.eulerAngles.y.ToString()); rotation.SetAttribute(\u0026#34;z\u0026#34;,go.transform.eulerAngles.z.ToString()); gameObject.AppendChild(rotation); //创建Scale XmlElement scale=xmlDoc.CreateElement(\u0026#34;Scale\u0026#34;); scale.SetAttribute(\u0026#34;x\u0026#34;,go.transform.localScale.x.ToString()); scale.SetAttribute(\u0026#34;y\u0026#34;,go.transform.localScale.y.ToString()); scale.SetAttribute(\u0026#34;z\u0026#34;,go.transform.localScale.z.ToString()); gameObject.AppendChild(scale); //添加物体到根节点 scene.AppendChild(gameObject); } } xmlDoc.Save(savePath); } 好了，在这段代码中我们以 Scene 作为根节点，然后以每个 GameObject 作为 Scene 的子节点，重点在 Xml 文件中记录了每个 GameObject 的名称、Prefab、坐标、旋转和缩放等信息。下面是一个导出场景的 Xml 文件的部分内容：\n\u0026lt;Scene Name=\u0026#34;DoneStealth\u0026#34; Asset=\u0026#34;Assets/Done/DoneScenes/DoneStealth.unity\u0026#34;\u0026gt; \u0026lt;GameObject Name=\u0026#34;char_robotGuard_002\u0026#34; Asset=\u0026#34;Prefabs/char_robotGuard_002.prefab\u0026#34;\u0026gt; \u0026lt;Transform x=\u0026#34;-18.99746\u0026#34; y=\u0026#34;0\u0026#34; z=\u0026#34;37.2443\u0026#34; /\u0026gt; \u0026lt;Rotation x=\u0026#34;0\u0026#34; y=\u0026#34;0\u0026#34; z=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;Scale x=\u0026#34;1\u0026#34; y=\u0026#34;1\u0026#34; z=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;/GameObject\u0026gt; \u0026lt;GameObject Name=\u0026#34;fx_laserFence_lasers_003\u0026#34; Asset=\u0026#34;Prefabs/fx_laserFence_lasers_003.prefab\u0026#34;\u0026gt; \u0026lt;Transform x=\u0026#34;-17.90294\u0026#34; y=\u0026#34;1.213998\u0026#34; z=\u0026#34;24.07678\u0026#34; /\u0026gt; \u0026lt;Rotation x=\u0026#34;0\u0026#34; y=\u0026#34;90.00001\u0026#34; z=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;Scale x=\u0026#34;1\u0026#34; y=\u0026#34;1\u0026#34; z=\u0026#34;3.735847\u0026#34; /\u0026gt; \u0026lt;/GameObject\u0026gt; \u0026lt;GameObject Name=\u0026#34;door_generic_slide_001\u0026#34; Asset=\u0026#34;Prefabs/door_generic_slide_001.prefab\u0026#34;\u0026gt; \u0026lt;Transform x=\u0026#34;-15.91264\u0026#34; y=\u0026#34;-0.001293659\u0026#34; z=\u0026#34;7.006886\u0026#34; /\u0026gt; \u0026lt;Rotation x=\u0026#34;0\u0026#34; y=\u0026#34;90.00001\u0026#34; z=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;Scale x=\u0026#34;1\u0026#34; y=\u0026#34;1\u0026#34; z=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;/GameObject\u0026gt; \u0026lt;GameObject Name=\u0026#34;door_generic_slide_003\u0026#34; Asset=\u0026#34;Prefabs/door_generic_slide_003.prefab\u0026#34;\u0026gt; \u0026lt;Transform x=\u0026#34;-7.910765\u0026#34; y=\u0026#34;-0.001293659\u0026#34; z=\u0026#34;37.01304\u0026#34; /\u0026gt; \u0026lt;Rotation x=\u0026#34;0\u0026#34; y=\u0026#34;90.00001\u0026#34; z=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;Scale x=\u0026#34;1\u0026#34; y=\u0026#34;1\u0026#34; z=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;/GameObject\u0026gt; 在这里我们假设所有的 Prefab 是放置在 Resources/Prefabs 目录中的，那么此时我们便有了两种动态加载场景的方式\n通过每个 GameObject 的 Asset 属性，配合 Resources.Load()方法实现动态加载 通过每个 GameObject 的 Name 属性，配合 AssetBundle 的 Load()方法实现动态加载 这两种方法大同小异，区别仅仅在于是否需要从服务器下载相关资源。因此本文的主题是使用 AssetBundle 和 Xml 实现场景的动态加载，因此，接下来我们主要以第二种方式为主，第一种方式请大家自行实现吧！ 动态加载物体到场景中 首先我们来定义一个根据配置文件动态加载 AssetBundle 中场景的方法 LoadDynamicScene\n/// \u0026lt;summary\u0026gt; /// 根据配置文件动态加载AssetBundle中的场景 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;bundle\u0026#34;\u0026gt;从服务器上下载的AssetBundle文件\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;xmlFile\u0026#34;\u0026gt;AssetBundle文件对应的场景配置文件\u0026lt;/param\u0026gt; public static void LoadDynamicScene(AssetBundle bundle,string xmlFile) { //加载本地配置文件 XmlDocument xmlDoc=new XmlDocument(); xmlDoc.LoadXml(((TextAsset)Resources.Load(xmlFile)).text); //读取根节点 XmlElement root=xmlDoc.DocumentElement; if (root.Name == \u0026#34;Scene\u0026#34;) { XmlNodeList nodes = root.SelectNodes(\u0026#34;/Scene/GameObject\u0026#34;); //定义物体位置、旋转和缩放 Vector3 position = Vector3.zero; Vector3 rotation = Vector3.zero; Vector3 scale=Vector3.zero; //遍历每一个物体 foreach (XmlElement xe1 in nodes) { //遍历每一个物体的属性节点 foreach(XmlElement xe2 in xe1.ChildNodes) { //根据节点名称为相应的变量赋值 if (xe2.Name == \u0026#34;Transform\u0026#34;) { var x = float.Parse(xe2.GetAttribute(\u0026#34;x\u0026#34;)); var y = float.Parse(xe2.GetAttribute(\u0026#34;y\u0026#34;)); var z = float.Parse(xe2.GetAttribute(\u0026#34;z\u0026#34;)); position = new Vector3(x, y, z); } else if (xe2.Name == \u0026#34;Rotation\u0026#34;) { var x = float.Parse(xe2.GetAttribute(\u0026#34;x\u0026#34;)); var y = float.Parse(xe2.GetAttribute(\u0026#34;y\u0026#34;)); var z = float.Parse(xe2.GetAttribute(\u0026#34;z\u0026#34;)); rotation = new Vector3(x, y, z); } else { var x = float.Parse(xe2.GetAttribute(\u0026#34;x\u0026#34;)); var y = float.Parse(xe2.GetAttribute(\u0026#34;y\u0026#34;)); var z = float.Parse(xe2.GetAttribute(\u0026#34;z\u0026#34;)); scale = new Vector3(x, y, z); } } //生成物体 var obj = bundle.Load(xe1.GetAttribute(\u0026#34;Name\u0026#34;)); GameObject go=(GameObject)GameObject.Instantiate(obj, position, Quaternion.Euler(rotation)); go.transform.localScale=scale; } } } 因为该方法中的 AssetBundle 是需要从服务器下载下来的，因此我们需要使用协程来下载 AssetBundle：\nIEnumerator Download() { WWW _www = new WWW (\u0026#34;http://localhost/DoneStealth.unity3d\u0026#34;); yield return _www; //检查是否发生错误 if (string.IsNullOrEmpty (_www.error)) { //检查AssetBundle是否为空 if (_www.assetBundle!=null) { LoadDynamicScene(_www.assetBundle,\u0026#34;DoneStealth.xml\u0026#34;); } } } 好了，现在运行程序，可以发现场景将被动态地加载到当前场景中:)，哈哈\n效果展示\r小结 使用这种方式来加载场景主要是为了提高游戏的性能，如果存在大量重复性的场景的时候，可以使用这种方式来减小游戏的体积，可是这种方式本质上是一种用时间换效率的方式，因为在使用这种方法前，我们首先要做好游戏场景，然后再导出相关的配置文件和 AssetBundle，从根本上来讲，工作量其实没有减少。 当场景导出的 Xml 文件中的内容较多时，建议使用内存池来管理物体的生成和销毁，因为频繁的生成和销毁是会带来较大的内存消耗的。说到这里的时候，我不得不吐槽下公司最近的项目，在将近 300 个场景中只有 30 个场景是最终发布游戏时需要打包的场景，然后剩余场景将被用来动态地加载到场景中，因为领导希望可以实现动态改变场景的目的，更为郁闷的是整个场景要高度 DIY,模型要能够随用户拖拽移动、旋转，模型和材质要能够让用户自由替换。从整体上来讲，频繁地销毁和生成物体会耗费大量资源，因此如果遇到这种情况建议还是使用内存池进行管理吧！ 好了，今天的内容就是这样子了，如果大家对此有什么疑问，欢迎给我留言，谢谢大家！\n","date":"2015-06-15T07:24:17Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1467630055/","slug":"1467630055","tags":["Unity3D","动态加载","AssetBundle"],"title":"Unity3D 游戏开发之使用 AssetBundle 和 Xml 实现场景的动态加载"},{"categories":["游戏开发"],"content":"各位朋友，大家好，欢迎大家关注我的博客，我是秦元培，我的博客地址是blog.csdn.net/qinyuanpei。 今天想和大家分享的是目前在移动平台上较为流行的关卡系统，关卡系统通常是单机手机游戏如《愤怒的小鸟》、《保卫萝卜》中对游戏内容的组织形式，玩家可通过已解锁的关卡(默认第一关是已解锁的)获取分数进而解锁新的关卡，或者是通过付费购买解锁新的关卡。那么好了，在今天的文章中博主将带领大家快速实现一个可扩展的关卡系统，这个实例的灵感来自博主最近的工作经历，希望对大家学习 Unity3D 游戏起到一定帮助性的作用。\n原理 在本地配置一个 Xml 文件，在这个文件中定义当前游戏中关卡的相关信息，通过解析该文件并和 UI 绑定最终实现一个完整的关卡系统。\n1、定义关卡 首先我们来定义一个关卡的基本结构：\npublic class Level { /// \u0026lt;summary\u0026gt; /// 关卡ID /// \u0026lt;/summary\u0026gt; public string ID; /// \u0026lt;summary\u0026gt; /// 关卡名称 /// \u0026lt;/summary\u0026gt; public string Name; /// \u0026lt;summary\u0026gt; /// 关卡是否解锁\t/// \u0026lt;/summary\u0026gt; public bool UnLock = false; } 在这里，我们假定关卡的名称和该关卡在 Unity3D 中场景名称一致。其中最为重要的一个属性是 UnLock，该值是一个布尔型变量，表明该关卡是否解锁，因为在游戏中，只有解锁的场景是可以访问的。\n2、定义关卡配置文件 从关卡的基本结构 Level 可以定义出如下的配置文件，这里使用 Xml 作为配置文件的存储形式：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;levels\u0026gt; \u0026lt;level id=\u0026#34;0\u0026#34; name=\u0026#34;level0\u0026#34; unlock=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;level id=\u0026#34;1\u0026#34; name=\u0026#34;level1\u0026#34; unlock=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;level id=\u0026#34;2\u0026#34; name=\u0026#34;level2\u0026#34; unlock=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;level id=\u0026#34;3\u0026#34; name=\u0026#34;level3\u0026#34; unlock=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;level id=\u0026#34;4\u0026#34; name=\u0026#34;level4\u0026#34; unlock=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;level id=\u0026#34;5\u0026#34; name=\u0026#34;level5\u0026#34; unlock=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;level id=\u0026#34;6\u0026#34; name=\u0026#34;level6\u0026#34; unlock=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;level id=\u0026#34;7\u0026#34; name=\u0026#34;level7\u0026#34; unlock=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;level id=\u0026#34;8\u0026#34; name=\u0026#34;level8\u0026#34; unlock=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;level id=\u0026#34;9\u0026#34; name=\u0026#34;level9\u0026#34; unlock=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;/levels\u0026gt; 和关卡结构定义类似，这里使用 0 和 1 来表示关卡的解锁情况，0 表示未解锁，1 表示解锁，可以注意到默认情况下第一个关卡是解锁的，这符合我们在玩《愤怒的小鸟》这类游戏时的直观感受。那么好了，在完成了关卡的结构定义和配置文件定义后，接下来我们开始思考如何来实现一个关卡系统，因为此处并不涉及到 Unity3D 场景中的具体逻辑，因此我们在关卡系统中主要的工作就是维护好主界面场景和各个游戏场景的跳转关系，我们可以注意到这里要完成两件事情，即第一要将配置文件中的关卡以一定形式加载到主界面中，并告诉玩家哪些关卡是已解锁的、哪些关卡是未解锁的，当玩家点击不同的关卡时可以得到不同的响应，已解锁的关卡可以访问并进入游戏环节，未解锁的关卡则需要获得更多的分数或者是通过付费来解锁关卡；第二是要对关卡进行编辑，当玩家获得了分数或者是支付一定的费用后可以解锁关卡进入游戏环节。这两点综合起来就是我们需要对关卡的配置文件进行读写，因为我们注意到一个关卡是否解锁仅仅取决于 unlock 属性，那么好了，明白了这一点后我们来动手编写一个维护关卡的类。\n3、编写一个维护关卡的类 这里直接给出代码，因为从严格的意义上来说，这段代码并非我们此刻关注的重点，可能这让大家感到难以适应，因为文章明明就是在教我们实现一个关卡系统，可是此刻博主却说这部分不重要了，请大家稍安勿躁，因为这里有比代码更为深刻的东西。\nusing UnityEngine; using System.Collections; using System.Collections.Generic; using System.Xml; public static class LevelSystem { /// \u0026lt;summary\u0026gt; /// 加载Xml文件\t/// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;The levels.\u0026lt;/returns\u0026gt; public static List\u0026lt;Level\u0026gt; LoadLevels() { //创建Xml对象 XmlDocument xmlDoc = new XmlDocument(); //如果本地存在配置文件则读取配置文件 //否则在本地创建配置文件的副本 //为了跨平台及可读可写，需要使用Application.persistentDataPath string filePath = Application.persistentDataPath + \u0026#34;/levels.xml\u0026#34;; if (!IOUntility.isFileExists (filePath)) { xmlDoc.LoadXml (((TextAsset)Resources.Load (\u0026#34;levels\u0026#34;)).text); IOUntility.CreateFile (filePath, xmlDoc.InnerXml); } else { xmlDoc.Load(filePath); } XmlElement root = xmlDoc.DocumentElement; XmlNodeList levelsNode = root.SelectNodes(\u0026#34;/levels/level\u0026#34;); //初始化关卡列表 List\u0026lt;Level\u0026gt; levels = new List\u0026lt;Level\u0026gt;(); foreach (XmlElement xe in levelsNode) { Level l=new Level(); l.ID=xe.GetAttribute(\u0026#34;id\u0026#34;); l.Name=xe.GetAttribute(\u0026#34;name\u0026#34;); //使用unlock属性来标识当前关卡是否解锁 if(xe.GetAttribute(\u0026#34;unlock\u0026#34;)==\u0026#34;1\u0026#34;){ l.UnLock=true; }else{ l.UnLock=false; } levels.Add(l); } return levels; } /// \u0026lt;summary\u0026gt; /// 设置某一关卡的状态 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;name\u0026#34;\u0026gt;关卡名称\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;locked\u0026#34;\u0026gt;是否解锁\u0026lt;/param\u0026gt; public static void SetLevels(string name,bool unlock) { //创建Xml对象 XmlDocument xmlDoc = new XmlDocument(); string filePath=Application.persistentDataPath + \u0026#34;/levels.xml\u0026#34;; xmlDoc.Load(filePath); XmlElement root = xmlDoc.DocumentElement; XmlNodeList levelsNode = root.SelectNodes(\u0026#34;/levels/level\u0026#34;); foreach (XmlElement xe in levelsNode) { //根据名称找到对应的关卡 if(xe.GetAttribute(\u0026#34;name\u0026#34;)==name) { //根据unlock重新为关卡赋值 if(unlock){ xe.SetAttribute(\u0026#34;unlock\u0026#34;,\u0026#34;1\u0026#34;); }else{ xe.SetAttribute(\u0026#34;unlock\u0026#34;,\u0026#34;0\u0026#34;); } } } //保存文件 xmlDoc.Save (filePath); } } 这里我们首先将关卡配置文件 levels.xml 放置在 Resources 目录下，这是因为我们可以使用 Resources.Load()这种方式来加载本地资源，这种方式对于 Unity3D 来说有着得天独厚的优势：\n它使用相对于 Resources 目录的相对路径，所以在使用的时候不用考虑是相对路径还是绝对路径的问题 它使用名称来查找一个本地资源，所以在使用的时候不用考虑扩展名和文件格式的问题 它可以是 Unity3D 支持的任意类型，从贴图到预制体再到文本文件等等，可以和 Unity3D 的 API 完美地结合 说了这么多它的优点，我们自然要痛心疾首地说说它的缺点，它的缺点是什么呢？那就是不支持写入操作，这当然不能责怪 Unity3D，因为当 Unity3D 导出游戏的时候会将 Rsources 目录下的内容压缩后再导出，我们当然不能要求在一个压缩后的文件里支持写入操作啦，所以我们是时候来总结下 Unity3D 中资源读写的常见方案了，那么 Unity3D 中常见的资源读写方案由哪些呢？\n1、Resources.Load:只读，当我们的资源不需要更新且对本地存储无容量要求的时候可以采用这种方式 2、AssetBundle：只读，当我们的资源需要更新且对本地存储有容量要求的时候可以采用这种方式 3、WWW:只读，WWW 支持 http 协议和 file 协议，因此可以 WWW 来加载一个网络资源或者本地资源 4、PlayerPrefs：可读可写，Unity3D 提供的一种的简单的键-值型存储结构，可以用来读写 float、int 和 string 三种简单的数据类型，是一种较为松散的数据存储方案 5、序列化和反序列化：可读可写，可以使用 Protobuf、序列化为 Xml、二进制或者 JSON 等形式实现资源读写。 6、数据库：可读可写，可以使用 MySQL 或者 SQLite 等数据库对数据进行存储实现资源读写。\n好了，在了解了 Unity3D 中资源读写的常见方案后，我们接下来来讨论下 Unity3D 中的路径问题： 1、Application.dataPath：这个路径是我们经常使用的一个路径，可是我们真的了解这个路径吗？我看这里要打个大大的问号，为什么这么说呢？因为这个路径在不同的平台下是不一样的，从官方 API 文档中可以了解到这个值依赖于运行的平台：\nUnity 编辑器：\u0026lt;工程文件夹的路径\u0026gt;/Assets Mac：\u0026lt;到播放器应用的路径\u0026gt;/Contents IOS: \u0026lt;到播放器应用的路径\u0026gt;/\u0026lt;AppName.app\u0026gt;/Data Win：\u0026lt;.exe 文件目录\u0026gt;\\Data Web：\u0026lt;.unity3d 文件的绝对路径\u0026gt; 这个路径是在 PC 上支持读写的，可是因为到了不同的平台上文件的路径发生变动，因此我们在程序中设置的路径可能就变成了一个错误的路径。在网上大家找到类似的内容，这一点是网上说的最多、坑最多的一块儿，希望大家在以后遇到这个问题的时候能够留心点，尽量能不用这个路径就不用这个路径吧！什么?不用这个路径，那该用什么路径呢？呵呵，不要着急啊，下面隆重向大家推荐 Application.persistentDataPath 这个路径。 2、Application.persistentDataPath：这个路径是 Unity3D 中的一个数据持久化路径，呵呵，千万不要问我什么叫做数据持久化路径，我不会告诉你我今天这篇文章的关键就是数据持久化啊！总之呢，我们把握住一点，这个路径是可以在移动平台上使用的一个可以读写的路径，当然在路径这块儿可能同样会碰到和 Application.dataPath 类似的问题，因为博主写这篇文章的时候并没有对移动平台进行测试，这一点希望大家能够注意啊，这并不是我偷懒，实在是公司最近的事情比较多，没有时间做进一步的测试，不过除了路径的问题以外，我可以向大家保证，这个路径是可以读写的，所以如果我们在开发 Unity3D 游戏过程中需要在本地存储某些文件的话，这个路径是个不错的选择。 好了，现在我们回到维护关卡的这个类中，大家可以注意到我在加载配置文件的时候做了这样一个处理： 如果本地(指游戏外部)存在配置文件则直接读取配置文件，否则使用 Resources.Load()方法加载 Resources 目录下的配置文件，并在本地创建一个配置文件的副本。这样做的目的是为了方便对配置文件进行修改，因为 Resources 目录下的配置文件在导出游戏后是没有路径的，我们没有办法用常规的访问文件的方式来读取这个文件，这个时候我们就用到 Application.persistentDataPath 这个路径，因为我们在本地创建了副本，所以只要读取副本文件就可以对其进行读取和修改了。那么，接下来，我们来写一个 Main 文件作为项目的入口文件吧！\n4、编写入口文件 using UnityEngine; using System.Collections; using System.Collections.Generic; using UnityEngine.UI; using System.Xml.Serialization; public class Main : MonoBehaviour { //关卡列表 private List\u0026lt;Level\u0026gt; m_levels; void Start () { //获取关卡 m_levels = LevelSystem.LoadLevels (); //动态生成关卡 foreach (Level l in m_levels) { GameObject prefab=(GameObject)Instantiate((Resources.Load(\u0026#34;Level\u0026#34;) as GameObject)); //数据绑定 DataBind(prefab,l); //设置父物体 prefab.transform.SetParent(GameObject.Find(\u0026#34;UIRoot/Background/LevelPanel\u0026#34;).transform); prefab.transform.localPosition=new Vector3(0,0,0); prefab.transform.localScale=new Vector3(1,1,1); //将关卡信息传给关卡 prefab.GetComponent\u0026lt;LevelEvent\u0026gt;().level=l; prefab.name=\u0026#34;Level\u0026#34;; } //人为解锁第二个关卡 //在实际游戏中玩家需要满足一定条件方可解锁关卡 //此处仅作为演示 LevelSystem.SetLevels (\u0026#34;level1\u0026#34;, true); } /// \u0026lt;summary\u0026gt; /// 数据绑定 /// \u0026lt;/summary\u0026gt; void DataBind(GameObject go,Level level) { //为关卡绑定关卡名称 go.transform.Find(\u0026#34;LevelName\u0026#34;).GetComponent\u0026lt;Text\u0026gt;().text=level.Name; //为关卡绑定关卡图片 Texture2D tex2D; if(level.UnLock){ tex2D=Resources.Load(\u0026#34;nolocked\u0026#34;) as Texture2D; }else{ tex2D=Resources.Load(\u0026#34;locked\u0026#34;) as Texture2D; } Sprite sprite=Sprite.Create(tex2D,new Rect(0,0,tex2D.width,tex2D.height),new Vector2(0.5F,0.5F)); go.transform.GetComponent\u0026lt;Image\u0026gt;().sprite=sprite; } } 在这段脚本中，我们首先加载了关卡信息，然后将关卡信息和界面元素实现绑定，从而实现一个简单的关卡选择界面，并人为地解锁了第二个关卡。好吧，如果这是一个正式游戏的配置关卡配置文件，相信大家都知道怎么免费玩解锁的关卡了吧，哈哈！当然，我不推荐大家这样做，因为作为一个程序员，当你全身心地投入到一个项目中的时候，你就会明白完成一款软件或者游戏需要投入多少精力，所以大家尽量还是不要想破解或者盗版这些这些事情，毕竟作为开发者可能他的出发点是想做出来一个让大家都喜欢的产品，可是更现实的问题是开发者一样要生活，所以请善待他们吧。好了，言归正传，这里的 UI 都是基于 UGUI 实现的，不要问我为什么不用 NGUI，因为我就是喜欢 UGUI！我们知道我们需要为每个关卡的 UI 元素绑定一个响应的事件，因此我们需要为其编写一个 LevelEvent 的脚本：\nusing UnityEngine; using System.Collections; using UnityEngine.UI; using UnityEngine.EventSystems; public class LevelEvent : MonoBehaviour { //当前关卡 public Level level; public void OnClick() { if(level.UnLock){ //假设关卡的名称即为对应场景的名称 //Application.LoadLevel(level.Name); Debug.Log (\u0026#34;当前选择的关卡是:\u0026#34;+level.Name); }else{ Debug.Log (\u0026#34;抱歉!当前关卡尚未解锁!\u0026#34;); } } } 记得在本文开始的时候，博主提到了一个假设，就是关卡的名称和其对应的游戏名称一致的假设，相信到此处大家都知道为什么了吧！为了让每个关卡的 UI 元素知道自己对应于哪个关卡，我们设置了一个 level 变量，这个变量的值在加载关卡的时候已经完成了初始化，所以此时我们可以在这里知道每个关卡的具体信息，从而完成事件的响应。好了，今天的内容就是这样了，我们来看看最终的效果吧！\nDEMO1\rDEMO2\r可以注意到在第二次打开游戏后，第二个关卡已经解锁了，说明我们在最开始设计的两个目标都达到了，那么内容就是这样子啦，如果大家有什么好的想法或者建议，欢迎在文章后面给我留言，谢谢大家！\n","date":"2015-06-11T08:11:01Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1424645834/","slug":"1424645834","tags":["关卡系统","Unity3D","游戏"],"title":"Unity3D 游戏开发之快速打造流行的关卡系统"},{"categories":["生活感悟"],"content":"大家好，欢迎大家关注我的博客，我是秦元培，我的博客地址是：http://blog.csdn.net/qinyuanpei。到公司上班已经一周了，趁着今天周末休息的时间，想将最近在工作和生活中的感受和想法写下来，因为生命就是一个不断积累、厚积薄发，最终实现自我超越的一个过程。作为第一份工作，尽管没有想象中那样理想，可我还是在很努力的工作。工作后接手的第一个项目是一个房地产的漫游展示项目，因为这家公司之前是做影视后期的，所以在决定做这个项目后，公司领导层对这个项目具体要做到什么样的效果并没有一个明确的认识，所以在项目开展前期无论是在对项目所使用的技术的熟悉程度上还是项目整体的策划上，都没有一个具体的的可操作的方案。因为公司领导是美术出身，所以从我进了公司以后，整个公司上下一直沉浸在一种加班加点赶制模型的压抑氛围当中。\n我进公司的第一天，公司负责技术的人向我演示了一个视频，告诉我项目做出来大概就是这样一个样子，然后就让我开始写所谓的\u0026quot;框架\u0026quot;，因为他对 Unity3D 的技术并不熟悉，所以基本上从我上班开始，所有和 Unity3D 相关的工作都由我一个人来完成，让我这样一个新入职的人来担当\u0026quot;主程\u0026quot;，我感到受宠若惊而压力山大，不过因为他和我年龄相差不大，一直都比较尊重我的想法，所以 Unity3D 这块整个项目就比较放心地交给了我来做，这样的结果就是我大概花了一周时间就写好了整体的框架[偷笑：)]。可是在设计整个项目的过程中，因为美术都忙着建模，所以 UI 设计这块儿基本上都是空白，作为一个刚进公司不久没有什么话语权的新人，在这种情况下我只能自己先大致做出来一个 DEMO，然后再听取领导的意见反复进行修改，可是如果这样，到了项目后期如果因为项目需求发生变动，可能 UI 设计就需要重新制作，我个人是比较讨厌做 UI，因为 UI 有时候会因为参数设置不合理等等的原因造成无法调试的错误，这样你折腾了大半天找了可能出现的各种错误，最终却发现是因为一个参数设置不合理，这该有多蛋疼啊！我比较喜欢 Cocos Studio 这种制作 UI 的方式，就是让美术直接在 UI 编辑器里做好 UI 然后导出为程序可以解析的数据类型，这样程序只需要负责将这些数据解析出来为它们绑定相关的 UI 事件就好了。然而现实是残酷的，在这个项目中，因为楼盘、户型、家装等等因素的不可控性，所以在设计 UI 的时候全部都是以动态加载的形式来处理的，因为你并不能确定这些 UI 里显示的元素到底有多少个，这样我在设计这个框架的时候是这样考虑的，就是把所有需要人力来调整、控制的部分(如模型摆放、场景设计等等)都手动完成，所以和 UI 相关的部分(如 UI 元素的动态加载、模型的加载、本地配置文件等等)都通过动态加载来实现，因为在整个项目中第三部分的家装会涉及到大量的模型，所以这部分考虑的是将模型文件打包成 AssetBundle 文件从服务器加载。\n我不知道公司领导当初是怎么样确定使用 Unity3D 来做这个项目，因为考虑到虚拟展示的需要，这个项目最终展示给用户的是一个网页，这样就更需要考虑资源组织的问题，就这样在工作的第一周时间内我想到了以前在学校做游戏的时候都没\u0026quot;舍得\u0026quot;使用的技术方案，基本的思路是本地的游戏文件最终仅仅保留一个主场景文件(MainMenu.cs)，主场景负责维护从楼盘到户型再到家装的所有逻辑，各个场景中的动态的部分则是通过 Resource.Load()和 AssetBundle 来实现，将这些场景放到服务器上，主场景将决定具体加载哪一个场景。因为整个项目主要分成楼盘、户型、家装这三个部分，这些场景除了模型以外逻辑都是一样的，因此将这部分的逻辑都写成公用的脚本，在制作这些场景时只需要将脚本拖拽到某些物体上就可以了。因为需要从服务器上获取符合筛选要求的楼盘信息，因此还需要编写服务器端的相关逻辑，目前项目组中还没有服务器端的程序，这部分我表示无能为力啊，哈哈。如果希望将最终的网页做得漂亮些，可能还需要前端工程师的加入吧，目前这块同样是空白！好吧，做项目的时候即使是程序员都会有分身乏术的时候，成为全栈工程师是我的梦想，可是目前做不到啊！我不知道在游戏开发中程序和美术的关系怎么样，反正在我目前的项目组里我这个程序的存在感实在是太弱了啊，可能是项目组程序的比例太低，可能是我和大家还不熟悉吧，不过昨天居然有个美术跑过来问我能不能教他 Unity3D，因为他觉得建模做得再好做出来的模型终究是死的，哈哈，瞬间感觉有种相见恨晚的感觉啊。好了，这些闲话先聊到这里吧，今天想和大家分享的是我在开发过程中遇到的某些坑，因为我是一个程序员，归根到底我和大家要聊的还是程序嘛！\n下载 AssetBundle 时遇到\u0026quot;跨域\u0026quot;的问题 这个问题主要是因为服务器上缺少一个叫做 crossdomain.xml 的文件，这是由 Adobe 提出的以保证 Flash 能够跨域访问文件的一种策略，当发生这个错误时具体的表现就是你可以通过浏览器从服务器上下载 AssetBundle 文件，可是当你试图在 Unity 里使用 WWW 访问该文件时就会报错，具体的错误信息我已经不记得了，不过错误信息中特别明确的指出了是因为缺少 crossdomain.xml 这个文件，所以解决的方案就是在服务器根目录里增加这样一个文件，文件的内容如下：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;cross-domain-policy\u0026gt; \u0026lt;site-control permitted-cross-domain-policies=\u0026#34;master-only\u0026#34; /\u0026gt; \u0026lt;allow-access-from domain=\u0026#34;*\u0026#34; to-ports=\u0026#34;*\u0026#34;/\u0026gt; \u0026lt;/cross-domain-policy\u0026gt; 具体使用的时候需要将*号部分分别替换成允许跨域访问的地址和端口，因为我是用 WAMP 这个软件搭建的本地服务器，所以这里都采用的是默认值，具体怎么去设置这里的内容还需要大家自己去探索，不过这里就是像告诉大家使用 Unity3D 做网页游戏或者是从服务器上下载文件是一定要考虑这个问题的啊！\n动态生成的 UI Prefab 被拉伸的问题 这个问题出现在动态生成 UI 元素的过程中，就是生成物体以后物体的大小和位置会发生变化，这个问题在宣雨松的博客中曾经读到过，不过当时他并没有说清楚产生这个问题的原因，所以当同样的问题发生在我身上的时候我果断选择和他一样，哈哈，解决方法是把物体的 localScale 设为(1,1,1)、localPosition 设为(0,0,0)，当然按照我的传统如果大家知道是为什么的话还是告诉我吧！\nAssetBundle 的 mainAsset 问题 这个问题产生在最初确定 AssetBundle 打包是将单个物体打包还是将多个物体一起打包的时候，后来发现 mainAsset 取决于 bool BuildAssetBundle (Object mainAsset,Object[] assets,string pathName, BuildAssetBundleOptions optionsBuildAssetBundleOptions.CollectDependencies | BuildAssetBundleOptions.CompleteAssets, BuildTarget targetPlatform= BuildTarget.WebPlayer) 这个方法中的第一个参数，就是说指定了一个参数则可以通过 mainAsset 来获取 AssetBundle 中的主物体，否则只能通过 Load 方法传入一个名称来获取指定物体。这里想说一件诡异的事情，比如说我们选中两个物体然后将其打包，但是通过 LoadAll 方法获取到的物体的数目却不是两个，因为打包的时候 GamObject 和 Transform 是分开打包的，父物体下的子物体同样是被分开打包的，因此这个方法使用起来并不是那么地尽如人意，这点希望大家注意！\n场景打包为 AssetBundle 的问题 我们知道在 Unity 中可以通过 BuildStreamedSceneAssetBundle 方法将场景打包为 AssetBundle 文件，然后按照如下方法加载到游戏中。场景打包的方法如下所示：\nstatic function MyBuild(){ var levels : String[] = [\u0026#34;Assets/Level1.unity\u0026#34;]; BuildPipeline.BuildStreamedSceneAssetBundle( levels, \u0026#34;Streamed-Level1.unity3d\u0026#34;, BuildTarget.WebPlayer); } 接下来我们就可以通过 WWW 方法将其加载到游戏中\nfunction Start () { // Download compressed scene. If version 5 of the file named \u0026#34;Streamed-Level1.unity3d\u0026#34; was previously downloaded and cached. // Then Unity will completely skip the download and load the decompressed scene directly from disk. var download = WWW.LoadFromCacheOrDownload (\u0026#34;http://myWebSite.com/Streamed-Level1.unity3d\u0026#34;, 5); yield download; // Handle error if (download.error != null) { Debug.LogError(download.error); return; } // In order to make the scene available from LoadLevel, we have to load the asset bundle. // The AssetBundle class also lets you force unload all assets and file storage once it is no longer needed. var bundle = download.assetBundle; // Load the level we have just downloaded Application.LoadLevel (\u0026#34;Level1\u0026#34;); 注意到最后一行我们是使用 LoadLevel 方法来加载一个场景的，该方法需要一个参数，它是我们在 Unity3D 中注册过的关卡，即在编译游戏的时候需要将其加入到关卡列表中。那么现在问题来了，这个 Level11 到底是本地的场景还是下载的场景啊，既然我们选择了从服务器上加载一个场景，那么本地应该是不会有这个场景了，那么游戏关卡列表中就不会有这个关卡，因此如果调用最后一样代码应该会提示找不到这个关卡。我在这里纠结了好久，最后发现是这样，就是现在本地做好关卡，然后将其加入到关卡列表中，当本地关卡打包成 AssetBundle 后，从本地删除当前关卡，依然可以从服务器上加载这个场景。这是我自己做实验的结果，不知道对不对，希望有知道这个的朋友能够告诉我这样到底对不对，因为这种方法感觉有些猥琐啊，哈哈。\n好了，今天的内容就是这样了，因为目前项目暂时就发现了这些问题，所以更多的关于 Unity3D 的内容需要等到项目慢慢推进的过程中去发现了，希望大家能够喜欢啊！\n","date":"2015-06-11T08:02:45Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/719322223/","slug":"719322223","tags":["游戏开发","工作","Unity3D"],"title":"Unity3D 游戏开发之路：一周工作总结"},{"categories":["生活感悟"],"content":"毕业就像指尖流沙，而我是那从指尖流过的沙子。我不知道该怎样来总结我的大学，即使我努力地寻找、努力地回避，我依然觉得大学对我而言就是这样一个讽刺的过程，曾经努力地想要摆脱这个专业最后却留了下来，最初对这个专业的热情随着时间一天天地消逝，到最后却发现自己夹在某种缝隙中左右为难。\n或许大家都认为我这个人比较冷淡，可是对我而言，我只是想做一个简简单单的人而已，我不会因为即将毕业就表现出某种殷切的神情，在我的心里我当做朋友的会一直当作朋友，即使以后大家都鲜有机会再聚在一起，我讨厌酒桌上的朋友，所以我不会用喝酒这件事情来作为我们彼此情感的见证。我是一个不善于卖弄和殷勤的人，可能我心直口快，可能我又爱又恨，可是那是因为我想做一个表里如一的人。朋友对我而言，一直是我所珍视的人，我觉得君子之交淡如水这样的关系就很好，我不是一个善于表演的人，不管是和大家一起拍 VCR 还是和大家在一起的时候，我想我喜欢的是大家在一起的时光，而不是在一起喝酒的时候。\n我承认，因为长期做编程设计的这样一个习惯，让我的思维方式里只有 0 和 1，只有对和错。可是人却是一种奇怪的感情动物，我有时候甚至会觉得自己更喜欢计算机而不是人，人在一个人的时候会感到孤独，可是当大家都聚在一起的时候真的会快乐吗？人与人之间的关系实在是微妙而复杂，即使是互联网甚至物联网都不能与之匹敌。我知道在中国人的思维里，这种想法特别地正常，因为中国就是这样一个人情社会，有时候我会听到别人跟我说，你自己怎么样并不重要，真正重要的是你处在一个什么样的圈子里。虽然在我阅读过的书籍里、接触过的人的话语里都有类似的结论，可是道理终究是道理，当你试图去将这个道理真正实践的时候，你会发现一切是如此的艰难。\n大家或许觉得我对这个班没有什么感情，可是人和人相处不能单纯地看重对方目前、以后可能会对你有什么样的帮助，因为这不叫朋友叫做关系，我眼中的朋友是那种即使自身没有什么强大的社会资源，可当你需要帮助的时候，他仍然会真心实意地为你付出，我对于朋友和关系的界定实在是困难，因此当我面对这场不知是送别还是交友的毕业聚会的时候，我会突然陷入某种迷茫，即使麦克风音量开到最大、嗓子喊到声嘶力竭，当一切都结束了的时候能够留下什么呢？终有一天大家都会奔向各自的前程，去做自己想做的事情，就像慕容紫英一样，百年蹉跎岁月不过转瞬，当朋友们都不再需要他照顾的时候，他会义无反顾地踏上自己的路，他心里记得那句“承君此诺，必守一生”，可是千百年后当昆仑山上下起雪的时候，这世上留给他又有什么呢？他终究是一个人，上天怜悯他却不曾心疼他，当心中信念坍塌的时候，我不知道他是不是和我一样有过这种迷茫？\n昨天我和班里大部分的人喝了酒，目的单纯而简单，就是想感谢大家在这四年里对我的帮助，就是想记住和大家在一起的这段时光，我是一个随和的人，所以我不会强迫别人喝多少酒，因为喝酒就是一种助兴的形式而已，真正让我们铭记于心的不是这顿酒，而是我们彼此在各自的生命里出现过。或许我就是这样一个尚不成熟的人吧，或许以后我会变成让大家、让每一个人都喜欢的样子，或许我以后依然会是这个样子……生命中有太多的或许让人无法预料，可你的生命会是什么样子完全取决于你的选择，我不想为未来埋下太多的伏笔，我就是一个普通的、平凡的人，仅此而已……\n","date":"2015-05-16T08:45:05Z","image":"/posts/3461518355/cover.jpg","permalink":"https://qinyuanpei.github.io/posts/3461518355/","slug":"3461518355","tags":["毕业","梦想","人生"],"title":"毕业就像指尖流沙"},{"categories":["单机游戏"],"content":" 今天想来说说MMD。MMD是MikuMikuDance的简称，是由日本人樋口优开发的一组3D动画制作软件。该软件最初希望能够将3D建模软件完成的VOCALOID的初音未来等角色模型制作成可以随着音乐跳舞的动画，因此称为MMD。作者在此基础上开发了能够将歌曲让初音未来等角色歌唱的MikuMikuVoice。2011年9月11日，樋口优宣布停止MMD新版本的开发工作。不过人们对制作MMD的热情丝毫没有减少，在动漫、游戏等领域总是能够不断看到MMD的影子。例如MMD/宇月和千本樱/夏侯瑾轩都是较为典型的MMD。\n好了，相信现在大家都对MMD有了一定的了解了，作为一名单机游戏爱好者，我目前最为遗憾的两件事情：\n不会制作游戏MV(或者说视频) 不会制作MMD(因为我是个程序嘛) 在我看来以同人形式去发掘一个作品中优秀的东西，这件事情本身就是一件让人觉得快乐的事情，因为可能某一个人和你有相同的想法，当它看到你的东西的时候，发觉你想表达的东西就是它想要表达的。我每次玩完一款游戏以后都会去网上搜集比较好的MV，因为我觉得随着人一天天地慢慢长大，有时候你发觉自己再没有时间去玩游戏的时候，通过看视频能让你想起很多的事情，有时候看着别人做的MV会哭，我便觉得当时的经历其实挺值的去回味的。好了，说了这么多毫不相干的事情，差点忘了今天的正事。首先我们来了解下一个完整的制作MMD的过程：\n使用Maya、Blender或者3DsMax等3D软件建模(或者从游戏中提取) 使用PMDEditor或者PMXEditor对模型进行绑骨、动作和表情制作等操作 将处理过的.pmd或者.pmx模型导入MikuMikuDance完成场景、音乐完成动画制作 从这样一个过程我们了解到，制作MMD还是需要一定的技术门槛的，因为并不是每一个人都能够完成模型的绑骨、动画这些任务的。这篇文章不提供以上软件的下载和使用方法，因为我们接下来的内容基本与以上软件无关，我们的重点依然是Unity3D，因为我是一个游戏开发者嘛，哈哈。好了，下面的内容基于两点假设：\n你有一个PMD或者PMX模型 你有一个VMD的动作文件 首先，第一步我们需要一个Unity3D插件MMD4Unity,将这个插件导入项目后，为了使整个项目结构较为清晰，我们将这个插件的文件夹命名为MMDPlugins。在MMDFiles文件中我们准备了三个文件:\n模型文件：初音.pmd 动作文件：动作1.vmd和动作2.vmd 好了，现在我们注意到Unity3D菜单栏上会增加一个Plugin菜单项，我们单击这个菜单项会发现MMD Loader和XFile Importer这两个项目，这里我们选择MMD Loader这个菜单项：\nMMD1\r这两个子菜单项的意义十分地明确了，PMD Loader负责加载PMD模型并将其转化为Unity3D可以识别的模型文件，VMD负责将一个动作文件套用到一个模型上。所以：\n1、通过PMD Loader打开加载PMD文件的窗口，建议这里将ShaderType设置为Default，因为如果使用MMD的Shader的话，待会转换出来的模型可能会存在找不到材质的问题。接下来我们点击Convert，稍等片刻就会在场景中看到一个模型(prefab)文件。 MMD2\rMMD3\r2、接下来通过VMD Loader打开加载VMD文件的窗口，选择场景中的模型文件和项目资源中的XMD动作文件，点击Convert，大概有1分钟多一点的样子就好了。此时我们选择场景中的模型文件，找到它的Animation组件，然后点击Animation右侧的按钮为其指定一个动画文件，因为刚刚我们已经为它添加了一个动作，所以我们可以很容易的在项目资源中找到名为初音_动作2的动画片段(AnimationClip)。 MMD4\r好了，现在我们就来看看这个MMD的效果吧！\nMMD5\r哈哈，感觉效果还不错吧！\n现在来说说我在使用这个插件过程中遇到的问题：\n在转换PMD模型的时候如果选择Default转换出的模型可以找到对应的材质，可是模型是错误的；如果选择MMDShader，转换出的模型会找不到对应得材质，比如说我在尝试转换下面这个模型的时候，因为MMD对模型的精细程度的要求，所以模型会被分得很细，因此像这个模型当贴图数目较少的时候，就没有办法自动对应贴图，所以这快目前还是个问题吧！ 如果使用的是PMX模型，可以用PMEditor这个软件转换下格式，转成PMD格式后，后然后再按照本文的方法去做就可以了。 PMD转换出来的模型没有办法选择其中的某一个部分，因此在操作模型的时候可能会不太方便吧，以前都是选择某一部分然后给模型贴图，现在这招不行了啊。 好了，今天的内容就是这样了，有什么问题大家给我留言哦！\n","date":"2015-04-19T23:31:30Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/4088452183/","slug":"4088452183","tags":["Unity3D","单机游戏","MMD"],"title":"Unity3D 游戏开发之 MMD For Unity 插件研究"},{"categories":["开发工具"],"content":" Sublime Text,是这个地球上最好的代码编辑器，没有之一。因为在过去的一段时间里，我使用的版本是SublimeText2，所以听说Sublime Text3版本稳定后，决定开始尝鲜。哈哈，我就是这么一个\u0026quot;喜新厌旧\u0026quot;的人！Sublime的强大不仅仅在它优雅的外表，更为重要的是她无可匹敌的扩展性，就是说我们可以通过插件来扩展它的功能，这对于一个喜欢DIY的人来说简直是无法抗拒的诱惑。不过在接收这些诱惑前，我们需要一个工具Package Control，它是Sublime里最为基础、最为重要的插件，好了，现在问题来了，Sublime怎么安装Package Control！\n在Sublime Text2下我们可以通过CTRL+~打开控制台，然后输入代码：\nimport urllib2,os; pf=\u0026#39;Package Control.sublime-package\u0026#39;; ipp = sublime.installed_packages_path(); os.makedirs( ipp ) if not os.path.exists(ipp) else None; urllib2.install_opener( urllib2.build_opener( urllib2.ProxyHandler( ))); open( os.path.join( ipp, pf), \u0026#39;wb\u0026#39; ).write( urllib2.urlopen( \u0026#39;http://sublime.wbond.net/\u0026#39; +pf.replace( \u0026#39; \u0026#39;,\u0026#39;%20\u0026#39; )).read()); print( \u0026#39;Please restart Sublime Text to finish installation\u0026#39;) 可是到了Sublime Text3下，因为版本不同的关系，内部API发生变化，因此需要使用新的代码：\nimport urllib.request,os; pf = \u0026#39;Package Control.sublime-package\u0026#39;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); open(os.path.join(ipp, pf), \u0026#39;wb\u0026#39;).write(urllib.request.urlopen( \u0026#39;http://sublime.wbond.net/\u0026#39; + pf.replace(\u0026#39; \u0026#39;,\u0026#39;%20\u0026#39;)).read()) 当代码因为某些原因无法正常工作的时候，我们可以手动安装Package Control：\n下载PackageControl或者通过Github获取 git clone git@github.com:wbond/package_control.git 通过Preferences-\u0026gt;Browser Packages进入Installed Packages目录 重新启动Sublime，然后Enjoy it！ ","date":"2015-04-17T12:54:41Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/570137885/","slug":"570137885","tags":["Sublime","编辑器","IDE"],"title":"在 Sublime Text3 下安装 Package Control"},{"categories":["编程语言"],"content":" Lua 5.3 已经发布好长时间了，可是因为 Lua For Windows 的 Lua 版本无法和官方保持一致，所以想尝试下编译 Lua 5.3 的源代码，因为作为一名合格的程序员，是应该要懂得编译原理的相关内容的啊(可是我真的没有学过编译原理啊!\u0026hellip;..)。好了，那么今天博主将在文章中和大家分享自己编译 Lua 5.3的过程，希望能够对大家学习和使用 Lua 有些帮助吧！\n我们知道 Lua 由三部分组成，即\nLua 链接库 Lua 解释器 Lua 编译器 因此，对于 Lua 源代码的编译主要就是编译 Lua 链接库、 Lua 解释器 和 Lua 编译器\n编译 Lua 链接库 使用 Visual Studio 创建一个 VC++ 项目，项目命名为 Lua53，项目类型为静态库、不设置预编译头。 删除 Visual Studio 自动创建的 .cpp 文件及其对应的.h文件。 将下载的 Lua 代码解压，将 src 目录下的全部文件拷贝到项目中，然后删除 lua.c、luac.c 和 lua.hpp 这三个文件。 编译项目会得到一个 Lua53.lib 的文件，这就是我们编译得到的 Lua 链接库。 编译 Lua 解释器 我们知道 Lua 解释器是一个可以直接运行Lua代码的可执行文件，因此\n在同一个解决方案下继续创建 VC++ 项目，项目命名为 Lua，项目类型为控制台应用程序、需设置预编译头。 删除 Visual Studio 自动创建的 .cpp 文件及其对应的 .h 文件。 将下载的 Lua 代码解压，将 src 目录下的全部文件拷贝到项目中，然后删除 luac.c 这个文件。 设置当前项目依赖于 Lua53 项目 编译项目会得到一个 Lua.exe 文件，这就是我们编译得到的 Lua 解释器。 运行该程序，我们可以看到下面的结果：\nLua解释器程序\r好了，现在我们来写一个简单的 Lua 程序：\nio.write(\u0026#34;Hello I get a powerful program language called Lua \\n\u0026#34;) io.write(string.format(\u0026#34;This Lua is %s and now is %s \\n\u0026#34;,_VERSION,os.date())) 程序运行结果为：\nHello I get a powerful program language called Lua This Lua is Lua5.3 and now is 04/16/15 16:06:43\n编译 Lua 编译器 和Lua类似地，\n在同一个解决方案下继续创建 VC++ 项目，项目命名为 Lua，项目类型为控制台应用程序、需设置预编译头。 删除 Visual Studio 自动创建的 .cpp 文件及其对应的 .h 文件。 将下载的Lua代码解压，将 src 目录下的全部文件拷贝到项目中，然后删除 lua.c 这个文件。 设置当前项目依赖于 Lua53 项目 编译项目会得到一个 Luac.exe 文件，这就是我们编译得到的 Lua 解释器。 使用 Lua 编译器需要在环境变量中增加对 Lua 编译器路径地引用，比如 Luac.exe 放在 D:\\Program Files\\Lua\\build\\ 这个目录下，就在 PATH 这个变量中增加：\nD:\\Program Files\\Lua\\build; 因为每个人的 Lua 编译器存放的位置都不同，所以这个就不再赘述了。\n好了，今天的内容就是这样了。\n链接 本文编译的 Lua 官方编译的 Lua ","date":"2015-04-16T14:50:35Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3642630198/","slug":"3642630198","tags":["Lua","编译","Visual Studio"],"title":"在 Windows 下使用 Visual Studio 编译 Lua 5.3"},{"categories":["读书笔记"],"content":"贝塞尔曲线(Bezier Curve)是由法国工程师皮埃尔·贝塞尔(Pierre Bezier)于 1962 年提出的一种曲线。在数学的数值分析领域中，贝塞尔曲线是计算机图形学中相当重要的参数曲线，其主要意义在于无论是直线还是曲线都能在数学上予以描述。最早贝塞尔曲线被用来对汽车主体进行设计，现在贝塞尔曲线被广泛地运用到计算机图形软件(如 Photoshop、Flash 等)中，是计算机图形领域重要的一个数学曲线。\n贝塞尔曲线主要内容 贝塞尔曲线就是这样的一条曲线，它是依据四个位置任意的点坐标绘制出的一条光滑曲线。在历史上，研究贝塞尔曲线的人最初是按照已知曲线参数方程来确定四个点的思路设计出这种矢量曲线绘制法。贝塞尔曲线的有趣之处更在于它的皮筋效应，也就是说，随着点有规律地移动，曲线将产生皮筋伸引一样的变换，带来视觉上的冲击。1962 年，法国数学家 Pierre Bezier 第一个研究了这种矢量绘制曲线的方法，并给出了详细的计算公式，因此按照这样的公式绘制出来的曲线就用他的姓氏来命名是为贝塞尔曲线。贝塞尔曲线按照阶数可以从一次扩展到 n 次，这里例举出常见的一次贝塞尔曲线、二次贝塞尔曲线和三次贝塞尔曲线。\n一次贝塞尔曲线 一次贝塞尔曲线，即线性贝塞尔曲线，其定义是:给定点 P0、P1，贝塞尔曲线是两点间的一条直线。线性贝塞尔曲线由下列公式给出：\nB(t)=P0+(P1-P0)t=(1-t)P0+tP1,其中 t 是一个 0 到 1 之间的数值\n该公式等同于对 P1,P0 两点进行线性插值。\n一次贝塞尔曲线\r二次贝塞尔曲线 二次贝塞尔曲线的路径由给定点 P0、P1、P2 的函数 B(t)给出:\nB(t)=(1-t)^2 * P0+2t(1-t)P1+t^2P2,其中 t 是一个 0 到 1 之间的数值\n为构建二次贝塞尔曲线，可以中介点 Q0 和 Q1 作为由 0 至 1 的 t:\n由 P0 至 P1 的连续点 Q0，描述一条线性贝塞尔曲线。 由 P1 至 P2 的连续点 Q1，描述一条线性贝塞尔曲线。 由 Q0 至 Q1 的连续点 B(t)，描述一条二次贝塞尔曲线。 二次贝塞尔曲线原理图\r二次贝塞尔曲线演示效果\r三次贝塞尔曲线 P0、P1、P2、P3 四个点在平面或在三维空间中定义了三次方贝塞尔曲线。曲线起始于 P0 走向 P1，并从 P2 的方向来到 P3。P0 和 P1 之间的间距，决定了曲线在转而趋进 P3 之前，走向 P2 方向的“长度有多长”。三次贝塞尔曲线的公式是:\nB(t)=P0(1-t)^3+3P1t(1-t)^2+3P2t^2(1-t)+P3t^3,其中 t 是一个 0 到 1 之间的数值\n三次贝塞尔曲线原理图\r三次贝塞尔曲线的绘制这里采取的是一种已知曲线参数方程来确定四个点的方法，这种方法称为矢量曲线绘制法。这里以二维平面为例(如需三维空间同理构造出 z(t)即可):\nx(t)=axt^3+bxt^2+cxt+x0 y(t)=ayt^3+byt^2+cyt+y0\n因为 x0、y0 已知，因此我们可以用下列公式计算出剩余三个点的坐标:\nx1 = x0 + cx / 3 x2 = x1 + ( cx + bx ) / 3 x3 = x0 + cx + bx + ax\ny1 = y0 + cy / 3 y2 = y1 + ( cy + by ) / 3 y3 = y0 + cy + by + ay\n在 x0、y0 已知的前提下，可以通过变换得到:\ncx = 3 * ( x1 - x0 ) bx = 3 * ( x2 - x1 ) - cx ax = x3 - x0 - cx - bx\ncy = 3 * ( y1 - y0 ) by = 3 * ( y2 - y1 ) - cy ay = y3 - y0 - cy - by\n因此只要给定四个点，总能构造出一条三次贝塞尔曲线，这种方法是一种较为实用和可靠的方法。\n三次贝塞尔曲线效果演示\r下面我们来看一个简单的示例，该示例以 Unity3D 为基础建立，希望对大家理解贝塞尔曲线有所帮助。\n代码示例 (尝试着实现了下，发现暂时有些问题，等实现了再更新上来吧！)\n2015 年 12 月 19 日更新： 在《CG Programming in Unity》一书中提到了贝塞尔曲线，实现了一个基础版本的贝塞尔曲线绘制，即在给定 P0 和 P2 的前提下，由用户通过滑杆对 P1 进行控制，可以实时预览当前曲线的样式，感兴趣的朋友可以阅读该书的第 9 章部分。下面给出代码示例：\nusing UnityEngine; using System.Collections; public class QuadraticBezier : MonoBehaviour { /// \u0026lt;summary\u0026gt; /// 3个基础点 /// \u0026lt;/summary\u0026gt; public Vector3 P0; public Vector3 P1; public Vector3 P2; /// \u0026lt;summary\u0026gt; /// 调整第二个参数的水平分量 /// \u0026lt;/summary\u0026gt; private float paramX = 0.5f; /// \u0026lt;summary\u0026gt; /// 调整第二个参数的垂直分量 /// \u0026lt;/summary\u0026gt; private float paramY = 0.5f; /// \u0026lt;summary\u0026gt; /// 线条宽度 /// \u0026lt;/summary\u0026gt; public float LineWidth = 0.15f; /// \u0026lt;summary\u0026gt; /// 线条颜色 /// \u0026lt;/summary\u0026gt; public Color LineColor = Color.white; /// \u0026lt;summary\u0026gt; /// 顶点数目 /// \u0026lt;/summary\u0026gt; public int PointsCount = 10; /// \u0026lt;summary\u0026gt; /// 线渲染器 /// \u0026lt;/summary\u0026gt; private LineRenderer lineRenderer; void Start () { //初始化线渲染器 lineRenderer = this.GetComponent\u0026lt;LineRenderer\u0026gt;(); if(lineRenderer == null) lineRenderer = this.gameObject.AddComponent\u0026lt;LineRenderer\u0026gt;(); lineRenderer.useWorldSpace = true; lineRenderer.SetColors(LineColor,LineColor); lineRenderer.material = new Material(Shader.Find(\u0026#34;Particles/Additive\u0026#34;)); lineRenderer.SetWidth(LineWidth,LineWidth); lineRenderer.SetVertexCount(PointsCount); } void Update() { //根据滑杆参数计算P1 P1 = new Vector3(Mathf.Abs(P0.x-P2.x) * paramX, Mathf.Abs(P0.x-P2.x) * paramY, 0); //绘制曲线 for (int i = 0; i \u0026lt; PointsCount; i++) { float t = i / (PointsCount - 1.0f); Vector3 position = (1.0f - t) * (1.0f - t) * P0 + 2.0f * (1.0f - t) * t * P1 + t * t * P2; lineRenderer.SetPosition(i, position); } } void OnGUI() { GUILayout.Label(string.Format(\u0026#34;第一个参数：P0=({0},{1},{2})\u0026#34;, P0.x, P0.y, P0.z)); GUILayout.Label(string.Format(\u0026#34;第二个参数：P1=({0},{1},{2})\u0026#34;, P1.x, P1.y, P1.z)); GUILayout.Label(\u0026#34;请拖动下面的滑杆调整第二个参数P1观察曲线变化\u0026#34;); paramX = GUILayout.HorizontalSlider(paramX, 0, 1); paramY = GUILayout.HorizontalSlider(paramY, 0, 1); GUILayout.Label(string.Format(\u0026#34;第三个参数：P2=({0},{1},{2})\u0026#34;, P2.x, P2.y, P2.z)); } } 在这段代码中通过两个参数来调整 P1，这里取 Z 分量为 0 主要是方便研究，扩展到三维空间需要给定第三个参数来对应的调整。这个示例的运行效果如下：\n效果展示\r","date":"2015-04-08T12:25:28Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2186770732/","slug":"2186770732","tags":["贝塞尔曲线","计算机图形","数学"],"title":"贝塞尔曲线学习笔记"},{"categories":["单机游戏"],"content":" 各位朋友，大家好，我是秦元培。今天博主想和分享的是使用 disunity 提取 Unity3D 游戏素材。这个工具呢，博主在Unity3D 游戏开发之反编译 AssetBundle 提取游戏资源这篇文章中其实已经提到过了，不过因为有些朋友对如何使用这个工具依然存在问题，所以博主决定特地写一篇文章来讲解如何使用 disunity 来提取 Unity3D 游戏中的素材。\n准备工作 disunity:负责对 Unity3D 的数据文件进行解包 Unity3D:负责将导出的数据文件显示出来 Bleander或者 3DsMax:负责 Unity3D 数据文件的转换处理，二选一即可。个人推荐 Blender。 Java:负责为 disunity 提供编译环境 测试文件 《新仙剑 OL》下载 《轩辕剑 6 外传穹之扉》 《雨血前传:蜃楼》下载 提取流程 好了，在确定做好所有的准备工作后，我们就可以正式开始今天的内容了！\n编译 disunity 虽然我们可以从 disunity 的项目主页中下载 release 版本，不过为了保险起见，博主依然建议大家自行编译 disunity。编译的方法是在命令行中切换到 disunity 的目录，然后执行命令:\njava -jar disunity.jar 如果大家的 Java 环境没有任何问题的话，那么接下来我们就应该可以看到:\n[Info] DisUnity v0.3.4 以及各种关于这个工具的使用方法和参数选项。那么好了，现在我们就来熟悉下 disunity 这个工具的常用命令。disunity 命令的基本形式是:\ndisunity [CommandName] [CommandOptions] disunity 命令 dump:将一个二进制的对象转化成人类可以阅读的文本信息。 dump-struct:将一个二进制的对象转化为结构化的信息。 extract:将 Unity3D 的数据文件转化为常见的文本、声音、图片等信息。 extract-raw:将 Unity3D 的数据文件转化为可序列化的对象，在 extract 命令不被支持的情况下使用。 extract-txt:和 dump 命令类似输出转换结果到命令行。 extract-struct:和 dump-struct 命令类似输出转换结果到命令行。 info:输出 Unity3D 的数据文件和 AssetBundle 文件的变量信息。 bundle-extract:释放所有的被打包到 AssetBundle 中的文件。 bundle-inject:将从 AssetBundle 中打包的文件重新打包 暂时先介绍这些，因为其它的命令我们基本用不到，如果需要深入研究这些命令，可以参考 disunity 项目中的 README.md 文件。\n解析《新仙剑 OL 》的 AssetBundle 文件 这里我们以游戏目录/assetbundles/NPC/Models/下的 s049.unity3d_CC9026FB 为例来讲解游戏模型的提取。\n模型文件提取 首先我们将这个文件的扩展名改为 s049.unity3d，因为这是它原始的扩展名，是 Unity3D 中导出 AssetBundle 的一种文件格式。好了，我们将这个文件放在一个无中文路径的目录下，这里以 C:\\Users\\Robin\\Desktop 即桌面为例。注意首先进入 disunity 的目录，然后执行命令：\ndisunity extract C:\\Users\\Robin\\Desktop\\s049.unity3d 接下来会在桌面生成一个名为 s049 的文件夹，在这个文件夹中找到 Mesh 的子文件夹，会得到一个 s049.obj 的文件，这个文件就是我们提取到的模型文件。\n模型贴图提取 好了，下面我们再来看看怎么提取这个模型文件对应的贴图，在游戏目录/assetbundles/NPC/Texture/下有一个名为 s049_1.unity3d_1D2446B9 的文件，这就是 s049 这个模型对应的贴图了。同样地，我们将其重命名为 s049_1.unity3d 然后执行命令：\ndisunity extract C:\\Users\\Robin\\Desktop\\s049_1.unity3d 接下来在桌面上生成一个名为 s049_1 的文件夹，在这个文件夹中找到 Texture2D 的子文件夹，会得到一个名为 s049_1.dds 的贴图文件，这就是我们要提取的模型 s049 的贴图文件。\n将模型和贴图合并 我们打开 Blender 并将 s049.obj 文件导入，然后将场景中默认的灯光和摄像机都删除，因为我们只需要一个模型文件，我们发现在 Blender 中已经可以看到模型了，因为 Unity3D 中使用的是 FBX 模型，所以我们这里将模型文件导出为 FBX 备用。因为 Unity3D 可以识别 dds 类型的贴图，所以对贴图我们不用做任何处理。\n童年林月如的模型\r打开 Unity3D 将童年林月如的模型和贴图一起导入，将童年林月如的模型拖入到游戏场景中，因为模型的尺寸没有经过调整，所以模型刚开始可能会比较小，我们可以在 Unity3D 进行局部的调整。接下来我们会发现模型没有贴图，只要选择这个模型然后在属性窗口为它附上 s049_1.dds 的贴图文件即可。下面是童年林月如的模型导入 Unity3D 以后的效果:\n童年林月如导入Unity3D后的效果\r解析《新仙剑 OL》的 assets 文件 和 AssetBundle 不同，assets 文件是整个 Unity3D 项目中项目资源的打包集合，比如说 Asset 文件下的资源都会被打包到这里，所以说解析 assets 文件可能会有更大的收获吧！因为所有的 Unity3D 游戏都会有这样的文件，而 AssetBundle 文件只有在使用了这项技术的游戏项目中才有。比如说在 Unity3D 中有一个重要的 Resource 文件夹，这个文件夹打包后被被打包成 resources.assets 文件。这里我们以 xianjian_Data/resources.assets 文件为例。首先执行命名:\ndisunity extract C:\\Users\\Robin\\Desktop\\resources.assets 接下来会在桌面生成一个 resources 的文件夹，打开这个文件夹我们会发现三个子文件夹，分别是 Shader、TextAsset 和 Texture2D。解析的结果似乎有点失望，不过在 TextAsset 文件夹下我们会找到一个叫做 ResourceFiles.txt 的文件，这是一个纯文本文件，我们可以直接打开，打开后我们发现它的内容是一个 Xml 文件，并且在这个 Xml 文件中定义了游戏中使用的各种资源的路径，不过这些资源都是以 AssetBundle 的形式来定义的。这说明什么呢？这说明《新仙剑 OL》的场景和界面资源是通过动态加载的方式加载到游戏当中的，而这些资源则是通过这个 Xml 文件来配置和管理的，这符合我们平时在 Unity3D 游戏开发中的观点和方法。通过这个文件，我们找到了 assetbundles/config/movieconfig.unity3d 这个文件，这是一个负责维护游戏中场景过场动画的文件。下面我们就来尝试解析这个文件，不过游戏制作方对 config 文件夹下的内容进行了加密，因为在这个文件夹下面是两个 AssetBundle 文件，博主尝试用 extract 和 bundle-extract 两个命令进行解析，可是得到的只是些文本文件，对我们继续研究没有什么帮助。那么好了，现在我们能够进行解析的只有 xinjian_Data/sharedassets0.assets 文件了：\ndisunity extract C:\\Users\\Robin\\Desktop\\sharedassets0.assets 这个解出来的话是些没有什么用的贴图文件，看来如果要提取音乐或者图片的话，还需要进行更加深入的研究才行啊。\n解析《雨血前传.蜃楼》的 assets 文件 因为解析《新仙剑 OL》的 assets 文件没有得到什么有用的东西，所以我们接下来来尝试解析《雨血前传.蜃楼》的 assets 文件。这款游戏是博主比较喜欢的一款游戏，基于 Unity3DY 引擎，而且这款游戏是作为 Unity3D 官方范例来推广的，因此研究这款游戏对我们提高 Unity3D 的资源打包机制会比较有帮助。好了，我们直接上手：\ndisunity extract C:\\Users\\Robin\\Desktop\\resources.assets 哈哈，这款游戏果然没有让我们失望，我们得到了什么呢？\n蜃楼中各种Boss的头像\r蜃楼中游戏连招视频1\r蜃楼中游戏连招视频2\r总结 不同的游戏采用的资源配置方案都不同，不过一般可以从 resources.assets 这个文件入手作为突破点。 如果能拿到游戏中数据配置方案，对于我们提取游戏中的素材会有较大的帮助，因为这样方向性会更强些。 通过 AssetBundle 动态加载到场景中最好还是采用一个配置表来进行配置，这样便于我们管理和维护整个游戏项目。 如果没有服务器段的干预，理论上只要修改了本地的 AssetBundle 文件就可以实现对游戏内容和数据的更改，换句话说，可以做外挂和修改器。 ","date":"2015-04-03T13:29:18Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1082185388/","slug":"1082185388","tags":["穹之扉","Unity3D","disunity","反编译"],"title":"Unity3D 游戏开发之使用 disunity 提取 Unity3D 游戏资源"},{"categories":["游戏开发"],"content":"各位朋友，大家好，欢迎大家关注我的博客，我是秦元培，我的博客地址是：https://blog.yuanpei.me ，今天我们来说说通过反编译 Unity3D 的 AssetBundle 来提取游戏资源，博主写这篇文章的目的并非是要教大家如何去破解一款基于 Unity3D 引擎开发的游戏，而是想通过今天这篇文章来告诉大家如何在开发 Unity3D 游戏的过程中保护自己的游戏资源。\n漫话 Unity3D 的 AssetBundle 对于 AssetBundle，其实博主是在以前的文章中是有提到的。不知道大家还记不记得，博主曾经在写游戏开发和 Lua 的不解之缘这个系列文章的时候，提到并且使用过 AssetBundle 这种技术。具体来说呢，AssetBundle 在 Unity3D 中是一种用于资源打包盒资源动态加载的解决方法，比如我们平时玩的单机游戏容量一般都比较大，这是因为制作人员在制作游戏的时候将所有的项目资源都整合到了一起。可是如果我们用 AssetBundle 来做这个游戏的话，我们就可以只在发布的游戏中提供支持游戏功能的核心部分，而将游戏当中的场景、模型等资源以 AssetBundle 的形式打包然后放到服务器上，这样当游戏客户端处于联网的时候就可以从服务器上下载这些资源，从而实现游戏资源的动态加载，由此可见 AssetBundle 可以帮助我们减少游戏的容量。如果是在需要安装包的场合下，那么游戏包容量的大小无疑会为游戏加些印象分。\n为什么这幅图总让我想起仙剑四里四人在即墨那晚的时光呢？\r比如最近《轩辕剑 6 外传穹之扉》这部单机游戏发布了，从各大游戏网站的评测到和一样我喜欢单机游戏的各位朋友们的亲身体验，大家一致的认为这部游戏整体表现还不错，应该考虑玩一玩。这样难免让博主有些心动，可是看到 17 个 G 的游戏容量时还是犹豫了下。DOMO 小组从《轩辕剑 6》就开始使用 Unity3D 引擎，在经历了第一部游戏的失败后，或许此次 DOMO 小组会将游戏优化的比较好吧。这里如果有喜欢单机游戏的朋友不妨去玩玩看，毕竟我们学习游戏开发的初衷就是做出好游戏，如果不热爱游戏又怎么能做出好游戏呢？好了，扯得有点远了，这里我们注意到一个重要的因素就是游戏容量，如果 DOMO 采用 AeestBundle 的话，游戏的容量肯定会减少很多。可是这样一来，它就不是单机游戏了嘛，对吧！\n在 Unity3D 中 AssetBundle 是专业版中的一个功能，在免费版的 Unity3D 中是无法使用这个功能的，不知道在 Unity5 中这个功能是不是划分到了个人版中。好了，下面我们来看看如何使用 AssetBundle。我们主要从使用 AssetBundle 打包和加载 AssetBundle 这两个方面来说：\n使用 Assetbundle 打包 使用 AssetBundle 打包主要通过 BuildPipeline.BuildAssetBundle()这个方法来实现，该方法原型为：\nbool BuildAssetBundle (Object mainAsset,Object[] assets,string pathName, BuildAssetBundleOptions options = BuildAssetBundleOptions.CollectDependencies | BuildAssetBundleOptions.CompleteAssets, BuildTarget targetPlatform = BuildTarget.WebPlayer) 在这个方法中，第一个参数是一个 Object 类型，表示一个激活的物体;第二个参数是一个 Object[]类型，表示所有选中的物体;第三个参数是一个 string 类型，表示要导出的资源包的路径，资源包的扩展名可以是 assetbundle 或者 unity3d;第四个参数表示的是打包选项，默认是完全打包和依赖打包。这里重点解释下这两个概念，完全打包是指所有资源都参与打包，比如说一个模型带有贴图和动画，那么打包模型的时候贴图和动画都会被作为资源打包。而依赖打包是相对于 Prefab 来说的，比如说 PrefabA 中引用了 PrefabB 这个对象，那么打包的时候这两个对象都会被打包，并且它们之间的这种依赖关系会在打包后继续保持；第五个参数是平台的选择，因为 Unity3D 是一个跨平台的游戏引擎，而各个平台现在的情况又不尽相同，因此现在 Unity3D 采取的方案是各个平台只能使用自己平台对应的 AssetBundle，这一点希望大家在使用的时候注意啊。好了，现在我们来看一个简单的例子：\n/// \u0026lt;summary\u0026gt; /// 输出AssetBundle /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;type\u0026#34;\u0026gt;平台类型\u0026lt;/param\u0026gt; static void ExportToAssetBundle(ExportType type,BuildTarget target) { //获取存储路径 string savePath = EditorUtility.SaveFilePanel(\u0026#34;输出为AssetBundle\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;New Resource\u0026#34;,\u0026#34;unity3d\u0026#34;); if (savePath == string.Empty) return; //获取选中的对象 Object[] selection = Selection.GetFiltered(typeof(Object),SelectionMode.DeepAssets); if (selection.Length == 0) return; //打包 if (type == ExportType.All) { BuildPipeline.BuildAssetBundle(null, selection, savePath, BuildAssetBundleOptions.CollectDependencies, target); } else { BuildPipeline.BuildAssetBundle(obj,null, savePath, BuildAssetBundleOptions.CollectDependencies, target); } } 这是一个简单的导出 AssetBundle 资源包的方法，它有两个参数，第一个参数表示是一个枚举类型，定义为 ExportType，取 Single 时表示打包一个特定的激活物体，比如说一个模型、一个场景等等;取 All 时表示打包所有选中的物体，比如一个场景。第二个参数表示打包的平台，这个不用多说了。因为博主的免费版的 Unity3D 不支持 AssetBundle，所以这里没法给大家演示了，具体效果请自行测试，有问题的话给博主留言就是了。\n加载 AssetBundle 加载 AssetBundle 是一个从网络中下载资源的过程，因此需要使用 Unity3D 的 WWW 功能，这是一个简单的网络协议的封装，可以像浏览器一样访问某个 URL 地址或者是本地地址，访问 WEB 地址需要使用 HTTP 协议，访问本地地址需要使用 File 协议。我们来看一个具体的例子：\n/// \u0026lt;summary\u0026gt; /// 加载一个unity3d格式的文件 /// WEB地址——http://server.com/xxx.unity3d /// 本地地址——file://.unity3d文件的绝对路径 /// \u0026lt;/summary\u0026gt; IEnumerator LoadUnity3DFile(string url) { WWW www = new WWW(url); yield return www; if (www.error!=null) { Debug.Log(www.error); } else { AssetBundle bundle = www.assetBundle; Instantiate(bundle.mainAsset,Vector3.zero,Quaternion.identity); } } 在这里我们直接使用 bundle.assetBundle 获取了全部的资源，如果只需要获取资源中的一部分，则只需要通过 bundle.Load()方法就可以了，这里需要传入资源的名称。当我们使用完资源后可以通过 bundle.Unload()方法来卸载资源，达到释放内存的目的。\n从反编译《新仙剑 OL》看 AssetBundle 打包 好了，下面我们以《新仙剑 OL》这款游戏的 AssetBundle 的反编译来探索下在使用 AssetBundle 打包应该注意哪些问题。《新仙剑 OL》这款游戏呢，是采用 Unity3D 引擎开发的一款横跨客户端游戏和网页游戏的网络游戏，游戏以《仙剑奇侠传》初代游戏剧情为主，玩家将第三人称视角再次跟随主人公展开一段荡气回肠的感人故事。这款游戏总体来说还不错吧，因为毕竟是网游，我们不能用单机游戏的视角去评价，具体的原因大家都是知道的。\n好了，为什么我们要选择这款游戏呢？\n第一，这款游戏的客户端只有 30 余 M,体积小适合拿来研究(这就是 AssetBundle 的好处啊) 第二，博主是一位仙剑玩家，一直希望有一天《仙剑奇侠传 1》能够用 3D 技术重现，这个游戏满足了博主的好奇心 第三，网络上已经有朋友对这个游戏的打包进行了研究，这里感谢网友朋友提供部分.unity3d 文件及相关文件。 我们选择的解包工具是一款叫做disunity的命令行工具，经过博主的尝试，这个工具真心强悍啊，可以解开.unity3d 文件和.assets 文件，可以拿到的数据形式有贴图、声音、模型等。具体的情况大家可以在稍后看到。\n首先我们找到《新仙剑 OL》的安装目录，然后我们就能发现一个叫做 assetbundles 的文件夹，这是怕大家不知道吗？这太明显了吧！我们打开文件夹会发现 Charachers、NPC、Scene 等等文件夹，继续往下找我们发现了好多的.unity3d 文件，不过这些文件都是以.unity3d 然后跟些随机字符串的形式存在的。根据网友朋友们的提示，这些文件就是.unity3d 文件，不过游戏制作组为了干扰我们故意接了下随机字符在后面(呵呵，还有比这更弱的加密方式吗？)。博主看到这里的第一感觉就是想先用加载 AssetBundle 的方式来看看能不能将这些 AssetBundle 读取出来，因此果断改了文件扩展名，然后开始在 Unity3D 中读取，结果程序报错看来是我们想的简单了啊。没办法的办法，强行解包吧！在命令行中输入：\ndisunity extract C:\\Users\\Robin\\Desktop\\s049.unity3d 接下来程序会在桌面上生成一个上 s049 的文件夹，打开文件夹一看，尼玛，竟然直接拿到了模型的网格数据(.obj)和贴图数据(.dds)以及相关的 Shader。这让我突然间有点不能接受啊，马上打开 Blender 将网格数据导入，结果童年的林月如就出现在了我们的面前：\n林月如灰模\r因为博主不会在 Blender 中给模型贴图，所以我们到 Unity3D 中完成贴图，首先需要将模型导出为 FBX 格式。好了，将模型导入 Unity3D 后，将贴图赋给模型，童年的林月如就闪亮登场了，哈哈！\n林月如贴图效果\r好了，再来一张，不过这张没有贴图，需要大家自己来辨别这是谁啊，哈哈！\n柳梦璃灰模\r通过 disunity 这个工具我们还能获取更多的资源，剩下的内容就由大家自己去探索吧。通过这部分的研究，我们可以总结出以下观点，希望大家在使用 AsssetBundle 这项技术时注意：\n尽量在一个 AssetBundle 中打包多个资源，这样做的好处是别人没法通过加载 AssetBundle 拿到你做好的 Prefab。 尽量将一个预制件分割成不同的部分分别存放，这样做的好处是即使别人拿到了你的预制件却是不完整的。 尽量利用动态脚本来加载场景而不是将整个场景打包，即使将整个场景打包，要把贴图和模型分开放置(因此如此，我虽然拿到了游戏的场景贴图，可是没有用啊) 尽量利用加密的方法来隐藏本地的 AssetBundle 或者使用不易察觉的存储位置作为 AssetBundle 的存储位置，不要用明文数据进行存储。 好了，今天的内容就是这样了，希望大家喜欢，AssetBundle 打包是一个值得去深入研究的问题，今天博主提出的这些观点不过是对《新仙剑 OL》这个游戏的打包提出的一些看法，如果大家有不同的看法，欢迎一起来交流！\n","date":"2015-04-02T20:37:52Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2799263488/","slug":"2799263488","tags":["Unity3D","游戏开发","AssetBundle","资源提取"],"title":"Unity3D 游戏开发之反编译 AssetBundle 提取游戏资源"},{"categories":["游戏开发"],"content":"各位朋友大家好，欢迎关注我的博客，我的博客地址是http://www.qinyuanpei.com。今天我们来说说如何在 Unity3D 中为编辑器开发扩展程序。提到扩展程序，相信大家都不会陌生了。不管是 Google 的 Chrome 浏览器还是经典的 FireFox，这些浏览器最为人所称道的就是它支持各种各样的扩展。扩展程序是一种插件，它遵循插件式设计的原则，可以随时在宿主程序中安装、卸载而不会影响宿主程序的正常运行。我们知道在 Unity3D 中有各种各样的插件，如 NGUI、2DToolKit、EasyTouch 等等都是一种扩展程序。扩展程序在丰富宿主程序功能的基础上，可以帮助宿主程序完成大量额外的工作。可以说正是因为 Unity3D 拥有大量的插件和资源支持，Unity3D 才能够受到大家如此的追捧。可是作为一个有节操的程序员，如果仅仅会使用工具，那么我们和普通用户有什么区别啊，所以在今天的文章中博主将通过三个具体的实例来教大家如何为 Unity3D 的编辑器开发扩展程序，希望对大家学习 Unity3D 技术有所帮助！\n常用的命名空间和类 开发 Unity3D 编辑器扩展程序的命名空间主要是 UnityEditor，在该命名空间下常用的类有 EditorGUI、EditorGUILayout、EditorWindow(可能还有其它的类，不过到目前为止博主就用过这些，如果有其它的类，欢迎大家来补充啊)。为 Unity3D 编辑器开发的扩展程序同样是一种脚本，通常需要将脚本文件放在项目资源文夹下的 Editor 文件夹中，即 Assets/Editor。不过该脚本不再继承自 MonoBehaviour，具体的内容我们会放到后面的实例中来讲 Unity3D 编辑器扩展程序的形式通常有两种，一种是没有界面的(如案例 1)、一种是有界面的(如案例 2、案例 3)。对于没有界面的这种扩展程序，我们只需要定义一个类(无需继承任何父类)然后再这个类中定义一个静态的方法就可以了;而对于有界面的这种扩展程序，我们需要让定义的这个类继承 EditorWindow 并实现 OnGUI()方法,因为在 OnGUI()方法中我们将会对扩展程序的界面进行绘制，不过无需担心啦，因此扩展程序的界面绘制和 Unity3D 脚本中的 OnGUI()方法是相似的，我们要做的就是要熟悉常见的控件。好了，下面进入今天的实战环节，大家准备好了吗？\nUnity3D 编辑器扩展程序开发实例 案例 1 快速修改贴图类型 Unity3D4.6 版本的一个重要更新就是 UGUI 和 Unity2D 的支持，因为有了对 Unity2D 的支持，所以 Unity3D 的贴图类型就增加了一个 Sprite 的类型。如果导入到 Unity3D 中的贴图是那种打好的小图的图集，那么 Unity3D 能够自动识别为 Sprite 类型。可是对于那种单张的贴图，Unity3D 默认还是按照默认的设置来处理，因此如果每次需要用到这些图片，就必须手动地将其 TextureType 设为 sprite，如果贴图数量比较少，那么手动修改也没有什么了。可是如果项目中的贴图数量较多的话，这样一张一张地去调整 TextureType 可能会浪费大量的时间啊！怎么办呢？简单！写代码！\nusing UnityEngine; using UnityEditor; using System.Collections; public class ImportSprite { /// \u0026lt;summary\u0026gt; /// 批量将贴图格式转换为Sprite /// \u0026lt;/summary\u0026gt; [MenuItem(\u0026#34;Tools/ConvertToSprite\u0026#34;)] static void ConvertToSprite() { //获取所有被选中的物体 Object[] selection=(Object[])Selection.objects; //合法性处理 if(selection.Length==0) return; //批量导入贴图 foreach(Object obj in selection) { //取得每一张贴图 Texture texture=(Texture)obj; //获得贴图路径 string localpath=AssetDatabase.GetAssetPath(texture); //贴图导入 TextureImporter importer=(TextureImporter)AssetImporter.GetAtPath(localpath); //设置贴图类型 importer.textureType=TextureImporterType.Sprite; //导入项目资源 AssetDatabase.ImportAsset(localpath); } //刷新项目资源 AssetDatabase.Refresh(); } } 我们将这个脚本放到 Editor 文件夹中，如果不出现什么意外的话，Unity3D 的菜单栏中会增加一个 Tools 的菜单项，该菜单项目前只有一个子菜单项 ConvertToSprite。好了，现在我们要做的事情就是在项目资源文件夹中选中要转换成 sprite 类型的贴图，然后单击 Tools-\u0026gt;ConvertToSprite。很快(具体有多快可以自己在编辑器窗口中去尝试，总之就是很快就对了，哈哈)所有的贴图的都如我们所愿地被转换成了 sprite 类型，此时此刻你有没有懊悔当年手动创建的 92 个空物体，反正博主是后悔当初做塔防游戏的时候手动创建了 92 个空物体，如果那个时候我知道 Unity3D 可以做这些事情，我打死都不会手动去创建 92 个空物体的，现在想想都佩服当时自己的勇气啊。好了，作为第一个编辑器扩展程序，我们稍微总结下主要的内容：\n在 Unity3D 中我们可以通过 TextureImporter、ModelImporter、AudioImporter、MovieImporter 等来分别向 Unity3D 中导入贴图、模型、音频、视频等等，经过设置后最终通过 AssetDatabase.ImportAsset()来将其添加到项目中热完全，最后需要使用 AssetDatabase.Refresh()方法来刷新本地资源，使导入的资源生效。 Selection.objects 取得的物体无法区分是从场景中选取的还是从项目资源文件夹中选取的，如果需要从场景中来选取，建议使用 Selection.transforms 来代替。 案例 2 动态生成 Prefab 首先让我们来回顾一下大家平时制作 Prefab 的流程：\n在项目资源文件夹中选取素材拖放到场景中 在场景中调整名称、位置、缩放、组件等等 将物体拖放到 Prefabs 文件夹下生成 Prefab 尽管这是 Unity3D 官方推荐的一种做法，可是如果我们现在有大量的 Prefab 要制作怎么办呢？一个最直观的例子就是游戏里的敌人。在一个中等规模的游戏中，敌人的种类通常很多，而且每一个敌人的行为可能都不相同。然后从宏观的角度来看，敌人的大部分特征都是相同的，因此我们这里考虑使用程序动态生成 Prefab，这里假定 Prefab 不需要附加脚本，因为如何给 Prefab 附加脚本博主还没有研究出来。好了，下面我们来看代码： using UnityEngine; using UnityEditor; using System.Collections; public class PrefabWrap : EditorWindow { //预设物体名称 private string prefabName; //预设物体tag private static string prefabTag; //预设物体Layer private static int prefabLayer; //当前插件窗口实例 private static PrefabWrap instance; /// \u0026lt;summary\u0026gt; /// 显示插件窗口 /// \u0026lt;/summary\u0026gt; [MenuItem(\u0026#34;Tools/PrefabWrapTool\u0026#34;)] static void PrefabWrapTool() { //获取当前窗口实例 instance=EditorWindow.GetWindow\u0026lt;PrefabWrap\u0026gt;(); //显示窗口 instance.Show(); } /// \u0026lt;summary\u0026gt; /// 在OnGUI方法中实现界面定制 /// \u0026lt;/summary\u0026gt; private void OnGUI() { //绘制一个文本框 prefabName=EditorGUILayout.TextField(\u0026#34;预设物体名称:\u0026#34;,prefabName); //绘制预设物体标签选择框 prefabTag=EditorGUILayout.TagField(\u0026#34;预设物体tag:\u0026#34;,prefabTag); //绘制预设物体层级选择框 prefabLayer=EditorGUILayout.LayerField(\u0026#34;预设物体Layer:\u0026#34;,prefabLayer); //绘制一个按钮 if(GUILayout.Button(\u0026#34;生成预设物体\u0026#34;,GUILayout.Height(20))) { if(prefabName!=string.Empty) { CreatePrefab(prefabName); } } } /// \u0026lt;summary\u0026gt; /// 批量创建Prefab /// \u0026lt;/summary\u0026gt; static void CreatePrefab(string name) { //获取所有被选中的物体 Object[] selection=(Object[])Selection.objects; //合法性处理 if(selection.Length==0) return; //批量处理 foreach(Object obj in selection) { //生成预设 GameObject prefab=(GameObject)PrefabUtility.CreatePrefab(\u0026#34;Assets/Prefabs/\u0026#34;+name+\u0026#34;.prefab\u0026#34;,(GameObject)obj); //设置tag和Layer prefab.tag=prefabTag; prefab.layer=prefabLayer; //导入项目 AssetDatabase.ImportAsset(AssetDatabase.GetAssetPath(prefab)); } //刷新本地资源 AssetDatabase.Refresh(); } //当界面发生变化时重绘 void OnInspectorUpdate() { Repaint(); } } 首先我们让这个脚本继承自 EditorWindow，这样它将在 Unity3D 中显示一个窗口。在 OnGUI()方法中我们定义了窗口需要绘制的内容为一个文本框、两个选择框和一个按钮，当单击按钮后会执行 CreatePrefab()方法。当界面发生变化的时候，需要对窗口进行重绘。最终的程序演示效果如下：\n动态生成Prefab效果演示\r当我们在场景中选择好物体后，只要填好预设物体的名称、tag、Layer 就可以直接生成 Prefab 了，不过这里有个问题，因为生成 Prefab 必须要传入一个 GameObject，因此如果直接选择项目资源文件夹里的内容可能会报错，因为你选择的不是一个 GameObject。博主做这样一个功能的初衷原本是想直接为每一个精灵图片生成预设文件，现在看来需要寻找其它的方法了，不过基本思路是创建一个空物体，然后向这个空物体中增加子物体，如果大家对此有兴趣的话，可以结合本文的方法自行去尝试。\n案例 3 快速为 Sprite 设置图集 tag 接下来这个案例呢，同样是和贴图有关的内容。我们知道在没有 UGUI 以前，我们使用 NGUI 的时候要做的第一件事情就是把要用到的贴图打成图集，现在在 Unity3D 里面我们可以通过贴图的 Packing Tag 来实现图集打包，就是说具有相同 Packing Tag 的物体会被打到一张大图上，这样做的好处是节省资源。如果大家对这部分内容不太熟悉，可以了解下我的这篇文章。既然明白了原理，那么我们为什么不来尝试着通过程序将这件事情一次完成呢？好了，直接给出代码：\nusing UnityEngine; using UnityEditor; using System.Collections; public class PackageTools : EditorWindow { /// \u0026lt;summary\u0026gt; /// 图集标签 /// \u0026lt;/summary\u0026gt; private string tagName; /// \u0026lt;summary\u0026gt; /// 当前实例 /// \u0026lt;/summary\u0026gt; private static PackageTools instance; /// \u0026lt;summary\u0026gt; /// 在OnGUI方法中实现界面定制 /// \u0026lt;/summary\u0026gt; private void OnGUI() { //绘制一个文本框 tagName=EditorGUILayout.TextField(\u0026#34;PackageTagName:\u0026#34;,tagName); //绘制一个按钮 if(GUILayout.Button(\u0026#34;Package\u0026#34;,GUILayout.Height(20))) { if(tagName!=string.Empty) { PackgeTextureWidthTag(tagName); } } } /// \u0026lt;summary\u0026gt; /// 显示插件窗口 /// \u0026lt;/summary\u0026gt; [MenuItem(\u0026#34;Tools/ShowPackageTools\u0026#34;)] static void ShowPackageTools() { //获取当前窗口实例 instance=EditorWindow.GetWindow\u0026lt;PackageTools\u0026gt;(); //显示窗口 instance.Show(); } /// \u0026lt;summary\u0026gt; /// 快速为图片生成图集 /// \u0026lt;/summary\u0026gt; static void PackgeTextureWidthTag(string tagName) { //获取所有被选中的物体 Object[] selection=(Object[])Selection.objects; //合法性处理 if(selection.Length==0) return; //批量处理贴图 foreach(Object obj in selection) { //取得每一张贴图 Texture texture=(Texture)obj; //获得贴图路径 string localpath=AssetDatabase.GetAssetPath(texture); //贴图导入 TextureImporter importer=(TextureImporter)AssetImporter.GetAtPath(localpath); //判断贴图类型,只有贴图类型为Sprite且精灵类型为SpriteMode if(importer.textureType==TextureImporterType.Sprite) { importer.spritePackingTag=tagName; //导入项目资源 AssetDatabase.ImportAsset(localpath); } } //刷新本地资源 AssetDatabase.Refresh(); } //当界面发生变化时重绘 void OnInspectorUpdate() { Repaint(); } } 因为打包图集只需要一个参数，因此这个打包工具只需要一个文本框和一个按钮，整个过程和案例 2 是一样的，这里就不做分析了。这个扩展程序的演示效果如下：\n图集打包效果演示\r好了，这就是今天的内容了，今天的内容基本上涵盖了为 Unity3D 开发扩展程序的基本内容，我们接下来要做的就是积极地在平时生活、工作和学习中寻找问题和解决问题，\u0026ldquo;授人以鱼不如授人以渔\u0026rdquo;，向他人传授知识和技能，这件事情本身对博主而言就是是快乐的，博主希望今天的内容大家能够喜欢。好了，谢谢大家！\n","date":"2015-03-31T00:53:22Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/70687890/","slug":"70687890","tags":["Unity3D","编辑器","扩展"],"title":"Unity3D 游戏开发之编辑器扩展程序开发实例"},{"categories":["游戏开发"],"content":"随着游戏制作技术的不断发展，在经历了从 2D 到 3D、从单机到网游、从 PC 游戏到移动游戏的种种演变后，玩家对于游戏质量的要求越来越高，游戏制作的难度相应地增加，整个游戏研发的体系开始变得庞大而复杂，由此就产生了游戏数据配置和管理的相关问题。本文将从游戏中的\u0026quot;复活\u0026quot;和\u0026quot;暂停/恢复\u0026quot;这两个应用场景的角度来谈谈在游戏开发中如何对游戏中的数据进行管理和配置。\n为什么要谈游戏数据的配置和管理 不知道大家是不是会和博主有一样的想法，就是当你回头来思考游戏开发的时候，你常常会发现，如果忽略游戏的画面、情节、特效等等这些游戏中的可视化的东西，那么其实游戏从本质上来说就是一个大型的有限状态机(FSM)，而我们通常所做的事情基本就是在维护这个有限状态机里面的各种状态，从游戏加载到游戏开始、从游戏开始到游戏中各种事件的发生再到各种事件影响到整个有限状态机的状态，我们通常所做的事情无外乎是在维护各种状态。这种感觉在 RPG 游戏中可能会更明显些，因为在 RPG 中玩家可能是在场景中行走或者奔跑、可能是在和场景中的某个 NPC 进行对话、可能是在和面前的敌人进行战斗、可能是在和杂货店的老板讨价还价……可以说在整个游戏当中无时无刻不在进行游戏状态的切换，那么在不同的状态间切换的时候，什么最为重要呢？答案是数据。什么是数据呢？玩家的生命值、魔法值、战斗力、防御力，物品的用途、价格、数量，游戏的剧情、对话、音乐等等这些都是数据。当我们在状态间进行切换的时候，其实真正改变的就是这些数据。由此可见，面对复杂而庞大的游戏体系，如何对游戏中的数据进行配置和管理是一件值得我们去思考的问题。\n从应用场景来看游戏数据的配置与管理 首先我们来从游戏当中的两个常见的应用场景:\u0026ldquo;复活\u0026quot;和\u0026quot;暂停/恢复\u0026quot;来看看游戏数据配置和管理的重要性。 这里以博主的一款跑酷游戏为例：\n游戏截图\r应用场景——\u0026ldquo;复活\u0026rdquo; \u0026ldquo;复活\u0026quot;是一个在游戏中特别常见的功能，复活这一设定的好处在于无需重新开始游戏就能再次回到游戏当中，当然这只是我们最为直观的一个感受，更为深刻的原因是，游戏者巧妙地利用了玩家在游戏任务失败那一刻的心理。现在生活中每一个人都喜欢胜利，这种心理到了游戏世界中同样是适用的，因为游戏的目的无非就是让玩家有种成就感以获得快乐。可是当游戏任务失败的时候，玩家会竭尽全力不断尝试去打败 Boss 以获得游戏的胜利，因此在游戏中有这样一个设定，可以引导玩家在游戏中形成消费的习惯，这样游戏就能从玩家身上盈利。好了，我们来看看一个基本的\u0026quot;复活\u0026quot;的逻辑吧！\nprivate void Update() { //如果玩家的生命值大于0则游戏正常进行 if(Player.Hp\u0026gt;0) { //游戏状态为Normal GameManager.Instance.GameState=GameStateEnum.Normal; //执行正常的游戏逻辑 DoNormalEvent(); }else { //游戏状态为Over GameManager.Instance.GameState=GameStateEnum.Over; //显示GameOver ShowGameOver(); //玩家复活 ReLive() } } 玩家复活需要做两件事情：\n将游戏的状态从 Over 调整到 Normal 将玩家的状态从死亡调整到正常 调整游戏的状态特别容易，因为 GameManager 是一个典型的单例模式，因此我们可以直接将 GameState 从 Over 变成 Noral。可是对于玩家状态的调整，我们却遇到了困难。问题出在什么地方呢？问题出在我们将玩家的生命值等一系列属性都写在了 PlayerController 这个类中，如果我们将玩家的属性全部都设为 Private，那么我们将无法从外部来调整这些属性。比如我们想让玩家满血复活，可是因为这些属性都是私有的，我们无法从外部访问，所以我们在给玩家恢复生命值的时候，无法获得玩家当前的生命值以及最大生命值。可是如果我们将玩家的属性全部都设为 Public，我们可能不得不去面对在编辑器窗口中为每一个属性去赋值，因为一旦我们试图调整游戏双方力量的平衡时，这将是我们不得不去面对的问题，更为致命的玩家的属性并不是永远不变的，比如在 RPG 游戏中玩家的生命值等属性会随着角色等级的提升而不断增加。因此不管我们将这些属性设为 Public 还是 Private，我们都无法保证每次访问到的这些数据都是最新的数据。换句话说，我们不能想当然地在脚本中将玩家的属性写成一个不变的值，因为这些数据随时都在发生着变化，当然如果像敌人和 Boss 这种数值相对稳定的情况，我们可以直接在脚本中将其写成一个固定值，不过我并不推荐大家这样做。由此可见，游戏中数据配置和管理的一个重要作用是维持各个状态间的正常切换。如图是雨血前传.蜃楼中的复活界面，每次复活需要消耗一个复活玉：\n雨血\r那么博主在这款跑酷游戏里面是怎样做这个复活的呢？因为博主当时在设计这个游戏的时候考虑不周，直接将玩家的生命值写成了 100，所以在复活玩家时候，同样是先将游戏的状态调整过来，然后再将相关的 GUI 窗口隐藏，然后将玩家的生命值重新设置为 100，重新生成玩家就好了。正是因为感觉这段时间做游戏缺乏一种良好的游戏架构，所以每次游戏做到最后都是自己把逼到了绝路上，留给了自己一个自己都不想再去维护的烂摊子，这样显然是不好的，所以以后需要在正式动手写代码前做好规划，相信这样就能够保证游戏的质量了吧！任何东西学习到一定阶段都会遭遇瓶颈，尽管打破这种瓶颈的过程是痛苦的，可是如果不去打破它，那么你永远都只能停留在这个位置。\n应用场景——\u0026ldquo;暂停/恢复\u0026rdquo; 和\u0026quot;复活\u0026quot;一样，\u0026ldquo;暂停/恢复\u0026quot;同样是一个在游戏中常见的功能，该功能是给了玩家暂时离开游戏的一种选择，可以保证玩家在做其它事情的时候不会影响到游戏的进程。比如在仙剑奇侠传、古剑奇谭等游戏中，玩家可以按下 ESC 键调出游戏设置界面，在玩家进入游戏设置界面的这段时间，游戏世界里的时间似乎是静止的，场景中的敌人不会因为玩家在查看系统设置界面就去主动偷袭玩家，因为这种情况下游戏是暂停的。而当玩家退出系统设置界面后，游戏恢复为正常状态。到了移动互联网时代，游戏中出现\u0026quot;暂停/恢复\u0026quot;的情况更为普遍，这是由移动互联网时代人们玩游戏更注重休闲和娱乐这样的性质来决定的。记得天天酷跑刚刚在微信上线的那段时间，我身边好多同学都在上课的时候玩，可是因为这游戏一跑起来就根本停不下来，所以经常是一次游戏玩下来一节课就结束了。博主不提倡这样啊，玩游戏归玩游戏，可是什么事情都要有个度啊，不然就会变成玩物丧志。好了，我们分析这个案例的目的无非就是想告诉大家在游戏里增加这样一个\u0026quot;暂停/恢复\u0026quot;的功能还是十分必要的。好了，现在我们来分析下在这个应用场景中发生状态转换的时候都会牵扯到那些数据吧！\n首先，游戏暂停后，场景内所有的物体都会停止运动，此时游戏中每个物体的状态都发生了变化，不过因为在 Unity3D 中控制游戏暂停/的恢复主要是通过调整 Time.timeScale 的值来实现的。当 Time.timeScale 取值为 0 时，游戏暂停；当 Time.timeScale 取值为 1 时，游戏恢复正常。不过需要注意的是 Time.timeScale 会对 Unity3D 中所有的时间产生影响如 FixedUpdate()、协程、Destroy()、动画组件等等，所以如果对暂停后的游戏状态有特殊要求的话，建议还是通过其它的方法来实现吧！这里没有提到 Update() 和 LaterUpdate() 这是因为这两个方法不会受到影响。我们来看这样一段代码：\n//游戏是否暂停 private bool isPause = false; //暂停/恢复游戏的方法 private void Resume() { if (!isPause) { Time.timeScale = 0; isPause = true; } else { Time.timeScale = 1; isPause = false; } } 通过这段代码我们就能够实现一个基本的游戏\u0026quot;暂停/恢复\u0026quot;的功能。在游戏管理类 GameManager 中我们定义了一个玩家的得分。正常情况下，当玩家没有死亡的时候会在 GUI 中更新玩家的得分，而玩家的得分是直接采用在 Update()中累加的方式实现的，因此玩家的得分会在游戏暂停后继续更新，这当然是不符合实际情况的，因此可以在这个增量前乘上一个 Time.deltaTime 就可以解决这个问题了。博主举这个例子无非就是想告诉大家使用这种方法来暂停游戏会存在这样的问题，希望大家以后注意啊！\n跑酷游戏复活界面\r游戏数据配置和管理的思路和方法 既然我们在今天的的文章中主要阐述的就是游戏数据配置和管理，那么下面我们就来说说游戏数据配置和管理的常见的思路和方法。根据游戏中数据变动的相对大小，我们将游戏中的数据分为静态数据和动态数据两类。\n静态数据 静态数据是指在游戏中基本不变或者不需要变动的数据。比如游戏中 Boss 的等级和生命值一般都是确定的，因此这种类型的数据可以称为静态数据。同样地，游戏中 NPC 对话的内容是一种静态数据，因为 NPC 的对话内容是在设计剧情的时候就设计好的无需再对它进行修改。那么对于静态数据，我们可以考虑下列方法：\n将静态数据作为常量定义在一个类中，这样做的好处是无需对每一个脚本进行修改。 将静态数据存储在文件当中，这样做的好处是可以对数据进行管理，缺点是需要针对不同的文件编写解析接口，游戏开发中常用的数据存储形式有：Json、Xml、Excel、CSV 等。 将静态数据存储在数据库当中，如 SQLIite 等，可是这样做的缺点同样很明显，从本地读取数据库会消耗大量的资源，而且数据库文件一旦丢失，整个游戏都将无法运行。 动态数据 动态数据是指在游戏中会不断变化的数据，比如玩家的得分、玩家的生命值、玩家的经验值等等。动态数据的处理方式除作为常量写在类中以外，其它的都和静态数据是一样的，在此就不再多说了。\n总结 可能今天这篇文章显得唠叨些，甚至从技术的角度来看，这篇文章都没有讲到什么有价值的技术要点。可是在博主看来，不管一项技术有多么伟大，如果没有良好的架构或者说结构，那么当这个项目的规模到了一定程度以后，这个项目就会出现问题。因为根据破窗户理论，当你看到窗户破了而不去及时修补的话，那么时间一长你破掉的就是整个房子了。回顾博主这么长时间的游戏开发，其实做过的好多游戏到最后之所以没有做完，都是因为到最后项目基本失控、变成了一个连自己都不愿意去维护的项目，这样的情况是可怕的。平时是你一个人做项目，可能你觉得这些都没有什么，可是当你和别人一起去完成这样一个项目的时候，你的这些问题都会成为整个团队的问题。博主一直想知道自己做游戏和团队在一起做游戏会有什么不同，因为博主感觉自己在这一块确实不是掌握得很好。虽然说架构这种事情你做多了才会有经验，可是你现在发现了问题，为什么不在现在改掉呢？架构真的很重要，致那些因为架构死去的项目，真正的项目应该死在实践中，因为架构的问题最终变得不可收拾的，这件事情本身就是可耻的。好了，今天就说这么多了。\n","date":"2015-03-27T02:12:58Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3356910090/","slug":"3356910090","tags":["游戏","数据","状态"],"title":"从「复活」和「暂停/恢复」谈游戏数据配置管理"},{"categories":["独立博客"],"content":" 当你打开这篇文章的时候，你听到一首熟悉的旋律，那是因为在你的心里始终装着一张泛黄的 CD，你从来不愿意打开它，即使外表早已积满了灰尘，你依然视它如新的一般，我们喜欢把它叫做时光或者说是青春。正如你所看到的，我在博客文章中插入了网易云音乐的播放器。这是一个基于 Flash 技术实现的组件。因为博主特别喜欢网易云音乐这个音乐产品，所以博主想将这个播放器带入到我的博客。在博客中使用这个播放器，只要在文章中添加如下代码：\n\u0026lt;embed src=\u0026#34;http://music.163.com/style/swf/widget.swf?sid=29713754\u0026amp;type=2\u0026amp;auto=1\u0026amp;width=278\u0026amp;height=32\u0026#34; width=\u0026#34;298\u0026#34; height=\u0026#34;52\u0026#34; allowNetworking=\u0026#34;all\u0026#34;\u0026gt;\u0026lt;/embed\u0026gt; 可是这样的结构对于一个写博客的人来说还是显得臃肿了，能不能让这个结构更简单写呢？简单到这样：\n[music:29713754] 因为在这段代码中真正和音乐有关的只有 sid 这个参数，所以我们我们只需要关注这个参数就好了。那么，现在我们其实就是在做这样一件事情，我们在文章中插入这样一个[key:value]的结构，然后通过程序将它替换成相应的 HTML 结构，这样就实现了在 Hexo 的文章中使用[key:value]结构来编写简单插件的功能，如果经历过使用 wordpress 建站的朋友一定知道，在 wordpress 中存在许多这样的类似插件，可以帮助写作者简化某些输入内容。好了，那么今天我们就来试着为 hexo 编写这样一个小插件吧！为了避免将插件写到网页里的时候出现错误，我们首先在 NodeJS 中测试，测试程序如下：\n//定义测试内容 var str = \u0026#34;这是一条测试内容以测试这个程序是否能够正确运行,现在让我们来听一首《匆匆那年》[music:29713754]\u0026#34;; //获得匹配内容 var dicts = str.match(/\\[(.*?):(.*?)\\]/g); if(dicts.length == 0) return; //对每一个匹配项进行处理 for(var i = 0; i \u0026lt; dicts.length; i++) { //对匹配项进行分割,拆分结果为\u0026#39;[music\u0026#39;和\u0026#39;29713754]\u0026#39; //我TM就郁闷了，在这里写成\u0026#39;/:/\u0026#39;报错 //可是写成/:/就顺利通过 var dict = dicts[i].split(/:/); //获得键名 var key = dict[0].substring(1,dict[0].length); //获得ID var id = dict[1].substring(0,dict[1].indexOf(\u0026#39;]\u0026#39;)); //判断键名的类型 if (key == \u0026#39;music\u0026#39;) { str = str.replace(dicts[i],\u0026#39;\u0026lt;embed src=\u0026#34;http://music.163.com/style/swf/widget.swf?sid=\u0026#39;+ id + \u0026#39;\u0026amp;type=2\u0026amp;auto=1\u0026amp;width=278\u0026amp;height=32\u0026#34; width=\u0026#34;298\u0026#34; height=\u0026#34;52\u0026#34; allowNetworking=\u0026#34;all\u0026#34;\u0026gt;\u0026lt;/embed\u0026gt;\u0026#39;); } } //输出结果 console.log(str); 这段代码主要就是通过正则表达式来匹配文章正文中所有的[key:value]结构，然后根据 key 来确定当前结构表什么类型，根据 id 来确定当前类型的参数，尽管这里只提到了一个 music 的类型，不过我相信只要大家开动脑筋、发挥想像相信会有更好的想法产生吧！这段代码博主在本地使用 NodeJS 测试了没有不过什么问题，大家可以在截图中清楚地看到 [music:29713754] 已经被替换称了网易云音乐的 Flash 组件，这样在网页中就会显示出网易云音乐的播放器，我们就能听到这熟悉而温暖的旋律了。\nNodeJS程序演示效果\r不过真的想要吐槽下 JavaScript。本来 JavaScript 就是弱类型了吧，NodeJS 再给弄一个除了报错什么都不会的命令行，因此在本地调试 JavaScript 代码实在是太困难了。昨天为了写出一个正确的正则表达式奋战到三点钟，后来终于给写出来了，实在压抑不住内心的喜悦匆匆忙忙地就往模板里面放，结果刚放进去再重新生成页面地时候博客就华丽地挂了，尝试了若干次无果后，果断放弃然后用备份地主题文件进行了覆盖替换。总之，经过这次事情，我是再不想接触 Javacript 了，你不让我面向对象我可以容忍，因为 JavaScript 本来就不是一门面向对象的语言。可是你总得让我知道我写的这个变量是个什么类型吧？因为这个插件的编写涉及到 JavaScript、NodeJS、Hexo 所以整个过程中编程的效率特别低，因为在没有文档、没有语法提示的情况下来写这样一段代码，在我看来完全就是摸着石头过河啊！\n好了，停止吐槽！我们接下来看看这段代码怎样和 Hexo 整个到一起。按照博主的理解既然我们要对文章的内容中的[key:value]结构进行替换，我们首先应该知道文章的内容在哪里。经过这么长时间对 Hexo 的学(zhe)习(teng),我们知道文章的内容是定义在 layout_partial 下的 article.ejs 这个文件中，在这个文件中有一个叫做 item 的变量,这个 item 的真实身份其实是 Hexo 中定义的一个全局变量 post。顾名思义，这个 post 就是我们发表的文章啦，它有一个重要的属性叫做 content，就是我们这里要用到的东西了。我们来 article.ejs 这个文件中定义的一段代码：\n\u0026lt;div class=\u0026#34;entry\u0026#34;\u0026gt; \u0026lt;% if (item.excerpt \u0026amp;\u0026amp; index){ %\u0026gt; \u0026lt;%- item.excerpt %\u0026gt; \u0026lt;% } else { %\u0026gt; \u0026lt;% if (!index){ %\u0026gt; \u0026lt;% if (!index \u0026amp;\u0026amp; item.toc){ %\u0026gt; \u0026lt;%- partial(\u0026#39;extra/toc\u0026#39;) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;%- item.content %\u0026gt; \u0026lt;% } else { %\u0026gt; \u0026lt;%- item.content.substring(0,item.content.indexOf(\u0026#39;\\n\u0026#39;)) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;/div\u0026gt; 这段代码是关于文章的内容的，因此我们要对文章内容进行修改的话，就要先读懂这块儿的代码。这块儿代码首先判断文章是不是处于首页位置(index)，接着判断文章中有没有 ReadMore 标记(excerpt)。如果文章在首页位置(index)且存在 ReadMore 标记(excerpt)，那么文章显示的是 ReadMore 标记前的内容(item.excerpt);如果文章没有 ReadMore 标记(excerpt)且文章在首页位置(index)，那么文章显示的整篇文章(item.content);如果文章在首页(index)可是没有 ReadMore 标记(excerpt)，那么选取文章的第一段作为文章的摘要来显示。这就是博主的博客目前采用的方案了，如果大家对这部分感兴趣的话，可以看看自己的博客使用的主题是如何定义这部分内容的，然后在此基础上做些调整以满足个性化的需求。\n好了，那么在了解了这部分内容后，我们应该马上就能想到，我们需要掉正的代码应该是在第 9 行这个位置，即当文章不在首页的时候要显示的内容。好了，按照我们在测试程序中的写法，我们可以写出如下代码：\n\u0026lt;% var dicts=item.content.match(/\\[(.*?):(.*?)\\]/g); %\u0026gt; \u0026lt;% if(dicts.length==0){ %\u0026gt; \u0026lt;%- item.content %\u0026gt; \u0026lt;% } else { %\u0026gt; \u0026lt;% for(i=0;i\u0026lt;dicts.length;i++) %\u0026gt; \u0026lt;% { %\u0026gt; \u0026lt;% var dict=dicts[i].split(/:/); %\u0026gt; \u0026lt;% var key=dict[0].substring(1,dict[0].length); %\u0026gt; \u0026lt;% var id=dict[1].substring(0,dict[1].length-1); %\u0026gt; \u0026lt;% if(key==\u0026#39;music\u0026#39;) %\u0026gt; \u0026lt;% { %\u0026gt; item.content=item.content.replace(dicts[i],\u0026#39;\u0026lt;embed src=\u0026#34;http://music.163.com/style/swf/widget.swf?sid=\u0026#39;+ id + \u0026#39;\u0026amp;type=2\u0026amp;auto=1\u0026amp;width=278\u0026amp;height=32\u0026#34; width=\u0026#34;298\u0026#34; height=\u0026#34;52\u0026#34; allowNetworking=\u0026#34;all\u0026#34;\u0026gt;\u0026lt;/embed\u0026gt;\u0026#39;) \u0026lt;% } %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;%- item.content %\u0026gt; \u0026lt;% } %\u0026gt; 相信经过博主的一番介绍，大家已经对这段代码相当熟悉了吧，这里就是先做判断，判断文章内容中是否有[key:vaule]这样的结构，如果没有就直接输出 item.content;如果有就需要对其进行替换后再输出 item.content。好了，现在我们用这段代码替换掉第 9 行代码，然后再次输出网页。可是结果让我们白忙活了一场，因为 Hexo 在输出网页的时候会报错，可能是因为博主写的 JavaScript 脚本 Hexo 无法解析吧！好了，从昨天下午开始差不多都在想着怎样解决这个问题，到现在还是没有一点头绪，文章里讲述的方法可以作为一种尝试，如果有兴趣、有精力、有能力解决这个问题的人，可以去进一步深入地探索这个问题。今天的内容就是这样了，睡觉！\n2015 年 11 月 10 日更新: 昨天抽空研究了下 Hexo 的插件机制，发现在 Hexo 中提供了一种标签插件，可以帮助我们快速完成这种需求，所以就记录在这里。首先，它的原理是根据这样一个简单的标记来进行处理，当我们输入默认的这样的标记的时候它会被渲染为普通的引用标记，当我们在这个标记内传入参数后就可以利用程序进行处理。例如我们编写这样的简单的 JS 文件：\n/** * hexo-tag-cloudmusic * https://github.com/qinyuanpei/hexo-tag-cloudmusic.git * Copyright (c) 2015, qinyuanpei * Licensed under the MIT license. */ hexo.extend.tag.register(\u0026#39;cloudmusic\u0026#39;, function(args){ var sid = args[0]; var config = hexo.config.cloudmusic || {}; var widgetType = hexo.config.widgetType || \u0026#39;flash\u0026#39;; var widgetSize = config.widgetSize || \u0026#39;small\u0026#39;; var autoPlay = config.autoPlay || 1; var width = config.width || 278; var height = config.height || 32; if(widgetType == \u0026#39;iframe\u0026#39;){ if(widgetSize==\u0026#39;small\u0026#39;){ return \u0026#39;\u0026lt;iframe frameborder=\u0026#34;no\u0026#34; border=\u0026#34;0\u0026#34; marginwidth=\u0026#34;0\u0026#34; marginheight=\u0026#34;0\u0026#34; width=298 height=52 src=\u0026#34;http://music.163.com/outchain/player?type=2\u0026amp;id=\u0026#39; + sid + \u0026#39;\u0026amp;auto=\u0026#39; + autoPlay +\u0026#39;\u0026amp;height=32\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026#39;; }else if(widgetSize==\u0026#39;big\u0026#39;){ return \u0026#39;\u0026lt;iframe frameborder=\u0026#34;no\u0026#34; border=\u0026#34;0\u0026#34; marginwidth=\u0026#34;0\u0026#34; marginheight=\u0026#34;0\u0026#34; width=351 height=86 src=\u0026#34;http://music.163.com/outchain/player?type=2\u0026amp;id=\u0026#39; + sid + \u0026#39;\u0026amp;auto=\u0026#39; + autoPlay + \u0026#39;\u0026amp;height=66\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026#39;; }else if(widgetSize==\u0026#39;custom\u0026#39;){ return \u0026#39;\u0026lt;iframe frameborder=\u0026#34;no\u0026#34; border=\u0026#34;0\u0026#34; marginwidth=\u0026#34;0\u0026#34; marginheight=\u0026#34;0\u0026#34; width=\u0026#39; + width +\u0026#39; height=\u0026#39; + height +\u0026#39; src=\u0026#34;http://music.163.com/outchain/player?type=2\u0026amp;id=\u0026#39; + sid + \u0026#39;\u0026amp;auto=\u0026#39; + autoPlay + \u0026#39;\u0026amp;height=66\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026#39;; } }else{ if(widgetSize==\u0026#39;small\u0026#39;){ return \u0026#39;\u0026lt;embed src=\u0026#34;http://music.163.com/style/swf/widget.swf?sid=\u0026#39; + sid + \u0026#39;\u0026amp;type=2\u0026amp;auto=\u0026#39; + autoPlay + \u0026#39;\u0026amp;width=278\u0026amp;height=32\u0026#34; width=\u0026#34;298\u0026#34; height=\u0026#34;52\u0026#34; allowNetworking=\u0026#34;all\u0026#34;\u0026gt;\u0026lt;/embed\u0026gt;\u0026#39;; }else{ return \u0026#39;\u0026lt;embed src=\u0026#34;http://music.163.com/style/swf/widget.swf?sid=\u0026#39; + sid + \u0026#39;\u0026amp;type=2\u0026amp;auto=\u0026#39; + autoPlay + \u0026#39;\u0026amp;width=320\u0026amp;height=66\u0026#34; width=\u0026#34;340\u0026#34; height=\u0026#34;86\u0026#34; allowNetworking=\u0026#34;all\u0026#34;\u0026gt;\u0026lt;/embed\u0026gt;\u0026#39;; } } }); 大家可以看到这个 JS 文件本质上就是根据传入的 sid 来拼接生成网易云音乐的 widget 代码，这样当我们需要在博客中引用网易云音乐的时候只需要采用下面的标记：\n{% cloudmusic 20744792 %} 这个项目我目前发布在我的 Github 上，欢迎大家 Start 和 Fork!\n","date":"2015-03-24T10:32:39Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/828223375/","slug":"828223375","tags":["网易","云音乐","插件","Hexo"],"title":"为 Hexo 开发一个网易云音乐的文章插件"},{"categories":["独立博客"],"content":"本文将尝试借助 Coding.NET 的项目演示功能，通过对 Hexo 中支持的发布类型进行扩充，实现可以在 Hexo 中发布网页游戏，从而方便博主展示游戏作品和帮助读者了解游戏效果。\n为什么要这样做 博主是一名至今为止都还没有做出一款完整游戏(指已上线)的游戏开发者,可是即使这样，博主依然愿意将自己在游戏开发过程中的感悟和体会分享给大家，因为博主在学习编程的路上摸索了这么久，首先要感谢的就是那些愿意在互联网上分享技术的人们，不管是 Github 上愿意将项目开源的那些技术大牛，还是在博客圈子里不断探索追逐梦想的人们，如果没有他们不求回报的辛勤付出，我是绝对不可能在环境科学这样一个专业中学好编程技术的。作为一名开源技术的追逐者，我们应该抱着\u0026quot;既取之，必与之\u0026quot;来回馈开源社区。况且将自己的知识分享给其他人，不仅可以敦促自己不断地学习，更能促进相互之间的学习，所以说写博客其实本来就是一件百利而无一害的事情。\n好了，说了这么多，其实博主的想法就是能够在博客中增加项目演示的需求。博主经常在博客上写一些游戏开发的技术文章，每次都会在文章最后给出这篇文章中具体实现了一种怎样的效果。如果是静态图片当然没有什么争议，可是我们知道游戏或者说程序是一种动态的东西，所以如果采用静态图片似乎不能完全展示出具体的效果。而游戏作为一种可互动的产品，它更加强调玩家的互动和代入感，所以能够为玩家提供一个可控的操作环境就显得特别重要。以往在CSDN博客都是利用 GIFCam 录制屏幕 Gif 动画来展示效果的，可是受制于 CSDN 每次上传图片必须小于 2M 的容量限制的要求，所以使用 Gif 基本只能让读者了解一个大概。现在博客采用七牛云存储来存储博客中的图片，这是在使用 CSDN 博客时所不能相提并论的，所以现在博主的博客基本上是以这个独立博客为主，CSDN 博客只是负责将独立博客的内容更新过去，博主每隔一段时间会去维护下 CSDN 博客，所以如何有时候没有及时回复大家的评论，还希望大家能够谅解啊。\n那么，在 Gif 动画的基础上，有没有比这个方案更好的方案呢？博主的想法是直接将游戏嵌入到博客当中，这样读者在学习了技术上的实现以后可以立即体验到实际到操作游戏的感觉，从而更快地了解游戏的实现。因为博主认为只有真正热爱游戏的人才能够设计出好的游戏，所以博主最近打算在博客中开设一个专门推荐好游戏的栏目，这样可以让我们一边玩游戏、一边学习技术，这样的想法可好？哈哈，好了下面我们来说说怎样实现这个目标！\n怎样实现这样的目标 Coding.NET 是一个类似于 Github 的网站，提供了免费的项目托管服务。和 Github 不同的是 Coding.NET 为所有的 Web 项目提供了提供一个在线的演示环境，就是说只要 Coding.NET 支持你的项目，那么你的项目就可以托管在这个网站上面进行演示。基于这样一种机制，博主便想到下面两种实现的思路：\n思路 1 因为 Unity3D 可以将项目导出为 WebPlayer 项目，在 Unity5.0 中更是提供了 WebGL 的支持，可以将 Unity3D 游戏导出为网页游戏。既然可以导出网页游戏，那么我们就可以将网页项目托管到 Coidng.NET 上，然后 iframe 框架引用到博客当中。不过这种方法可能会影响到网页的加载速度和搜索引擎优化，因为所有的搜索引擎都讨厌 iframe。所以这种思路果断放弃咯！\n思路 2 将 Unity3D 导出的.unity3d 文件托管到七牛云存储上，然后通过在 Hexo 中增加一个新的模版，来实现.unity3d 文件和模版文件的对接，模板文件采用 Unity3D 的 WebPlayer 插件进行编写，在实现目的的基础上保证整个页面布局美观大方。这种方法的优点是完全原生，没有第三方依赖关系。缺点是页面是针对某一个 Hexo 主题的，没有办法做到一次编写、完全通用的效果。好了，下面我们就以这种思路来开始实现这个伟(zhuang)大(bi)的目标吧！\n模板修改 首先我们的目的是要实现在博客中集成游戏的功能，因此我们的游戏是不能作为博客的文章出现的首页，我们知道在 Hexo 中可以通过 hexo new page[PageName]这个命令来生成一个自定义页面，而且生成的自定义页面不会出现在博客首页，只有通过链接才可以访问到这个页面，因此我们可以从这里作为突破口。在输入 hexo new page[PageName]命令后我们发现在 hexo 的 source 文件夹下会生成一个以 PageName 命名的文件夹，在这个文件夹中有一个 index.md 的文件，通过修改这个文件的内容就能实现自定义页面。可是我们发现一个问题，每次生成一个新的页面就需要创建一个新的文件夹，这样的结构对我们管理游戏项目十分不便。怎么办呢？首先我们在 source 文件夹下创建一个 games 的文件夹，然后再该文件夹下创建一个子文件夹，子文件夹的命名可以任意(此处以 example 为例)关键是要在这个子文件夹里需要放置我们前面通过命令生成的 index.md 文件，此时我们就可以通过http://YourSite.com/games/example来访问这个页面了，此后如果需要发布新的游戏，可以将 example 文件夹复制一份然后重命名即可。好了，下面我们来重点说下这个 index.md 里的内容。通过查看 Unity3D 生成的网页文件我们了解到一个完整的 Unity3D 游戏需要两个东西，一个是描述页面结构的 HTML，一个是.unity3d 文件。这里面麻烦点的是 HTML，开始觉得蛮容易的，后来发现修改模板实在困难，所以不得不放弃这种思路了。\n思路 3 博主是最近了解到，Github 除了可以用 xxx.github.io 这种方式搭建博客外，还可以通过 gh-pages 分支来实现，换句话说只要我们把静态的网页放到项目的 gh-pages 下，Github 就能帮你把页面显示出来，因此我们就可以将 Unity3D 导出的网页版本游戏放到 Github 上，从而实现游戏的在线演示。好吧，满满的罪恶感啊，要是有一天 Github 被 GFW 了，我就是那个断送它的人啊！\n效果演示","date":"2015-03-24T08:54:48Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1150071886/","slug":"1150071886","tags":["Hexo","游戏","部署"],"title":"使用 Coding.NET 和 Hexo 实现网页游戏的发布"},{"categories":["编程语言"],"content":"本文将在 C#中 Socket 同步通信的基础上，分析和研究 Socket 异步编程的实现方法，目的是深入了解 Socket 编程的基本原理，增强对网络游戏开发相关内容的认识。\n什么是 Socket 编程的异步的实现 所谓 Socket 编程的异步实现是指按照异步过程来实现 Socket 编程，那么什么是异步过程呢，我们把在完成了一次调用后通过状态、通知和回调来告知调用者的方式成为异步过程，换句话说，在异步过程中当调用一个方法时，调用者并不能够立刻得到结果，只有当这个方法调用完毕后调用者才能获得调用结果。这样做的好处是什么呢？答案是高效。相信大家还记得我们在《C#中 Socket 通信编程的同步实现》这篇文章中使用多线程来实现简单聊天的案例吧，在这个案例中我们需要开启两个线程来不断监听客户端的连接和客户端的消息，这样的效率肯定是很低的。那么现在好了，我们可以通过异步过程来解决这个问题，下面我们就来看看如何实现 Socket 的异步通信。\n如何实现 Socket 异步通信 服务端 基本流程 创建套接字 绑定套接字的 IP 和端口号——Bind() 使套接字处于监听状态等待客户端的连接请求——Listen() 当请求到来后，使用 BeginAccept()和 EndAccept()方法接受请求，返回新的套接字 使用 BeginSend()/EndSend 和 BeginReceive()/EndReceive()两组方法与客户端进行收发通信 返回，再次等待新的连接请求 关闭套接字 代码示例 using System; using System.Collections.Generic; using System.Text; using System.Net; using System.Net.Sockets; namespace AsyncServer { public class AsyncTCPServer { public void Start() { //创建套接字 IPEndPoint ipe = new IPEndPoint(IPAddress.Parse(\u0026#34;127.0.0.1\u0026#34;), 6065); Socket socket = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp); //绑定端口和IP socket.Bind(ipe); //设置监听 socket.Listen(10); //连接客户端 AsyncAccept(socket); } /// \u0026lt;summary\u0026gt; /// 连接到客户端 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;socket\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; private void AsyncAccept(Socket socket) { socket.BeginAccept(asyncResult =\u0026gt; { //获取客户端套接字 Socket client = socket.EndAccept(asyncResult); Console.WriteLine(string.Format(\u0026#34;客户端{0}请求连接...\u0026#34;, client.RemoteEndPoint)); AsyncSend(client, \u0026#34;服务器收到连接请求\u0026#34;); AsyncSend(client, string.Format(\u0026#34;欢迎你{0}\u0026#34;,client.RemoteEndPoint)); AsyncReveive(client); }, null); } /// \u0026lt;summary\u0026gt; /// 接收消息 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;client\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; private void AsyncReveive(Socket socket) { byte[] data = new byte[1024]; try { //开始接收消息 socket.BeginReceive(data, 0, data.Length, SocketFlags.None, asyncResult =\u0026gt; { int length = socket.EndReceive(asyncResult); Console.WriteLine(string.Format(\u0026#34;客户端发送消息:{0}\u0026#34;, Encoding.UTF8.GetString(data))); }, null); } catch (Exception ex) { Console.WriteLine(ex.Message); } } /// \u0026lt;summary\u0026gt; /// 发送消息 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;client\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;p\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; private void AsyncSend(Socket client, string p) { if (client == null || p == string.Empty) return; //数据转码 byte[] data = new byte[1024]; data = Encoding.UTF8.GetBytes(p); try { //开始发送消息 client.BeginSend(data, 0, data.Length, SocketFlags.None, asyncResult =\u0026gt; { //完成消息发送 int length = client.EndSend(asyncResult); //输出消息 Console.WriteLine(string.Format(\u0026#34;服务器发出消息:{0}\u0026#34;, p)); }, null); } catch (Exception e) { Console.WriteLine(e.Message); } } } } 客户端 基本流程 创建套接字并保证与服务器的端口一致 使用 BeginConnect()和 EndConnect()这组方法向服务端发送连接请求 使用 BeginSend()/EndSend 和 BeginReceive()/EndReceive()两组方法与服务端进行收发通信 关闭套接字 代码示例 using System; using System.Collections.Generic; using System.Text; using System.Net; using System.Net.Sockets; namespace AsyncClient { public class AsyncTCPClient { /// \u0026lt;summary\u0026gt; /// 连接到服务器 /// \u0026lt;/summary\u0026gt; public void AsynConnect() { //端口及IP IPEndPoint ipe = new IPEndPoint(IPAddress.Parse(\u0026#34;127.0.0.1\u0026#34;), 6065); //创建套接字 Socket client = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp); //开始连接到服务器 client.BeginConnect(ipe, asyncResult =\u0026gt; { client.EndConnect(asyncResult); //向服务器发送消息 AsynSend(client,\u0026#34;你好我是客户端\u0026#34;); AsynSend(client, \u0026#34;第一条消息\u0026#34;); AsynSend(client, \u0026#34;第二条消息\u0026#34;); //接受消息 AsynRecive(client); }, null); } /// \u0026lt;summary\u0026gt; /// 发送消息 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;socket\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;message\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; public void AsynSend(Socket socket, string message) { if (socket == null || message == string.Empty) return; //编码 byte[] data = Encoding.UTF8.GetBytes(message); try { socket.BeginSend(data, 0, data.Length, SocketFlags.None, asyncResult =\u0026gt; { //完成发送消息 int length = socket.EndSend(asyncResult); Console.WriteLine(string.Format(\u0026#34;客户端发送消息:{0}\u0026#34;, message)); }, null); } catch (Exception ex) { Console.WriteLine(\u0026#34;异常信息：{0}\u0026#34;, ex.Message); } } /// \u0026lt;summary\u0026gt; /// 接收消息 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;socket\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; public void AsynRecive(Socket socket) { byte[] data = new byte[1024]; try { //开始接收数据 socket.BeginReceive(data, 0, data.Length, SocketFlags.None, asyncResult =\u0026gt; { int length = socket.EndReceive(asyncResult); Console.WriteLine(string.Format(\u0026#34;收到服务器消息:{0}\u0026#34;, Encoding.UTF8.GetString(data))); AsynRecive(socket); }, null); } catch (Exception ex) { Console.WriteLine(\u0026#34;异常信息：\u0026#34;, ex.Message); } } } } 从总体上来讲 Socket 异步编程的逻辑性更加明确了，因为我们只需要为每一个过程写好回调函数就好了。那么这个示例的效果如何呢？我们来看看它的演示效果：\nSocket异步编程效果演示\r总结 和 Socket 同步编程的案例相比，今天的这个案例可能只是对 Socket 异步编程内容的一个简单应用，因为博主到现在为止都还没有写出一个可以进行交互聊天的程序来。在 Socket 的异步编程中，服务端不需要为一个客户端单独创建一个线程来维护其连接，可是这样带来的一个问题就是博主不知道该如何实现一个多客户端的异步编程的实例。如果有朋友知道如何实现的话，还希望能够告诉我，毕竟学习就是一个相互促进的过程啊。好了，最后想说的是博主这段时间研究 Socket 异步编程中关于异步方法调用的写法问题。我们知道 Socket 异步编程中的方法是成对出现的，每一个方法都有一个回调函数，对于回调函数，这里有两种写法，以 BeginConnect 方法为例：\nm_Socket.BeginConnect(this.m_ipEndPoint, new AsyncCallback(this.ConnectCallBack), this.m_Socket);//其中ConnectCallBack是一个回调函数 或者\nm_Socket.BeginConnect(this.m_ipEndPoint,asyncResult=\u0026gt; { //在这里添加更多代码 },null) 博主为什么要在这里说这两种写法呢，有两个原因：\n第二种写法更为简洁，无需去构造容器传递 Socket 和消息，因为它们都是局部变量。如果我们使用第一种方法，因为主函数和回调函数是两个不同的函数，因此如果想要共享变量就需要通过 IAsyncResult 接口来访问容器中的值，这样显然增加了我们的工作量。 第二种写法更为优雅，这似乎是 C#语言中某种高级语法，具体叫什么我忘了，反正在 Linq 中经常看到这种写法的影子。 综合以上两个观点，博主还是建议大家使用第二种写法，博主打算有空的话将之前写的程序再重新写一遍，看看能不能找出代码中的问题。好了，今天的内容就是这样了，谢谢大家，希望大家喜欢！\n","date":"2015-03-22T09:37:04Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2041685704/","slug":"2041685704","tags":["Socket","异步","通信","编程"],"title":"C# 中 Socket 通信编程的异步实现"},{"categories":["编程语言"],"content":"本文通过分析和总结 C# 中 Socket 通信编程的关键技术，按照同步实现的方式实现了一个简单的 Socket 聊天程序，目的是通过这个程序来掌握 Socket 编程，为进一步开发 Unity3D 网络游戏打下一个坚实的基础。\nSocket 编程基础 关于 Socket 编程基础部分的内容，主要是了解和掌握.NET 框架下为 Socket 编程提供的相关类和接口方法。.NET 中常见的网络相关的 API 都集中在 System.Net 和 System.Net.Socket 这两个命名空间下，大家可以通过 MSDN 去了解这两个命名空间下相关的类和方法。这里援引一位朋友总结的一篇文章http://www.cnblogs.com/sunev/archive/2012/08/05/2604189.html，大家可以从这里获得更为直观的认识。\n什么是 Socket 编程的同步实现 本文的目的是按照同步实现的方式来实现一个简单的 Socket 聊天程序，因此在解决这个问题前，我们首先来看看什么是 Socket 编程的同步实现。所谓 Socket 编程的同步实现就是指按照同步过程的方法来实现 Socket 通信。从编程来说，我们常用的方法或者函数都是同步过程。因为当我们调用一个方法或者函数的时候我们能够立即得到它的返回值。可是我们知道在 Socket 通信中，我们不能保证时时刻刻连接都通畅、更不能够保证时时刻刻都有数据收发，因为我们就需要不断去读取相应的值来确定整个过程的状态。这就是 Socket 编程的同步实现了，下面我们来看具体的实现过程。\n如何实现 Socket 同步通信 服务端 服务端的主要职责是处理各个客户端发送来的数据，因此在客户端的 Socket 编程中需要使用两个线程来循环处理客户端的请求，一个线程用于监听客户端的连接情况，一个线程用于监听客户端的消息发送，当服务端接收到客户端的消息后需要将消息处理后再分发给各个客户端。\n基本流程 创建套接字 绑定套接字的 IP 和端口号——Bind() 将套接字处于监听状态等待客户端的连接请求——Listen() 当请求到来后，接受请求并返回本次会话的套接字——Accept() 使用返回的套接字和客户端通信——Send()/Receive() 返回，再次等待新的连接请求 关闭套接字 代码示例 using System; using System.Collections.Generic; using System.Text; using System.Net; using System.Net.Sockets; using System.Threading; namespace TCPLib { public class TCPServer { private byte[] result = new byte[1024]; /// \u0026lt;summary\u0026gt; /// 最大的监听数量 /// \u0026lt;/summary\u0026gt; private int maxClientCount; public int MaxClientCount { get { return maxClientCount; } set { maxClientCount = value; } } /// \u0026lt;summary\u0026gt; /// IP地址 /// \u0026lt;/summary\u0026gt; private string ip; public string IP { get { return ip; } set { ip = value; } } /// \u0026lt;summary\u0026gt; /// 端口号 /// \u0026lt;/summary\u0026gt; private int port; public int Port { get { return port; } set { port = value; } } /// \u0026lt;summary\u0026gt; /// 客户端列表 /// \u0026lt;/summary\u0026gt; private List\u0026lt;Socket\u0026gt; mClientSockets; public List\u0026lt;Socket\u0026gt; ClientSockets { get { return mClientSockets; } } /// \u0026lt;summary\u0026gt; /// IP终端 /// \u0026lt;/summary\u0026gt; private IPEndPoint ipEndPoint; /// \u0026lt;summary\u0026gt; /// 服务端Socket /// \u0026lt;/summary\u0026gt; private Socket mServerSocket; /// \u0026lt;summary\u0026gt; /// 当前客户端Socket /// \u0026lt;/summary\u0026gt; private Socket mClientSocket; public Socket ClientSocket { get { return mClientSocket; } set { mClientSocket = value; } } /// \u0026lt;summary\u0026gt; /// 构造函数 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;port\u0026#34;\u0026gt;端口号\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;count\u0026#34;\u0026gt;监听的最大树目\u0026lt;/param\u0026gt; public TCPServer(int port, int count) { this.ip = IPAddress.Any.ToString(); this.port = port; this.maxClientCount=count; this.mClientSockets = new List\u0026lt;Socket\u0026gt;(); //初始化IP终端 this.ipEndPoint = new IPEndPoint(IPAddress.Any, port); //初始化服务端Socket this.mServerSocket = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp); //端口绑定 this.mServerSocket.Bind(this.ipEndPoint); //设置监听数目 this.mServerSocket.Listen(maxClientCount); } /// \u0026lt;summary\u0026gt; /// 构造函数 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;ip\u0026#34;\u0026gt;ip地址\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;port\u0026#34;\u0026gt;端口号\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;count\u0026#34;\u0026gt;监听的最大数目\u0026lt;/param\u0026gt; public TCPServer(string ip,int port,int count) { this.ip = ip; this.port = port; this.maxClientCount = count; this.mClientSockets = new List\u0026lt;Socket\u0026gt;(); //初始化IP终端 this.ipEndPoint = new IPEndPoint(IPAddress.Parse(ip), port); //初始化服务端Socket this.mServerSocket = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp); //端口绑定 this.mServerSocket.Bind(this.ipEndPoint); //设置监听数目 this.mServerSocket.Listen(maxClientCount); } /// \u0026lt;summary\u0026gt; /// 定义一个Start方法将构造函数中的方法分离出来 /// \u0026lt;/summary\u0026gt; public void Start() { //创建服务端线程，实现客户端连接请求的循环监听 var mServerThread = new Thread(this.ListenClientConnect); //服务端线程开启 mServerThread.Start(); } /// \u0026lt;summary\u0026gt; /// 监听客户端链接 /// \u0026lt;/summary\u0026gt; private void ListenClientConnect() { //设置循环标志位 bool flag = true; while (flag) { //获取连接到服务端的客户端 this.ClientSocket = this.mServerSocket.Accept(); //将获取到的客户端添加到客户端列表 this.mClientSockets.Add(this.ClientSocket); //向客户端发送一条消息 this.SendMessage(string.Format(\u0026#34;客户端{0}已成功连接到服务器\u0026#34;, this.ClientSocket.RemoteEndPoint)); //创建客户端消息线程，实现客户端消息的循环监听 var mReveiveThread = new Thread(this.ReceiveClient); //注意到ReceiveClient方法传入了一个参数 //实际上这个参数就是此时连接到服务器的客户端 //即ClientSocket mReveiveThread.Start(this.ClientSocket); } } /// \u0026lt;summary\u0026gt; /// 接收客户端消息的方法 /// \u0026lt;/summary\u0026gt; private void ReceiveClient(object obj) { //获取当前客户端 //因为每次发送消息的可能并不是同一个客户端，所以需要使用var来实例化一个新的对象 //可是我感觉这里用局部变量更好一点 var mClientSocket = (Socket)obj; // 循环标志位 bool flag = true; while (flag) { try { //获取数据长度 int receiveLength = mClientSocket.Receive(result); //获取客户端消息 string clientMessage = Encoding.UTF8.GetString(result, 0, receiveLength); //服务端负责将客户端的消息分发给各个客户端 this.SendMessage(string.Format(\u0026#34;客户端{0}发来消息:{1}\u0026#34;,mClientSocket.RemoteEndPoint,clientMessage)); } catch (Exception e) { //从客户端列表中移除该客户端 this.mClientSockets.Remove(mClientSocket); //向其它客户端告知该客户端下线 this.SendMessage(string.Format(\u0026#34;服务器发来消息:客户端{0}从服务器断开,断开原因:{1}\u0026#34;,mClientSocket.RemoteEndPoint,e.Message)); //断开连接 mClientSocket.Shutdown(SocketShutdown.Both); mClientSocket.Close(); break; } } } /// \u0026lt;summary\u0026gt; /// 向所有的客户端群发消息 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;msg\u0026#34;\u0026gt;message\u0026lt;/param\u0026gt; public void SendMessage(string msg) { //确保消息非空以及客户端列表非空 if (msg == string.Empty || this.mClientSockets.Count \u0026lt;= 0) return; //向每一个客户端发送消息 foreach (Socket s in this.mClientSockets) { (s as Socket).Send(Encoding.UTF8.GetBytes(msg)); } } /// \u0026lt;summary\u0026gt; /// 向指定的客户端发送消息 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;ip\u0026#34;\u0026gt;ip\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;port\u0026#34;\u0026gt;port\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;msg\u0026#34;\u0026gt;message\u0026lt;/param\u0026gt; public void SendMessage(string ip,int port,string msg) { //构造出一个终端地址 IPEndPoint _IPEndPoint = new IPEndPoint(IPAddress.Parse(ip), port); //遍历所有客户端 foreach (Socket s in mClientSockets) { if (_IPEndPoint == (IPEndPoint)s.RemoteEndPoint) { s.Send(Encoding.UTF8.GetBytes(msg)); } } } } } 好了，现在我们已经编写好了一个具备接收和发送数据能力的服务端程序。现在我们来尝试让服务端运行起来：\nusing System; using System.Collections.Generic; using System.Text; using TCPLib; using System.Net; using System.Net.Sockets; namespace TCPLib.Test { class Program { static void Main(string[] args) { //指定IP和端口号及最大监听数目的方式 TCPLib.TCPServer s1 = new TCPServer(\u0026#34;127.0.0.1\u0026#34;, 6001, 10); //指定端口号及最大监听数目的方式 TCPLib.TCPServer s2 = new TCPServer(6001, 10); //执行Start方法 s1.Start(); } } } 现在我们来看看编写客户端 Socket 程序的基本流程\n客户端 客户端相对于服务端来说任务要轻许多，因为客户端仅仅需要和服务端通信即可，可是因为在和服务器通信的过程中，需要时刻保持连接通畅，因此同样需要两个线程来分别处理连接情况的监听和消息发送的监听。\n基本流程 创建套接字保证与服务器的端口一致 向服务器发出连接请求——Connect() 和服务器端进行通信——Send()/Receive() 关闭套接字 代码示例 using System; using System.Collections.Generic; using System.Text; using System.Net; using System.Net.Sockets; using System.Threading; namespace TCPLib { public class TCPClient { /// \u0026lt;summary\u0026gt; /// 定义数据 /// \u0026lt;/summary\u0026gt; private byte[] result = new byte[1024]; /// \u0026lt;summary\u0026gt; /// 客户端IP /// \u0026lt;/summary\u0026gt; private string ip; public string IP { get { return ip; } set { ip = value; } } /// \u0026lt;summary\u0026gt; /// 客户端端口号 /// \u0026lt;/summary\u0026gt; private int port; public int Port { get { return port; } set { port = value; } } /// \u0026lt;summary\u0026gt; /// IP终端 /// \u0026lt;/summary\u0026gt; private IPEndPoint ipEndPoint; /// \u0026lt;summary\u0026gt; /// 客户端Socket /// \u0026lt;/summary\u0026gt; private Socket mClientSocket; /// \u0026lt;summary\u0026gt; /// 是否连接到了服务器 /// 默认为flase /// \u0026lt;/summary\u0026gt; private bool isConnected = false; /// \u0026lt;summary\u0026gt; /// 构造函数 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;ip\u0026#34;\u0026gt;IP地址\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;port\u0026#34;\u0026gt;端口号\u0026lt;/param\u0026gt; public TCPClient(string ip, int port) { this.ip=ip; this.port=port; //初始化IP终端 this.ipEndPoint = new IPEndPoint(IPAddress.Parse(this.ip), this.port); //初始化客户端Socket mClientSocket = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp); } public void Start() { //创建一个线程以不断连接服务器 var mConnectThread = new Thread(this.ConnectToServer); //开启线程 mConnectThread.Start(); } /// \u0026lt;summary\u0026gt; /// 连接到服务器 /// \u0026lt;/summary\u0026gt; private void ConnectToServer() { //当没有连接到服务器时开始连接 while (!isConnected) { try { //开始连接 mClientSocket.Connect(this.ipEndPoint); this.isConnected = true; } catch (Exception e) { //输出Debug信息 Console.WriteLine(string.Format(\u0026#34;因为一个错误的发生，暂时无法连接到服务器，错误信息为:{0}\u0026#34;,e.Message)); this.isConnected = false; } //等待5秒钟后尝试再次连接 Thread.Sleep(5000); Console.WriteLine(\u0026#34;正在尝试重新连接...\u0026#34;); } //连接成功后 Console.WriteLine(\u0026#34;连接服务器成功，现在可以和服务器进行会话了\u0026#34;); //创建一个线程以监听数据接收 var mReceiveThread = new Thread(this.ReceiveMessage); //开启线程 mReceiveThread.Start(); } /// \u0026lt;summary\u0026gt; /// 因为客户端只接受来自服务器的数据 /// 因此这个方法中不需要参数 /// \u0026lt;/summary\u0026gt; private void ReceiveMessage() { //设置循环标志位 bool flag = true; while (flag) { try { //获取数据长度 int receiveLength = this.mClientSocket.Receive(result); //获取服务器消息 string serverMessage = Encoding.UTF8.GetString(result, 0, receiveLength); //输出服务器消息 Console.WriteLine(serverMessage); } catch (Exception e) { //停止消息接收 flag = false; //断开服务器 this.mClientSocket.Shutdown(SocketShutdown.Both); //关闭套接字 this.mClientSocket.Close(); //重新尝试连接服务器 this.isConnected = false; ConnectToServer(); } } } /// \u0026lt;summary\u0026gt; /// 发送消息 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;msg\u0026#34;\u0026gt;消息文本\u0026lt;/param\u0026gt; public void SendMessage(string msg) { if(msg==string.Empty || this.mClientSocket==null) return; mClientSocket.Send(Encoding.UTF8.GetBytes(msg)); } } } 同样地，我们现在来运行客户端程序，这样客户端就可以和服务端进行通信了：\nusing System; using System.Collections.Generic; using System.Text; using TCPLib; using System.Net; using System.Net.Sockets; namespace TCPLib.Test { class Program { static void Main(string[] args) { //保证端口号和服务端一致 TCPLib.TCPClient c = new TCPClient(\u0026#34;127.0.0.1\u0026#34;,6001); //执行Start方法 c.Start(); while(true) { //读取客户端输入的消息 string msg = Console.ReadLine(); //发送消息到服务端 c.SendMessage(msg); } } } } 注意要先运行服务端的程序、再运行客户端的程序，不然程序会报错，嘿嘿！好了，下面是今天的效果演示图：\n聊天窗口效果演示\r客户端下线效果演示\r总结 今天我们基本上写出了一个可以使用的用例，不过这个例子目前还存在以下问题：\n这里仅仅实现了发送字符串的功能，如何让这个程序支持更多的类型，从基础的 int、float、double、string、single 等类型到 structure、class 甚至是二进制文件的类型？\n如何让这个用例更具有扩展性，我们发现所有的 Socket 编程流程都是一样的，唯一不同就是在接收到数据以后该如何去处理，因为能不能将核心功能和自定义功能分离开来？\n在今天的这个用例中，数据传输的缓冲区大小我们人为设定为 1024，那么如果碰到比这个设定更大的数据类型，这个用例该怎么来写？\n好了，这就是今天的内容了，希望大家喜欢，同时希望大家关注我的博客！\n2016 年 1 月 24 日更新： 要解决“支持更多类型的问题”，可以从两种思路来考虑，即实现所有类型到 byte[]类型的转换或者是实现所有类型到 string 类型的转换，对于第二种思路我们通常称之为序列化，序列化可以解决所有类型到 string 类型的转换问题，唯一可能需要考量的一个部分就是缓冲区的大小问题。\n要解决“将核心功能和自定义功能分离”这个问题，可以考虑使用委托机制来实现，委托机制可以理解为一个函数的指针，在需要将函数的控制权交给用户来处理的场景中，委托都是一种有效而明智的选择。\n","date":"2015-03-15T15:05:56Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3959327595/","slug":"3959327595","tags":["Socket","通信","同步","多线程"],"title":"C# 中 Socket 通信编程的同步实现"},{"categories":["游戏开发"],"content":"今天我们来做点和游戏无关的事情吧！博主最近情绪一直比较低落，因为在找工作的过程中遇到了些挫折。当一个人内心缺乏斗志的时候，通常会难以静下心来认真地做事情，所以这段时间博主并不打算再去为大家分享新的游戏案例，希望大家能够谅解啊。\n好了，博主今天想和大家分享的是一个叫做幸运转盘的案例。我们知道平时在节假日商场为了促销商品，通常都会推出诸如转盘抽奖这样的游戏。在学了概率以后，虽然我们都知道中奖是一个小概率事件，可是人们对买彩票中奖这样的事情仍然乐此不疲。就像腾讯通过今年的春晚成功地为微信支付培养了大量忠实用户一样，虽然大家抢红包抢到的钱都不算多，可是大家都还是愿意去抢红包啊。为什么呢？呵呵，不就图一乐嘛。好了，那么下面我们一起乐一乐吧，因为激动人心的抽奖环节就要开始了！\n首先我们来看看在 Unity3D 中如何实现转盘抽奖：\n转盘游戏示意图\r从这张图片我们可以看出，转盘抽奖有两部分组成：转盘是可以旋转的、转盘指针是固定不动的。那么，好了，抽奖无非就是让转盘转起来然后再停下来嘛，直接给出代码：\nusing UnityEngine; using System.Collections; public class LuckyRoll : MonoBehaviour { //幸运转盘 private Transform mRoolPanel; //初始旋转速度 private float mInitSpeed; //速度变化值 private float mDelta=0.5f; //转盘是否暂停 private bool isPause=true; void Start () { //获取转盘 mRoolPanel = this.transform.FindChild(\u0026#34;Background\u0026#34;); } //开始抽奖 public void OnClick() { if (isPause) { //随机生成一个初始速度 mInitSpeed=Random.Range(100,500); //开始旋转 isPause = false; } } void Update() { if(!isPause) { //转动转盘(-1为顺时针,1为逆时针) mRoolPanel.Rotate(new Vector3(0,0,-1) * mInitSpeed * Time.deltaTime); //让转动的速度缓缓降低 mInitSpeed -= mDelta; //当转动的速度为0时转盘停止转动 if (mInitSpeed \u0026lt;= 0) { //转动停止 isPause = true; } } } } 这里我们随机给出一个速度 mInitSpeed，然后让它按照 mDelta 的速率缓慢的减少，当 mInitSpeed 的数值为 0 时表示转盘停止转动。好了，我们来看看最后的效果：\n转盘游戏演示\r从现在的效果来看，这个案例基本上成功了，所以以后如果碰到需要这种抽奖活动的场合，大家就可以跟美术协调好，快速地制作出这样一个幸运转盘来向身边的人们炫耀了。不过这个案例同样存在问题：\n基于随机数的转盘转动不受玩家控制，玩家无法参与到互动当中，可以考虑触摸操作，这样可以根据玩家的操作来模拟转动，提高游戏的真实性和可玩性。 因为抽奖的结果是由美术设计在转盘上的，所以程序无法根据转盘停止后指针的位置直接判断出玩家抽奖的结果以及本次抽奖是否为有效的抽奖(指针恰好停留在两个扇形区域的分界线上)。 因为这里转盘的旋转并没有严格地按照实际情况下转盘的受力情况来设计，因此可以说这个游戏中的概率分布可能不是均匀的，因此计算机里使用的随机数是伪随机数。 好了，暂时就发现这些问题，如果有朋友知道如何模拟触屏操作和阻尼运动，可以在这篇文章后面给我留言，今天的内容就是这样了，希望大家会喜欢！\n2015 年 3 月 31 日更新 今天找到了关于转盘游戏概率设计的相关内容，所以经过整理后补充在这里：\n首先，这种转盘游戏概率设计的前提是转盘固定不动，转盘指针绕中心位置旋转，与这篇文章中的恰好相反。如下图所示，在这个转盘游戏的设计中主要遵循基本的三角函数,这里以指针默认位置朝上来讲解该原理。如果指针的默认位置在其它位置上的，可以此类推。\n转盘游戏示意图\rx += xcosᶱ y += ycosᶱ\n好了，现在我们就可以通过调整指针的角度来实现抽奖游戏了。比如我们将转盘平均分成 8 份，指针角度为 0 表示奖品 A,指针角度为 45 度表示奖品 B 等等，以此类推。这样的话，我们只要调整指针的角度就可以控制抽奖的结果。可是在实际生活中，指针不可能一次就指到对应的奖品上去，通常会在旋转若干圈后慢慢地停下来。因此我们可以使用下列公式：\n指针角度 = 360 * 圈数 + (目标角度与初始角度的差值)\n这里的圈数可以通过随机数来生成，这样可以让每次抽奖更加随机些，当然为了增加抽奖的真实感，我们可以采用这篇文章中提到的减速的方法来实现一个缓停的效果。那么问题来了，如果转盘上的奖项不是均匀分布的怎么呢？这种情况可以根据转盘上圆心角的大小为每一个奖项设定一个范围，然后在这个范围内随机生成一个角度来计算指针的角度，好了，下面给出代码实现：\nusing UnityEngine; using System.Collections; using System.Collections.Generic; public class LuckyRoll2 : MonoBehaviour { //对奖项进行封装 private class WrapItem { public WrapItem(string name, float rank, float ang1, float ang2) { this.ItemName = name; this.ItemRank = rank; this.MinAngle = ang1; this.MaxAngle =ang2; } //奖项名称 public string ItemName { get; set; } //奖项概率 public float ItemRank { get; set; } //最大角度 public float MaxAngle { get; set; } //最小角度 public float MinAngle { get; set; } } //全部的奖项 private List\u0026lt;WrapItem\u0026gt; allItems; void Start () { //初始化奖品 allItems = new List\u0026lt;WrapItem\u0026gt;() { //圆心角为5°，概率为10%，以此类推 new WrapItem(\u0026#34;奖品1\u0026#34;, 10, 0, 30), new WrapItem(\u0026#34;奖品2\u0026#34;, 15, 30, 90), new WrapItem(\u0026#34;奖品3\u0026#34;, 20, 90, 165), new WrapItem(\u0026#34;奖品4\u0026#34;, 25, 165, 255), new WrapItem(\u0026#34;奖品5\u0026#34;, 30, 255, 360), }; //模拟抽奖10次 for (int i = 0; i \u0026lt; 10; i++ ) { Debug.Log(Roll()); } } /// \u0026lt;summary\u0026gt; /// 抽奖方法 /// \u0026lt;/summary\u0026gt; private string Roll() { //抽奖结果 string result = \u0026#34;\u0026#34;; //概率总精度为100 float totalRank = 100; foreach(WrapItem item in allItems) { //产生一个0到100之间的随机数 float random = Random.Range(0,totalRank); //将该随机数和奖品的概率比较 if(random \u0026lt;= item.ItemRank) { //抽奖结果 result = item.ItemName; //为转盘指针随机生成旋转角度 float angle = Random.Range(item.MinAngle, item.MaxAngle); //旋转转盘指针,此处略 break; }else { totalRank -= item.ItemRank; } } return \u0026#34;抽奖结果为:\u0026#34; + result; } } 好了，这里我们没有写转盘旋转的功能，这部分内容大家自己去实现好了，因为在 Unity3D 里面实现这样一个功能实在是太简单了。今天我们主要关注的内容是概率，所以我们重点对概率做了些研究，这里我们来讨论下算法中的概率计算问题，首先奖品 1、奖品 2、奖品 2、奖品 4、奖品 5 的概率分别为 10%、15%、20%、25%、30%，其概率之和为 100。因此\n奖品 1：从 0~100 中随机抽取一个数，这个数值小于 10 的概率显然是 10%，这是第一轮数组遍历。\n奖品 2：在第一轮数组遍历没有返回的情况下，进入第二轮遍历，此时从 0~90 中随机抽取一个数，其概率为：(1 - (10 / 100) * (15 / (100 - 10)) = 15%。同样的，抽到奖品 3 的概率为(1 - (25 / 100))*(20 / (100 - 25)) = 20%，以此类推。\n好了，这部分内容终于补充到这篇文章里了，对于这个问题的研究基本上可以告一段落，不得不说概率对于游戏开发来说还是蛮重要的啊，有时间学习下数学吧，反正咱底子不弱啊，哈哈。\n下面给出程序演示效果：\n转盘游戏概率设计效果演示\r参考资料 大家快来玩转盘抽奖游戏(走在网页游戏开发的路上)(七) PHP + jQuery 实现转盘抽奖概率可任意调整\n","date":"2015-03-12T19:13:38Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3449402269/","slug":"3449402269","tags":["转盘","游戏","概率"],"title":"使用 Unity3D 创建一个幸运转盘"},{"categories":["游戏开发"],"content":"今天来介绍博主最近捣腾的一个小游戏“贪吃蛇”。“贪吃蛇”这个游戏相信大家都不会感到陌生吧。今天博主将通过Love2D这款游戏引擎来为大家实现一个简单的贪吃蛇游戏,在本篇文章当中我们将会涉及到“贪吃蛇”的基本算法、Lua 语言编程等基本的内容，希望能够对大家开发类似的游戏提供借鉴和思考，文章中如有不足之处，还希望大家能够谅解，因为博主的游戏开发基本就是这样慢慢摸索着学习，所以难免会有不足的地方。\n游戏算法 我们首先来看看贪吃蛇是怎么移动的？ 贪吃蛇游戏算法演示1\r贪吃蛇游戏算法演示2\r贪吃蛇游戏算法演示3\r贪吃蛇游戏算法演示4\r通过这四张图的演示，我们可以发现这样一个规律：\n蛇的移动其实是将蛇身体的最后一个元素移动到第一个元素的位置\n那么完成这样一个工作需要两个步骤：\n1、将在蛇头位置插入一个新的元素 2、移除蛇尾位置的最后一个元素\n好了，了解了蛇的移动后我们再来考虑一个问题，怎样判断蛇吃到了食物？思路和蛇的移动类似，主要考虑在蛇头插入的这个元素和食物的关系，如果这个元素的坐标和食物的坐标是相同的，那么就可以认为蛇吃到了食物，此时蛇的身体应该是变长的，所以只要在蛇头位置插入一个元素就可以了。反之，如果蛇没有吃到食物，那么蛇应该是移动的，所以就可以按照移动的方法来处理了。那么在蛇头位置插入的这个元素该如何确定呢？我们来看下面这段程序：\n--计算下一个目标点 function getNextPoint() --计算下一个目标点 snake = {} if (dir == 0) then snake.x = snakes[1].x snake.y = snakes[1].y - 20 end if (dir == 1) then snake.x = snakes[1].x snake.y = snakes[1].y + 20 end if (dir == 2) then snake.x = snakes[1].x - 20 snake.y = snakes[1].y end if (dir == 3) then snake.x = snakes[1].x + 20 snake.y = snakes[1].y end return snake end 这里定义了 getNextPoint()的方法，目的是计算在蛇头位置添加的下一个元素，这里我们注意到根据蛇的移动方向(dir)的不同，其中 0 表示上、1 表示下、2 表示左、3 表示右，计算出下一个元素的位置，因为在这个游戏中网格大小是 20，所以这里可以直接根据坐标来计算一个元素的位置。snakes 是一个 table，保存的是当前的蛇的全部元素的坐标。通过维护这个 table，我们就可以利用绘图的函数绘制出蛇的身体，这样蛇就可以移动起来了。我们来看看蛇是怎样移动的：\n--核心算法——蛇的移动 function SnakeUpdate() --获取元素个数 local n = table.maxn(snakes) if (table.maxn(snakes) \u0026gt; 0) then if (getNextPoint().x == foodX and getNextPoint().y == foodY) then --将下一个目标点的位置插入表中 table.insert(snakes, 1, getNextPoint()) --将食物状态设置为BeEated foodState=\u0026#34;BeEated\u0026#34; else --将下一个目标点的位置插入表中 table.insert(snakes, 1, getNextPoint()) --移除最后一个元素 table.remove(snakes,n+1) end end end 在这里我们定义了一个 foodState 变量以保存食物的状态，当食物的状态为 BeEated 的时候表示食物被蛇吃掉了，此时应该重新生成一个食物的坐标，此时事物的状态将变成 WaitToEat。食物的坐标保存在 foodX 和 foodY 这两个变量中，大家可以到完整的代码中去查看。\n游戏状态 我们知道蛇碰到四周墙壁的时候就会死亡，此时游戏结束。这个比较简单，只要判断蛇头的坐标和屏幕的关系就可以了。因为在这个游戏中屏幕的尺寸为 640X640，所以判断游戏是否结束的代码可以这样写：\n--判断游戏状态 if(snakes[1].x \u0026lt;= 0 or snakes[1].x \u0026gt;= 640 or snakes[1].y \u0026lt;= 0 or snakes[1].y \u0026gt;= 640) then gameState = 0 else gameState = 1 end 这里 gameState 为 0 表示游戏结束，gameState 为 1 表示游戏正常进行。\n完整代码 在完成了这些核心的算法以后，剩下的事情就交给 Love2D 引擎来绘制吧，最后给出完整的程序代码：\n--定义窗口宽度和高度 local w\t= 640 local h\t= 640 --定义网格单元大小 local unitSize = 20; --方块的初始位置 local initX\t= 320 local initY\t= 320 --移动方向 local dir = 1 --贪吃蛇集合 local snakes = {} --食物状态 --WaitToEat：绘制食物 --BeEated：随机生成食物 local foodState = \u0026#34;WaitToEat\u0026#34; --游戏状态 --0：游戏结束 --1：游戏正常 local gameState = 1 --食物的位置 local foodX = 0 local foodY = 0 --Love2D加载事件 function love.load() --设置窗口标题 love.window.setTitle(\u0026#34;Love2D-贪吃蛇游戏\u0026#34;) --设置窗口大小 love.window.setMode(w,h) --定义字体 local myFont = love.graphics.newFont(30) --设置字体 love.graphics.setFont(myFont) --设置背景色 love.graphics.setBackgroundColor(255,255,255,255) --设置线条类型为平滑 love.graphics.setLineStyle(\u0026#34;smooth\u0026#34;) --设置线宽 love.graphics.setLineWidth(0.1) --蛇的初始化(蛇的长度为5) for i=1,5 do local snake = { } snake.x = initX + (i-1) * 20 snake.y = initY snakes[i] = snake end --食物初始化 foodX = love.math.random(32-1)*20 foodY = love.math.random(32-1)*20 end --Love2D绘制事件 function love.draw() --绘制竖线 love.graphics.setColor(0,0,0,255) for i = 0, w, unitSize do love.graphics.line(0,i,h,i) end --绘制横线 for j = 0, h, unitSize do love.graphics.line(j,0,j,w) end --绘制蛇 for i = 1,table.maxn(snakes) do love.graphics.setColor(0,0,255,255) love.graphics.rectangle(\u0026#34;fill\u0026#34;,snakes[i].x,snakes[i].y,20,20) end --绘制食物 if(foodState == \u0026#34;WaitToEat\u0026#34;) then love.graphics.setColor(255,0,0,255) love.graphics.rectangle(\u0026#34;fill\u0026#34;,foodX,foodY,20,20) end --如果游戏结束则显示GameOver if(gameState == 0) then love.graphics.setColor(255,0,0,255) love.graphics.print(\u0026#34;Game Over\u0026#34;,250,300) end end -- function love.update(dt) --判断游戏状态 if (snakes[1].x \u0026lt;= 0 or snakes[1].x \u0026gt;= 640 or snakes[1].y \u0026lt;= 0 or snakes[1].y \u0026gt;= 640) then gameState = 0 else gameState = 1 end --如果游戏状态为正常 if (gameState == 1) then SnakeUpdate() FoodUpdate() end end --核心算法——蛇的移动 function SnakeUpdate(dt) --获取元素个数 local n = table.maxn(snakes) if(table.maxn(snakes) \u0026gt; 0) then if(getNextPoint().x == foodX and getNextPoint().y == foodY) then --将下一个目标点的位置插入表中 table.insert(snakes, 1, getNextPoint()) --将食物状态设置为BeEated foodState=\u0026#34;BeEated\u0026#34; else --将下一个目标点的位置插入表中 table.insert(snakes, 1, getNextPoint()) --移除最后一个元素 table.remove(snakes,n+1) end end end --随机生成食物 function FoodUpdate() --如果食物被蛇吃掉则重新生成食物 if(foodState == \u0026#34;BeEated\u0026#34;) then foodX=love.math.random(32-1)*20 foodY=love.math.random(32-1)*20 foodState = \u0026#34;WaitToEat\u0026#34; end end --根据玩家按下的键位定义不同的方向 function love.keypressed(key) if (key == \u0026#34;a\u0026#34;) then dir = 2 end if (key == \u0026#34;d\u0026#34;) then dir = 3 end if (key == \u0026#34;w\u0026#34;) then dir = 0 end if (key==\u0026#34;s\u0026#34;) then dir = 1 end end --计算下一个目标点 function getNextPoint() --计算下一个目标点 local snake = {} if(dir == 0) then snake.x = snakes[1].x snake.y = snakes[1].y - 20 end if(dir == 1) then snake.x = snakes[1].x snake.y = snakes[1].y + 20 end if(dir == 2) then snake.x=snakes[1].x - 20 snake.y=snakes[1].y end if(dir == 3) then snake.x = snakes[1].x + 20 snake.y = snakes[1].y end return snake end 将代码压缩成.love 文件后就可以运行了，我们来看看最终的效果：\nLove2D 贪吃蛇游戏示例1\rLove2D 贪吃蛇游戏示例2\r本文的项目作为开源项目托管在 Github 上，可以通过Github来获取项目源代码。谢谢大家，今天的内容就是这样了。\n","date":"2015-03-10T10:51:19Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/426338252/","slug":"426338252","tags":["Love2D","游戏开发","贪吃蛇"],"title":"使用 Love2D 引擎开发贪吃蛇游戏"},{"categories":["生活感悟"],"content":"回到学校将近一周了，工作依然没有着落。每天穿梭在校园里，看着身边熙熙攘攘的人群来来往往，面对着许许多多陌生的面孔，看着太阳每天的起起落落，听着校园广播吟唱那些恍若隔世的时光。我深切地感受到作为一名即将毕业的大四学生，此时此刻站在这里是多么的多余。\n我一直想到外面去看看，因为我觉得只有到外面去才能找到让我梦想扎根的地方。我和家里人说了我的这个想法，家里建议我先在宁夏找工作，如果在宁夏实在找不到工作再考虑出去。我听从了这个建议，可是当我开始找工作的时候，我却突然发现自己错了。如果说以前想去外面是因为觉得外面的工作找起来容易的话，那么此时此刻当我在宿舍里写下这篇文章的时候，这无疑是一种赤裸裸的讽刺了，因此我发现我在银川都找不到合适的工作。第一次推开一家公司的门，我以为只要我的技术达到了应聘的要求我就可以胜任这份工作，可是我错了，环境科学这个专业会牢牢地拴住我一辈子。当我在网上投递简历的时候，我以为我的专业对我的求职不会产生什么影响，可是我又错了，我都没想到我的简历会因为专业不符合要求而被直接忽略。在这一瞬间，我突然好后悔当初的决定，我后悔选择了宁夏大学这样一个本地的 211 ，我后悔选择了环境科学这样一个我从来没有喜欢过却要羁绊我一生的专业。\n三叔从我大三的时候就开始关注我的工作问题，他当年农校毕业后工作辗转多次终于在镇上做了一个司法所的所长，有次他跟我说起当年农校里同学现在的情况时，提出了一个叫做不务正业的观点，就是说当年农校里的那些同学现在都是在做着和农校时候学到无关的工作。我不知道这样的现象是不是中国教育的一个缩影，因此我不能评价这种想法到底是对还是错。可是当我那天把去年买的教材送给一名前来借书的学弟的时候，我突然沉默了好久。我的父母含辛茹苦地供我上大学，即使是这样一个我从来没有喜欢的专业，可是我却轻而易举地将这种付出像丢垃圾一样撇在一边，到底环境科学专业对我意味着什么？到底这大学四年对我意味着什么？是我一直在努力想要想明白却始终想不明白的问题。\n我就是想单纯地去解决技术上的问题，然后通过技术让我们的生活更加美好。我一直觉得这样的想法会成为我一生都去奋斗的信仰，可是当我回顾四周的时候看到有的人早已找到工作，我真的不明白，到底是我想要的太多不肯放下身段去做些没有技术含量的工作，还是这个世界的价值存在某种扭曲：一个人要花费四年的时光去做些一辈子里只能做一次的事情，可是当付出了那么多以后突然发现生命里早已有了定数，你想要试图改变的可能仅仅因为环境科学这四个字就变成空想，你最终踏上的注定是和大部分一样平凡的生命。\n此时此刻，我处在环境科学和游戏的夹缝里，环境科学我本不喜欢，从来没有想要在这个领域谋生的打算，可是当我想要做些什么的时候，我却在这座城市里渐渐迷失了方向。网页开发、电子商务、设备维护这些仅仅是计算机行业中的一部分工作，可是大部分人都认为这就是我喜欢的计算机行业，这是吗？我在心里暗暗问自己。我去那家公司应聘的时候，公司里有个人建议我到外面去报个培训班，因为他觉得我在大学里学的和企业需求完全是两回事，可是我作为一个应届毕业生，我原本就是希望到公司里去学习实际的项目经验的，你为什么就要要求我有两到三年的经验呢，我想就算是研究生都不可能天天待在公司里慢慢积累经验吧。我没有抱怨的意思，我只是想让自己快些找到合适的工作，我实在不想继续在这样纠结下去了，希望明天到招聘会上会有所收获吧！\n","date":"2015-03-10T10:45:51Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/3321992673/","slug":"3321992673","tags":["梦想","现实","毕业季"],"title":"当梦想照进现实"},{"categories":["读书笔记"],"content":"随着 HTML5 标准最终敲定，HTML5 将有望成为游戏开发领域的的热门平台。HTML5 游戏能够运行于包括 iPhone 系列和 iPad 系列在内的计算机、智能手机以及平板电脑上，是目前跨平台应用开发的最佳实施方案。本文系根据 《HML5 Canvas 游戏开发实战》 一书中的内容整理而成，是了解和学习 HTML5 游戏开发的基础内容，希望能够帮助到那些和博主一样致力于游戏开发的朋友们！\nJavaScript 中的面向对象编程 对于游戏开发来说，面向对象编程(OOP)是一种重要而且必要的方法，所以在了解 HTML5 游戏开发前，首先应该了解 JavaScript 中的面向对象编程。JavaScript 是一种基于对象的语言，可它并不是一种真正的面向对象的编程语言，因为在 JavaScript 的语法中不存在类(Class)的概念。下面我们将分析和解决在 JavaScript 中实现封装、继承等面向对象的问题。\n在 JavaScript 中函数(function)就是就是一个类(class) //声明一个函数 function MyClass(){} //实例化一个对象 var cls1 = new MyClass(); 使用 this 关键字就可以为类增加属性 //声明一个类并定义其构造函数 function MyClass(name,age) { this.name = name; this.age = age; }; //实例化一个对象 var cls1 = new MyClass(\u0026#34;张三\u0026#34;,20) //输出cls1的两个属性值 alert(\u0026#34;name=\u0026#34; + cls1.name + \u0026#34;\u0026amp;\u0026#34; + cls1.age) 使用 prototype 属性可以为类添加方法 //声明一个类并定义其构造函数 function MyClass(name,age) { this.name = name; this.age = age; }; //为MyClass增加方法 MyClass.prototype= { toString:function() { alert(\u0026#34;name=\u0026#34; + this.name + \u0026#34;\u0026amp;\u0026#34; + this.age) }, getName:function() { alert(\u0026#34;name=\u0026#34; + this.name) }, getAge:function() { alert(\u0026#34;age=\u0026#34; + this.age) } }; 使用 apply 方法实现属性和方法的继承 //定义一个父类People function People() { this.type=\u0026#34;人\u0026#34; }; //为父类定义一个方法 People.prototype= { getType:function() { alert(\u0026#34;type=\u0026#34; + this.type) } }; //定义一个子类Student function Student(name,age,sex) { //继承父类的属性type People.apply(this,arguments); this.name = name; this.age = age; this.sex = sex; }; //声明一个Student实例 var stu = new Student(\u0026#34;张三\u0026#34;,20,\u0026#34;男\u0026#34;)； //输出type alert(stu.type) //下面我们来了解下如何继承父类的方法，继承父类方法主要通过循环使用父对象的prototype进行复制来实现，如 //重新定义子类Student function Student(name,age,sex) { //继承父类的属性type People.apply(this,arguments); //继承父类的方法，略显抽象 var prop; for(prop in People.prototype) { var proto = this.constructor.prototype; if(!proto[prop]) { proto[prop] = People.prototype[prop]; } proto[prop][\u0026#34;super\u0026#34;] = People.prototype; } //属性定义 this.name = name; this.age = age; this.sex = sex; }; //实例化Student对象 var stu = new Student(\u0026#34;张三\u0026#34;,20,\u0026#34;男\u0026#34;); stu.getType(); 静态类的实现 function staticClass() { staticClass.name = \u0026#34;张三\u0026#34;; staticClass.toString=function { alert(\u0026#34;name=\u0026#34; + staticClass.name ) }; }; alert(staticClass.name); staticClass.toString(); Canvas 绘图基础 HTML5 提供了图像、视频、音频、表单、位置、本地数据库、离线存储、websocket 等各种全新的特性，对于 HTML 游戏开发而言，我们主要关注图像、音频、本地数据库以及 websocket 等，首先我们来了解下 Canavs 绘图的基础内容。\nCanvas 是 HTML5 为我们提供的一张画布，可以让我们在 HTML 上直接绘制图形，因此 Canvas 可以作为 HTML5 游戏开发的基本元素，即 HTML5 游戏引擎的底层都是以 Canvas 元素来驱动的。Canvas 本身没有绘图的能力，需要借助于 JavaScript 来实现绘图的功能。使用 Canvas 元素只需要在网页中添加 canvas 标记即可，如\n\u0026lt;canvas id=\u0026#34;myCanavs\u0026#34; width=\u0026#34;800\u0026#34; height=\u0026#34;480\u0026#34;\u0026gt;\u0026lt;/canvas\u0026gt; 接下来我们通过 JavaScript 来获取这个 Canvas 并通过相关 API 实现绘图环境的初始化\n//获取Canvas元素 var canvas = document.getElementById(\u0026#39;myCanvas\u0026#39;); //检查canvas合法性 if(canvas \u0026amp;\u0026amp; canvas.getContext) { //获取当前上下文 var ctx = canvas.getContext(\u0026#39;2d\u0026#39;) } 因为目前 Canvas 只支持 2D 绘图，因此，这里的参数暂时只能为 2d。因为 Cnavas 绘图的 API 都封装在 ctx 这个实例中，因此下面的所有操作都是基于 ctx 来实现的：\n使用 Canvas 绘制线 //设置线宽 ctx.lineWidth = 10; //设置画笔颜色 ctx.strokeStyle = \u0026#34;red\u0026#34;; //创建一个路径 ctx.beginPath(); //路径起点 ctx.moveTo(10,10); //路径终点 ctx.lineTo(150,50); //绘制路径 ctx.stroke(); 使用 Cnavas 绘制矩形 //设置线宽 ctx.lineWidth=5; //设置画笔颜色 ctx.strokeStyle-\u0026#34;red\u0026#34; //创建路径 ctx.beginPath(); //绘制矩形 ctx.strokeRect(10,10,70,40); 或者\n//定义矩形 ctx.rect(10,10,70,40); //绘制矩形 ctx.stroke(); 如果需要对矩形进行填充\n//创建路径 ctx.beginPath() //绘制矩形 ctx.fillRect(10,10,70,40) 使用 Canvas 绘制圆 //创建路径 ctx.beginPath(); //定义圆 ctx.arc(100,100,50,0,360*Math.PI/180,true); //绘制圆 ctx.stroke(); 同样地，可以使用 fill 进行填充绘制\n//创建路径 ctx.beginPath(); //定义圆 ctx.arc(100,100,50,0,360*Math.PI/180,true); //绘制圆 ctx.fill(); 使用 Canvas 绘制圆角矩形 绘制圆角矩形需要 arcTo 函数配合 lineTo 来完成\n//创建路径 ctx.beginPath(); ctx.moveTo(40,20); ctx.lineTo(100,20); ctx.arcTo(100,20,120,40,20); ctx.lineTo(120,70); ctx.arcTo(120,90,100,90,20); ctx.lineTo(40,90); ctx.arcTo(20,90,100,70,20); ctx.lineTo(20,40); ctx.arcTo(20,20,40,20,20); //绘制圆角矩形 ctx.stroke(); 使用 Canvas 绘制复杂图形 在 HTML5 中可以通过 quadraticCurveTo 函数绘制二次贝塞尔曲线，通过 bezierCurveTo 函数绘制三次贝塞尔曲线,具体代码请参考 API 文档。\n使用 Canvas 绘制文字 //设置字体 ctx.font=\u0026#34;30px Arial\u0026#34;; //绘制文字 ctx.strokeText(\u0026#34;Hello HTML5\u0026#34;,100,50); 使用 Canvas 绘制图片 绘制图片使用 drawImage 函数，其函数原型如下：\ndrawImage(image,dx,dy); 其中 image 可以是 HTML 中的标签或者是 JavaScript 中的 Image 对象。如\n//定义一个img标签 \u0026lt;img id=\u0026#34;img_source\u0026#34; src=\u0026#34;source.jpg\u0026#34; width=\u0026#34;240\u0026#34; height=\u0026#34;240\u0026#34;/\u0026gt; 接下来通过 getElementById 来取得图像数据，并将其绘制出来\nvar img=document.getElementById(\u0026#34;img_source\u0026#34;); ctx.draw(img,200,200); 如果直接使用 JavaScript 代码\nvar img=new Image(); img.src=\u0026#34;source.jpg\u0026#34;; ctx.draw(img,200,200) 图形的平移操作 使用 translate 函数实现在水平和垂直方向上的平移\n图形的旋转操作 使用 rotate 函数实现旋转，需要注意的是传入的参数是弧度\n图形的伸缩操作 使用 scale 函数实现伸缩，当参数为负值时表示在该方向上翻转\n图形高级特效 这里主要介绍线性渐变、径向渐变、颜色反转、灰度。\n线性渐变 //创建一个线性渐变容器 var grd=ctx.createLinearGradient(0,0,200,0); //添加颜色 grd.addColorStop(0.2,\u0026#34;#00ff00\u0026#34;); grd.addColorStop(0.8,\u0026#34;#ff0000\u0026#34;); //应用渐变 ctx.fillStyle=grd; 径向渐变 //创建一个径向渐变容器 var grd=ctx.createRadialGradient(100,100,10,100,100,50); //添加颜色 grd.addColorStop(0,\u0026#34;#00ff00\u0026#34;); grd.addColorStop(,\u0026#34;#ff0000\u0026#34;); //应用渐变 ctx.fillStyle=grd; 颜色反转 遍历每个像素并对 RGB 值进行取反 灰度 灰度计算公式：gary = red * 0.3 + green * 0.59 + blue * 0.11 基础的内容就是这些了，以后如果碰到需要 HTML5 的地方可以回过头来看看。\n","date":"2015-03-08T19:14:44Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2038378679/","slug":"2038378679","tags":["游戏","HTML5","技术","笔记"],"title":"HTML5 游戏开发技术基础整理"},{"categories":["读书笔记"],"content":" 在移动互联网时代地大潮中，微信无疑是整个行业的弄潮儿。从“微信之父”张小龙最初定义微信这个产品的那一刻开始，就注定微信将走上一条平台化的道路，各种各样可能的商业模式成为人们开始不断探索微信的价值，可是这一切和你有关系吗？\n在中国这种人情社会，越来越脆弱的信任纽带，急需有微信这样的工具来加固；越来越高的交往成本，更急需微信这样的工具来降低。\n微信给中国社会带来的强大震感和冲击，其实仅仅是一个很小的圈子。互联网讲究的是大格局，强调的是跨越时空。可是大部分的中国人都不在这个格局里，我们不具备穿越时空的条件。每个人都有自己的中国梦，可是你扪心自问：你跟别人的中国梦共同的有多少呢？换句话说，我们每一个人都很宅，而微信不过是让我们更宅而已，不管有没有微信，我们都是在过着柴米油盐酱醋茶这样的普通人的生活，而中国人性格里会不由自主的对外界进行自我戒备，我们的圈子其实就那么大。\n每个人都不容易，不能再彼此伤害。互联网思维，是一种开明的思维，一种能看到大格局的思维，一种超越自我的思维。\n就像大部分人所理解的互联网一样，互联网的自由精神在带给我们便利的生活的同时，给某些人以可乘之机，使得我们距离互联网近在咫尺而又遥不可及，我们都习惯于互联网带给我们的这种便利，可是在我们内心深处我们仍然将互联网视为洪水猛兽。\n在世界范围内，全民制衡历来是种对社会有价值的力量。这种制衡一旦制度变化，就会变得持续而稳定，直接后果就是国家的回归。依次是三种境界：回归社会、回归国民、回归自然。\n我从来不喜欢政治，因为是政治是一种极其微妙的东西，正直且单纯的人永远不适合政治，可是因为互联网的舆论作用，互联网所代表的全民制衡开始不断地和政治发生着反应，互联网精神的终极目标从来都不是取代和颠覆现有的模式，而是希望在和政治博弈的过程中让这个世界更加美好。\n因为互联网文明将摧毁一切旧的东西，一切大家习以为常的东西。 在互联网面前，每个人都是平等的，大家可以通过网络获取信息、获取服务，互联网发展到今天，当我们每个人都对身边的一切习以为常的时候，互联网却在不断地改变我们与世界接触的方式，这就是创新，这就是变革。\n当互联网以一种新的模式去颠覆现有模式的时候，任何的以政治理由为出发点的干预在我看来都是徒然的，因为干预者之所以干预无非是因为这种变革影响到了其利益，可是不管怎样，政治本身并不具备互联网的思维，国家队的搜索在市场竞争中失败便是最好的例子。\n不仅仅上流社会和主流社会漠视互联网，全社会都漠视互联网。\n我们每个人每天都在使用互联网，可是我们真的了解互联网吗？我们不可或缺地依赖着互联网，却从来不对为这个行业努力付出的程序员们表示尊敬，如果有一天这个世界没有了互联网，有多少人的生活会变得混乱不堪？\n互联网的人文属性，决定了盈利模式的地域局限和社会差异。 从互联网第一天进入中国，一直到今天，无数美国的盈利模式，都没在中国火起来，这就是互联网的人文属性。\n每一个国家、每一个地区的互联网盈利模式都会存在差异，这就是互联网的人文属性。例如国外用户都有购买付费软件的习惯，可是在中国人们更喜欢盗版甚至破解。中国互联网的一个特点是免费，可是免费从来都是最贵的。如果你需要一个功能，国外用户会选择一个软件，国内用户则会选择XX助手，这就是人文上的差异。\n张小龙给了我们三个启示\n关系能为技术带来超乎想象的附加值 移动互联网的附加值压迫基于本土实现 新型移动应用在垄断用户数据方面更具魔力 当我们感慨移动互联网的时候，大数据时代已经悄然来临，在大数据时代每个人的数据都是一个数据云，如果我们能够将这些数据收集起来加以利用的话，那么这可能就是大数据时代留给我们的机会吧。\n无论是什么思维，不能解决问题，便没有价值。\n如果要问我为什么想去做一名程序员，我会说因为我喜欢解决问题。\n马化腾的故事告诉我们：不回到原点，不立足初始化的逻辑，世界对你而言，将会越来越陌生。\n以前我的叔叔告诉我，就算没有百度，这个世界仍会有千度甚至万度。可是此时此刻我终于明白，复制一种产品相对容易，可是如何发挥自我的优势将产品做到极致，这是制胜的关键，所以我仍然认为没有人可以比李彦宏更了解搜索、做好搜索。微信的成功并非是微信在技术上的成功，而是腾讯懂得重新审视自我、突破自我，这是用户关系上的成功。\n其实人生如白驹过隙，到头来不过是关系这两个字而已。\n正如儒家研究人与人间的关系、道家研究人与自己的关系、佛家研究人的今生与前世的关系，归根到底都是关系，人活着所以幸苦，不过是将大量的时间用到了维护人与人的关系上去，这就是互联网的本质——关系。\n任何形式的大大小小的互联网，无非是让我们彼此、让我们跟世界无限趋近于无缝连接而已。这种连接，就是彼此的回归。\n桌面互联网让我们更好地获取信息、移动互联网让我们更好地获取服务、大数据则让我们更好地获取关系。\n有意无意间，机会会变得有限而无限。超越是以别人为目标的颠覆，就是想打垮对方;颠覆是以自己为目标的超越，首先是孵化自己。虽然都是有意的，目标很明确，结果却不同。\n同一价值体系内的超越和颠覆，固然有意义，然而体系与体系间的超越和颠覆却更加致命。\n搜索引擎，至今仍然是互联网的制高点，搜索决定访问，离开有效访问，就没有一切，因此搜索确定一切。\n尊重自然搜索、搜索决定一切是现代网络营销的核心，离开这些一切都是徒劳的。\n","date":"2015-02-11T15:50:55Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1930050594/","slug":"1930050594","tags":["互联网","微信","哲学","人文"],"title":"互联网黑洞读书笔记(2)"},{"categories":["游戏开发"],"content":"各位朋友，大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://blog.csdn.net/qinyuanpei。今天我想和大家分享的话题是在 Unity3D 中使用 Mecanim 动画系统来控制 2D 动画。\n相信在大家的印象中，Mecanim 动画系统主要运用在 3D 动画中，因为 Mecanim 动画系统提供了像动画重定向、人体骨骼动画等 3D 动画的特性，那么 Unity3D 的 Mecanim 动画系统能不能用来控制 2D 动画呢？如果在以前，博主和大家的理解是一样的，认为 Mecanim 只能运用到 3D 动画中，对于 2D 动画只能使用传统的逐帧动画和骨骼动画。可是前不久有位朋友问我，为什么不使用动画组件来控制 2D 动画呢？博主心想啊，这 Mecanim 动画系统真的能控制 2D 动画吗？经过博主查找大量资料和亲身实践，发现 Mecanim 是可以用来控制 2D 动画的，而且由于状态机的引入，我们对动画状态的控制会变得更为简单，从写代码的角度来看，这样可以减少我们的代码量便于维护。那么好了，今天我们就来一起学习下如何使用 Mecanim 动画系统来控制 2D 动画吧！\n传统 2D 动画的实现方式 在 Unity3D 中传统 2D 动画的实现方式是基于逐帧动画的原理实现的，这种实现方式在 Unity3D 没有推出 Unity2D 前甚至在 Unity2D 推出后相当长的一段时间内，基本上我们最为常用的实现方式，博主在刚开始学习 Unity3D 的时候通常是以 2D 形式来展开的，因为博主认为 2D 和 3D 在原理上基本是相通的，如果我们掌握了 2D 游戏的基本原理，那么在实现 3D 游戏的时候就会相对容易些。我们来看看一个最简单的 2D 动画的脚本实现：\n//精灵渲染器 private SpriteRenderer mRenderer; //精灵集合 public Sprite[] Sprites; //FPS,即每秒钟的画面帧数 public float FPS = 24; //精灵索引 private int index = 0; //当前时间 private float currentTime = 0; void Start () { mRenderer = GetComponent\u0026lt;SpriteRenderer\u0026gt;(); } void Update () { //获取当前时间 currentTime += Time.deltaTime; //如果达到了更新画面的时间 if(currentTime \u0026gt;= 1 / FPS) { //使索引增加 index += 1; //清除时间记录 currentTime = 0; //当索引更新到最后一帧时,索引重置 if(index \u0026gt;= Sprites.Length) { index = 0; } } //更新画面 mRenderer.sprite = Sprites[index]; } 通过分析，我们可以发现这段脚本存在以下问题：\n动画维护困难：每增加一个动画就需要添加一个数组，不仅增加了动画的维护难度，同时降低了脚本的效率。 状态维护困难：因为在 Update 方法里实现的是一个动画，因此当我们需要在各个动画状态间进行切换的时候，我们需要使用更多的代码来维护相关逻辑。 使用 Mecanim 动画系统的实现方式 为了解决传统的 2D 动画实现方式中存在的动画维护困难、状态维护困难这两个问题，我们需要一种更好的方案来实现 2D 动画的控制，这种方案需要提供较为方便的动画维护功能，即各个动画是独立的，当改变了某一个动画时，其余的动画不会发生改变。其次，这种方案需要提供较为方便的状态维护功能，即各个动画状态切换是方便的，我们可以更好地从这一种状态切换到另一种状态。关于动画状态切换，大家可以去了解下有限状态机(FSM)的概念，这里我们不做深入的探究，这里我们选择 Unity3D 的 Mecanim 动画系统，因为 Mecanim 动画系统正好解决了这两个问题。好了，下面我们来一起学习一个 2D 动画的实例：\n首先我们在场景中创建一个名为 PlayerController 的空物体，然后在该物体的下面增加一个精灵组件(Sprite),并将其命名为 PlayerSprite，这样做的好处是 Unity3D 将为我们自动创建较为规范的命名。好了，现在我们选择 PlayerController 这个物体，然后通过 Window -\u0026gt; Animation 菜单打开 Animation 窗口：\n创建第一个动画\r首先我们点击 AddCurve 按钮，此时将弹出一个对话框让我们保存动画文件，这里我们存储为 Player@Idle.anim,并将其保存在项目目录下的 Animations\\Player 目录下，这样可以方便我们维护和查找特定的动画文件。在保存完动画文件后，此时会弹出如下的界面，我们选择 PlayerSprite 节点下的 SpriteRenderer，然后选择 Sprite，因为这里我们的 2D 动画主要是通过改变 SpriteRenderer 的 Sprite 属性来实现的，最后我们点击 Sprite 节点后面的加号来完成对象的选取。此时会在动画窗口中显示时间轴和刻度线，我们将在这里完成动画的编辑。大家可以注意到默认情况下，动画面板添加了两帧，即第 1 帧和最后一帧，其总时间是 1 秒，同时我们注意到这里有一个采样率(Sample),其实这就是当前动画的 FPS 了。好了，现在我们开始制作第一个动画：\n动画序列\r在资源文件夹中，我们可以找到当前动画的图片素材，注意到这个图片中总共有 12 帧画面，因此我们可以按照 0.05s 的间隔来分配整个时间轴，所以我们可以这样添加帧：\n为 Idle 动画添加帧\r好了，现在我们就完成了一个 Idle 动画的制作，现在打开角色的动画控制器 PlayerController，这是 Unity3D 为我们自动创建的一个动画控制器，因为我们现在只有一个 Idle 动画，所以在 Animaotr 窗口中我们可以看到只有一个 Idle 状态，现在我们将这个状态设为默认状态。好了，现在我们可以直接运行游戏，发现在场景中角色开始循环播放 Idle 动画了。好了，现在让我们重复刚才的步骤，来完成角色的其余动画。\nIdle 动画效果演示\r经过一番努力，我们现在已经完成了角色所有动画的制作，现在我们来设计角色的动画状态机：\n玩家角色控制器设计\r设计好角色的动画状态机后我们开始来编写脚本，以实现角色动画的控制：\nusing UnityEngine; using System.Collections; public class PlayerController : MonoBehaviour { public enum PlayerState { Idle, Move, LightAttack, WeightAttack } public PlayerState State=PlayerState.Idle; //玩家移动速度 public float WalkSpeed = 0.75f; public float RunSpeed = 1.5f; //玩家跳跃力的强度 public float JumpForce = 200f; //位置限制 public float MinX = -5.80f; public float MaxX = 5.80f; public float MinY = -1.80f; public float MaxY = 0.35f; //玩家朝向，默认朝右 public bool isFaceRight = true; //动画组件 private Animator mAnim; //2D刚体 private Rigidbody2D mRig2D; void Start () { mAnim=GetComponent\u0026lt;Animator\u0026gt;(); mRig2D=GetComponent\u0026lt;Rigidbody2D\u0026gt;(); } void Update() { SpriteMove(); SpriteAttack(); SpriteJump(); SpriteIdle(); } /// \u0026lt;summary\u0026gt; /// 精灵Idle /// \u0026lt;/summary\u0026gt; private void SpriteIdle() { //当玩家无任何操作时恢复到Idle状态 if (!Input.anyKey) { mAnim.SetBool(\u0026#34;Jump\u0026#34;, false); mAnim.SetBool(\u0026#34;Attack\u0026#34;, false); mAnim.SetBool(\u0026#34;BigAttack\u0026#34;, false); mAnim.SetBool(\u0026#34;Skill\u0026#34;, false); mAnim.SetBool(\u0026#34;BigSkill\u0026#34;, false); State=PlayerState.Idle; } } /// \u0026lt;summary\u0026gt; /// 精灵攻击 /// \u0026lt;/summary\u0026gt; private void SpriteAttack() { //轻击，键位J if(Input.GetKey(KeyCode.J)) { mAnim.SetBool(\u0026#34;Attack\u0026#34;, true); State=PlayerState.LightAttack; } //重击，键位K if(Input.GetKey(KeyCode.K)) { mAnim.SetBool(\u0026#34;BigAttack\u0026#34;, true); State=PlayerState.WeightAttack; } } /// \u0026lt;summary\u0026gt; /// 精灵跳跃 /// \u0026lt;/summary\u0026gt; private void SpriteJump() { if (Input.GetKey(KeyCode.I)) { mAnim.SetBool(\u0026#34;Jump\u0026#34;, true); mRig2D.AddForce(new Vector2(0, Time.deltaTime * JumpForce), ForceMode2D.Impulse); } } private void SpriteMove() { float h = Input.GetAxis(\u0026#34;Horizontal\u0026#34;); float v = Input.GetAxis(\u0026#34;Vertical\u0026#34;); Vector2 mPos = mRig2D.position; mAnim.SetFloat(\u0026#34;Speed\u0026#34;, Mathf.Sqrt(h * h + v * v)); float mPosX, mPosY; if (Mathf.Sqrt(h * h + v * v) \u0026gt; 0.5f){ mPosX = mPos.x + h * Time.deltaTime * RunSpeed; mPosY = mPos.y + v * Time.deltaTime * RunSpeed; }else{ mPosX = mPos.x + h * Time.deltaTime * WalkSpeed; mPosY = mPos.y + v * Time.deltaTime * WalkSpeed; } mRig2D.MovePosition(new Vector2(mPosX, mPosY)); if (h \u0026gt; 0 \u0026amp;\u0026amp; !isFaceRight) { FlipSrite(); } else if (h \u0026lt; 0 \u0026amp;\u0026amp; isFaceRight) { FlipSrite(); } } void FlipSrite() { if(isFaceRight){ transform.rotation=Quaternion.Euler(0,180,0); isFaceRight=false; }else{ transform.rotation=Quaternion.Euler(0,0,0); isFaceRight=true; } } } 好了，现在我们可以来看看最终的效果，博主这里是想利用这些素材来制作一个横板过关的游戏，可是因为文章篇幅有限，所以这部分内容只能留到以后再和大家分享了。\n最终效果演示\rMecanim 动画系统应用扩展 好了，到现在为止，基于 Mecanim 动画系统的 2D 动画控制基本上讲解完了。下面我们说说 Mecaanim 动画系统应用扩展。通过前面的学习，我们知道 Unity2D 使用的 Mecanim 动画系统主要是通过改变游戏体的属性来实现某种特定的动画效果的，例如我们这里的动画是通过改变角色精灵附加的 SpriteRenderer 组件的 Sprite 属性来实现的，因此从本质上来说 Unity2D 的动画控制器是一种属性动画。总体来说，Unity2D 可以实现以下类型的动画：\n位移动画：通过 Transform 组件的 Position 属性实现 旋转动画：通过 Transform 组件的 Rotation 属性实现 伸缩动画：通过 Transform 组件的 Scale 属性实现 渐变动画：通过更改指定组件的颜色或材质实现 脚本动画：通过更改指定脚本的变量或字段实现 好了，这就是今天这篇文章的全部内容了，希望大家喜欢！\n","date":"2015-02-11T13:35:58Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2583252123/","slug":"2583252123","tags":["Mecanim","动画","Unity3D"],"title":"使用 Mecanim 动画系统来控制 2D 动画"},{"categories":["读书笔记"],"content":"Lua 是一门简单而强大的语言，其本身强大的扩展性使得这门语言在游戏设计等领域发挥着重要的作用。博主曾在 Unity3D 中使用过这门语言，并且针对 Lua 和 Unity、C++等方面的内容进行了学习和讨论。最近因为在【游戏脚本高级编程】这本书中详细介绍了 Lua 脚本的相关内容，因此在这里记录下博主的读书心得，方便以后在需要的时候查阅。\nLua 系统构成 Lua 系统由 Lua 链接库、Luac 编译器、Lua 解释器三部分构成。\nLua 链接库主要由 lua.lib 和 lua.h 这两个文件组成。Lua 链接库主要负责对自身进行初始化及关闭操作、装载脚本与执行脚本、提 供可调用交互接口。 Luac 编译器是一个由命令行驱动的编译器，其名称为 Luac。当我们需要使用 Luac 编译器来编译一个脚本时，只需输入 \u0026gt;luac \u0026lt;FileName\u0026gt; //FileName为脚本名称 我们可以直接通过 Lua 链接库来装载脚本并在装载的过程中实现动态编译，可是这样会造成两个问题，即无法在动态编译过程中获取错误信息和动态编译使脚本加载速度变慢，在使用的时候应该注意到这个问题。\nLua 解释器是一个由命令行驱动的代码运行环境，我们可以直接在这个环境中运行和测试脚本代码。 Lua 脚本语法 注释：Lua 脚本的注释以\u0026ndash;开始，如 \u0026gt; --这是一句注释 当我们需要对多行脚本进行注释的时候，可以采取手动换行的方式进行多个单行的注释。\n变量：Lua 脚本中的变量是无类型的、隐式声明、首个字符必须是非数字字符、对大小写敏感。Lua 脚本中变量的一个重要特性生支持多重赋值，即允许在赋值运算符的左边同时写下多个变量。如 -- 变量个数等于数值个数 x,y,z=1,2,3 -- 变量个数大于数值个数,z的值为nil x,y,z=1,2 -- 变量个数小于数值个数,3这个数值将被忽略 x,y=1,2,3 数据类型：在 Lua 中支持 6 种数据类型，即数字(number)、字符串(string)、函数(function)、表(table)、用户数据(userdata)、空值(nil)。 数字(number)指整型和浮点型的数据。 字符串(string)指字符串类型的数据。 函数(function)指一个正式声明的函数的引用。如： function fib(n) if(n\u0026lt;2) then return n else return fib(n-1)+fib(n-2) end end -- 在Lua中函数可以赋值给变量 fib2=fib -- 调用fib函数 print(fib2(5)) 表(table)是Lua语言中最简单同时是最复杂的数据结构：简单如普通数组，复杂如链表、字典、类等。 -- 我们在构造一个数据集合时，不需要指定数据类型和数据大小 -- 完成初始化后的数据集合默认索引从1开始，除非显示地声明索引0处的数值 -- 构造一个数字类型的数组 IntArray={1,2,3,4,5} -- 构造一个字符串类型的数组 StringArray={\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;D\u0026#34;} -- 打印IntArray的第一个元素,输出为1 print(IntArray[1]) -- 显示声明StringArray索引0处的数值 StringArray[0]=\u0026#34;E\u0026#34; -- 打印StringArray的第一个元素和第二个元素，输出为E,A print(StringArray[0],StringArray[1]) -- 打印一个越界的数组值，输出为nil print(IntArray[10]) -- 在Lua中表的数据类型可以是不同的 table[0]=\u0026#34;table\u0026#34; table[1]=1 -- 在Lua中表的索引可以是任意类型,因为表是基于键-值原理来工作的 Enemy={} Enemy[\u0026#34;Name\u0026#34;]=\u0026#34;Enemy\u0026#34; Enemy[\u0026#34;HP\u0026#34;]=100 Enemy[\u0026#34;Speed\u0026#34;]=30 -- 特别地，如果Key是一个合法的字符串类型，那么Table[Key]与Table.Key是等价的。 Enemy={} Enemy.Name=\u0026#34;Enemy\u0026#34; Enemy.HP=100 Enemy.Speed=30 用户数据(userdata)是Lua语言中一个特殊的数据类型，它允许在Lua脚本的变量中存放C语言中的指针。 空值(nil)是各种语言中通用的一种数据类型，在此不再赘述。 在Lua脚本中我们可以使用type()函数来获取任意数据的类型 逻辑与表达式：Lua 和大部分的编程类似支持加减乘除等运算，不同的是在 Lua 中使用~=来表示不等关系。 Lua 支持的条件逻辑主要有 if-then-else 以及嵌套的 if-then-else，Lua 不支持 switch 结构。Lua 支持的循环结构主要有 while、for、repea 三种结构，如： -- 这是一个while循环 i=0 while(i\u0026lt;10) do i++ print(i) end -- 这是一个for循环 for i=0,10 do print(i) end -- 这是一个repeat循环 repeat print(i) i++ until(i\u0026gt;10) -- 这是一个扩展的for循环，类似于Foreach结构,主要用来遍历表(table) for key,value in tables do print(k,value) end Lua 与 C/C++交互 Lua 与 C/C++交互主要通过 Lua 提供的 C API 来完成，其核心是 Lua 堆栈，一个简单的 C++代码调用 Lua 脚本的示例代码如下：\n#include \u0026lt;iostream\u0026gt; using namespace std; #include \u0026lt;iostream\u0026gt; extern \u0026#34;C\u0026#34; { #include \u0026#34;lua.h\u0026#34; #include \u0026#34;lualib.h\u0026#34; #include \u0026#34;lauxlib.h\u0026#34; } using namespace std; int main() { //创建Lua环境 lua_State* L=luaL_newstate(); //打开Lua标准库,常用的标准库有luaopen_base、luaopen_package、luaopen_table、luaopen_io、 //luaopen_os、luaopen_string、luaopen_math、luaopen_debug luaL_openlibs(L); //下面的代码可以用luaL_dofile()来代替 //加载Lua脚本 luaL_loadfile(L,\u0026#34;script.lua\u0026#34;); //运行Lua脚本 lua_pcall(L,0,0,0); //将变量arg1压入栈顶 lua_getglobal(L,\u0026#34;arg1\u0026#34;); //将变量arg2压入栈顶 lua_getglobal(L,\u0026#34;arg2\u0026#34;); //读取arg1、arg2的值 int arg1=lua_tonumber(L,-1); int arg2=lua_tonumber(L,-2); //输出Lua脚本中的两个变量 cout \u0026lt;\u0026lt;\u0026#34;arg1=\u0026#34;\u0026lt;\u0026lt;arg1\u0026lt;\u0026lt;endl; cout \u0026lt;\u0026lt;\u0026#34;arg2=\u0026#34;\u0026lt;\u0026lt;arg2\u0026lt;\u0026lt;endl; //将函数printf压入栈顶 lua_getglobal(L,\u0026#34;printf\u0026#34;); //调用printf()方法 lua_pcall(L,0,0,0); //将函数sum压入栈顶 lua_getglobal(L,\u0026#34;sum\u0026#34;); //传入参数 lua_pushinteger(L,15); lua_pushinteger(L,25); //调用printf()方法 lua_pcall(L,2,1,0);//这里有2个参数、1个返回值 //输出求和结果 cout \u0026lt;\u0026lt;\u0026#34;sum=\u0026#34;\u0026lt;\u0026lt;lua_tonumber(L,-1)\u0026lt;\u0026lt;endl; //将表table压入栈顶 lua_getglobal(L,\u0026#34;table\u0026#34;); //获取表 lua_gettable(L,-1); //输出表中第一个元素 cout \u0026lt;\u0026lt;\u0026#34;table.a=\u0026#34;\u0026lt;\u0026lt;lua_tonumber(L,-2)\u0026lt;\u0026lt;endl; //调用C++方法首先需要注册该方法 lua_register(L, \u0026#34;AverageAndSum\u0026#34;, AverageAndSum); } static int AverageAndSum(lua_State *L) { //返回栈中元素的个数 int n = lua_gettop(L); //存储各元素之和 double sum = 0; for (int i = 1; i \u0026lt;= n; i++) { //参数类型处理 if (!lua_isnumber(L, i)) { //传入错误信息 lua_pushstring(L, \u0026#34;Incorrect argument to \u0026#39;average\u0026#39;\u0026#34;); lua_error(L); } sum += lua_tonumber(L, i); } //传入平均值 lua_pushnumber(L, sum / n); //传入和 lua_pushnumber(L, sum); //返回值的个数，这里为2 return 2; } 请确保在计算机中安装了 Lua 环境，并在 VC++目录中添加相关的头文件引用和库文件引用。相应的 Lua 脚本代码定义如下：\n--在Lua中定义两个变量 arg1=15 arg2=20 --在Lua中定义一个表 table= { a=25, b=30 } --在Lua中定义一个求和的方法 function sum(a,b) return a+b end --在Lua中定义一个输出的方法 function printf() print(\u0026#34;This is a function declared in Lua\u0026#34;) end --在Lua中调用C++中定义并且注册的方法 average,sum=AverageAndSum(20,52,75,14) print(\u0026#34;Average=\u0026#34;.average) print(\u0026#34;Sum=\u0026#34;.sum) ","date":"2015-02-03T16:06:31Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1940333895/","slug":"1940333895","tags":["Lua","脚本语言","语法","游戏"],"title":"脚本语言编程：Lua 脚本编程入门"},{"categories":["读书笔记"],"content":" 互联网是什么？当我们都渐渐习惯将互联网当作生活的一部分的时候，面对这样一个问题，我们似乎是迷茫和困惑的。因为当互联网渐渐地开始改变我们生活的那一刻起，我们就在和互联网不断地发生着剧烈反应。\n本书作者仲昭川在互联网经济这一章曾这样描述过互联网：\n东方的先哲曾以“天人合一论”兴起了以道德为主线的农业文明，随后西方的民主法治和流水线分工率先把社会重新组合为以金钱为主线的工业文明。而Internet带来的信息革命，又极大地破坏了以这个分析为哲学、以垄断为目标、以金钱为动力的西方体系，人类世界的文明再次转向以神秘为哲学、以分享为目标、以道德为动力的东方体系。\n这样的描述让我们开始重新审视互联网，互联网时代的本质是数据对数据、信息对信息交换，这种经济的特点是，在给了人们更多选择的自由的同时，让更多的东西逐渐从黑暗中走出来，成为公信力引导下的公开博弈，而更加不可思议的是，这种近乎原始时代的信誉体系却是人们主动地、自发地建立并维护的。社会的一切都是价格和价值，而到了互联网时代一切都变成了利益和兴趣。互联网时代更多的是在以价值为导向，而非以技术为导向。因为真正能改变甚至颠覆传统行业的互联网模式，可能并不是建立在一项伟大的技术的基础之上，而是这种新的模式能为人们的生活带来怎样的利用价值，互联网的本质是服务行业，这种属性到了移动互联网时代变得更加地明确了。\nIT行业和互联网行业是完全不同的两个行业。这种界定，是科技与人文、技术与关系之间的分野。\n具体的理解是，互联网是一个泛行业或者说全行业，因为对于任何一个行业，只要你有足够的线上用户就可以称之为互联网行业。而在不久的将来，任何行业都将具有互联网行业的属性，因为互联网将深刻地影响到各个行业。可以说互联网的本质就是关系，而且是全新的、广泛的关系、跨越时空的关系;而且是不管什么需求，都要发生关系;而且是跟任何人发生关系，不管是熟人还是陌生人;而且是随时随地发生关系，不管是此时此地还是彼时彼地。而IT是什么呢？IT是科技，解决技术问题，当所有的技术集成到一起并能成功运行的时候就是IT,当所有技术集成到一起而且足够轻的时候就是便携，便携就会产生移动，当人们使用这些便携的IT产品相互发生关系时就是移动互联网。\n互联网本来就是要让不同兴趣的人分开，让不同利益的人早点各奔前程\n互联网表面上没有边界，可是实际上互联网中的每个人都有自己的倾向和归属。因此互联网上不同的兴趣、不同的利益，使互联网上到处都输部落，不管聊天群还是网站，本质上都是一群兴趣相投的人组成的部落。\n当大家都希望长期地、大范围地、频繁地、深入地发生关系的时候，就会产生对品牌的渴望和依赖。\n因为品牌代表了一种稳定的关系，它包含了实用性、信任度、荣誉感，即品牌权威所说的三种关系：利益关系、情感关系、社会关系。\n人类文明发展到今天，基本就分为两类：人文与科技。人文崇尚感受，科技崇尚数理\n人文的东西，生来就是为了创造经典并流传后世，一到顶峰就很难被超越，给世人留下了恒久的享受或遗产。人文所对应的，是精神、道义、经验、规范、模式、规则等，诸如此类，人文是人们用来管理、调整并保持欲望的一种手段。\n科技的东西，是人们用来满足旧欲望并刺激新欲望的，所以它注定昙花一现，随时准备离场、时刻等待淘汰。那些驻足在时光之海的科技发明，不是因为不伟大，而是因为存活时间太短。\n互联网的本质是关系\n你了解关系，就了解相互需求，就知道怎么让对象找你，而不是用大成本去寻找对象。\n互联网思维有两个特点：与众不同、与己不同\n与众不同就是要存同求异，因为多元共生的互联网才能有创新和活力。 与己不同就是找出自己的优势和不足，和自己发生关系。\n","date":"2015-02-03T09:45:57Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1478979553/","slug":"1478979553","tags":["互联网","哲学","思维","价值"],"title":"互联网黑洞读书笔记(1)"},{"categories":["游戏开发"],"content":"各位朋友，大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://blog.csdn.net/qinyuanpei。今天我们来聊聊常用办公软件 Excel 和游戏开发那不为人知的秘密。今天的内容将涉及到 Excel 在游戏开发中的应用以及如何利用程序解析 Excel 中的数据。作为常用的办公软件的 Excel 相信大家都不陌生啦。可是如果我们认为 Excel 只是办公软件的话，那么这就不只是天真而是 Out 了。事实上 Excel 和游戏开发有着密切的联系，不知道大家还记不记得那款利用 Excel 开发出来的三国杀，这可能是 Excel 第一次以游戏开发的身份出现在大家面前吧。我们知道在游戏开发领域有一种工作叫做策划，就像在软件开发领域有一种工作叫做产品经理一样。而在诸多的策划工作中，数值策划是一个可以直接影响游戏进程的工作，因为数值策划体现了一个游戏在整体数值上的平衡，设计者需要维护好这样一个平衡，确保游戏外的玩家和游戏里的敌人面对的是同一个公平的虚拟世界。\n例如，在《仙剑奇侠传四》这款游戏中，韩菱纱在游戏后期的速度可以说是完全打破了游戏的平衡性，因为韩菱纱本身的速度就比较快，再加上仙风云体术的加速效果完全对玄霄产生了戏剧性压制，导致在游戏结尾的 Boss 战中经常是韩菱纱出手 N 次后才挨到玄霄出手，我们知道韩菱纱的乾坤一掷每次消耗气 15，可是因为韩菱纱的速度足够快，所以韩菱纱完全可以通过普通物理攻击快速地积满气进而施展乾坤一掷，这就是游戏的平衡性被打破了呀，更不要说这部游戏里最为经典的千方残光剑 Bug 了，这同样是游戏平衡性的问题，归根到底是紫英的这个技能在配置数据时出现了错误，这充分说明数据的正确合理与否是会对游戏产生重要影响的。\n慕容紫英千方残光剑\r韩菱纱乾坤一掷\r尽管我们可以使用 Xml、Json、ini、数据库等存储形式来存储这些数据，可是毫无疑问的是，Excel 是 Window 平台上最好的数据处理软件，因此数值策划更倾向于使用 Excel 来设计游戏中的数据，面对如此重要的数值策划工作，我们自然希望在解析 Excel 文件时不会出现错误，可是我们总不能指望着策划把 Excel 数据转换成我们能处理的数据类型吧，因此就有了博主今天的这篇文章，所以在今天的文章中我们主要的内容就是如何通过程序来解析 Excel 文件。\n项目需求 最近博主一个朋友向我抱怨，说手头上有好几百个 Excel 工作表要处理，大概几十万条数据吧。原因是当时公司分配任务时交待不清，等到了向公司交接数据的时候，朋友忽然发现这些 Excel 文件的表格格式和公司规定的不一样啊。这可急坏了博主的这位朋友，博主的朋友只好不断地的复制、黏贴，因为这些数据是分布在不同的数据表里，朋友整天都忙得焦头烂额，可是即使这样效率还是得不到保证啊，朋友最后找到了博主这里，问我能不能编写程序帮他解决这个问题。因为平时经常与技术圈子里的朋友聊天，所以在博主印象里 Excel 的解析在游戏开发中还是较为常见的，而且博主知道对于微软的 Office 办公软件是可以通过 VBA 编程来实现某些功能的，可是因为博主一直在用国产的 WPS，所以对于 Excel 的解析基本上是停留在一个概念性的认识上，可是朋友的忙不能不帮不是，所以博主决定借着这个机会好好研究下 Excel 文件的解析。\n解决方案 因为博主在之前并没有过解析 Excel 文件的经历，所以博主就到 Github 上淘了些开源项目。和很多人爱逛天猫、淘宝的经历类似，如果你发现有一个人经常喜欢到 Github 上晃荡、喜欢关注技术类的博客或者资讯、经常再看 PDF 版的技术文档或书籍，请千万不要怀疑，这个人绝对是程序员。哈哈，好了，玩笑就此打住啊。经过博主对这些开源项目的简单分析和整理，目前，对 Excel 文件解析的解决方案主要有以下三种：\nMicrosoft.Office.Interop.Excel 第一种解决方案是基于微软提供的 Office API,这组 API 以 COM 组件的形式给出，我们可以通过调用该 API 实现对 Excel 文件的解析。使用这组 API 非常简单,博主稍后会为大家给出一个示例代码。微软的 Office API 特点是使用起来方便，可以使用 C#、Visual Basic 等语言进行相关开发。可是这种解决方案的的缺点同样很明显，因为 COM 组件主要依赖于系统，因此使用 COM 组件需要在系统中注册，这将对代码的可移植性产生影响，而且受制于 COM 技术，这种解决方案只能运行在 Windows 平台上，无法实现跨平台，加之解析速度较慢，故，这种方案通常只适合在解析速度要求不高，运行环境为 Windows 平台的应用场景。\nExcelReader 第二种解决方案得益于 OpenOffice 标准,OpenOffice 标准可以让我们使用一种标准来解析和处理 Excel 文件而无需关注 Excel 文件是来自微软的 Misrosoft Office、金山的 WPS 还是其它的办公软件。如果说第一种解决方案是 Windows 平台上解析 Excel 文件的选择之一，那么 ExcelRead 就是跨平台目标下解析 Excel 文件的首选方案。尤其像 Unity3D 这样的跨平台解决方案下，选择一个跨平台的类库或者组件能够保证我们的游戏在各种平台下稳定地运行，所以 ExcelRead 是博主向大家推荐的一个跨平台的 Excel 解析方案。\nFastExcel 第三种解决方案 FastExcel 是博主在解决博主的这位朋友的问题时所采取的方案。FastExcel 是一个在开源世界里比较著名的 Excel 读写的类库，因此使用这个类库可以得到较为广泛的社区支持，而且在 FastExcel 这个项目的源代码中，作者为我们提供了使用 FastExel 进行 Excel 解析的相关示例，具有较高的参考价值，基本上可以在这个示例的基础上写出可以运行的代码。根据示例代码的运行结果使用 FastExcel 单独读写 100000 行数据基本上维持在 3~4 秒，读写速度还是蛮快的。不过 FastExcel 使用的是迭代器和 Linq to Xml 来读取 Excel 文件的，所以当数据表中存在空白单元格时，读写的时候会比较诡异，这一点希望大家注意。\n工程案例 既然今天的主题是 Unity3D 游戏开发，所以无论我们在前面提出了什么样的解决方案，最后我们都要落实到游戏开发上，所以最后和大家分享的是一个 Unity3D 配合 ExcelReader 实现 Excel 解析的简单案例。为什么要选择 ExcelReader 呢？因为 ExcelReader 是一个跨平台的解决方案。好了，下面我们一起来学习这个案例：\nusing UnityEngine; using System.Collections; using System.IO; using Excel; using System.Data; public class ExcelScripts : MonoBehaviour { void Start () { FileStream m_Stream=File.Open(Application.dataPath + \u0026#34;\\\\Excel\\\\UserLevel.xlsx\u0026#34;,FileMode.Open,FileAccess.Read); //使用OpenXml读取Excel文件 IExcelDataReader mExcelReader=ExcelReaderFactory.CreateOpenXmlReader(m_Stream); //将Excel数据转化为DataSet DataSet mResultSets=mExcelReader.AsDataSet(); //读取行数 int rowCount=mResultSets.Tables[0].Rows.Count; //逐行读取,从第一行读以跳过表头 for(int i=1;i\u0026lt;rowCount;i++) { //将读取的Excel数据转化成数据实体 UserLevel mUser=new UserLevel(); mUser.Name=mResultSets.Tables[0].Rows[i][0].ToString(); mUser.Level=mResultSets.Tables[0].Rows[i][1].ToString(); mUser.Description=mResultSets.Tables[0].Rows[i][2].ToString(); mUser.Skill=mResultSets.Tables[0].Rows[i][3].ToString(); //输出Debug信息 Debug.Log(mUser.ToString()); //ADD:更多逻辑 } } //定义一个数据实体类UserLevel private class UserLevel { private string m_Name; public string Name { get { return m_Name;} set { m_Name = value;} } private string m_Level; public string Level { get {\treturn m_Level;} set {\tm_Level = value;} } private string m_Description; public string Description { get { return m_Description;} set { m_Description = value;} } private string m_Skill; public string Skill { get {\treturn m_Skill;}\tset {\tm_Skill = value;} } public override string ToString() { return string.Format(\u0026#34;Name={0}\u0026amp;Level={1}\u0026amp;Description={2}\u0026amp;Skill={3}\u0026#34;, m_Name,m_Level,m_Description,m_Skill); } } } 好了，这就是今天这篇文章的全部内容了，希望大家喜欢！\n","date":"2015-01-25T19:41:57Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/906436376/","slug":"906436376","tags":["Unity3D","游戏","Excel"],"title":"当 Unity3D 游戏开发遇上 Excel"},{"categories":["游戏开发"],"content":"各位朋友，大家好，我是秦元培，欢迎大家关注我的博客，我的博客地址是http://blog.csdn.net/qinyuanpei。我们知道一个完整的塔防游戏由地图、敌人、防守单位三个部分组成，在上一篇文章中我们已经对地图这块儿进行了全面的讲解，今天我们来说说敌人和防守单位。\n敌人篇 敌人自动寻路的实现 敌人在游戏中有一个基本的行为，即沿着寻路路径向我方阵地移动并发起攻击。在地图篇中，我们详细地介绍了敌人寻路路径的生成原理。既然有了敌人寻路的路线，那么怎么让敌人沿着路线移动呢？其实只要指定敌人寻路的起点就可以了，因为在寻路路径的设计中，我们使用的是一个类似于链表的结构，这样我们就能根据每个结点获取它的目标结点，从而实现敌人沿着寻路路径移动的效果了。因为敌人寻路的路线是定义在PathNode类中的，因此我们可以写出下面这样的代码：\nvoid Move() { Vector3 mPos1=this.transform.position; Vector3 mPos2=this.StartNode.transform.position; //计算敌人与路径节点间的距离 float mDis=Vector2.Distance(new Vector2(mPos1.x,mPos1.y),new Vector2(mPos2.x,mPos2.y)); if(mDis\u0026lt;0.1F){ if(StartNode.ThatNode==null){ //对防守阵地进行摧毁 GameManager.Instance.PlayerHP-=20; //从敌人列表中移除自身 GameManager.Instance.Enemys.Remove(this); //销毁自身 Destroy(this.gameObject); //销毁血条 Destroy(mHPBar.gameObject); }else{ StartNode=StartNode.ThatNode; } } //计算敌人的移动方向 Vector3 mDir=new Vector3(mPos2.x-mPos1.x,mPos2.y-mPos1.y,0).normalized; transform.Translate(mDir * MoveSpeed * Time.deltaTime); } 好了，现在我们来一起分析这段代码。首先，我们计算了敌人与路径结点间的距离，这里我们用0.1来近似地表示敌人已经到了路径结点上，此时如果该结点的目标结点为null则表示此时敌人已经到了最后一个结点处，所以敌人会对我方的阵地造成20点的伤害并销毁敌人。在GameManager我们使用了一个列表来管理和维护当前场景中的所有敌人，因此当当前敌人销毁时需要从列表中移除，GameManager类是一个静态类，负责对游戏的全局维护，这个类我们放到稍后来讲啊。那么如果敌人没有走到最后一个结点怎么办呢？我们只需要将StartNode指向StartNode的目标节点，这样我们就可以对整个路径结点实现遍历。这里是不是有种数据结构的感觉呢？哈哈，数据结构和算法是编程中最基础、最重要的内容，这些内容到了游戏开发领域同样是适用的。那么，好了，既然知道敌人是怎么移动的，现在我们就来对敌人进行移动吧，这里是采用计算移动方向的方式来实现，这个很简单啦。\n好了，现在我们来说说敌人的血条吧，我们知道当怪物沿着寻路路径向我方阵地发起攻击的时候，我方防守单位会自动地对敌人进行防御性攻击，那么此时血条就可以显示敌人的实时血量，从而方便玩家根据战场的情况来调整兵力部署情况。我们知道从Unity4.6以后Unity开始使用全新的GUI系统UGUI，因为博主在之前的项目中一直使用NGUI，加上博主不是很喜欢做UI，所以每次用NGUI的时候整个人的心情都是不好的，有段时间被NGUI虐得体无完肤，感觉整个人都不好了。好了，既然现在我们有了新的选择UGUI，那么就让我们先睹为快吧！如图，全新的NGUI位于GameObect-\u0026gt;UI菜单下，基本覆盖了常用的如Button、Image、Slider、ScrollBar等控件，因为UGUI刚推出不久，所以博主希望大家还是能客观、公正的对待UGUI和NGUI，博主认为在短期内这两个GUI系统将处于共存的状态，不存在相互替代的可能性。 好了，UGUI所有的控件都是放到一个叫做Canvas的父控件下的，这一点和NGUI的UIRoot有些类似吧！Canvas提供了三种模式的UI系统，即Screen Space-Overlay、Screen Space-Camera、World Space。第一种Screen Space-Overlay它是完全以当前屏幕的像素大小创建的一个矩形范围，即控件是以屏幕坐标来绘制的；第二种Screen Space-Camera它是根据相机的视线范围来确定的一个矩形范围，其控件是根据Camera的ViewPortPoint坐标来绘制的;第三种从名称我们就可以知道，它是完全3D化的UI,使用的是常用的世界坐标。博主是刚开始研究UGUI,如果有不对的地方还希望大家能够原谅啊。好了，下面我们正式来做血条吧，在这里我们使用的是默认的Slider控件，用Slider控件来制作血条需要将Slider控件自带的滑块删除，然后我们通过改变value就可以实现一个简单的血条了。在UGUI中所有的图片素材都是以Sprite的形式来出现的，所以UGUI可以自己生成图集，不需要像NGUI在做UI前首先要生成图集。这是博主做的一个简单的血条。现在血条做好了，可是问题来了：这UGUI的所有控件都必须要放到Canvas下面啊，所以我们没法像NGUI一样直接把做好的血条放到怪物下面。怎么办呢？既然不能放到怪物下面，那我们就放到Canvas下面吧，不过我们需要自己计算血条的位置。好了，下面来看代码：\npublic class Enemy : MonoBehaviour { //敌人的生命值 public float MaxHP; public float HP; //敌人的初始路径节点 public PathNode StartNode; //敌人的移动速度 public float MoveSpeed=0.15F; //敌人的旋转速度 public float RotateSpeed=0.3F; //敌人血条预制件 public GameObject HPBar; //敌人血条组件 private Slider mHPBar; //public EnemySpawn mSpawn; void Awake() { //在敌人列表中增加一个敌人 GameManager.Instance.Enemys.Add(this.GetComponent\u0026lt;Enemy\u0026gt;()); //查找UI Transform mUiRoot=GameObject.Find(\u0026#34;UIManager\u0026#34;).transform; //计算血条位置 Vector3 mPos=this.transform.FindChild(\u0026#34;EnemyHP\u0026#34;).transform.position; //mPos=Camera.main.WorldToViewportPoint(mPos); mPos.z=-5; //生成血条 GameObject go=(GameObject)Instantiate(HPBar,mPos,Quaternion.identity); //使血条成为Canvas的子物体 go.transform.parent=mUiRoot; //对血条进行放缩 go.GetComponent\u0026lt;RectTransform\u0026gt;().localScale=new Vector3(0.5F,0.30F,1); //获取Slider mHPBar=go.transform.GetComponent\u0026lt;Slider\u0026gt;(); } void Move() { Vector3 mPos1=this.transform.position; Vector3 mPos2=this.StartNode.transform.position; //计算敌人与路径节点间的距离 float mDis=Vector2.Distance(new Vector2(mPos1.x,mPos1.y),new Vector2(mPos2.x,mPos2.y)); if(mDis\u0026lt;0.1F){ if(StartNode.ThatNode==null){ //对防守阵地进行摧毁 GameManager.Instance.PlayerHP-=20; //从敌人列表中移除自身 GameManager.Instance.Enemys.Remove(this); //销毁自身 Destroy(this.gameObject); //销毁血条 Destroy(mHPBar.gameObject); }else{ StartNode=StartNode.ThatNode; } } //计算敌人的移动方向 Vector3 mDir=new Vector3(mPos2.x-mPos1.x,mPos2.y-mPos1.y,0).normalized; transform.Translate(mDir * MoveSpeed * Time.deltaTime); } void Rotate() { //初始角度 float mStartAngle=this.transform.eulerAngles.z; transform.LookAt(StartNode.transform); //目标角度 float mTargetAngle=this.transform.eulerAngles.z; //计算旋转量 float mAngle=Mathf.MoveTowardsAngle(mStartAngle,mTargetAngle,RotateSpeed *Time.deltaTime); this.transform.eulerAngles = new Vector3(0,0,mAngle); } void Update () { Move(); UpdateHPBar(); } private void UpdateHPBar() { //更新血条位置 Vector3 mPos=this.transform.FindChild(\u0026#34;EnemyHP\u0026#34;).transform.position; //使血条位于顶层 mPos.z=-5; mHPBar.transform.position=mPos; //更新血量 mHPBar.value=(float)HP/MaxHP; } public void SetDamage(int mValue) { HP-=mValue; if(HP\u0026lt;=0){ Destroy(this.gameObject); Destroy(mHPBar.gameObject); GameManager.Instance.Enemys.Remove(this.GetCopmonent\u0026lt;Enemy\u0026gt;()); } } } 在这里我们做了三件事情：\n第一，在Awake方法中我们首先计算出血条的位置然后在这个位置生成血条，并取得相关的变量备用。 第二，在Update方法中增加一个UpdateHPBar方法以实现对血条血量的更新。 第三，增加了一个SetDamage方法，当敌人血量为0时销毁自身、销毁血条、从敌人列表中移除敌人 敌人按波次进攻的实现 好了，到现在为止，对于敌人的逻辑我们就全部实现了。可是我们知道在塔防游戏中敌人通常是一波一波出现的，所以我们需要一个敌人生成器EnemySpawn。那么，怎么来生成敌人呢，这里我们使用Xml文件来配置要生成的敌人列表，首先我们来构建一个Xml文件：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; standalone=\u0026#34;yes\u0026#34;?\u0026gt;\r\u0026lt;Enemies\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;1\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;1\u0026#34; Wait=\u0026#34;0.5\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;2\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;2\u0026#34; Wait=\u0026#34;0.45\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;2\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;2\u0026#34; Wait=\u0026#34;0.45\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;3\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;3\u0026#34; Wait=\u0026#34;0.4\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;3\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;3\u0026#34; Wait=\u0026#34;0.4\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;3\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;3\u0026#34; Wait=\u0026#34;0.4\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;4\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;4\u0026#34; Wait=\u0026#34;0.35\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;4\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;4\u0026#34; Wait=\u0026#34;0.35\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;4\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;4\u0026#34; Wait=\u0026#34;0.35\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;4\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;4\u0026#34; Wait=\u0026#34;0.35\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;5\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;5\u0026#34; Wait=\u0026#34;0.3\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;5\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;5\u0026#34; Wait=\u0026#34;0.3\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;5\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;5\u0026#34; Wait=\u0026#34;0.3\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;5\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;5\u0026#34; Wait=\u0026#34;0.3\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;5\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;5\u0026#34; Wait=\u0026#34;0.3\u0026#34;/\u0026gt;\r\u0026lt;Enemy Wave=\u0026#34;6\u0026#34; EnemyName=\u0026#34;Enemy\u0026#34; Level=\u0026#34;99\u0026#34; Wait=\u0026#34;0.15\u0026#34;/\u0026gt;\r\u0026lt;/Enemies\u0026gt; 从这个Xml文件中我们可以看到这样一个结构：\nusing UnityEngine; using System.Collections; using System.Collections.Generic; using System.Xml; public class SpawnData { //敌人进攻波数 public int Wave; ///敌人名称，我们将根据这个名称来生成不同的敌人 public string EnemyName; //敌人等级，我们将根据这个值来调整敌人的生命值和移动速度 public int Level; public float Wait; } 在SpawnData这个结构中，我们可以得到敌人攻击的波数、敌人的名称、敌人等级、敌人生成需要等待的时间，因为博主在游戏中只有一种敌人，所以敌人的名称都是一样的。好了，现在我们可以开始解析Xml了：\n//解析Xml文件 void ReadXml() { //创建一个字典以存储敌人列表 mEnemyDatas=new List\u0026lt;SpawnData\u0026gt;(); //加载Xml文档 XmlDocument mDocument=new XmlDocument(); mDocument.LoadXml(ConfigFile.text); XmlElement mRoot=mDocument.DocumentElement; //解析Xml文档 XmlNodeList mNodes=mRoot.SelectNodes(\u0026#34;/Enemies/Enemy\u0026#34;); foreach(XmlNode mNode in mNodes) { //为每一个SpawnData赋值 SpawnData mData=new SpawnData(); mData.Wave=int.Parse(mNode.Attributes[0].Value); mData.EnemyName=mNode.Attributes[1].Value; mData.Level=int.Parse(mNode.Attributes[2].Value); mData.Wait=float.Parse(mNode.Attributes[3].Value); mEnemyDatas.Add(mData); } } 那么好了，在解析完Xml后我们得到了所有的敌人数据，接下来我们只需要按照顺序生成敌人就可以了。具体怎么做呢，我们知道在塔防游戏中生成敌人有两种情况：\n一个是要生成的敌人和当前敌人是同一波的，这种情况只要继续生成就好了。 一个是要生成的敌人的波数大于当前波数，这种情况需要等待这一波敌人被消灭完。 好了，现在来写代码：\nusing UnityEngine; using System.Collections; using System.Collections.Generic; using System.Xml; public class EnemySpawn : MonoBehaviour { //敌人寻路起点 public PathNode SpawnPath; //敌人预制件 public GameObject Enemy; //Xml文件 public TextAsset ConfigFile; //存放敌人的数组 private List\u0026lt;SpawnData\u0026gt; mEnemyDatas; //当前敌人进攻波数 private int mWave=0; //当前敌人索引 private int mIndex=0; //当前等待的时间 private float mWait; void Start() { //读取Xml数据 ReadXml(); Debug.Log(mEnemyDatas.Count); //初始化攻击波数 SpawnData mData=mEnemyDatas[mIndex]; //设置攻击波数和等待时间 mWave=mData.Wave; mWait=mData.Wait; GameManager.Instance.AttackWave=mWave; //生成第一个敌人 CreateEnemy(mData); mIndex+=1; } void CreateEnemy(SpawnData mData) { GameObject go=(GameObject)Instantiate(Enemy,SpawnPath.transform.position,Quaternion.identity); Enemy _Enemy=go.GetComponent\u0026lt;Enemy\u0026gt;(); //根据Level计算敌人的生命值和移动速度 _Enemy.MaxHP= (float)mData.Level*0.25F * 100; _Enemy.HP= (float)mData.Level*0.25F * 100; go.GetComponent\u0026lt;Enemy\u0026gt;().MoveSpeed=(float)mData.Level * 0.15F; go.GetComponent\u0026lt;Enemy\u0026gt;().StartNode=SpawnPath; } void Update () { if(mIndex\u0026lt;=mEnemyDatas.Count-1){ SpawnEnemy(); }else { //当索引数目大于敌人列表中的数目时，表示所有敌人以及生成完毕，此时 //如果所有的敌人都被消灭，则表示玩家获胜。 if(GameManager.Instance.Enemys.Count==0){ GameManager.Instance.IsWin=true; Debug.Log(\u0026#34;玩家胜\u0026#34;); } } } private void SpawnEnemy() { //取得下一个生成的敌人的数据 SpawnData mData=mEnemyDatas[mIndex]; //开始计时 mWait-=Time.deltaTime; if(mWait\u0026lt;=0 ){ //如果当前是同一波敌人，则继续生成敌人 if(mWave==mData.Wave){ //设置等待时间 mWait=mEnemyDatas[mIndex].Wait; //设置进攻波数 mWave=mEnemyDatas[mIndex].Wave; GameManager.Instance.AttackWave=mWave; //生成一个敌人 if(mData!=null){ CreateEnemy(mData); } mIndex+=1; }//如果是下一波敌人，则需要等待这一波敌人全部死亡后再生成 else if(mWave\u0026lt;mData.Wave \u0026amp;\u0026amp; GameManager.Instance.Enemys.Count==0){ //设置等待时间 mWait=mData.Wait; //设置进攻波数 mWave=mData.Wave; GameManager.Instance.AttackWave=mWave; //生成一个敌人 CreateEnemy(mData); mIndex+=1; } } } //解析Xml文件 void ReadXml() { //创建一个字典以存储敌人列表 mEnemyDatas=new List\u0026lt;SpawnData\u0026gt;(); //加载Xml文档 XmlDocument mDocument=new XmlDocument(); mDocument.LoadXml(ConfigFile.text); XmlElement mRoot=mDocument.DocumentElement; //解析Xml文档 XmlNodeList mNodes=mRoot.SelectNodes(\u0026#34;/Enemies/Enemy\u0026#34;); foreach(XmlNode mNode in mNodes) { //为每一个SpawnData赋值 SpawnData mData=new SpawnData(); mData.Wave=int.Parse(mNode.Attributes[0].Value); mData.EnemyName=mNode.Attributes[1].Value; mData.Level=int.Parse(mNode.Attributes[2].Value); mData.Wait=float.Parse(mNode.Attributes[3].Value); mEnemyDatas.Add(mData); } } } 我们可以注意到，到现在为止敌人相关的内容博主都已经为大家讲解完了，这里博主和大家开了一个小玩笑，不知道大家有没有发现，在敌人的Xml配置文件中博主最后设计了一个等级为99级的敌人，哈哈，这个敌人在游戏中的特点大家要自己从代码中来探索了，大家可以按照博主的思路做出这个塔防游戏然后自己去试试看，相信大家会更加深刻地理解数值平衡的重要性吧！\n防守单位篇 防守单位是塔防游戏中玩家可以支配和控制的一种资源，玩家通过合理地分布防守单位的位置来对玩家的防守阵地进行防御，当玩家的防守阵地被摧毁时玩家将无法继续部署防守单位。这就是防守单位在游戏中的主要作用。通常为了增加游戏的可玩性，游戏设计者往往会设计多种防守单位，在博主的这个小游戏中，我们只设计了一种防守单位，更多的防守单位的设计大家可以参考《保卫萝卜》和《植物大战僵尸》这两个游戏。好了，说了这么多，那么防守单位在整个塔防游戏中主要的作用是什么呢？答案就是防守，哈哈，这是一句不折不扣的废话。可是就是这样一句废话，却足以让我们知道防守单位需要对敌人进行自动攻击，这就要涉及到简单的AI算法了。好了，我们来看下面的脚本：\nusing UnityEngine; using System.Collections; public class Defender : MonoBehaviour { //目标敌人 private Enemy mTarget; //攻击半径 public float AttackArea=2.5F; //与敌人的距离 private float mDistance=0; //防守单位的旋转速度 public float RotateSpeed=1.5F; //防守单位攻击间隔 public float AttakTime=2.5F; //防守单位攻击间隔 private float mTime=0.0F; //炮弹预设 public GameObject BulletObject; void Start () { //初始化防守单位攻击间隔 mTime=AttakTime; } //查找攻击范围内的敌人 void FindEnemy() { //初始化目标敌人 mTarget=null; //获取敌人列表 ArrayList mEnemys=GameManager.Instance.Enemys; //遍历每个敌人 foreach(Enemy _enemy in mEnemys) { //忽略生命值为0的敌人 if(_enemy.HP==0) continue; //计算防守单位与敌人间的距离 Vector3 mPos1=transform.position; Vector3 mPos2=_enemy.transform.position; float mDis=Vector2.Distance(new Vector2(mPos1.x,mPos1.y),new Vector2(mPos2.x,mPos2.y)); if(mDis\u0026gt;AttackArea){ //Debug.Log(\u0026#34;敌人\u0026#34; + _enemy.transform.name + \u0026#34;未进入攻击范围,距离为:\u0026#34; + mDis); //return; }else{ //Debug.Log(\u0026#34;敌人\u0026#34; + _enemy.transform.name + \u0026#34;已进入攻击范围,距离为:\u0026#34; + mDis); //选择最近的敌人 if(mDistance==0 || mDistance \u0026gt; mDis){ mTarget=_enemy; mDistance=mDis; } /*//选择生命值最低的敌人 if(mLife==0 || mLife \u0026gt; _enemy.HP){ mTarget=_enemy; mLife=_enemy.HP; } */ } } mDistance=0; } void RotateTo() { //判断目标敌人是否为空 if(mTarget==null) return; //计算要旋转到敌人方向的角度 Vector3 mPos1=this.transform.position; Vector3 mPos2=mTarget.transform.position; Vector3 mDir=(mPos2-mPos1).normalized; //使得两向量共面 mDir.z=0; //计算两向量角度 float mAngle=getAngle(Vector3.up,mDir); this.transform.eulerAngles=new Vector3(0,0,mAngle) * RotateSpeed; } //根据向量数学计算角度 private float getAngle(Vector3 v1,Vector3 v2) { float mDot=Vector3.Dot(v1,v2); float mv1=Mathf.Sqrt(v1.x*v1.x+v1.y*v1.y+v1.z*v1.z); float mv2=Mathf.Sqrt(v2.x*v2.x+v2.y*v2.y+v2.z*v2.z); if(v2.x\u0026gt;v1.x){ return -Mathf.Acos(mDot/(mv1*mv2))* Mathf.Rad2Deg ; }else{ return Mathf.Acos(mDot/(mv1*mv2))* Mathf.Rad2Deg ; } } void Attack() { RotateTo(); if(mTarget==null) return; //以下添加攻击逻辑 mTime-=Time.deltaTime; if(mTime\u0026lt;0){ Vector3 _angle=transform.Find(\u0026#34;Bullet\u0026#34;).eulerAngles; Vector3 _pos=new Vector3(this.transform.position.x,this.transform.position.y,-2); Instantiate(BulletObject,_pos,Quaternion.Euler(_angle)); mTime=AttakTime; } } void Update () { FindEnemy(); Attack(); } } 防守单位的脚本定义在Defender这个类中，主要的行为有两个，即发现敌人后转向敌人、向敌人发射炮弹，这块的代码较为简单，大家自己去领会就好啦。我们知道在塔防游戏中玩家可以通过点击屏幕来自由地增加或移动防守单位，这部分的内容主要是和GUI相关的，因为目前博主对UGUI掌握地还不是很熟，所以就等以后博主有时间了再来补充吧！好了，这个塔防游戏的讲解教程就是这样了，希望大家能够喜欢，我知道大家等这篇下篇已经好久了，哈哈！ 最后想说的是，博主的独立博客http://qinyuanpei.github.io正式开始使用了，以后发表的文章会在独立博客和CSDN同时更新，希望大家能继续关注博主的博客！谢谢大家\n","date":"2015-01-21T13:50:48Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/1176959868/","slug":"1176959868","tags":["游戏开发","Unity3D","塔防","教程"],"title":"Unity3D 塔防游戏开发项目讲解(下)"},{"categories":["读书笔记"],"content":"\r在此将【剑指 Offer】中的经典问题和重要内容整理出来，便于以后遇到类似的问题再次查阅。博主强烈为大家推荐这本书，因为这本书中的题目都来自真实的公司笔试，对于大家求职、找工作会有很大的帮助。\n1、在定义一个赋值运算符时，通常需要考虑以下四点： 是否将返回值的类型声明为该类型的引用，并在函数结束前返回实例自身的引用(即*this)。只有一个返回引用，才可以允许连续赋值，否则如果函数的返回值是void，应用该赋值运算符将不能做连续赋值。 是否将传入的参数类型声明为常量引用。如果传入的参数不是引用而是实例，那么从形参到实参会调用一次复制构造函数，把参数声明为引用可以避免这样的无谓消耗，能提高代码的效率。同时，我们在赋值运算函数内部不会改变传入的实例状态，因此应该在传入的引用参数前加上const关键字。 是否释放实例已有的内存，如果我们忘记在分配新内存之前释放自身已有的空间，恒旭将出现内存泄漏。 是否判断传入的参数和当前的实例是否是同一个实例。如果是同一个，则不进行赋值运算，直接返回，如果事先不判断就进行赋值，那么在释放实例自身内存的时候就会导致严重的问题，当*this和传入的参数是同一个实例时，一旦释放了自身的内存，传入的参数的内存将同时被释放，因此将再也找不到需要赋值的内容了。 当我们完整的考虑了上述四个方面以后，我们可以写出如下的代码：\nCMyString\u0026amp; CMyString::operator = (const CMyString \u0026amp;str) { if(this == \u0026amp;str) return; delete []m_pData; m_pData = NULL; m_pData = new char[strlen(str.m_pData) + 1]; strcpy(m_pData,str.m_pData); return *this; } 2、要想在赋值运算符函数中实现异常安全性，我们有两种方法 方法一：先用new分配新内容再用delete释放已有内容。这样只有当分配内容成功后再释放原来的内容，换句话说当分配内存失败时我们可以确定CMyString的实例不会被修改。 方法二：先创建一个临时实例，再交换临时实例和原来的实例 下面给出第二种方法的实现代码：\nCMyString\u0026amp; CMyString::operator = (const CMyString \u0026amp;str) { if(this != \u0026amp;str){ CMyString strTemp(str); char* pTemp = strTemp.m_pData; strTemp.m_pData = m_pData; m_pData = pTemp; } return *this } 3、对于C++和C#中的struct和class的认识 C++:在C++中如果没有标明成员函数或者成员变量的访问权限级别，则在struct中默认的是public，在class中的默认的private。 C#:在C#中如果没有标明成员函数或者成员变量的访问级别，则struct和class默认都是private，不同的是struct定义的是值类型，其实例在栈上分配内存；class定义的是引用类型，其实例在堆上分配内存。 4、在C#中实现单例模式 原理：在C#语法中C#是在调用静态函数时初始化静态变量，.NET运行时可以保证只调用一次静态构造函数，这样我们就可以保证仅初始化一次Instance; 下面给出代码示例： public sealed class Singleton { private Singleton() { } private static Singleton instance=new Singleton(); public static Singleton Instance { get{ return instance;} } } 5、C++数组重要概念 数组是最简单的一种数据结构，它占据一块连续的内存并按照顺序存储数据。 在C/C++中，数组和指针是相互关联又有区别的两个概念。当我们声明一个数组时，其数组的名字同时是一个指针，该指针指向数组的第一个元素，因此我们可以使用一个指针来访问数组。可是值得注意的是，C/C++并没有记录数组的大小，因此使用指针访问数组中的元素时要注意不能超出数组的边界。 使用sizeof计算指针的大小时，在32位操作系统中，对于任意指针结果都是4。 二维数组在内存中占据连续的空间。在内存中从上到下存储各行元素，在同一行中按照从左到右的顺序存储。因此我们可以根据行号和列号计算出相对于数组首地址的偏移量，从而找到对应的元素。 6、C#中的String类型 在C#中封装字符串的类型Sysytem.String有一个非常特殊的性质，即String中的内容是不能改变的。当尝试改变String中的内容，就会产生一个新的实例。 如果要连续多次修改字符串内容，可以考虑使用StringBuilder。 当我们需要在函数或者方法中返回一个String实例时，我们需要在传入的参数前加上ref或者out标记 7、链表 链表是一种动态数据结构，因为在创建链表的时候，不需要知道链表的长度。当插入一个结点时，我们只需要为新结点分配内存，然后调整指针的指向来确保新结点被链接到链表当中。内存分配不是在创建链表时一次性完成，而是每添加一个结点分配一次内存。由于没有闲置的内存，因此链表的空间效率比数组要高。 因为链表中的内存不是一次性分配的，所以我们不能确定链表的内存和数组一样是连续的，因此如果想在链表中找到第i个结点，我们只能从头结点开始，沿着指向下一个结点的指针遍历链表，其效率是O(n)。而在数组中，我们可以根据下标i直接找到第i个元素，其效率是O(1)。 当我们需要从尾到头输出链表时，第一个遍历到的结点最后一个输出，而最后一个遍历到的结点第一个输出，这是典型的后进先出，因此我们可以考虑使用栈来实现这种顺序。下面是具体的代码实现： void PrintListReversingly_Iteratively(ListNode* pHead) { std::stack\u0026lt;ListNode*\u0026gt; nodes; ListNode* pNode = pHead; while (pNode != NULL) { nodes.push(pNode); pNode = pNode-\u0026gt;m_pNext; } while (!nodes.empty()) { pNode = nodes.top(); printf(\u0026#34;%d\\t\u0026#34;, pNode-\u0026gt;m_nValue); nodes.pop(); } } 8、树 除了根节点之外每个结点只有一个父结点，根节点没有父结点。 除了叶节点以外所有结点都有一个或者多个子结点，叶结点没有子结点。父结点和子结点间用指针链接。 二叉树是树的一类特殊结构，在二叉树的每个结点最多只能有两个子结点。二叉树有三种主要的遍历方式，即前序遍历(根、左、右)、中序遍历(左、根、右)、后序遍历(左、右、根)。 二叉搜索树是二叉树的一个特例，其特点是左子节点总是小于或等于根节点，右子结点总是大于或等于根节点。 9、栈和队列 栈的特点是后进先出，即最后一个被压入(Push)栈的元素会第一个被弹出(Pop)。 队列的特点是先进先出，即第一个进入队列(入队)的元素将会第一个出来(出队)。 10、递归与循环 递归实现的效率无法和循环相比，因此函数调用会造成时间和空间的损失、会造成重复计算、可能会造成栈溢出。 在经典的斐波那契数列问题中，我们可以采用下面的方法来代替传统的递归方法： int Fiboncci(int n) { int[] result=new int[] { 0, 1 }; if (n \u0026lt; 2) return result[n]; int m = 1; int n = 0; int k = 0; for (int i = 2; i \u0026lt;= n; i++) { k = m + n; n = m; m = k; } return k; } 11、位运算 左移m \u0026lt;\u0026lt; n表示把 m 左移 n 位。左移 n 位的时候，最左边的 n 位将被丢弃，同时在最右边补上 n 个0。如： 00001010 \u0026laquo; 2 = 00101000 10001010 \u0026laquo; 3 = 01010000\n右移m \u0026gt;\u0026gt; n表示把 m 右移 n 位。右移 n 位的时候，最右边的 n 位将被丢弃。此时如果数字是一个无符号数值，则用0填补最左边的 n 位;如果数字是一个正数，则在右移之后在左边补 n 个0;如果数字是一个负数，则右移之后在左边补 n 个1。如： 00001010 \u0026raquo; 2 = 00000010 10001010 \u0026raquo; 3 = 11110001\n","date":"2015-01-20T10:04:41Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/123663202/","slug":"123663202","tags":["读书","面试","剑指Offer","技术"],"title":"剑指 Offer 读书笔记(1)"},{"categories":["生活感悟"],"content":"即将到来的是新的一天，我却不能在疲惫中很快入睡，听着耳边再熟悉不过的歌，即使不是大家都喜欢的那种慷慨激昂的曲调，然而在这安静得无从察觉一个人内心世界的夜晚，这样温婉柔和的小调反而更容易让人静下心来想些事情。今天新住的宾馆简约而整洁，最为重要的是终于有了一张属于自己的桌子。以前每次趴在床上画图斑，等到再站起来时背部便开始痛起来。偶尔盘腿坐在床上录数据，等到再站起来时脚已经麻了。这样做的一个坏处是每次都会把中性笔的墨水弄到床单上，虽然顾客是上帝，可是上帝不断地给人类制造麻烦，这样真的好吗？与此同时，开始意识到一个良好的姿势对于健康是多么的重要了。\n此时此刻，床上已经堆满了各种各样的图纸，那些来自我上个星期处理过的内业。所谓内业就是在室内加班，如果公司上下一致通过，理论上是符合 SA8000 社会责任标准体系的，所以内业是可以借休息的名义来进行的。这两天想了很多的问题，从什么地方说起呢？我想我可以从这些地方来谈谈我的想法吧。\n首先是交流，我承认我是一个不喜欢说话的人。我不喜欢说话，从来不是因为我不敢而是因为我不愿意。我不喜欢看选秀节目、我不喜欢娱乐八卦、我不喜欢猎奇找刺激。我平时最为喜欢的是科技报道、技术博客和游戏资讯，因为此时此刻我站在科技和游戏的一个十字路口。我喜欢古香古色的传统文化，喜欢在书籍中寻找精神的依托，我就是这样一个传统而现代的人。可是这样一个时代注定是一个娱乐化的时代，当你打开电视，你会发现到处都是导师、到处都是明星、到处都是嘉宾，因为我们生活在一个每天都在造星的浮躁时代。我承认再这样一个时代充满了机会，可是当我们每一个人都缺乏耐心来专注于一件事情的时候，这样的机会对于我们又有什么意义呢？\n我今天早上看《我是演说家》这个节目，我觉得这是一个比较有品位的节目，如乐嘉所说，这个节目的收视率可能不会像其它的娱乐节目那样高，可它能让你的内心真正有所触动，因为讲故事的人可能是一个你素不相识的人，可是它讲的故事可能会是你身上正在发生的事情。我从来不怀疑语言的力量，从春秋战国的张仪、苏秦等纵横家到三国时期诸葛亮舌战群儒再到世界上著名的营销专家，语言可以说世界上最锋利的武器，可是当大家都在讨论时下最为流行的娱乐节目时，我忽然觉得自己实在没有开口的必要了。那次领导说我分不清书记和主任，因为我觉得人与人之间的交流应该是随着时间的推移而不断深入的，所谓慢工出细活，我觉得交朋友应该像喝茶一样慢慢地品，像喝酒一样交朋友只能交到酒肉朋友。我们为了赶上工作进度，可能一周会去三四个地方，因此和许多人都只是一面之缘，所以我觉得分不清主任和书记尚属情有可原。\n我每隔一段时间就会去我的博客看看，每次碰到问我问题的朋友我都很热心的回答，因为我觉得人与人之间真的存在某种特殊的联系。我特别喜欢和技术圈子里的这些朋友聊天，因为我觉得在这些人中间可以找到共同的话题，或许大家的经济状况都不宽裕，可是听着每一个人的故事都是一部奋斗史，没有人能随随便便成功，可是每一个人都在用一种近乎纯粹的信仰去努力实现，而不是随波逐流。或许这就是我喜欢这行的真正原因吧，每一天都充满新的挑战，每一天都是在不断创造，我就是喜欢这样的创造性工作，而不繁琐单调的机械重复。回到交流这个问题上，人是一种麻烦的动物，所以在和人打交道的这个问题上，我们把大量的时间浪费在了交流上。我承认团队间的交流十分重要，可是因为人的惰性和私欲，交流有时候会占用大量的时间成本，在这一点上计算机更值得人类学习，因为它忠诚可靠而且一视同仁。\n接下来是工作，我一直坚信伟大的工作和平庸的工作间的区别就是能否让工作的人感到自豪。我的理想并不伟大，我只想某天朋友们聚到一起时候，我惊讶地发现你们的手机或者智能设备上运行着我设计的应用程序或者游戏，然后我平静地告诉你这些都是我设计的，这时候轮到你们惊讶：天啊，它简直堪称完美！在舜土实习已经快一个月了，可是我并没有对自己的工作感到自豪过，即使我能将 10 个左右的自然村在一个下午全部做完，可是我依然感到这是一份平庸的工作。首先，为了追赶进度，将一个行政村集中到一天来做，相比土地确权工作两天一个自然村的进度，整体质量较为粗放。其次，将希望完全寄托在队长身上，队长如果不配合，工作很难开展，如果队长失去耐心，工作会变得更加艰难。说实话，这些村民生活在山区生活困顿不堪，可是每次到村里都很热心地招待我们，这让我在感受到他们的质朴时更加自责，我们所做的工作从本质上来讲根本没有什么意义，每次和这些队长交流，他们最关心的问题就是他们的土地会不会被收回去，可是他们视如生命的土地却往往只是国土局任意划定的一个范围。作为一个实习生，或许抱怨工作是不合适的，因为无论到任何一个企业，前 3~5 年所做的始终都是些基础而琐碎的东西，可是不管让员工做什么，都要让员工觉得他此刻的这份工作是有意义的，因为这关系到一个企业的品位。工作固然是为了挣钱，可是如果工作能带给人幸福感和自豪感，那么我相信对于工作和人都是会有好处的吧，因为兴趣才是最好的老师啊。\n今天搬到宾馆准备处理上周没处理完的事情，领导说明天要去的地方已经联系好了，可是公司的图纸还没有打印出来，他问我怎么处理，我就说跟人家实话实说吧，问题出在我们这边啊。然后他就说我傻，可是我觉得很多问题不是想掩盖就能掩盖的了的，就像我们所做的这个工作，一旦那些队长明白过来我们所做的事情的真正意图，那么整个结果就完全取决于对方，因为如果队长本身就告诉你错误的信息，那么你是无法辨别的。很多的技术公司都会形成自己公司内部的框架或者工具类，因为过度依赖第三方的组件很容易受到被动影响。或许乔布斯的专制和傲慢让很多苹果的员工和用户都曾受到伤害，可是他不过是使用了一个最基本的原则，要想不受制于别人，就必须掌控一切的主动权，所以苹果用自己的软件和自己的硬件打造了一个无可匹敌的强大壁垒。当然，逝者长已，可能苹果的产品中那些属于乔布斯个人标志的风格已经渐渐地淡出了我们的视野，可是我们依然会对这位智者表示无比崇高的敬意。\n回到我的工作，这是一个完全没有追求的工作，昨天下午坐在车上和一个刚认识的朋友，他问我为什么不去找个和开发相关的实习工作啊，突然沉默了好半天，最后只能含糊其辞地糊弄过去。记得上次回银川，和一个实习生一起到楼下的打印店裁剪图册，结果半路上问我是什么专业？结果环境科学再次被鄙视了啊，哈哈，我不知道这样是喜还是悲，总之工作还是喜欢的好。以前我爸说我不喜欢这不喜欢那的，我到底喜欢什么啊？那个时候我没有说什么。可是这一刻我想告诉他我喜欢的就是开发。\n最后要说的是做人，在同学的生日上，突然被人说我单纯，我不知道这该庆幸呢还是该感慨啊。今年的我 22 岁了，可我还是喜欢简简单单的生活。我不喜欢带着面具生活、我不喜欢口是心非、我不喜欢心机深重。我从来不觉得穿上大人的衣服、梳起大人的发型就能让你成熟起来，真正的成熟是要有责任、有担当。虽然不是每一个人都值得你敞开心扉去交往，可是做人坦诚、正直有什么不对吗？难道你希望每天面对人的时候都盘算着怎么玩弄那些可笑的伎俩吗？我承认在这个世界上有太多的事情我都还不懂，可我会努力去尝试着了解。我不喜欢抽烟、不喜欢喝酒，到现在不会划拳等那些酒场上应该会的东西。如果真想喝酒了，围炉小酌几杯便可以了，何必要让饮酒变成这样一件麻烦的事情呢？\n我承认人生在成长的过程中难免会变得世故圆滑，可是难道这就是我们变得不再单纯的理由吗？这个世界上有两样永远不会变的东西，一样是我们头顶灿烂的星空，一样是我们心中崇高的道德准则。我并不觉得成长就要放弃那些曾经坚持过的东西，如果这些东西注定有一天要丢弃掉，那么何苦要将这些观念从小就灌输给我们，一旦有一天当你发现以前坚守的东西都被自己推翻，而你变成了原来自己不喜欢的哪种人，这将无异于信仰的大厦的坍塌，人的成长竟然要以抛弃自己的过去为代价，这未免过于残酷了吧。我从来都不傻，我看得出每个人心中的想法，只是我不愿意那样做，不管这个社会再怎么变，我不会背叛自己，我只是想让自己在这个浮躁的时代里保持自己的这颗简简单单的心。\n众生虽苦，请诸恶莫作。我并不觉得自己比别人高贵多少，所以我始终像保留一份善良，不管是餐厅的服务员还是酒店的保洁员，每一个出来讨生活的人都不容易，所以多一份尊重比上帝的高傲更容易让人接受。而明天毕业后的我同样会有这一天，我真的不想小河变成大楼，我真的不想单纯变成冷漠，许嵩的这首《秋千坠》很多人都没有用心地听，因此这首歌轻快的节奏只会让人记得那段重复的飘啊飘\u0026hellip;以单纯的心去面对这个世界，可能会受到伤害。可是事实上，任何伤害都不足摧毁你，伤害和磨练在你自省和反思过后，只能让你的阅历更丰富，看人更包容，生活更感恩。有人靠着阿谀奉承、靠着耍心机生活，可你不必这样，因为坦荡真诚同样是一种人生，在这个世界上能真正改变你的只有你自己，人出于本能对人存有戒备之心固然无可厚非，可是在保护好自我的情况下以坦荡真诚的胸襟去面对这个世界则更加难能可贵。因为我们不能生活给了你一个杯具就随波逐流，只有你和别人不一样，你才能将这个杯具变成洗具，这就是我想要的生活。最后送上杨绛先生的一段话与大家分享：\n一个人经过不同程度的锻炼，就获得不同程度的修养、不同程度的效益。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容……我们曾如此期盼外界的认可，到最后才知道，世界是自己的，与他人毫无关系。\n已经凌晨三点了，可是写完这篇文字我很开心，这些话总是要经过些事情才能领悟出来，没有一个人能突然成长起来，我希望自己能一直单纯下去，单纯的人生不是痴傻无明，而是大智若愚，更好地爱这个父母送给我们的世界。\n","date":"2015-01-01T21:36:24Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/2752169106/","slug":"2752169106","tags":["贝塞尔曲线","计算机图形","数学"],"title":"写给永远单纯的自己"},{"categories":["生活感悟"],"content":"如果生命可以轻到像一枚羽毛，我愿意飞过那片时间的海，突然造访那些曾经让我怀念过的日子，看看那时的我是不是和现在一样，从来不曾后悔过当初的选择。2014 年 12 月 31 日，面对即将到来的 2015，我相信这将是我在平凡中蜕变的一年。回首 2014，从年初开始学习 Unity3D 游戏开发到现在，我的博客共积累了 230335 次访问量和 700 名粉丝的关注。首先要感谢各位长期以来关注和支持我的博客的朋友，因为你们的关注和支持就是我不断写下去的动力。2014 年，当我渐渐地从一名大四学生的角色转换到一个社会人的时候，我开始认识到原来生活的本质就是平凡。或许我们都只是在做着普普通通的工作，或许我们都只是一个普普通通的人，可是因为我们有一颗不甘于平凡的心，所以我们选择在追逐梦想的路上完成一次次的蜕变。整个 2014 年基本上可以分成三个阶段：\n沐春 这段时间是大三的第二学期，基本上就是上课看各种各样的技术类书籍，下课有时间就编程，因为这学期有 CAD 课程，所以有时候会练习画图。在这段时间。第一次接触到了 Linux，因为厌倦了在 Windows 平台上使用盗版和破解软件的现状，我曾一度想换到 Linux 平台，不过考虑到我仍然有大量的工作要依赖于 Windows，所以这样的愿望只能成为一种奢望。在这段时间，我喜欢上了一个叫做 Sublime 的代码编辑器，通过各种各样的插件，我将它扩展成了一个万能的编辑器，我会用它来写 C#、C++、Java、Lua 和 Python 的小程序，现在我依然很喜欢这个代码编辑器。前几天我给它装上了 Markdown Preview 插件，这样我便可以使用它来编写 Markdown 文件。五一的时候和同学骑自行车去了滚钟口，这大概是大三的时候最后一次和同学出去玩啦。大概 4 月底左右，我将《古剑奇谭》系列的第二部游戏通关。6 月份我萌生了利用 Unity 来开发仙剑同人游戏的想法。\n博客配图 1\r既望 在结束了这学期的考试后，7 月份我们在西安和上海进行了为期半个月的野外课程实习。7 月份的西安正值酷暑，整个实习过程总是与汗水为伴。不过因为我不喜欢本科的专业，所以这个实习除了让我有机会看看外面的世界开阔眼界以外，并不会有太实际的意义。在实习期间，我研究了动作类游戏中的三连击效果，这是我在实习中做过的唯一一件与专业无关的事情。8 月份回到家里，基本上天天宅在家里做游戏，做了两个射击类的小游戏和一个 RPG 游戏。9 月份到了学校，为了给我的仙剑同人游戏项目寻找素材，我研究了《仙剑奇侠传》、《古剑奇谭》等游戏的解包，并从中提取了大量的游戏模型。为了给模型添加动画，开始学习学习 Blender 软件，不过后来发现给模型绑骨是件相当麻烦的事情，所以只好放弃 3D 建模的学习。到目前为止，我只能利用 CAD 和 SU 建些简单的模型，希望以后有机会再学习这部分的知识吧。然后剩下的时间基本上都是在捣鼓 Linux 和 Github，剩下的就是做游戏啦。期间研究了 Unity 与 Android 平台、Lua 脚本的交互等问题，根据这些问题写成的文章深受大家的喜爱，这里再次对大家的支持表示感谢。\n博客配图 2\r秋云 这段时间按照双 11 来划分的话，双 11 以前是考试，双 11 以后是实习。考试没什么值得说的，永远是记忆的内容比实用的内容多。不过当我面对最后一次考试的时候，我的心情极其复杂，因为我真的不知道如果再给我四年，我是不是还会这样选择我的人生。实习的过程是段暂的，可是在实习中我收获的东西却很多，我收获了为人处世的道理，同时收获了一份珍贵的友谊，我想这就是我想去经历和体验的东西吧。在实习过程中，我利用空闲时间研究了塔防游戏和静态博客系统 Hexo，我的感觉就是编程环境还是 Linux 操作系统比较给力，像 Ruby 这样的环境在 Windows 下安装简直就是在找虐。好了，不吐槽了，我利用 Hexo 和 Github Page 搭建了我的独立博客http://qinyuanpei.github.io/ 以后我的文章会同时在这两个博客上发布，希望喜欢我的文章的朋友能一如既往的关注和支持我！\n博客配图 3\r2014 年即将过去，面对即将到来的 2015 年，我想做到 5 件事情：\n认真完成本科毕业设计——做好大学里的最后一件事情 找一份自己喜欢的工作——脚踏实地的追逐自己的梦想 努力做一个有责任感的人——给她想要的温暖 努力做一个善良的人——众生虽苦，诸恶莫作 努力做一个有内涵的人——读书、写作、编程、游戏 欢迎大家关注我的博客：\nCSDN 博客：http://blog.csdn.net/qinyuanpei 独立博客：http://qinyuanpei.github.io/ 最后祝大家在新的一年里事业有成、梦想成真！\n","date":"2014-12-30T19:34:34Z","image":null,"permalink":"https://qinyuanpei.github.io/posts/124807650/","slug":"124807650","tags":["感悟","平凡",2014],"title":"在平凡中蜕变，我的 2014"},{"categories":null,"content":"","date":"0001-01-01T00:00:00Z","image":null,"permalink":"https://qinyuanpei.github.io/works/","slug":"works","tags":null,"title":"个人作品"},{"categories":null,"content":"","date":"0001-01-01T00:00:00Z","image":null,"permalink":"https://qinyuanpei.github.io/musics/","slug":"musics","tags":null,"title":"听歌"},{"categories":null,"content":"","date":"0001-01-01T00:00:00Z","image":null,"permalink":"https://qinyuanpei.github.io/statics/","slug":"statics","tags":null,"title":"站点统计"},{"categories":null,"content":"","date":"0001-01-01T00:00:00Z","image":null,"permalink":"https://qinyuanpei.github.io/movies/","slug":"movies","tags":null,"title":"观影"},{"categories":null,"content":"","date":"0001-01-01T00:00:00Z","image":null,"permalink":"https://qinyuanpei.github.io/books/","slug":"books","tags":null,"title":"读书"}]